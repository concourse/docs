\title{Pipeline Guides}

\use-plugin{concourse-docs}
\split-sections
\table-of-contents

\section{
  \title{Common Pipeline Practices}
  \omit-children-from-table-of-contents

  The following are practices that we see a lot of people use in their
  pipelines. These are by no means "Best" practices, they are simply common and
  may or may not work for you and your team.

  \section{
    \title{Parallelizing Get Steps in Jobs}

    All jobs usually have \reference{get-step}{get steps} as their first set of
    steps.

    \codeblock{yaml}{{
    jobs:
    - name: awesome-job
      plan:
      - get: cool-code
      - get: funny-binary
      - get: the-weather
      - task: business-stuff
    }}

    To reduce the waiting time to the length of the longest running get step,
    put all get steps under an \reference{in-parallel-step}.

    \codeblock{yaml}{{
    jobs:
    - name: awesome-job
      plan:
      - in_parallel:
        - get: cool-code
        - get: funny-binary
        - get: the-weather
      - task: business-stuff
    }}
  }

  \section{
    \title{Specify Inputs for Put Steps}

    By default \reference{put-step}'s have all artifacts from a job mounted in
    their resource container. This can result in long initialization times for
    put steps. It's likely that a \reference{put-step} only needs a subset of
    all available artifacts generated throughout the job.

    There are two ways to specify which artifacts to send to a
    \reference{put-step}. You can specify \code{detect} as the
    \reference{schema.put.inputs} or you can pass in an exact list of
    all artifacts the \reference{put-step} needs.

    Using \code{detect}:
    \codeblock{yaml}{{
    jobs:
    - name: the-job
      plan: # Get some artifacts
      - in_parallel:
        - get: apples
        - get: apple-basket
        - get: monkeys
      # using detect will result in "apples-location" and "basket" being passed in
      # "monkeys" will not be passed in
      - put: apples-in-basket
        inputs: detect
        params:
          apples-location: apples/location # matches the first get step
          basket: apple-basket # matches the second get step
    }}

    Specifying the exact inputs needed for the \reference{put-step}:
    \codeblock{yaml}{{
    jobs:
    - name: the-job
      plan: # Get some artifacts
      - in_parallel:
        - get: apples
        - get: apple-basket
        - get: monkeys
      - put: apples-in-basket
        inputs: [apples, apple-basket] # specify the exact inputs needed
        params:
          apples-location: apples/location
          basket: apple-basket
    }}

  }

  \section{
    \title{Putting Task Configs in Files}

    A lot of the pipeline examples that you will find on this site and in
    resource repos will embed a \reference{schema.task.config} directly
    in the pipeline. This is a nice way of clearly seeing what inputs/outputs the
    task uses. Tasks are usually designed to be used in multiple places, maybe
    with slightly different configuration. To support this scenario, most users
    store task configs in files instead of embedding the config directly in the
    pipeline.

    Here's what this looks like in practice:

    \titled-codeblock{print-date.yml, stored in some git repo under a folder named "tasks"}{yaml}{{
    platform: linux

    image_resource: # define a default image for the task to use
      type: registry-image
      source:
        repository: busybox

    run:
      path: date
      args: ["+%Y-%m-%d"]
    }}
    \inset{} {- spacer -}

    Using the task in a pipeline:

    \titled-codeblock{pipeline.yml}{yaml}{{
    resources:
    - name: ci
      type: git
      source:
        uri: https://github.com/concourse/examples.git

    jobs:
    - name: the-best-job
      plan:
      - get: ci
      - task: today
        file: ci/tasks/print-date.yml
    }}
  }

  \section{
    \title{\code{Get} Images for Tasks Instead of using Anonymous Image Resources}

    It is easy to let Concourse fetch images for tasks right when they are
    needed by using the \reference{schema.task-config.image_resource} field in a
    \reference{tasks}{task config}. It's the easy out-of-the-box solution.
    Another way is to pass the image for a task as an input to the job by
    setting the \reference{schema.task.image} field. This also allows
    you to track the version of the image being used by the task and also avoid
    getting rate-limited by configuring the resource with credentials.

    \bold{Before - Anonymous Image Fetching for Tasks}

    \codeblock{yaml}{{
    jobs:
    - name: job
      plan:
      - task: simple-task
        config:
          platform: linux
          image_resource: # anonymous image resource
            type: registry-image
            source:
              repository: busybox
          run:
            path: echo
            args: ["Hello world!"]
    }}

    \inset{} {- spacer -}

    \bold{After - Passing Task Image as Job Inputs}

    \codeblock{yaml}{{
    resources:
    - name: busybox
      type: registry-image
      source:
        repository: busybox
        username: ((docker.user))
        password: ((docker.password))

    jobs:
    - name: job
      plan:
      - get: busybox # pass image into job
      - task: simple-task
        image: busybox # use image for task. Overrides anonymous image
        config:
          platform: linux
          run:
            path: echo
            args: ["Hello world!"]
    }}
  }
}

\section{
  \omit-children-from-table-of-contents
  \title{Exploring Task Input and Output Scenarios}
  Understanding how task inputs and outputs work in Concourse can be a little
  confusing initially. This guide will walk you through a few example pipelines
  to show you how inputs and outputs work within a single Concourse job. By the
  end you should understand how inputs and outputs work within the context of a
  single job.

  To run the pipelines in the following examples yourself you can get your own
  Concourse running locally by following the \reference{quick-start} guide.
  Then use \reference{fly-set-pipeline} to see the pipelines in action.

  \section{
    \title{#1 - Passing Inputs Between Tasks}

    This pipeline will show us how to create outputs and pass outputs as inputs
    to the next \reference{steps}{step} in a \reference{schema.job.plan}{job plan}.

    This pipeline has two tasks. The first task outputs a file with the date. The
    second task reads and prints the contents of the file from the first task.

    Here's a visualization of the job.
    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-one-10.gif}{100%}

    \titled-codeblock{passing-artifacts.yml}{yaml}{{
    busybox: &busybox #YAML anchor
      type: registry-image
      source:
        repository: busybox

    jobs:
    - name: the-job
      plan:
      - task: create-one-output
        config:
          platform: linux
          image_resource: *busybox
          outputs:
            # Concourse will make an empty dir with this name
            # and save the contents for later steps
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output/file
      - task: read-output-from-previous-step
        config:
          platform: linux
          image_resource: *busybox
          # You must explicitly name the inputs you expect
          # this task to have.
          # If you don't then outputs from previous steps
          # will not appear in the step's container.
          # The name must match the output from the previous step.
          # Try removing or renaming the input to see what happens!
          inputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                cat ./the-output/file
    }}

    Set and run this pipeline to see the results yourself. Save the pipeline in a
    file called \code{passing-artifacts.yml}.

    \codeblock{bash}{{
    fly -t tutorial set-pipeline -p passing-artifacts -c passing-artifacts.yml
    fly -t tutorial unpause-pipeline -p passing-artifacts
    fly -t tutorial trigger-job --job passing-artifacts/the-job --watch
    }}
  }

  \section{
    \title{#2 - Two tasks with the same output, who wins?}

    This scenario is to satisfy the curiosity cat inside all of us. Never do this
    in real life because you're definitely going to hurt yourself!


    There are two \reference{jobs} in this pipeline. The first job,
    \code{writing-in-parallel}, has two \reference{steps}; both steps will produce an
    artifact named \code{the-output} in parallel. If you run the
    \code{writing-to-the-same-output-in-parallel} job multiple times you'll see
    the file in \code{the-output} folder changes depending on which of the
    parallel tasks finished last. Here's a visualization of the first job.

    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-two-parallel.gif}{100%}

    The second job, \code{writing-to-the-same-output-serially}, is a serial
    version of the first job. In this job the second task always wins because
    it's the last task that outputs \code{the-output}, so only \code{file2}
    will be in \code{the-output} directory in the last step in the job plan.

    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-two-serial.gif}{100%}

    The lesson to take away from this example is that \bold{last to write wins}
    when it comes to the state of any particular artifact in your job.

    \titled-codeblock{parallel-artifacts.yml}{yaml}{{
    busybox: &busybox #YAML anchor
      type: registry-image
      source:
        repository: busybox

    jobs:
    - name: writing-to-the-same-output-in-parallel
      plan:
      # running two tasks that output in parallel?!?
      # who will win??
      - in_parallel:
        - task: create-the-output
          config:
            platform: linux
            image_resource: *busybox
            outputs:
              - name: the-output
            run:
              path: /bin/sh
              args:
                - -cx
                - |
                  ls -lah
                  date > ./the-output/file1
        - task: also-create-the-output
          config:
            platform: linux
            image_resource: *busybox
            outputs:
              - name: the-output
            run:
              path: /bin/sh
              args:
                - -cx
                - |
                  ls -lah
                  date > ./the-output/file2
      # run this job multiple times to see which
      # previous task wins each time
      - task: read-output-from-previous-step
        config:
          platform: linux
          image_resource: *busybox
          inputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah ./the-output
                echo "Get ready to error!"
                cat ./the-output/file1 ./the-output/file2

    - name: writing-to-the-same-output-serially
      plan:
      - task: create-the-output
        config:
          platform: linux
          image_resource: *busybox
          outputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output/file1
      - task: also-create-the-output
        config:
          platform: linux
          image_resource: *busybox
          outputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output/file2
      - task: read-output-from-previous-step
        config:
          platform: linux
          image_resource: *busybox
          inputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah ./the-output
                echo "Get ready to error! file1 will never exist"
                cat ./the-output/file1 ./the-output/file2
    }}

    Set and run this pipeline to see the results yourself. Save the pipeline in a
    file called \code{parallel-artifacts.yml}.

    \codeblock{bash}{{
    fly -t tutorial set-pipeline -p parallel-artifacts -c parallel-artifacts.yml
    fly -t tutorial unpause-pipeline -p parallel-artifacts
    fly -t tutorial trigger-job --job parallel-artifacts/writing-to-the-same-output-in-parallel --watch
    fly -t tutorial trigger-job --job parallel-artifacts/writing-to-the-same-output-serially --watch
    }}
  }

  \section{
    \title{#3 - Mapping the Names of Inputs and Outputs}

    Sometimes the names of inputs and outputs don't match between multiple
    \reference{schema.task.config}{task configs}, or they do match and
    you don't want them overwriting each other, like in the previous example.
    That's when
    \reference{schema.task.input_mapping}{\code{input_mapping}} and
    \reference{schema.task.output_mapping}{\code{output_mapping}}
    become helpful. Both of these features rename the inputs/outputs in the task's
    config to some other name in the job plan.

    This pipeline has one job with four tasks.

    The first task outputs a file with the date to the \code{the-output}
    directory.  \code{the-output} is mapped to the new name \code{demo-disk}.
    The artifact \code{demo-disk} is now available in the rest of the job plan
    for future steps to take as inputs.

    The second task reads and prints the contents of the file under the new name
    \code{demo-disk}.

    The third task reads and prints the contents of the file under another name,
    \code{generic-input}. The \code{demo-disk} artifact in the job plan is mapped
    to \code{generic-input}.

    The fourth task tries to use the artifact named \code{the-output} as its input.
    This task fails to even start because there was no artifact with the name
    \code{the-output} available in the \reference{schema.job.plan}{job plan}; it was remapped to \code{demo-disk}.

    Here's a visualization of the job.

    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-three-1.gif}{100%}

    \titled-codeblock{mapping-artifacts.yml}{yaml}{{
    busybox: &busybox #YAML anchor
      type: registry-image
      source:
        repository: busybox

    jobs:
    - name: the-job
      plan:
      - task: create-one-output
        # The task config has the artifact `the-output`
        # output_mapping will rename `the-output` to `demo-disk`
        # in the rest of the job's plan
        output_mapping:
          the-output: demo-disk
        config:
          platform: linux
          image_resource: *busybox
          outputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output/file
      # this task expects the artifact `demo-disk` so no mapping is needed
      - task: read-output-from-previous-step
        config:
          platform: linux
          image_resource: *busybox
          inputs:
            - name: demo-disk
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                cat ./demo-disk/file
      - task: rename-and-read-output
        # This task expects the artifact `generic-input`.
        # input_mapping will map the task's `generic-input` to
        # the job plans `demo-disk` artifact.
        # `demo-disk` is renamed to `generic-input`.
        input_mapping:
          generic-input: demo-disk
        config:
          platform: linux
          image_resource: *busybox
          inputs:
            - name: generic-input
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                cat ./generic-input/file
      - task: try-to-read-the-output
        input_mapping:
          generic-input: demo-disk
        config:
          platform: linux
          image_resource: *busybox
          # `the-output` is not available in the job plan
          # so this task will error while initializing
          # since there's no artiact named `the-output` in
          # the job's plan
          inputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                cat ./generic-input/file
    }}

    Set and run this pipeline to see the results yourself. Save the pipeline in a
    file called \code{mapping-artifacts.yml}.

    \codeblock{bash}{{
    fly -t tutorial set-pipeline -p mapping-artifacts -c mapping-artifacts.yml
    fly -t tutorial unpause-pipeline -p mapping-artifacts
    fly -t tutorial trigger-job --job mapping-artifacts/the-job --watch
    }}
  }

  \section{
    \title{#4 - Adding Files to an Existing Artifact}
    This pipeline will also have two jobs in order to illustrate this point. What
    happens if we add a file to an output? If you think back to example two you
    may already know the answer.

    The first task will create \code{the-output} with \code{file1}. The second
    task will add \code{file2} to the \code{the-output}. The last task will read
    the contents of \code{file1} and \code{file2}.

    As long as you re-declare the input as an output in the second task you can
    modify any of your outputs.

    This means you can pass something between a bunch of tasks and have each task
    add or modify something in the artifact.

    Here's a visualization of the job.
    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-four.gif}{100%}

    \titled-codeblock{existing-artifact.yml}{yaml}{{
    jobs:
    - name: add-file-to-output
      plan:
      - task: create-one-output
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          outputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output/file1
      - task: add-file-to-previous-output
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          # this task lists the same artifact as
          # its input and output
          inputs:
            - name: the-output
          outputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output/file2
      - task: read-output-from-previous-step
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          inputs:
            - name: the-output
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah ./the-output
                cat ./the-output/file1 ./the-output/file2
    }}

    Set and run this pipeline to see the results yourself. Save the pipeline in a
    file called \code{existing-artifact.yml}.

    \codeblock{bash}{{
    fly -t tutorial set-pipeline -p existing-artifact -c existing-artifact.yml
    fly -t tutorial unpause-pipeline -p existing-artifact
    fly -t tutorial trigger-job --job existing-artifact/add-file-to-output --watch
    }}
  }

  \section{
    \title{#5 - Task With Multiple Inputs and Outputs}

    What happens if you have a task that has multiple outputs and a second task
    that only lists one of the outputs? Does the second task get the extra
    outputs from the first task?

    The answer is no. A task will only get the artifacts that match the name of
    the inputs listed in the task's config.

    Here's a visualization of the job.
    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-five.gif}{100%}

    \titled-codeblock{multiple-artifacts.yml}{yaml}{{
    jobs:
    - name: multiple-outputs
      plan:
      - task: create-three-outputs
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          outputs:
            - name: the-output-1
            - name: the-output-2
            - name: the-output-3
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah
                date > ./the-output-1/file
                date > ./the-output-2/file
                date > ./the-output-3/file
      - task: take-one-output
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          # only one of the three outputs are
          # listed as inputs
          inputs:
            - name: the-output-1
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah ./
                cat ./the-output-1/file
      - task: take-two-outputs
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          # this task pulls in the other
          # two outputs, just for fun!
          inputs:
            - name: the-output-2
            - name: the-output-3
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah ./
                cat ./the-output-2/file
                cat ./the-output-3/file
    }}

    Set and run this pipeline to see the results yourself. Save the pipeline in a
    file called \code{multiple-artifacts.yml}.

    \codeblock{bash}{{
    fly -t tutorial set-pipeline -p multiple-artifacts -c multiple-artifacts.yml
    fly -t tutorial unpause-pipeline -p multiple-artifacts
    fly -t tutorial trigger-job --job multiple-artifacts/multiple-outputs --watch
    }}
  }

  \section{
    \title{#6 - Get Steps Generate Artifacts}

    The majority of Concourse pipelines have at least one
    \reference{resources}{resource}, which means they have at least one \reference{get-step}{get step}.
    Using a \reference{get-step}{get step} in a job makes an artifact with the name of the get step
    available for later steps in the job plan to consume as inputs.

    Here's a visualization of the job.
    \diagram{images/how-to-guides/pipelines/explore-task-inputs-outs-example-six.gif}{100%}

    \titled-codeblock{get-artifact.yml}{yaml}{{
    resources:
    - name: concourse-examples
      type: git
      source:
        uri: "https://github.com/concourse/examples"

    jobs:
    - name: get-job
      plan:
      # there will be an artifact named
      # "concourse-examples" available in the job plan
      - get: concourse-examples
      - task: take-one-output
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: busybox
          inputs:
            - name: concourse-examples
          run:
            path: /bin/sh
            args:
              - -cx
              - |
                ls -lah ./
                cat ./concourse-examples/README.md
    }}

    Set and run this pipeline to see the results yourself. Save the pipeline in a
    file called \code{get-artifact.yml}.

    \codeblock{bash}{{
    fly -t tutorial set-pipeline -p get-artifact -c get-artifact.yml
    fly -t tutorial unpause-pipeline -p get-artifact
    fly -t tutorial trigger-job --job get-artifact/get-job --watch
    }}
  }
}

\section{
  \title{Gated Pipeline Patterns}
  \omit-children-from-table-of-contents

  Gated pipelines provide control for administrators and release managers on
  when a given software release is deployed to a tightly protected environment
  (e.g. production).

  The execution of jobs that perform certain tasks (e.g. deployment) targeting
  the downstream environment beyond the "gate" step is done only upon either an
  approval coming from an external Change Control system or an explicit manual
  trigger of such step.

  \section{
    \title{#1 - A Simple Gated Pipeline}

    By default all \reference{jobs} only run when manually triggered. That
    means a user has to run \reference{fly-trigger-job} or click the plus
    button in the web interface for a job to run. A job only runs automatically
    if one of its resources has the
    \reference{schema.get.trigger}{\code{trigger: true}} parameter
    set.

    Therefore, in order to create a gated job in a pipeline you simply need to
    create a job that can only be manually triggered. That means not setting
    \code{trigger: true} for any of the jobs' \reference{get-step}{get steps}.

    \codeblock{yaml}{{
jobs:
- name: run-automatically
  plan:
  - get: my-repo
    trigger: true  # has trigger:true so automatically triggers
  # can include more steps to run other things before hitting the gate

- name: the-gate  # manually trigger this job
  plan:
  - get: my-repo
    trigger: false  # redundant but guarantees the job won't run automatically
    passed:
      - run-automatically

# runs immediately after the gate is triggered
- name: do-more-stuff-after-the-gate
  plan:
  - get: my-repo
    passed:
      - the-gate
    trigger: true
  # can include more steps to run other things

resources:
- name: my-repo
  type: git
  source:
    uri: https://github.com/concourse/examples.git
    }}

    \link{\diagram{images/how-to-guides/pipelines/simple-gated-pipeline.png}{100%}}{images/how-to-guides/pipelines/simple-gated-pipeline.png}
  }

  \section{
    \title{#2 - Gated Pipeline Fanning In and Out}

    You can also use a gate as way to fan-in from multiple jobs and/or fan-out
    to multiple jobs as well.

    \codeblock{yaml}{{
jobs:
# three pre-gate jobs
- name: job-a
  plan:
  - get: my-repo
    trigger: true
- name: job-b
  plan:
  - get: my-repo
    trigger: true
- name: job-c
  plan:
  - get: my-repo
    trigger: true

- name: the-gate  # manually trigger this job
  plan:
  - get: my-repo
    trigger: false
    passed:  # fan-in from the three pre-gate jobs
      - job-a
      - job-b
      - job-c

# fan-out to three post-gate jobs
- name: post-gate-job-a
  plan:
  - get: my-repo
    trigger: true
    passed: [the-gate]
- name: post-gate-job-b
  plan:
  - get: my-repo
    trigger: true
    passed: [the-gate]
- name: post-gate-job-c
  plan:
  - get: my-repo
    trigger: true
    passed: [the-gate]

resources:
- name: my-repo
  type: git
  source:
    uri: https://github.com/concourse/examples.git
    }}

    \link{\diagram{images/how-to-guides/pipelines/simple-gated-fan-in-fan-out.png}{100%}}{images/how-to-guides/pipelines/simple-gated-fan-in-fan-out.png}
  }

  \section{
    \title{#3 - A Gated Pipeline With Notifications}
    This pipeline shows you how you can send a notification, like an email, to
    notify someone that a new build of your application is ready to be shipped.

    \codeblock{yaml}{{{
jobs:
- name: build-it
  plan:
  - get: my-repo
    trigger: true
  # can add steps to build your app

- name: test-it
  plan:
  - get: my-repo
    trigger: true
    passed: [build-it]
  # can add steps to run tests
  - put: email-release-manager
    params:
      subject: "Ready to ship"
      body_text: |
        A build is ready to be shipped!
        Build to be shipped: ${ATC_EXTERNAL_URL}/teams/${BUILD_TEAM_NAME}/pipelines/${BUILD_PIPELINE_NAME}/jobs/${BUILD_JOB_NAME}/builds/${BUILD_NAME}
        Link to pipeline: ${ATC_EXTERNAL_URL}/teams/${BUILD_TEAM_NAME}/pipelines/${BUILD_PIPELINE_NAME}

- name: ship-it
  plan:
  - get: my-repo
    trigger: false
    passed: [test-it]

resources:
- name: my-repo
  type: git
  source:
    uri: https://github.com/concourse/examples.git
- name: email-release-manager
  type: email
  source:
    # other required fields for this resource have been omitted
    from: pipeline@example.com
    to: release-manager@example.com

resource_types:
- name: email
  type: registry-image
  source:
    repository: pcfseceng/email-resource
    }}}

    \link{\diagram{images/how-to-guides/pipelines/gated-pipeline-with-notification.png}{100%}}{images/how-to-guides/pipelines/gated-pipeline-with-notification.png}
  }
}

\section{
  \title{Time Triggered Pipeline Patterns}
  \omit-children-from-table-of-contents

  The \link{time resource}{https://github.com/concourse/time-resource/}
  produces a new \reference{versions}{version} for the time interval that was
  declared in its definition in the pipeline configuration file.

  The two most common usages are having the time resource trigger on an interval:

  \codeblock{yaml}{{
  resources:
  - name: trigger-every-3-minutes
    type: time
    source:
      interval: 3m
  }}

  Or trigger once within a certain time range:
  \codeblock{yaml}{{
resources:
- name: trigger-daily-between-1am-and-2am
  type: time
  source:
    start: 1:00 AM
    stop: 2:00 AM
    location: America/Toronto
  }}

  Check the README of the \link{time
  resource}{https://github.com/concourse/time-resource/} for more details.

  \section{
    \title{#1 - Single Time Trigger}
    The following is an example of a pipeline that is triggered by a time
    resource on a pre-determined interval.

    \codeblock{yaml}{{
resources:
- name: trigger-every-3-minutes
  type: time
  source:
    interval: 3m

jobs:
- name: run-forrest-run
  plan:
  - get: trigger-every-3-minutes
    trigger: true
  # can add other steps to run in this job

- name: run-bubba-run
  plan:
  - get: trigger-every-3-minutes
    trigger: true
    passed:
      - run-forrest-run
  # can add other steps to run in this job
    }}
    \link{\diagram{images/how-to-guides/pipelines/time-triggered-01.png}{100%}}{images/how-to-guides/pipelines/time-triggered-01.png}
  }
  \section{
    \title{#2 - Multiple Time Triggers}
    As an enhancement to the previous sample with a single time trigger, this
    pipeline example implements two time resource triggers and the ability to
    manually kick it off outside of the time resources schedules.

    The first time you set up a pipeline like this you will need to manually
    trigger it in order to satisfy the passed constraint of the
    \code{manual-trigger} resource. Once one version is available that
    satisfies the passed constraint all future triggers by the other resources
    will work as expected.

    \codeblock{yaml}{{
resources:
- name: trigger-every-4-minutes
  type: time
  source:
    interval: 4m
- name: trigger-every-10-minutes
  type: time
  source:
    interval: 10m
- name: manual-trigger
  type: time
  source:
    interval: 1m

jobs:
- name: manual-trigger
  plan:
  - put: manual-trigger

- name: run-forrest-run
  plan:
  - get: trigger-every-4-minutes
    trigger: true
  - get: trigger-every-10-minutes
    trigger: true
  - get: manual-trigger
    trigger: true
    passed:
      - manual-trigger
  # can add other steps to run in this job

- name: run-bubba-run
  plan:
  - get: trigger-every-4-minutes
    trigger: true
    passed:
      - run-forrest-run
  - get: trigger-every-10-minutes
    trigger: true
    passed:
      - run-forrest-run
  - get: manual-trigger
    trigger: true
    passed:
      - run-forrest-run
  # can add other steps to run in this job
    }}

    \link{\diagram{images/how-to-guides/pipelines/time-triggered-02.png}{100%}}{images/how-to-guides/pipelines/time-triggered-02.png}
  }
}

\section{
  \title{Manual Approval Step}

  This is an exampe of a \reference{task-step} you can add to your
  \reference{jobs} that requires a human to approve or reject the job from
  running. This is probably the most minimal version of a manual approval step
  you can have in Concourse that doesn't require pulling in a bunch of other
  tech into your stack. It's definitely not the best UX since you need to use
  the \code{fly} CLI to approve the step.

  Task configuration, \code{config.yml}:
  \codeblock{yaml}{{
  platform: linux

  inputs:
  - name: repo

  params:
    APPROVAL_TIMEOUT: 600 #default of 10mins

  run:
    path: repo/tasks/manual-approval/run.sh
  }}

  Task script, \code{run.sh}:
  \codeblock{bash}{{
  #!/usr/bin/env bash

  set -euo pipefail

  timeout=$((EPOCHSECONDS+APPROVAL_TIMEOUT))
  echo -n "Waiting for manual approval..."
  until [[ -f /tmp/approved || $EPOCHSECONDS -gt $timeout ]]; do
      sleep 5
      echo -n "."
  done

  if [[ -f /tmp/approved ]]; then
      echo "Step approved!"
  else
      echo "Approval timeout reached. Aborting job."
      exit 1
  fi
  }}

  To approve the job when it gets to this step you have to create
  \code{/tmp/approved} on the step's container. You can do that user
  \code{fly}'s \code{intercept} command, like so (replace \code{PIPELINE/JOB}
  with the name of your pipeline and job that the step resides in):

  \codeblock{bash}{{
  fly -t ci intercept --job PIPELINE/JOB --step manual-approval touch /tmp/approved
  }}

  Here's the step added in-line to a pipeline so you can see how it works on its own.

  \codeblock{yaml}{{
  jobs:
  - name: approval
    plan:
    - task: manual-approval
      params:
        APPROVAL_TIMEOUT: 600 #10mins
      config:
        platform: linux
        image_resource:
          type: mock
          source:
            mirror_self: true
        run:
          path: bash
          args:
          - -c
          - |
            #!/usr/bin/env bash

            set -euo pipefail

            timeout=$((EPOCHSECONDS+APPROVAL_TIMEOUT))
            echo -n "Waiting for manual approval..."
            until [[ -f /tmp/approved || $EPOCHSECONDS -gt $timeout ]]; do
                sleep 5
                echo -n "."
            done

            if [[ -f /tmp/approved ]]; then
                echo "Step approved!"
            else
                echo "Approval timeout reached. Aborting job."
                exit 1
            fi
  }}
}
