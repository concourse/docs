{"1---a-simple-gated-pipeline":{"location":"gated-pipeline-patterns.html#1---a-simple-gated-pipeline","title":"#1 - A Simple Gated Pipeline","text":"By default all Jobs only run when manually triggered. That means a user has to run fly trigger-job or click the plus button in the web interface for a job to run. A job only runs automatically if one of it's resources has the trigger: true parameter set.\n\nTherefore, in order to create a gated job in a pipeline you simply need to create a job that can only be manually triggered. That means not setting trigger: true for any of the jobs get steps.\n\n---\njobs:\n- name: run-automatically\n  plan:\n  - get: my-repo\n    trigger: true  # has trigger:true so automatically triggers\n  # can include more steps to run other things before hitting the gate\n\n- name: the-gate  # manually trigger this job\n  plan:\n  - get: my-repo\n    trigger: false  # redundant but guarantees the job won't run automatically\n    passed:\n      - run-automatically\n\n# runs immediately after the gate is triggered\n- name: do-more-stuff-after-the-gate\n  plan:\n  - get: my-repo\n    passed:\n      - the-gate\n    trigger: true\n  # can include more steps to run other things\n\nresources:\n- name: my-repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\nimages/how-to-guides/pipelines/simple-gated-pipeline.png","depth":5,"section_tag":"1---a-simple-gated-pipeline"},"1---passing-inputs-between-tasks":{"location":"exploring-task-input-and-output-scenarios.html#1---passing-inputs-between-tasks","title":"#1 - Passing Inputs Between Tasks","text":"This pipeline will show us how to create outputs and pass outputs as inputs to the next step in a job plan.\n\nThis pipeline has two tasks. The first task outputs a file with the date. The second task reads and prints the contents of the file from the first task.\n\nHere's a visualization of the job. images/how-to-guides/pipelines/explore-task-inputs-outs-example-one-10.gif\n\nbusybox: \u0026busybox #YAML anchor\n  type: registry-image\n  source:\n    repository: busybox\n\njobs:\n- name: the-job\n  plan:\n  - task: create-one-output\n    config:\n      platform: linux\n      image_resource: *busybox\n      outputs:\n        # Concourse will make an empty dir with this name\n        # and save the contents for later steps\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output/file\n  - task: read-ouput-from-previous-step\n    config:\n      platform: linux\n      image_resource: *busybox\n      # You must explicitly name the inputs you expect\n      # this task to have.\n      # If you don't then outputs from previous steps\n      # will not appear in the step's container.\n      # The name must match the output from the previous step.\n      # Try removing or renaming the input to see what happens!\n      inputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            cat ./the-output/file\nSet and run this pipeline to see the results yourself. Save the pipeline in a file called passing-artifacts.yml.\n\nfly -t tutorial set-pipeline -p passing-artifacts -c passing-artifacts.yml\nfly -t tutorial unpause-pipeline -p passing-artifacts\nfly -t tutorial trigger-job --job passing-artifacts/the-job --watch\n","depth":5,"section_tag":"1---passing-inputs-between-tasks"},"2---gated-pipeline-fanning-in-and-out":{"location":"gated-pipeline-patterns.html#2---gated-pipeline-fanning-in-and-out","title":"#2 - Gated Pipeline Fanning In and Out","text":"You can also use a gate as way to fan-in from multiple jobs and/or fan-out to multiple jobs as well.\n\n---\njobs:\n# three pre-gate jobs\n- name: job-a\n  plan:\n  - get: my-repo\n    trigger: true\n- name: job-b\n  plan:\n  - get: my-repo\n    trigger: true\n- name: job-c\n  plan:\n  - get: my-repo\n    trigger: true\n\n- name: the-gate  # manually trigger this job\n  plan:\n  - get: my-repo\n    trigger: false\n    passed:  # fan-in from the three pre-gate jobs\n      - job-a\n      - job-b\n      - job-c\n\n# fan-out to three post-gate jobs\n- name: post-gate-job-a\n  plan:\n  - get: my-repo\n    trigger: true\n    passed: [the-gate]\n- name: post-gate-job-b\n  plan:\n  - get: my-repo\n    trigger: true\n    passed: [the-gate]\n- name: post-gate-job-c\n  plan:\n  - get: my-repo\n    trigger: true\n    passed: [the-gate]\n\nresources:\n- name: my-repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\nimages/how-to-guides/pipelines/simple-gated-fan-in-fan-out.png","depth":5,"section_tag":"2---gated-pipeline-fanning-in-and-out"},"2---two-tasks-with-the-same-output-who-wins":{"location":"exploring-task-input-and-output-scenarios.html#2---two-tasks-with-the-same-output-who-wins","title":"#2 - Two tasks with the same output, who wins?","text":"This scenario is to satisfy the curiosity cat inside all of us. Never do this in real life because you're definitely going to hurt yourself!\n\nThere are two Jobs in this pipeline. The first job, writing-in-parallel, has two Steps; both steps will produce an artifact named the-output in parallel. If you run the writing-to-the-same-output-in-parallel job multiple times you'll see the file in the-output folder changes depending on which of the parallel tasks finished last. Here's a visualization of the first job.\n\nimages/how-to-guides/pipelines/explore-task-inputs-outs-example-two-parallel.gifThe second job, writing-to-the-same-output-serially, is a serial version of the first job. In this job the second task always wins because it's the last task that outputs the-output, so only file2 will be in the-output directory in the last step in the job plan.\n\nimages/how-to-guides/pipelines/explore-task-inputs-outs-example-two-serial.gifThe lesson to take away from this example is that last to write wins when it comes to the state of any particular artifact in your job.\n\nbusybox: \u0026busybox #YAML anchor\n  type: registry-image\n  source: repository: busybox\n\njobs:\n- name: writing-to-the-same-output-in-parallel\n  plan:\n  # running two tasks that output in parallel?!?\n  # who will win??\n  - in_parallel:\n    - task: create-the-output\n      config:\n        platform: linux\n        image_resource: *busybox\n        outputs:\n          - name: the-output\n        run:\n          path: /bin/sh\n          args:\n            - -cx\n            - |\n              ls -lah\n              date \u003e ./the-output/file1\n    - task: also-create-the-output\n      config:\n        platform: linux\n        image_resource: *busybox\n        outputs:\n          - name: the-output\n        run:\n          path: /bin/sh\n          args:\n            - -cx\n            - |\n              ls -lah\n              date \u003e ./the-output/file2\n  # run this job multiple times to see which\n  # previous task wins each time\n  - task: read-ouput-from-previous-step\n    config:\n      platform: linux\n      image_resource: *busybox\n      inputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah ./the-ouput\n            echo \"Get ready to error!\"\n            cat ./the-output/file1 ./the-output/file2\n\n- name: writing-to-the-same-output-serially\n  plan:\n  - task: create-the-output\n    config:\n      platform: linux\n      image_resource: *busybox\n      outputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output/file1\n  - task: also-create-the-output\n    config:\n      platform: linux\n      image_resource: *busybox\n      outputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output/file2\n  - task: read-ouput-from-previous-step\n    config:\n      platform: linux\n      image_resource: *busybox\n      inputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah ./the-ouput\n            echo \"Get ready to error! file1 will never exist\"\n            cat ./the-output/file1 ./the-output/file2\nSet and run this pipeline to see the results yourself. Save the pipeline in a file called parallel-artifacts.yml.\n\nfly -t tutorial set-pipeline -p parallel-artifacts -c parallel-artifacts.yml\nfly -t tutorial unpause-pipeline -p parallel-artifacts\nfly -t tutorial trigger-job --job parallel-artifacts/writing-in-parallel --watch\nfly -t tutorial trigger-job --job parallel-artifacts/writing-to-the-same-output-serially --watch\n","depth":5,"section_tag":"2---two-tasks-with-the-same-output-who-wins"},"3---a-gated-pipeline-with-notifications":{"location":"gated-pipeline-patterns.html#3---a-gated-pipeline-with-notifications","title":"#3 - A Gated Pipeline With Notifications","text":"This pipeline shows you how you can send a notification, like an email, to notify someone that a new build of your application is ready to be shipped.\n\n---\njobs:\n- name: build-it\n  plan:\n  - get: my-repo\n    trigger: true\n  # can add steps to build your app\n\n- name: test-it\n  plan:\n  - get: my-repo\n    trigger: true\n    passed: [build-it]\n  # can add steps to run tests\n  - put: email-release-manager\n    params:\n      subject: \"Ready to ship\"\n      body_text: |\n        A build is ready to be shipped!\n        Build to be shipped: ${ATC_EXTERNAL_URL}/teams/${BUILD_TEAM_NAME}/pipelines/${BUILD_PIPELINE_NAME}/jobs/${BUILD_JOB_NAME}/builds/${BUILD_NAME}\n        Link to pipeline: ${ATC_EXTERNAL_URL}/teams/${BUILD_TEAM_NAME}/pipelines/${BUILD_PIPELINE_NAME}\n\n- name: ship-it\n  plan:\n  - get: my-repo\n    trigger: false\n    passed: [test-it]\n\nresources:\n- name: my-repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\n- name: email-release-manager\n  type: email\n  source:\n    # other required fields for this resource have been omitted\n    from: pipeline@example.com\n    to: release-manager@example.com\n\nresource_types:\n- name: email\n  type: registry-image\n  source:\n    repository: pcfseceng/email-resource\nimages/how-to-guides/pipelines/gated-pipeline-with-notification.png","depth":5,"section_tag":"3---a-gated-pipeline-with-notifications"},"3---mapping-the-names-of-inputs-and-outputs":{"location":"exploring-task-input-and-output-scenarios.html#3---mapping-the-names-of-inputs-and-outputs","title":"#3 - Mapping the Names of Inputs and Outputs","text":"Sometimes the names of inputs and outputs don't match between multiple task configs, or they do match and you don't want them overwriting each other, like in the previous example. That's when input_mapping and output_mapping become helpful. Both of these features rename the inputs/outputs in the task's config to some other name in the job plan.\n\nThis pipeline has one job with four tasks.\n\nThe first task outputs a file with the date to the the-output directory.  the-output is mapped to the new name demo-disk. The artifact demo-disk is now available in the rest of the job plan for future steps to take as inputs.\n\nThe second task reads and prints the contents of the file under the new name demo-disk.\n\nThe third task reads and prints the contents of the file under another name, generic-input. The demo-disk artifact in the job plan is mapped to generic-input.\n\nThe fourth task tries to use the artifact named the-output as its input. This task fails to even start because there was no artifact with the name the-output available in the job plan; it was remapped to demo-disk.\n\nHere's a visualization of the job.\n\nimages/how-to-guides/pipelines/explore-task-inputs-outs-example-three-1.gifbusybox: \u0026busybox #YAML anchor\n  type: registry-image\n  source: repository: busybox\n\njobs:\n- name: the-job\n  plan:\n  - task: create-one-output\n    # The task config has the artifact `the-output`\n    # output_mapping will rename `the-output` to `demo-disk`\n    # in the rest of the job's plan\n    output_mapping:\n      the-output: demo-disk\n    config:\n      platform: linux\n      image_resource: *busybox\n      outputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output/file\n  # this task expects the artifact `demo-disk` so no mapping is needed\n  - task: read-ouput-from-previous-step\n    config:\n      platform: linux\n      image_resource: *busybox\n      inputs:\n        - name: demo-disk\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            cat ./demo-disk/file\n  - task: rename-and-read-output\n    # This task expects the artifact `generic-input`.\n    # input_mapping will map the task's `generic-input` to\n    # the job plans `demo-disk` artifact.\n    # `demo-disk` is renamed to `generic-input`.\n    input_mapping:\n      generic-input: demo-disk\n    config:\n      platform: linux\n      image_resource: *busybox\n      inputs:\n        - name: generic-input\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            cat ./generic-input/file\n  - task: try-to-read-the-output\n    input_mapping:\n      generic-input: demo-disk\n    config:\n      platform: linux\n      image_resource: *busybox\n      # `the-output` is not available in the job plan\n      # so this task will error while initializing\n      # since there's no artiact named `the-output` in\n      # the job's plan\n      inputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            cat ./generic-input/file\nSet and run this pipeline to see the results yourself. Save the pipeline in a file called mapping-artifacts.yml.\n\nfly -t tutorial set-pipeline -p mapping-artifacts -c mapping-artifacts.yml\nfly -t tutorial unpause-pipeline -p mapping-artifacts\nfly -t tutorial trigger-job --job mapping-artifacts/the-job --watch\n","depth":5,"section_tag":"3---mapping-the-names-of-inputs-and-outputs"},"4---adding-files-to-an-existing-artifact":{"location":"exploring-task-input-and-output-scenarios.html#4---adding-files-to-an-existing-artifact","title":"#4 - Adding Files to an Existing Artifact","text":"This pipeline will also have two jobs in order to illustrate this point. What happens if we add a file to an output? If you think back to example two you may already know the answer.\n\nThe first task will create the-output with file1. The second task will add file2 to the the-output. The last task will read the contents of file1 and file2.\n\nAs long as you re-declare the input as an output in the second task you can modify any of your outputs.\n\nThis means you can pass something between a bunch of tasks and have each task add or modify something in the artifact.\n\nHere's a visualization of the job. images/how-to-guides/pipelines/explore-task-inputs-outs-example-four.gif\n\njobs:\n- name: add-file-to-output\n  plan:\n  - task: create-one-output\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      outputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output/file1\n  - task: add-file-to-previous-output\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      # this task lists the same artifact as\n      # its input and output\n      inputs:\n        - name: the-output\n      outputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output/file2\n  - task: read-ouput-from-previous-step\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      inputs:\n        - name: the-output\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah ./the-output\n            cat ./the-output/file1 ./the-output/file2\nSet and run this pipeline to see the results yourself. Save the pipeline in a file called existing-artifact.yml.\n\nfly -t tutorial set-pipeline -p existing-artifact -c existing-artifact.yml\nfly -t tutorial unpause-pipeline -p existing-artifact\nfly -t tutorial trigger-job --job existing-artifact/add-file-to-output --watch\n","depth":5,"section_tag":"4---adding-files-to-an-existing-artifact"},"5---task-with-multiple-inputs-and-outputs":{"location":"exploring-task-input-and-output-scenarios.html#5---task-with-multiple-inputs-and-outputs","title":"#5 - Task With Multiple Inputs and Outputs","text":"What happens if you have a task that has multiple outputs and a second task that only lists one of the outputs? Does the second task get the extra outputs from the first task?\n\nThe answer is no. A task will only get the artifacts that match the name of the inputs listed in the task's config.\n\nHere's a visualization of the job. images/how-to-guides/pipelines/explore-task-inputs-outs-example-five.gif\n\njobs:\n- name: multiple-outputs\n  plan:\n  - task: create-three-outputs\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      outputs:\n        - name: the-output-1\n        - name: the-output-2\n        - name: the-output-3\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah\n            date \u003e ./the-output-1/file\n            date \u003e ./the-output-2/file\n            date \u003e ./the-output-3/file\n  - task: take-one-output\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      # only one of the three outputs are\n      # listed as inputs\n      inputs:\n        - name: the-output-1\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah ./\n            cat ./the-output-1/file\n  - task: take-two-outputs\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      # this task pulls in the other\n      # two outputs, just for fun!\n      inputs:\n        - name: the-output-2\n        - name: the-output-3\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah ./\n            cat ./the-output-2/file\n            cat ./the-output-3/file\nSet and run this pipeline to see the results yourself. Save the pipeline in a file called multiple-artifacts.yml.\n\nfly -t tutorial set-pipeline -p multiple-artifacts -c multiple-artifacts.yml\nfly -t tutorial unpause-pipeline -p multiple-artifacts\nfly -t tutorial trigger-job --job multiple-artifacts/multiple-outputs --watch\n","depth":5,"section_tag":"5---task-with-multiple-inputs-and-outputs"},"6---get-steps-generate-artifacts":{"location":"exploring-task-input-and-output-scenarios.html#6---get-steps-generate-artifacts","title":"#6 - Get Steps Generate Artifacts","text":"The majority of Concourse pipelines have at least one resource, which means they have at least one get step. Using a get step in a job makes an artifact with the name of the get step available for later steps in the job plan to consume as inputs.\n\nHere's a visualization of the job. images/how-to-guides/pipelines/explore-task-inputs-outs-example-six.gif\n\nresources:\n- name: concourse-examples\n  type: git\n  source: uri: \"https://github.com/concourse/examples\"\n\njobs:\n- name: get-job\n  plan:\n  # there will be an artifact named\n  # \"concourse-examples\" available in the job plan\n  - get: concourse-examples\n  - task: take-one-output\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: repository: busybox\n      inputs:\n        - name: concourse-examples\n      run:\n        path: /bin/sh\n        args:\n          - -cx\n          - |\n            ls -lah ./\n            cat ./concourse-examples/README.md\nSet and run this pipeline to see the results yourself. Save the pipeline in a file called get-artifact.yml.\n\nfly -t tutorial set-pipeline -p get-artifact -c get-artifact.yml\nfly -t tutorial unpause-pipeline -p get-artifact\nfly -t tutorial trigger-job --job get-artifact/get-job --watch\n","depth":5,"section_tag":"6---get-steps-generate-artifacts"},"LANDED-table":{"location":"internals.html#LANDED-table","title":"LANDED","text":"A worker in this state has successfully waited for all non-interruptible jobs on it after having concourse land-worker called. It will no longer be used to schedule any new containers or create volumes until it registers as RUNNING again.\n\n","depth":4,"section_tag":"worker-lifecycle"},"LANDING-table":{"location":"internals.html#LANDING-table","title":"LANDING","text":"The concourse land-worker command will put a worker in the LANDING state to safely drain its assignments for temporary downtime.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and transition the worker into LANDED state.\n\n","depth":4,"section_tag":"worker-lifecycle"},"RETIRING-table":{"location":"internals.html#RETIRING-table","title":"RETIRING","text":"The concourse retire-worker command will put a worker in the RETIRING state to remove it from the cluster permanently.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and remove the worker.\n\n","depth":4,"section_tag":"worker-lifecycle"},"RUNNING-table":{"location":"internals.html#RUNNING-table","title":"RUNNING","text":"A worker in this state is registered with the cluster and ready to start running containers and storing volumes.\n\n","depth":4,"section_tag":"worker-lifecycle"},"STALLED-table":{"location":"internals.html#STALLED-table","title":"STALLED","text":"A worker in this state was previously registered with the cluster, but stopped advertising itself for some reason. Ususally this is due to network connectivity issues, or the worker stopping unexpectedly.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":4,"section_tag":"worker-lifecycle"},"a-note-about-user-lookup":{"location":"generic-oidc-auth.html#a-note-about-user-lookup","title":"A note about user lookup","text":"When determining the user identity, Concourse will first look at the preferred_username claim. If this claim is empty or missing, it will then look at the claim specified by CONCOURSE_OIDC_USER_NAME_KEY (which defaults to username).\n\nLet's say that you want to tie each user to their email by using CONCOURSE_OIDC_USER_NAME_KEY=email.\n\nIf your OIDC provider returns the following claims, Concourse will still resolve the user to Jane Doe: {\n \"sub\": \"248289761001\",\n \"username\": \"j.doe\",\n \"preferred_username\": \"Jane Doe\",\n \"email\": \"janedoe@example.com\",\n}\n\n\nHowever, if the preferred_username claim is empty or missing, Concourse will respect the key and resolve the user to janedoe@example.com: {\n \"sub\": \"248289761001\",\n \"username\": \"j.doe\",\n \"preferred_username\": \"\",\n \"email\": \"janedoe@example.com\",\n}\n\n\n","depth":6,"section_tag":"a-note-about-user-lookup"},"action-matrix":{"location":"user-roles.html#action-matrix","title":"Action Matrix","text":"In this table, an action is marked as customizable if it is possible to change its permissions by providing the --config-rbac flag, documented below. Assigning an action to a role that is not customizable will have no effect on its permissions.\n\n| Action                        | fly commands affected                                       | UI actions affected                              | can be performed unauthenticated? | customizable |\n| GetBuild                      | n/a                                                                | view one-off build page                          | ✓                                 | ✓            |\n| BuildResources                | n/a                                                                | view build page                                  | ✓                                 | ✓            |\n| GetBuildPreparation           | n/a                                                                | view build page                                  | ✓                                 | ✓            |\n| BuildEvents                   | fly watch,Running tasks with fly execute                      | view build page                                  | ✓                                 | ✓            |\n| GetBuildPlan                  | n/a                                                                | view build page                                  | ✓                                 | ✓            |\n| ListBuildArtifacts            | n/a                                                                | n/a                                              | ✓                                 | ✓            |\n| AbortBuild                    | fly abort-build                                        | abort button on build page                       | ✘                                 | ✓            |\n| PruneWorker                   | fly prune-worker                                       | n/a                                              | ✘                                 | ✓            |\n| LandWorker                    | fly land-worker                                        | n/a                                              | ✘                                 | ✓            |\n| RetireWorker                  | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| ListDestroyingVolumes         | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| ListDestroyingContainers      | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| ReportWorkerContainers        | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| ReportWorkerVolumes           | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| GetPipeline                   | n/a                                                                | view pipeline page                               | ✓                                 | ✓            |\n| GetJobBuild                   | n/a                                                                | view build page                                  | ✓                                 | ✓            |\n| PipelineBadge                 | n/a                                                                | n/a                                              | ✓                                 | ✓            |\n| JobBadge                      | n/a                                                                | n/a                                              | ✓                                 | ✓            |\n| ListJobs                      | fly jobs                                               | view pipeline page                               | ✓                                 | ✓            |\n| GetJob                        | n/a                                                                | view job page                                    | ✓                                 | ✓            |\n| ListJobBuilds                 | fly builds                                             | view job page                                    | ✓                                 | ✓            |\n| ListPipelineBuilds            | fly builds                                             | n/a                                              | ✓                                 | ✓            |\n| GetResource                   | n/a                                                                | view resource page                               | ✓                                 | ✓            |\n| ListBuildsWithVersionAsInput  | n/a                                                                | expand version on resource page                  | ✓                                 | ✓            |\n| ListBuildsWithVersionAsOutput | n/a                                                                | expand version on resource page                  | ✓                                 | ✓            |\n| GetResourceCausality          | n/a                                                                | n/a                                              | ✓                                 | ✓            |\n| GetResourceVersion            | n/a                                                                | n/a                                              | ✓                                 | ✓            |\n| ListResources                 | fly resources                                               | view pipeline page                               | ✓                                 | ✓            |\n| ListResourceTypes             | n/a                                                                | n/a                                              | ✓                                 | ✓            |\n| ListResourceVersions          | fly resource-versions,fly pin-resource          | view resource page                               | ✓                                 | ✓            |\n| CreateBuild                   | Running tasks with fly execute                                            | n/a                                              | ✘                                 | ✓            |\n| GetContainer                  | n/a                                                                | n/a                                              | ✘                                 | ✓            |\n| HijackContainer               | fly intercept                                          | n/a                                              | ✘                                 | ✓            |\n| ListContainers                | fly containers                                         | n/a                                              | ✘                                 | ✓            |\n| ListWorkers                   | fly workers                                            | n/a                                              | ✘                                 | ✓            |\n| RegisterWorker                | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| HeartbeatWorker               | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| DeleteWorker                  | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| GetTeam                       | fly get-team                                           | n/a                                              | ✘                                 | ✓            |\n| SetTeam                       | fly set-team                                           | n/a                                              | ✘                                 | ✓            |\n| ListTeamBuilds                | fly builds                                             | n/a                                              | ✘                                 | ✓            |\n| RenameTeam                    | fly rename-team                                        | n/a                                              | ✘                                 | ✓            |\n| DestroyTeam                   | fly destroy-team                                       | n/a                                              | ✘                                 | ✓            |\n| ListVolumes                   | fly volumes                                            | n/a                                              | ✘                                 | ✓            |\n| DownloadCLI                   | fly sync                                               | icons on dashboard and pipeline pages            | ✓                                 | ✘            |\n| CheckResourceWebHook          | n/a                                                                | n/a                                              | ✓                                 | ✘            |\n| GetInfo                       | n/a                                                                | n/a                                              | ✓                                 | ✘            |\n| GetCheck                      | fly check-resource,fly check-resource-type | check button on resource page                    | ✘                                 | ✓            |\n| ListTeams                     | fly teams                                              | view dashboard page                              | ✓                                 | ✘            |\n| ListAllPipelines              | n/a                                                                | view dashboard page                              | ✓                                 | ✘            |\n| ListPipelines                 | fly pipelines                                          | n/a                                              | ✓                                 | ✓            |\n| ListAllJobs                   | fly teams                                              | view dashboard page                              | ✓                                 | ✘            |\n| ListAllResources              | n/a                                                                | view dashboard page                              | ✓                                 | ✘            |\n| ListBuilds                    | fly builds                                             | n/a                                              | ✓                                 | ✘            |\n| MainJobBadge                  | n/a                                                                | n/a                                              | ✓                                 | ✘            |\n| GetLogLevel                   | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| SetLogLevel                   | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| GetWall                       | n/a                                                                | n/a                                              | ✓                                 | ✘            |\n| SetWall                       | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| ClearWall                     | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| ListActiveUsersSince          | fly active-users                                       | n/a                                              | ✘                                 | ✘            |\n| GetInfoCreds                  | n/a                                                                | n/a                                              | ✘                                 | ✘            |\n| CheckResource                 | fly check-resource                                     | check button on resource page                    | ✘                                 | ✓            |\n| CheckResourceType             | fly check-resource-type                                | n/a                                              | ✘                                 | ✓            |\n| CreateJobBuild                | fly trigger-job                                        | trigger button on job and build pages            | ✘                                 | ✓            |\n| RerunJobBuild                 | fly rerun-build                                        | rerun button on build page                       | ✘                                 | ✓            |\n| CreatePipelineBuild           | Running tasks with fly execute                                            | n/a                                              | ✘                                 | ✓            |\n| DeletePipeline                | fly destroy-pipeline                                   | n/a                                              | ✘                                 | ✓            |\n| DisableResourceVersion        | fly disable-resource-version                           | version disable widget on resource page          | ✘                                 | ✓            |\n| EnableResourceVersion         | fly enable-resource-version                            | version enable widget on resource page           | ✘                                 | ✓            |\n| PinResourceVersion            | fly pin-resource                                       | pin buttons on resource page                     | ✘                                 | ✓            |\n| UnpinResource                 | fly unpin-resource                                          | pin buttons on resource page                     | ✘                                 | ✓            |\n| SetPinCommentOnResource       | fly pin-resource                                       | comment overlay on resource page                 | ✘                                 | ✓            |\n| GetConfig                     | fly get-pipeline                                       | n/a                                              | ✘                                 | ✓            |\n| GetCC                         | n/a                                                                | n/a                                              | ✘                                 | ✓            |\n| GetVersionsDB                 | n/a                                                                | n/a                                              | ✘                                 | ✓            |\n| ListJobInputs                 | n/a                                                                | n/a                                              | ✘                                 | ✓            |\n| OrderPipelines                | fly order-pipelines                                    | drag and drop on dashboard                       | ✘                                 | ✓            |\n| OrderPipelinesWithinGroup     | fly order-instanced-pipelines                          | drag and drop within instance group on dashboard | ✘                                 | ✓            |\n| PauseJob                      | fly pause-job                                          | pause button on job page                         | ✘                                 | ✓            |\n| PausePipeline                 | fly pause-pipeline                                     | pause button on pipeline or dashboard            | ✘                                 | ✓            |\n| RenamePipeline                | fly rename-pipeline                                    | n/a                                              | ✘                                 | ✓            |\n| UnpauseJob                    | fly unpause-job                                        | play button on job page                          | ✘                                 | ✓            |\n| UnpausePipeline               | fly unpause-pipeline                                   | play button on pipeline or dashboard             | ✘                                 | ✓            |\n| ExposePipeline                | fly expose-pipeline                                    | eyeball button on dashboard                      | ✘                                 | ✓            |\n| HidePipeline                  | fly hide-pipeline                                      | slashed eyeball button on dashboard              | ✘                                 | ✓            |\n| SaveConfig                    | fly set-pipeline                                       | n/a                                              | ✘                                 | ✓            |\n| ClearTaskCache                | fly clear-task-cache                                   | n/a                                              | ✘                                 | ✓            |\n| CreateArtifact                | Running tasks with fly execute                                            | n/a                                              | ✘                                 | ✓            |\n| GetArtifact                   | Running tasks with fly execute                                            | n/a                                              | ✘                                 | ✓            |\n| ClearResourceCache            | fly clear-resource-cache                               | n/a                                              | ✘                                 | ✓            |\n\n","depth":4,"section_tag":"action-matrix"},"add-a-job":{"location":"tutorial-hello-world.html#add-a-job","title":"Add a job","text":"We only need one job right now so let's add a single job named hello-world-job.\n\njobs:\n- name: hello-world-job\nAwesome, we have a job named hello-world! Now we need to add a step to our job. To define a list of steps a job should execute, we need to add the plan key to our job.\n\njobs:\n- name: hello-world-job\n  plan:\n","depth":4,"section_tag":"add-a-job"},"add-a-step":{"location":"tutorial-hello-world.html#add-a-step","title":"Add a step","text":"Unlike jobs, the order of steps does matter! Concourse will run the steps in the order that they are listed. Let's carefully place a task step as the first (and only) step in our job.\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\nFantastic! Now we need to tell Concourse how to run our task step. We do that by providing a task config.\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\nAt this point we are going to pause to explain steps a bit more.\n\n","depth":4,"section_tag":"add-a-step"},"adding-cf-users-to-the-main-team":{"location":"cf-uaa-auth.html#adding-cf-users-to-the-main-team","title":"Adding CF Users to the main Team","text":"CloudFoundry users and org/space members can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_CF_USER=username\nCONCOURSE_MAIN_TEAM_CF_ORG=org-name\nCONCOURSE_MAIN_TEAM_CF_SPACE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_ANY_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_DEVELOPER_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_AUDITOR_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_MANAGER_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_GUID=SPACE_GUID\nMultiple users, spaces, etc. may be specified by comma-separating them.\n\n","depth":6,"section_tag":"adding-cf-users-to-the-main-team"},"administration":{"location":"administration.html","title":"Administration","text":"","depth":3,"section_tag":"administration"},"algorithm":{"location":"scheduler.html#algorithm","title":"Algorithm","text":"The algorithm is a subcomponent of the scheduler which is used to determine the input versions to the next build of a job. There are many factors that contribute to figuring out the next input versions. It can be anything that affects which resource versions will be used to schedule a build, such as version constraints or passed constraints in a get step, disabling versions through the web UI, etc. The algorithm can also fail to determine a successful set of input versions, which the error will be propogated to the preparation view in the build page.\n\nIf the algorithm computes a successful set of input versions, it will figure out whether or not the versions it computed can be used to produce a new build. This is done by comparing the triggerable input versions to the versions used by the previous build and if any of them have a different version, then the scheduler will know to schedule a new build. Conversly, if the input versions produced by the algorithm are the same as the previous build, then the scheduler will not create a new build.\n\n","depth":5,"section_tag":"algorithm"},"architecture-worker":{"location":"internals.html#architecture-worker","title":"Workers Architecture","text":"Workers are machines running Garden and Baggageclaim servers and registering themselves via the TSA.\n\nNote: Windows and Darwin workers also run Garden and Baggageclaim servers but do not run containers. They both use houdini to fake making containers. Windows containers are not supported and Darwin does not have native container technology.\n\nWorkers have no important state configured on their machines, as everything runs in a container and thus shouldn't care about what packages are installed on the host (well, except for those that allow it to be a worker in the first place). This is very different from workers in other non-containerized CI solutions, where the state of packages on the worker is crucial to whether your pipeline works or not.\n\nEach worker registers itself with the Concourse cluster via the TSA.\n\nWorkers by default listen on port 7777 for Garden and port 7788 for Baggageclaim. Connections to both servers are forwarded over the SSH connection made to the TSA.\n\n","depth":3,"section_tag":"architecture-worker"},"audit-logs":{"location":"concourse-web.html#audit-logs","title":"Enabling audit logs","text":"A very simplistic form of audit logging can be enabled with the following vars:\n\n# Enable auditing for all api requests connected to builds.\nCONCOURSE_ENABLE_BUILD_AUDITING=true\n\n# Enable auditing for all api requests connected to containers.\nCONCOURSE_ENABLE_CONTAINER_AUDITING=true\n\n# Enable auditing for all api requests connected to jobs.\nCONCOURSE_ENABLE_JOB_AUDITING=true\n\n# Enable auditing for all api requests connected to pipelines.\nCONCOURSE_ENABLE_PIPELINE_AUDITING=true\n\n# Enable auditing for all api requests connected to resources.\nCONCOURSE_ENABLE_RESOURCE_AUDITING=true\n\n# Enable auditing for all api requests connected to system transactions.\nCONCOURSE_ENABLE_SYSTEM_AUDITING=true\n\n# Enable auditing for all api requests connected to teams.\nCONCOURSE_ENABLE_TEAM_AUDITING=true\n\n# Enable auditing for all api requests connected to workers.\nCONCOURSE_ENABLE_WORKER_AUDITING=true\n\n# Enable auditing for all api requests connected to volumes.\nCONCOURSE_ENABLE_VOLUME_AUDITING=true\nWhen enabled, API requests will result in an info-level log line like so:\n\n{\"timestamp\":\"2019-05-09T14:41:54.880381537Z\",\"level\":\"info\",\"source\":\"atc\",\"message\":\"atc.audit\",\"data\":{\"action\":\"Info\",\"parameters\":{},\"user\":\"test\"}}\n{\"timestamp\":\"2019-05-09T14:42:36.704864093Z\",\"level\":\"info\",\"source\":\"atc\",\"message\":\"atc.audit\",\"data\":{\"action\":\"GetPipeline\",\"parameters\":{\":pipeline_name\":[\"booklit\"],\":team_name\":[\"main\"]},\"user\":\"test\"}}\n","depth":5,"section_tag":"audit-logs"},"auth":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"authenticating-with-vault":{"location":"vault-credential-manager.html#authenticating-with-vault","title":"Authenticating with Vault","text":"There are many ways to authenticate with a Vault server. The web-node can be configured with either a token or an arbitrary auth backend and arbitrary auth params, so just about all of them should be configurable.\n\nWhen the web node acquires a token, either by logging in with an auth backend or by being given one directly, it will continuously renew the token to ensure it doesn't expire. The renewal interval is half of the token's lease duration.\n\n","depth":5,"section_tag":"authenticating-with-vault"},"aws-asm-credential-manager":{"location":"aws-asm-credential-manager.html","title":"The AWS Secrets Manager credential manager","text":"","depth":4,"section_tag":"aws-asm-credential-manager"},"aws-secretsmanager-access-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-access-key","title":"aws-secretsmanager-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-iam-permissions":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-iam-permissions","title":"IAM Permissions","text":"The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the Resource section can contain a wildcard to a secret or be restricted to an individual secret. In order for the health check to work properly (see Scaling), Concourse needs to have access to the __concourse-health-check secret.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Sid\": \"AllowAccessToSecretManagerParameters\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"secretsmanager:ListSecrets\"\n        ],\n          \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"AllowAccessGetSecret\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\"\n            ],\n            \"Resource\": [\n                \"arn:aws:secretsmanager:*:*:secret:/concourse/*\",\n                \"arn:aws:secretsmanager:*:*:secret:__concourse-health-check-??????\"\n            ]\n        }\n    ]\n}\nIf you wish to restrict concourse to only have access to secrets for a specific pipeline, you can replace \"arn:aws:secretsmanager:*:*:secret:/concourse/*\" in the example above with\n\n\"arn:aws:secretsmanager:*:*:secret:/concourse/TEAM_NAME/*\",\n\"arn:aws:secretsmanager:*:*:secret:/concourse/TEAM_NAME/PIPELINE_NAME/*\",\nwhere TEAM_NAME and PIPELINE_NAME are replaced with the team and name of the pipeline in question.\n\nFor more information on how to use IAM roles to restrict access to Secrets Manager, review the official documentation.\n\n","depth":5,"section_tag":"aws-secretsmanager-iam-permissions"},"aws-secretsmanager-pipeline-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-pipeline-secret-template","title":"aws-secretsmanager-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-region":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-region","title":"aws-secretsmanager-region","text":"The AWS region that requests to Secrets Manager will be sent to.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-scaling":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-scaling","title":"Scaling","text":"If your cluster has a large workload, in particular if there are many resources, Concourse can generate a lot of traffic to AWS and subsequently get rate-limited.\n\nAs long as Concourse has permission to get the value of the __concourse-health-check secret, you should be able to measure an error rate by polling the /api/v1/info/creds endpoint when authenticated as a Concourse Admin.\n\nDepending on your workflow for updating secrets and your reliability requirements it may be worth Caching credentials and/or Retrying failed fetches to mitigate rate-limit-related errors.\n\n","depth":5,"section_tag":"aws-secretsmanager-scaling"},"aws-secretsmanager-secret-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-secret-key","title":"aws-secretsmanager-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-session-token":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-session-token","title":"aws-secretsmanager-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-team-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-team-secret-template","title":"aws-secretsmanager-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-access-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-access-key","title":"aws-ssm-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SSM_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-credential-manager":{"location":"aws-ssm-credential-manager.html","title":"The AWS SSM credential manager","text":"","depth":4,"section_tag":"aws-ssm-credential-manager"},"aws-ssm-pipeline-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-pipeline-secret-template","title":"aws-ssm-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-region":{"location":"aws-ssm-credential-manager.html#aws-ssm-region","title":"aws-ssm-region","text":"The AWS region that requests to parameter store will be sent to.\n\nEnvironment variable CONCOURSE_AWS_SSM_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-secret-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-secret-key","title":"aws-ssm-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SSM_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-session-token":{"location":"aws-ssm-credential-manager.html#aws-ssm-session-token","title":"aws-ssm-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SSM_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-team-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-team-secret-template","title":"aws-ssm-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"badges":{"location":"observation.html#badges","title":"Badges","text":"The Concourse API supports returning SVG badges indicating the status of a job:\n\n/api/v1/teams/{team}/pipelines/{pipeline}/jobs/{job}/badge\n...and for an entire pipeline:\n\n/api/v1/teams/{team}/pipelines/{pipeline}/badge\nThis can be used to annotate your READMEs with a build status badge like so:\n\n![CI](https://ci.concourse-ci.org/api/v1/teams/main/pipelines/booklit/jobs/unit/badge)\n...which should render the following:\n\n{image: https://ci.concourse-ci.org/api/v1/teams/main/pipelines/booklit/jobs/unit/badge}\n\n","depth":3,"section_tag":"badges"},"basic-architecture":{"location":"internals.html#basic-architecture","title":"Basic architecture","text":"Concourse is a fairly simple distributed system built up from the following components. You'll see them referenced here and there throughout the documentation, so you may want to skim this page just to get an idea of what they are.\n\nimages/concourse_architecture.png","depth":3,"section_tag":"basic-architecture"},"basic-git-operations":{"location":"basic-git-operations.html","title":"Basic Git Operations","text":"All of these examples use the concourse/git-resource image. That image is probably the most popular git resource for Concourse since it is shipped in the concourse/concourse image and in the tarball on the Github release page. It is not the only resource available for working with git-related resources. If you don't see your use-case on this page then there is probably another resource that you can use. For example, Pull Request workflows can be accomplished with the teliaoss/github-pr-resource.\n\nCheck out the docs for the git resource for all configuration options.\n\n","depth":4,"section_tag":"basic-git-operations"},"basic-schemas":{"location":"config-basics.html#basic-schemas","title":"Basic Schemas","text":"Throughout the Concourse documentation you will come across schema definitions for each API.\n\nThe following are basic schema definitions that the other schemas refer to. You can probably skip past this and just make assumptions along the way; this is just here for completeness!\n\nAny integer, i.e. 1000.\n\nAn arbitrary string value with no content restrictions.\n\nAn arbitrary object representing configuration that is not directly interpreted by Concourse - typically given to a resource type.\n\nuri: https://github.com/vito/booklit\nbranch: master\nAll object keys must be strings, preferably snake_cased.\n\nAn arbitrary object representing key-value definitions for ((vars)).\n\nAs with config schema, all object keys must be strings, preferably snake_cased.\n\nAn object containing string keys and string values. Each pair represents an environment variable to set to the given value.\n\nSOME_SIMPLE_VAR: simple-var\nSOME_LONG_VAR: |\n  This is an example of using YAML multi-line string syntax to set a very\n  long environment variable value.\nSOME_NUMERIC_VALUE: \"1\"\nNote that in the last example we took special care to quote the number.\n\ntrue or false.\n\nYAML also supports the alias yes, no, on, or off, but...please don't.\n\nAn arbitrary YAML value. May be a number schema, string schema, boolean schema, config schema, or a [value schema].\n\nvalues:\n- 123\n- bar\n- true\n- key1: abc\n  key2: def\n- [hello, world]\nAn identifier is a string value. The following defines the allowed character set for an identifier:\n\n* Unicode lowercase letters, while still supporting languages that don't have any casing (e.g. Japanese).\n\n* Decimal numbers.\n\n* Hyphens - and underscores _ as word separators.\n\n* Periods . in order to support domain names and version numbers.\n\nThe validation rule is as follows:\n\n^[\\p{Ll}\\p{Lt}\\p{Lm}\\p{Lo}][\\p{Ll}\\p{Lt}\\p{Lm}\\p{Lo}\\d\\-_.]*$\nWhere all identifier must start with a Unicode lowercase letter, followed by any number of allowed characters.\n\nCurrently, the validation will only show as warnings. For the sake of future-proofing, you may want to conform to it.\n\nA string value specifying a (typically relative) path of a directory.\n\nA string value specifying a (typically relative) path of a file.\n\nA string value in Go time.ParseDuration format. 1h for one hour, 5m for 5 minutes.\n\nAn object with string keys and string values.\n\nThe following is an array of versions:\n\n- {\"ref\": \"33042e15e930b6786fc9b0a9ea5dec78689c5e4b\"}\n- ref: v1.2.0,\n  sha: 33042e15e930b6786fc9b0a9ea5dec78689c5e4b\n- foo: \"0\"\nNote that in the last example we took special care to quote the number.\n\nIn many scenarios where a version can be specified, e.g. get step version, only a subset of the full set of fields is necessary. The latest version matching the fields specified will be chosen.\n\n","depth":3,"section_tag":"basic-schemas"},"benefits-of-global-resources":{"location":"global-resources.html#benefits-of-global-resources","title":"Benefits of Global Resources","text":"","depth":4,"section_tag":"benefits-of-global-resources"},"bitbucket-cloud-auth":{"location":"bitbucket-cloud-auth.html","title":"BitBucket Cloud auth","text":"A Concourse server can authenticate against BitBucket Cloud to leverage its permission model.\n\n","depth":4,"section_tag":"bitbucket-cloud-auth"},"bitbucket-cloud-authentication":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authentication","title":"Authentication","text":"First, you'll need to create an OAuth consumer on Bitbucket Cloud.\n\nThe consumer will need the following permissions:\n\n* Account:\n\n  * Email\n\n  * Read\n\n* Team membership:\n\n  * Read\n\nThe \"Callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by BitBucket Cloud - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_ID=myclientid\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_SECRET=myclientsecret\n","depth":5,"section_tag":"bitbucket-cloud-authentication"},"bitbucket-cloud-authorization":{"location":"microsoft-auth.html#bitbucket-cloud-authorization","title":"Authorization","text":"Individual user auth is disabled due to a quirk with with Microsoft returning unique IDs but non-unique usernames\n\nGroups can be authorized for a team by passing the following flags to fly set-team:\n\n--microsoft-group=GROUP_NAME: Authorize an entire group's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --microsoft-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  microsoft:\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"bitbucket-cloud-authorization"},"build-log-retention":{"location":"concourse-web.html#build-log-retention","title":"Build log retention","text":"Build logs are stored in the DB - if they are not cleanup up every once in a while, the storage usage for build logs will continue to grow as more builds run. While this is usually fine for small Concourse instances, as you scale up, you may run into storage concerns.\n\nTo clean up old build logs, you can configure Concourse to periodically scan for builds whose logs should be reaped based on a log retention policy, skipping over any paused pipelines and jobs. When a build's logs are reaped, they are no longer visible in the UI.\n\nConcourse can be configured with a default build log retention policy for all jobs:\n\nCONCOURSE_DEFAULT_BUILD_LOGS_TO_RETAIN=50\nCONCOURSE_DEFAULT_DAYS_TO_RETAIN_BUILD_LOGS=14\nWith these settings, Concource will keep the latest 50 builds for each job. If a job runs more than 50 builds in 14 days, all of those builds will be retained until 14 days after they ran.\n\nSome jobs have differing retention requirements - you can configure build_log_retention_policy schema on a job-by-job basis.\n\nYou can also configure Concourse with maximum values for build log retention policies to prevent jobs from retaining their build logs for too long:\n\nCONCOURSE_MAX_BUILD_LOGS_TO_RETAIN=100\nCONCOURSE_MAX_DAYS_TO_RETAIN_BUILD_LOGS=30\nWith these settings, build_log_retention_policy.builds is capped at 100, and build_log_retention_policy.days is capped at 30.\n\n","depth":5,"section_tag":"build-log-retention"},"build-plans":{"location":"jobs.html#steps","title":"Steps","text":"Each job has a single build plan configured as job.plan. A build plan is a recipe for what to run when a build of the job is created.\n\nA build plan is a sequence of steps:\n\n* the task step runs a task\n\n* the get step fetches a resource\n\n* the put step updates a resource\n\n* the set_pipeline step configures a pipeline\n\n* the load_var step loads a value into a local var\n\n* the in_parallel step runs steps in parallel\n\n* the do step runs steps in sequence\n\n* the across step modifier runs a step multiple times; once for each combination of variable values\n\n* the try step attempts to run a step and succeeds even if the step fails\n\nWhen a new version is available for a get step with trigger: true configured, a new build of the job will be created from the build plan.\n\nWhen viewing the job in the pipeline, resources that are used as get steps appear as inputs, and resources that are used in put steps appear as outputs. Jobs are rendered downstream of any jobs they reference in passed constraints, connected by the resource.\n\nIf any step in the build plan fails, the build will fail and subsequent steps will not be executed. Additional steps may be configured to run after failure by configuring step.on_failure or step.ensure (or the job equivalents, job.on_failure and job.ensure).\n\n","depth":3,"section_tag":"steps"},"build-rerunning":{"location":"builds.html#build-rerunning","title":"Rerunning a Build","text":"Concourse supports build rerunning, which means to run a new build using the exact same set of input versions as the original build. There are two ways to rerun a build: through the web UI on the builds page and through the fly rerun-build.\n\nWhen a build is rerun, it will create a new build using the name of the original build with the rerun number appended to it, e.g. 3.1 for the first rerun of build 3.\n\nRerun builds are ordered chronologically after the original build, rather than becoming a new \"latest\" build. Similarly, when the scheduler is resolving passed constraints that reference a job with rerun builds, those rerun builds are processed in this same order. This ensures that the versions, which made it through a rerun build, do not become the new \"latest versions\". Instead, they act as if the original build had succeeded at its point in the build history.\n\nThis may sound a little confusing, but the summary is that reruns should behave as if they replace the original failed build.\n\n","depth":3,"section_tag":"build-rerunning"},"build-the-image":{"location":"building-an-image-and-using-it-in-a-task.html#build-the-image","title":"Build The Image","text":"To avoid repeating ourselves we're going to use the pipeline made in the other guide Building and Pushing an Image. We will start with the pipeline from the Defining the Build Context section.\n\nWe will add the UNPACK_ROOTFS parameter to the task. This paramter tells the oci-build-task to include the image in a special format that Concourse's container runtime uses. In the future this may not be necessary if Concourse starts using the OCI image format.\n\nresources:\n# The repo with our Dockerfile\n- name: concourse-examples\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/concourse/examples.git\n    branch: master\n\njobs:\n- name: build-and-run\n  plan:\n  - get: concourse-examples\n  - task: build-image\n    privileged: true # oci-build-task must run in a privileged container\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: concourse/oci-build-task\n      inputs:\n      - name: concourse-examples\n      outputs:\n      - name: image\n      params:\n        CONTEXT: concourse-examples/Dockerfiles/simple\n        UNPACK_ROOTFS: true #add this param\n      run: # binary used to build the image\n        path: build\n \n\nThe above pipeline will build a container image and also output it in Concourse's rootfs image format.\n\n","depth":5,"section_tag":"build-the-image"},"build-tracker":{"location":"build-tracker.html","title":"Build Tracker","text":"The build tracker is the component that runs the execution of a build. It picks up any started builds, which can be orphaned builds (builds that an ATC started but did not finish) or builds that have just been scheduled. There is one build tracker per ATC, which runs on an interval that is defaulted to 10 seconds.\n\n","depth":4,"section_tag":"build-tracker"},"building-an-image-and-using-it-in-a-task":{"location":"building-an-image-and-using-it-in-a-task.html","title":"Building an Image and Using it in a Task","text":"This guide will show you how to build and use an image within one job without pushing the image to an external image registry like Docker Hub.\n\n","depth":4,"section_tag":"building-an-image-and-using-it-in-a-task"},"building-and-pushing-an-image":{"location":"building-and-pushing-an-image.html","title":"Building and Pushing an Image","text":"In this guide we are going to show how to build and publish container images using the oci-build task and registry-image resource. This guide assumes you understand how to build container images with Dockerfile's and publish to Docker Hub or another image registry using the docker cli.\n\nThis is one way of building and pushing images. There are many other ways to accomplish this same task in Concourse.\n\nFirst we need a Dockerfile. You can store this in your own repo or reference the github.com/concourse/examples repo. The rest of this post assumes you use the examples repo. All files in this blog post can be found in the examples repo.\n\n","depth":4,"section_tag":"building-and-pushing-an-image"},"builds":{"location":"builds.html","title":"Builds","text":"A build is an execution of a build plan, which is either\n\n* configured as a sequence of steps in a job\n\n* generated by the Resource Checker to run a check: Check for new versions.\n\n* submitted directly to Concourse as a one-off build via Running tasks with fly execute\n\nContainers and volumes are created as get steps, put steps, and task steps run. When a build completes successfully, these containers go away.\n\nA failed build's containers and volumes are kept around so that you can debug the build via fly intercept. If the build belongs to a job, the containers will go away when the next build starts. If the build is a one-off, its containers will be removed immediately, so make sure you intercept while it's running, if you want to debug.\n\n","depth":2,"section_tag":"builds"},"bundled-resource-types":{"location":"concourse-worker.html#bundled-resource-types","title":"Bundled Resource Types","text":"Workers come prepackaged with a bundle of resource types. They are included in the tarball from the Github release page and are part of the concourse/concourse image.\n\nTo view the resource types available on a worker run:\n\nfly workers --details\nIf you want more details, like the version number of each resource, you can run:\n\nfly curl api/v1/workers\n","depth":6,"section_tag":"bundled-resource-types"},"ccxml":{"location":"observation.html#ccxml","title":"cc.xml","text":"The Concourse API can return the status of a team's pipelines in a format compatible with tools like CCMenu. This endpoint is available at the following route:\n\n/api/v1/teams/{team}/cc.xml\n","depth":3,"section_tag":"ccxml"},"cf-authentication":{"location":"cf-uaa-auth.html#cf-authentication","title":"Authentication","text":"You'll need to configure your UAA with a concourse client by setting the following under uaa.clients:\n\nconcourse:\n  id: myclientid\n  secret: myclientsecret\n  scope: openid,cloud_controller.read\n  authorized-grant-types: \"authorization_code,refresh_token\"\n  access-token-validity: 3600\n  refresh-token-validity: 3600\n  redirect-uri: https://concourse.example.com/sky/issuer/callback\nThe value for redirect-uri must be the external URL of your Concourse server with /sky/issuer/callback appended.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nNext, you'll need to take the same client ID and secret and configure it on the Running a web node by setting the following env:\n\nCONCOURSE_CF_API_URL=http://mycf.example.com\nCONCOURSE_CF_CLIENT_ID=myclientid\nCONCOURSE_CF_CLIENT_SECRET=myclientsecret\nNote: if you're integrating with Cloud Foundry, you're probably also deploying Concourse via BOSH - in which case you'll want to set the cf_auth.* properties in your manifest instead of setting the above env.\n\n","depth":5,"section_tag":"cf-authentication"},"cf-authorization":{"location":"cf-uaa-auth.html#cf-authorization","title":"Authorization","text":"CloudFoundry users and org/space members can be authorized for a team by passing the following flags to fly set-team:\n\n--cf-user=USERNAME: Authorize an individual user.\n\n\n--cf-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--cf-space=ORG_NAME:SPACE_NAME: Deprecated in favor of --cf-space-with-developer-role. Authorize the members with developer role of a space within an organization.\n\n\n--cf-space-with-any-role=ORG_NAME:SPACE_NAME: Authorize the members with any role of a space within an organization.\n\n\n--cf-space-with-developer-role=ORG_NAME:SPACE_NAME: Authorize the members with developer role of a space within an organization.\n\n\n--cf-space-with-auditor-role=ORG_NAME:SPACE_NAME: Authorize the members with auditor role of a space within an organization.\n\n\n--cf-space-with-manager-role=ORG_NAME:SPACE_NAME: Authorize the members with manager role of a space within an organization.\n\n\n--cf-space-guid=SPACE_GUID: Authorize the members with any role of a space within an organization by space GUID.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --cf-user my-username \\\n    --cf-org my-org \\\n    --cf-space my-other-org:my-space\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  cf:\n    users: [\"my-username\"]\n    orgs: [\"my-org\"]\n    spaces: [\"my-other-org:my-space\"]\n","depth":5,"section_tag":"cf-authorization"},"cf-uaa-auth":{"location":"cf-uaa-auth.html","title":"CF/UAA auth","text":"Cloud Foundry (CF) auth can be used for operators who wish to authenticate their users configured against their Cloud Foundry instance via the UAA auth component.\n\n","depth":4,"section_tag":"cf-uaa-auth"},"chaining-placement-strategies":{"location":"container-placement.html#chaining-placement-strategies","title":"Chaining Placement Strategies","text":"Container placement strategies can be chained together to apply multiple strategies in sequence. The first strategy in the chain receives the entire set of workers, filtering the set down in some way, and passing that new set of workers to the next strategy in the chain. If the last strategy in the chain returns multiple workers, one will be chosen at random.\n\nFor instance, consider the following configuration:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-containers,limit-active-volumes,volume-locality,fewest-build-containers\nCONCOURSE_MAX_ACTIVE_CONTAINERS_PER_WORKER=200\nCONCOURSE_MAX_ACTIVE_VOLUMES_PER_WORKER=100\nThis defines a chain of 4 placement strategies, plus the implicit random strategy. Let's look at what each strategy accomplishes:\n\n1. The limit-active-containers strategy removes all workers that already have more than 200 containers\n\n2. The limit-active-volumes strategy removes all remaining workers that already have more than 100 volumes\n\n3. The volume-locality strategy keeps only the remaining worker(s) that have the most inputs locally. This can keep more than one worker in the case of a tie\n\n4. The fewest-build-containers strategy will attempt to break ties by selecting the worker with fewer build containers. If all the remaining workers have the exact same number of containers, one will be selected at random.\n\n","depth":4,"section_tag":"chaining-placement-strategies"},"checker":{"location":"checker.html","title":"Resource Checker","text":"Resources represent external state such as a git repository, files in an S3 bucket, or anything else that changes over time. By modelling these as resources, it allows you to use this external state as inputs (or triggers) to your workloads.\n\n","depth":4,"section_tag":"checker"},"checks":{"location":"tutorial-resources.html#checks","title":"Checks","text":"Checks are how Concourse finds new versions of resources.  In order to find any new versions Concourse has to run the check scripts for all resources across all pipelines. By default, all resources have a check_every interval of one minute.\n\nThe default check_every interval helps keep Concourse feeling snappy and responsive to changes in external systems. The negative side-effect of this is that if you have a lot of resources all hitting the same system (e.g. an internal Git system) you may end up DDOS'ing your external system! In these cases you may want to increase the check_every interval of your resource or disable it and use webhooks instead.\n\nTo see the results of the last check that ran, go to the resource's page in the pipeline and expand the top line that starts with check: \u003cresource-name\u003e.\n\n{image: images/tutorial/view-check-status.png}\n\nIf your resource is having trouble finding new versions or is configured incorrectly the resource will print a message that will be visible on its resource page for you to debug.\n\nYou can also use fly to view the status by running fly builds and filtering for check's to see the result of the last check.\n\n$ fly -t tutorial builds | grep check\nid   name                    status     start\n106  hello-world/repo/check  succeeded  2021-04-10@12:00:20-0400\n","depth":4,"section_tag":"checks"},"cluster-wide-credential-manager":{"location":"vars.html#cluster-wide-credential-manager","title":"The cluster-wide credential manager","text":"Concourse can be configured with a single cluster-wide credential manager, which acts as a source for any vars which do not specify a source name.\n\nSee Credential Management for more information.\n\nIn the future we would like to introduce support for multiple cluster-wide var sources, configured using the var_source schema schema, and begin deprecating the The cluster-wide credential manager.\n\n","depth":4,"section_tag":"cluster-wide-credential-manager"},"common-pipeline-practices":{"location":"common-pipeline-practices.html","title":"Common Pipeline Practices","text":"The following are practices that we see a lot of people use in their pipelines. These are by no means \"Best\" practices, they are simply common and may or may not work for you and your team.\n\n","depth":4,"section_tag":"common-pipeline-practices"},"complications-with-reusing-containers":{"location":"global-resources.html#complications-with-reusing-containers","title":"Complications with reusing containers","text":"There is an exception to sharing check containers within a deployment, which is workers belonging to a team and workers with tags.\n\nIf a resource has resource.tags configured, and the resource's check interval ends up acquiring the checking lock, a new container will be created on a worker matching the appropriate tags, even if a check container already exists for the same resource config elsewhere.\n\nSimilarly, if a team has its own workers, and their check interval ended up acquiring the lock, a new container will be created on the team's workers, rather than re-using a container from the shared worker pool.\n\nThis is a bit complicated to reason about and we plan to stop re-using check containers to simplify all of this. See 3079 for more information.\n\n","depth":6,"section_tag":"complications-with-reusing-containers"},"component-atc":{"location":"internals.html#component-atc","title":"ATC: web UI \u0026 build scheduler","text":"The ATC is the heart of Concourse. It runs the web UI and API and is responsible for all pipeline scheduling. It connects to PostgreSQL, which it uses to store pipeline data (including build logs).\n\nMultiple ATCs can be running as one cluster; as long as they're all pointing to the same database, they'll synchronize using basic locking mechanisms and roughly spread work across the cluster.\n\nThe ATC by default listens on port 8080, and is usually colocated with the TSA and sitting behind a load balancer.\n\nNote: for fly intercept to function, make sure your load balancer is configured to do TCP or SSL forwarding, not HTTP or HTTPS.\n\nThere are multiple components within the ATC that each have their own set of responsibilities. The main components consist of the checker, scheduler, build tracker and the garbage collector.\n\nThe checker's responsibility is to continously checks for new versions of resources. The scheduler is responsible for scheduling builds for a job and the build tracker is responsible for running any scheduled builds. The garbage collector is the cleanup mechanism for removing any unused or outdated objects, such as containers and volumes.\n\nAll the components in a Concourse deployment can be viewed in the components table in the database as of v5.7.0. The intervals that the components run at can also be adjusted through editing that table, as well as pausing the component from running entirely.\n\n","depth":3,"section_tag":"component-atc"},"component-tsa":{"location":"internals.html#component-tsa","title":"TSA: worker registration \u0026 forwarding","text":"The TSA is a custom-built SSH server that is used solely for securely registering workers with the ATC.\n\nThe TSA by default listens on port 2222, and is usually colocated with the ATC and sitting behind a load balancer.\n\nThe TSA implements CLI over the SSH connection, supporting the following commands:\n\n* The forward-worker command is used to reverse-tunnel a worker's addresses through the TSA and register the forwarded connections with the ATC. This allows workers running in arbitrary networks to register securely, so long as they can reach the TSA. This is much safer than opening the worker up to the outside world.\n\n* The land-worker command is sent from the worker when landing, and initiates the state change to LANDING through the ATC.\n\n* The retire-worker command is sent from the worker when retiring, and initiates the state change to RETIRING through the ATC.\n\n* The delete-worker command is sent from the worker when draining is interrupted while a worker is retiring. It removes the worker from the ATC.\n\n* The sweep-containers command is sent periodically to facilitate garbage collection of containers which can be removed from the worker. It returns a list of handles for containers in the DESTROYING state, and it is the worker's job to subsequently destroy them.\n\n* The report-containers command is sent along with the list of all container handles on the worker. The ATC uses this to update the database, removing any DESTROYING containers which are no longer in the set of handles, and marking any CREATED containers that are not present as missing.\n\n* The sweep-volumes command is sent periodically to facilitate garbage collection of volumes which can be removed from the worker. It returns a list of handles for volumes in the DESTROYING state, and it is the worker's job to subsequently destroy them.\n\n* The report-volumes command is sent along with the list of all volume handles on the worker. The ATC uses this to update the database, removing any DESTROYING volumes which are no longer in the set of handles, and marking any CREATED volumes that are not present as missing.\n\n","depth":3,"section_tag":"component-tsa"},"concourse-admin":{"location":"user-roles.html#concourse-admin","title":"Concourse Admin","text":"Admin is a special user attribute granted only to owners of the The main team.\n\nAdmins have the ability to administrate teams using fly set-team, fly destroy-team, fly rename-team, etc.\n\nAdmins always have permission to perform any action on any team. You cannot assign actions to the admin role using the --config-rbac flag.\n\nThe following actions are also assigned to admins, and cannot be reconfigured:\n\n- GetLogLevel\n- ListActiveUsersSince\n- SetLogLevel\n- GetInfoCreds\n- SetWall\n- ClearWall\n","depth":4,"section_tag":"concourse-admin"},"concourse-cli":{"location":"concourse-cli.html","title":"The concourse CLI","text":"The concourse CLI can be downloaded from the latest GitHub release - make sure to grab the appropriate archive for your platform. Each concourse-* archive contains the following files:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # Linux only\nconcourse/fly-assets/...\nconcourse/resource-types/... # Linux only\nThe Linux release is the largest among all the platforms because it is prepackaged with a bundle of resource types like the git, time, and registry-image resources. Resources only run on Linux workers, that's why the other platforms are not bundled with resources; resources don't currently exist for non-linux platforms.\n\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere. On Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguring concourseAll Concourse web and worker node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nconcourse web --help\nconcourse worker --help\nconcourse quickstart --help\nconcourse migrate --help\nconcourse generate-key --help\nconcourse land-worker --help\nconcourse retire-worker --help\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name uppercased, preceded with CONCOURSE_ and dashes (-) replaced with underscores (_). These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent and interchangeable. Env vars are simply easier to reference in isolation and are more useful to copy-paste.\n\n","depth":3,"section_tag":"concourse-cli"},"concourse-generate-key":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"","depth":3,"section_tag":"concourse-generate-key"},"concourse-web":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"concourse-worker":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"config-basics":{"location":"config-basics.html","title":"Config Basics","text":"Concourse configuration for things like Pipelines and Tasks is done through declarative YAML files.\n\nConcourse configuration supports basic variable substitution by way of ((vars)). There is no built-in support for fancier templating constructs, e.g. loops and conditionals; users are free to use whatever templating system they like.\n\n","depth":2,"section_tag":"config-basics"},"configuration":{"location":"php-example.html#configuration","title":"Pipeline Configuration","text":"---\nresources:\n- name: larvel-websockets-git\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/beyondcode/laravel-websockets.git\n\njobs:\n- name: test\n  public: true\n  plan:\n  - get: larvel-websockets-git\n    trigger: true\n  - task: run-tests\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: { repository: composer }\n      inputs:\n        - name: larvel-websockets-git\n      run:\n        path: /bin/sh\n        args:\n          - -c\n          - |\n            cd larvel-websockets-git\n\n            composer install\n            vendor/bin/phpunit --coverage-text --coverage-clover=coverage.clover\n\n\n","depth":3,"section_tag":"configuration"},"configure-opa":{"location":"opa.html#configure-opa","title":"Configuring Concourse","text":"There are four configuration options you need to set on the concourse web nodes to have them interact with OPA.\n\n* CONCOURSE_OPA_URL: The OPA policy check endpoint. Should point to a specific package/rule that contains all Concourse rules for your cluster.\n\n  Example: http://opa-endpoint.com/v1/data/concourse/decision\n\n* CONCOURSE_POLICY_CHECK_FILTER_HTTP_METHODS: API http methods to go through policy check. You will need to make sure these match up with an API action in the next two configuration options.\n\n  Example: PUT,POST\n\n* CONCOURSE_POLICY_CHECK_FILTER_ACTION: Actions in this list will go through policy check.\n\n  Example: ListWorkers,ListContainers\n\n* CONCOURSE_POLICY_CHECK_FILTER_ACTION_SKIP: Actions in this list will not go through policy check\n\n  Example: PausePipeline,UnpausePipeline\n\nFor the last three configuration options you can refer to the this list of routes for a list of API actions and their respective HTTP method. There are also some Special Actions not directly in the API.\n\n","depth":4,"section_tag":"configure-opa"},"configuring-auth":{"location":"configuring-auth.html","title":"Configuring Auth","text":"The very first thing to configure with Concourse is how users will log in, and what those users should be able to do.\n\nThis is configured in two separate tiers:\n\n* Authentication, how users identify themselves, is configured on the Running a web node.\n\n* Authorization, how user access is determined, is configured on each team.\n\nConcourse currently supports the following auth methods:\n\nAny number of providers can be enabled at any one time. Users will be given a choice when logging in as to which one they would like to use.\n\nConcourse uses a fork of Dex for its authentication. You can find additional documentation on the supported auth providers in the Dex connectors documentation.\n\nAdding a new auth provider to Concourse is as simple as submitting a pull request to our fork of Dex and then adding a bit of configuration to the skymarshal component.\n\n","depth":3,"section_tag":"configuring-auth"},"configuring-kubernetes-rbac":{"location":"kubernetes-credential-manager.html#configuring-kubernetes-rbac","title":"Configuring Kubernetes RBAC","text":"As the Web nodes need to retrieve secrets from namespaces that are not their own, they needs extra permissions to do so.\n\nIf you have RBAC enabled, that means creating the necessary Kubernetes objects to identify the Web nodes and give them access to a predefined list of namespaces where the secrets live.\n\nRegardless of how the Kubernetes RBAC-related objects are created, the basic requirement is that web must be able to read secrets in the  namespaces where each teams' secrets live.\n\nFor instance, if you have the following teams which you want to read secrets from:\n\n* team-a\n\n* team-b\n\nAssuming the following Running a web node configuration:\n\nCONCOURSE_KUBERNETES_NAMESPACE_PREFIX=myprefix-\nweb must be able to get secrets from the following namespaces:\n\n* myprefix-team-a\n\n* myprefix-team-b\n\nTo allow web to interpolate credentials for \"team-a\" and \"team-b\", we'd then need to create a few Kubernetes RBAC objects.\n\nStarting with identifying the web service as an actor, we can use a ServiceAccount for that:\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: web\n  labels:\n    app: web\nTo allow actors to do something, in this case, retrieve secrets from a given namespace, a ClusterRole is then needed.\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: read-secrets\n  labels:\n    app: web\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\"]\nAs that role is useless if not bound to an actor, the next step is creating the the object that represents binding the role to the web ServiceAccount that we had created before.\n\nThis is accomplished through the RoleBinding object, which is per-namespace (thus, per-team).\n\nEven though in this example we're binding to a ClusterRole (which is not tied to any namespace), the use of such cluster role is (see metadata.namespace), making the effective  permissions restricted to the namespace applied in the RoleBinding.\n\n---\n# Role binding for the first team (`team-b`), allowing `web`\n# to consume secrets from it.\n#\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: web-team-a\n  namespace: myprefix-team-a\n  labels:\n    app: web\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: read-secrets\nsubjects:\n- kind: ServiceAccount\n  name: web\n  namespace: concourse\n\n---\n# Role binding for the second team (`team-b`), allowing `web`\n# to consume secrets from it.\n#\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: web-team-b\n  namespace: myprefix-team-b\n  labels:\n    app: web\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: read-secrets\nsubjects:\n- kind: ServiceAccount\n  name: web\n  namespace: concourse\nTo finish the example, we need to associate the web Pod with the service, granting the pod access to those namespaces through the roles that have been bound to it.\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      serviceAccountName: web\n      containers:\n        - name: web\n          image: \"concourse/concourse:5.3.0\"\n          args: [ web ]\n          env:\n            - name: CONCOURSE_KUBERNETES_NAMESPACE_PREFIX\n              value: \"myprefix-\"\n          # ...\n","depth":5,"section_tag":"configuring-kubernetes-rbac"},"configuring-ldap-group-search":{"location":"ldap-auth.html#configuring-ldap-group-search","title":"Configuring LDAP group search","text":"The LDAP provider can also be configured with group search configuration, so that users can be configured for team authorization by their 'group' in LDAP.\n\nFor example, to find groups and identify them by their ou attribute, you would configure:\n\nCONCOURSE_LDAP_GROUP_SEARCH_BASE_DN='cn=groups,dc=example,dc=com'\nCONCOURSE_LDAP_GROUP_SEARCH_NAME_ATTR=ou\nThe attributes correlating a user to a group must be specified like so:\n\nCONCOURSE_LDAP_GROUP_SEARCH_USER_ATTR=uid\nCONCOURSE_LDAP_GROUP_SEARCH_GROUP_ATTR=members\nThis specifies that the uid attribute of the user must be present in the members attribute of the group.\n\nAn additional filter may be specified, just like with users:\n\nCONCOURSE_LDAP_GROUP_SEARCH_FILTER='(objectClass=posixGroup)'\n","depth":6,"section_tag":"configuring-ldap-group-search"},"configuring-main-team-authorization":{"location":"generic-saml-auth.html#configuring-main-team-authorization","title":"Configuring main Team Authorization","text":"SAML users and groups can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_SAML_USER=my-user\nCONCOURSE_MAIN_TEAM_SAML_GROUP=my-group\nMultiple users and groups may be specified by comma-separating them.\n\n","depth":6,"section_tag":"configuring-main-team-authorization"},"configuring-metrics":{"location":"metrics.html#configuring-metrics","title":"Configuring Metrics","text":"The Running a web node can be configured to emit metrics on start.\n\nCurrently supported metrics emitters are InfluxDB, NewRelic, Prometheus, and Datadog. There is also a dummy emitter that will just spit the metrics out in to the logs at DEBUG level, which can be enabled with the --emit-to-logs flag.\n\nRegardless of your metrics emitter, you can set CONCOURSE_METRICS_BUFFER_SIZE to determine how many metrics emissions are sent at a time. Increasing this number can be helpful if sending metrics is regularly failing (due to rate limiting or network failures) or if latency is particularly high.\n\nThere are various flags for different emitters; run concourse web --help and look for \"Metric Emitter\" to see what's available.\n\n","depth":4,"section_tag":"configuring-metrics"},"configuring-rbac":{"location":"user-roles.html#configuring-rbac","title":"Configuring RBAC","text":"Configuring RBAC is experimental, and this may change in the future.\n\nIt is possible to promote or demote the roles to which actions are assigned by passing the --config-rbac to the concourse web command with a path to a .yml file, like the following:\n\nconcourse web --config-rbac=/path/to/rbac/config.yml\nThis file should be a YAML map where the keys are role names (owner, member, pipeline-operator, and viewer are valid). For each role, the value should be a list of actions. On startup, Concourse will assign each role to its associated list of actions.\n\nFor example, in the default configuration only pipeline-operators and above can abort builds. To restrict aborting builds to only members and above, you could pass this as a --config-rbac file:\n\nmember:\n- AbortBuild\nOn the other hand, only members and above can order pipelines by default. To extend this privilege down to pipeline-operators, you can use a --config-rbac file like the following:\n\npipeline-operator:\n- OrderPipelines\nYou do not need to specify a role for every possible action; if an action does not appear in the file, then the default role (as described in the sections above) will be assigned to that action. Also, please avoid specifying the same action under multiple roles in this file - it can have unpredictable results.\n\n","depth":4,"section_tag":"configuring-rbac"},"configuring-runtimes":{"location":"concourse-worker.html#configuring-runtimes","title":"Configuring Runtimes","text":"The worker can be run with multiple container runtimes - containerd, Guardian, and Houdini (an experimental and the only runtime for Darwin and Windows). Only containerd and Guardian are meant for production use. Guardian is the default runtime for Concourse.\n\nNote about architecture: The web node (ATC) talks to all 3 runtimes via a single interface called the Garden server. While Guardian comes packaged with a Garden server and its flags in Concourse are unfortunately prefixed with --garden-*, Guardian (a runtime) and Garden (an interface and server) are two separate tools. An analogy for Garden would be the Container Runtime Interface (CRI) used in Kubernetes. Kubernetes uses containerd via CRI. Concourse uses containerd via Garden.\n\n","depth":5,"section_tag":"configuring-runtimes"},"configuring-the-secrets-engine":{"location":"vault-credential-manager.html#configuring-the-secrets-engine","title":"Configuring the secrets engine","text":"Concourse is currently limited to looking under a single path, meaning enabling only one secrets engine is supported: kv, or kv_v2. This may change in the future - we're still collecting ideas in RFC #21.\n\nUsing kv version 2 enables versioned secrets and the ability to restore previous versions or deleted secrets. Concourse will read the latest version of a secret at all times and if it is deleted it will appear as if the secret does not exist. More information regarding the Vault KV backend and the differences in versions can be found here.\n\nSo, let's configure the kv secrets engine and mount it at /concourse:\n\n$ vault secrets enable -version=1 -path=concourse kv\nTo enable kv_v2 and versioned secrets: $ vault secrets enable -version=2 -path=concourse kv\n\n\nNext, you'll want to create a policy to allow Concourse to read from this path.\n\npath \"concourse/*\" {\n  policy = \"read\"\n}\nSave this to concourse-policy.hcl, and then run:\n\nvault policy write concourse ./concourse-policy.hcl\nThis configuration will allow Concourse to read all credentials under /concourse. This should match your configured path prefix.\n\n","depth":5,"section_tag":"configuring-the-secrets-engine"},"configuring-tracing":{"location":"tracing.html#configuring-tracing","title":"Configuring Tracing","text":"To export spans to Jaeger, specify the Thrift HTTP endpoint of the Jaeger collector:\n\nCONCOURSE_TRACING_JAEGER_ENDPOINT=http://jaeger:14268/api/traces\nTo export spans to Google Cloud Trace, specify the GCP Project ID:\n\nCONCOURSE_TRACING_STACKDRIVER_PROJECTID=your-gcp-project-id\nNote that suitable GCP credentials must be available, via the usual GOOGLE_APPLICATION_CREDENTIALS environment variable, the default location that the gcloud CLI expects, or from GCP's metadata server (if Concourse is deployed on GCP).\n\nTo export spans to Lightstep via the OTLP Exporter, specify your collector access token and endpoint:\n\nCONCOURSE_TRACING_OTLP_ADDRESS=ingest.lightstep.com:443\nCONCOURSE_TRACING_OTLP_HEADERS=lightstep-access-token:mysupersecrettoken\nTo export spans to Honeycomb.io, specify the API key, dataset and optionally the service name:\n\nCONCOURSE_TRACING_HONEYCOMB_API_KEY=your-honeycomb-api-key\nCONCOURSE_TRACING_HONEYCOMB_DATASET=your-honeycomb-dataset\nCONCOURSE_TRACING_HONEYCOMB_SERVICE_NAME=service-name-for-concourse  # NOTE: Optional. Defaults to \"concourse\"\n","depth":4,"section_tag":"configuring-tracing"},"conjur-account":{"location":"conjur-credential-manager.html#conjur-account","title":"conjur-account","text":"The Conjur account.\n\nEnvironment variable CONCOURSE_CONJUR_ACCOUNT.\n\n","depth":5,"section_tag":"configuration"},"conjur-appliance-url":{"location":"conjur-credential-manager.html#conjur-appliance-url","title":"conjur-appliance-url","text":"URL of the Conjur instance.\n\nEnvironment variable CONCOURSE_CONJUR_APPLIANCE_URL.\n\n","depth":5,"section_tag":"configuration"},"conjur-authn-api-key":{"location":"conjur-credential-manager.html#conjur-authn-api-key","title":"conjur-authn-api-key","text":"The api key that corresponds to the Conjur host username.\n\nEnvironment variable CONCOURSE_CONJUR_AUTHN_API_KEY.\n\n","depth":5,"section_tag":"configuration"},"conjur-authn-login":{"location":"conjur-credential-manager.html#conjur-authn-login","title":"conjur-authn-login","text":"A valid Conjur host username.\n\nEnvironment variable CONCOURSE_CONJUR_AUTHN_LOGIN.\n\n","depth":5,"section_tag":"configuration"},"conjur-authn-token-file":{"location":"conjur-credential-manager.html#conjur-authn-token-file","title":"conjur-authn-token-file","text":"Token file used if Conjur instance is running in k8s or iam. \n\nEnvironment variable CONCOURSE_CONJUR_AUTHN_TOKEN_FILE.\n\n","depth":5,"section_tag":"configuration"},"conjur-cert-file":{"location":"conjur-credential-manager.html#conjur-cert-file","title":"conjur-cert-file","text":"Cert file used if conjur instance is using a self signed cert.\n\nEnvironment variable CONCOURSE_CONJUR_CERT_FILE.\n\n","depth":5,"section_tag":"configuration"},"conjur-credential-manager":{"location":"conjur-credential-manager.html","title":"The Conjur credential manager","text":"","depth":4,"section_tag":"conjur-credential-manager"},"conjur-permissions":{"location":"conjur-credential-manager.html#conjur-permissions","title":"Conjur Permissions","text":"The following is an example Conjur policy that can be used to grant permissions to a Conjur host. In this example host/concourse  will have permissions to read and update all of the secrets within  the TEAM_NAME and PIPELINE_NAME policies.\n\n- !host concourse\n- !policy\n  id: concourse\n  owner: !host concourse\n  body:\n  - !policy \n    id: TEAM_NAME\n    body:\n    - !variable team-secret-variable\n    - !policy\n      id: PIPELINE_NAME\n      body:\n      - !variable pipeline-secret-variable\nNote that the TEAM_NAME and PIPELINE_NAME text above should be replaced to fit your Concourse setup.\n\nFor more information on how to create and load Conjur policies, review the official documentation.\n\n","depth":5,"section_tag":"conjur-permissions"},"conjur-pipeline-secret-template":{"location":"conjur-credential-manager.html#conjur-pipeline-secret-template","title":"conjur-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_CONJUR_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"conjur-secret-template":{"location":"conjur-credential-manager.html#conjur-secret-template","title":"conjur-secret-template","text":"The base path used when attempting to locate a vault or safe level secret.\n\nEnvironment variable CONCOURSE_CONJUR_SECRET_TEMPLATE.\n\nExample:\n\nDefault: vaultName/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"conjur-team-secret-template":{"location":"conjur-credential-manager.html#conjur-team-secret-template","title":"conjur-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_CONJUR_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"container-image-guides":{"location":"container-image-guides.html","title":"Container Image Guides","text":"","depth":3,"section_tag":"container-image-guides"},"container-placement":{"location":"container-placement.html","title":"Container Placement","text":"Each step in a build is executed inside a container. The Running a web node distributes containers across the worker cluster depending on the configured strategy. If no workers satisfies the configured strategy, the step will block until a worker becomes available.\n\n","depth":3,"section_tag":"container-placement"},"containerd-runtime":{"location":"concourse-worker.html#containerd-runtime","title":"containerd runtime","text":"To use the containerd runtime manually set the --runtime (CONCOURSE_RUNTIME) to containerd on the concourse worker command.\n\nThe following is a list of the containerd runtime specific flags for Concourse that can be set. They are all optional and have default values.\n\nContainerd Configuration:\n  --containerd-config=                               Path to a config file to use for the Containerd daemon. [$CONCOURSE_CONTAINERD_CONFIG]\n  --containerd-bin=                                  Path to a containerd executable (non-absolute names get resolved from $PATH). [$CONCOURSE_CONTAINERD_BIN]\n  --containerd-init-bin=                             Path to an init executable (non-absolute names get resolved from $PATH). (default: /usr/local/concourse/bin/init) [$CONCOURSE_CONTAINERD_INIT_BIN]\n  --containerd-cni-plugins-dir=                      Path to CNI network plugins. (default: /usr/local/concourse/bin) [$CONCOURSE_CONTAINERD_CNI_PLUGINS_DIR]\n  --containerd-request-timeout=                      How long to wait for requests to Containerd to complete. 0 means no timeout. (default: 5m) [$CONCOURSE_CONTAINERD_REQUEST_TIMEOUT]\n  --containerd-max-containers=                       Max container capacity. 0 means no limit. (default: 250) [$CONCOURSE_CONTAINERD_MAX_CONTAINERS]\n\nContainerd Container Networking:\n  --containerd-external-ip=                          IP address to use to reach container's mapped ports. Autodetected if not specified. [$CONCOURSE_CONTAINERD_EXTERNAL_IP]\n  --containerd-dns-server=                           DNS server IP address to use instead of automatically determined servers. Can be specified multiple times. [$CONCOURSE_CONTAINERD_DNS_SERVER]\n  --containerd-restricted-network=                   Network ranges to which traffic from containers will be restricted. Can be specified multiple times. [$CONCOURSE_CONTAINERD_RESTRICTED_NETWORK]\n  --containerd-network-pool=                         Network range to use for dynamically allocated container subnets. (default: 10.80.0.0/16) [$CONCOURSE_CONTAINERD_NETWORK_POOL]\n  --containerd-mtu=                                  MTU size for container network interfaces. Defaults to the MTU of the interface used for outbound access by the host. [$CONCOURSE_CONTAINERD_MTU]\n  --containerd-allow-host-access                     Allow containers to reach the host's network. This is turned off by default. [$CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS]\n\nDNS Proxy Configuration:\n  --containerd-dns-proxy-enable                      Enable proxy DNS server. Note: this will enable containers to access the host network. [$CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE]\nMake sure to read A note on allowing host access and DNS proxy to understand the implications of using --containerd-allow-host-access and --containerd-dns-proxy-enable\n\n","depth":6,"section_tag":"containerd-runtime"},"create-the-job":{"location":"building-and-pushing-an-image.html#create-the-job","title":"Create the Job","text":"Next we will create a job that will build and push our container image.\n\nTo build the job we will need to pull in the repo where the Dockerfile is.\n\nresources: ... # omitting resource section from above\n\njobs:\n- name: build-and-push\n  plan:\n  - get: concourse-examples\n","depth":5,"section_tag":"create-the-job"},"creating-a-pipeline":{"location":"tutorial-hello-world.html#creating-a-pipeline","title":"Creating a Pipeline","text":"Let's start where all tutorials start, with a Hello World! example!\n\nIn this section you're going to create a pipeline that simply prints Hello world! to the console. While building up the pipeline we will pause to explain the core pieces of the pipeline.\n\nLet's first answer: what is a pipeline made up from?\n\nThe simplest Concourse pipeline is made of two objects:\n\n* An unordered list of Jobs which contains...\n\n* An ordered list of Steps\n\n If you've used other pipeline building tools in the past, then what you think of as a pipeline is probably most similar to a job in Concourse.\n\nFor our Hello World! pipeline we will need one job with one step. This is the smallest pipeline you can make in Concourse. The single step is what will print Hello World! to the console.\n\nCreate and open a new file called hello-world.yml. Inside that file let's add the first top-level key, jobs.\n\njobs:\nThe jobs key is where we define the list of jobs that should make up our pipeline. The order of the jobs does not matter. The order of jobs does not define the structure of the pipeline. We'll get into pipeline structure and job ordering later when we talk about Resources and passed constraints.\n\n","depth":4,"section_tag":"creating-a-pipeline"},"creating-commits-and-tags":{"location":"basic-git-operations.html#creating-commits-and-tags","title":"Creating Commits and Tags","text":"Here's a simple way to create a commit using a bash script.\n\nresources:\n- name: repo-main\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/user/my-repo\n    branch: main\n\njobs:\n- name: create-a-commit\n  plan:\n  - get: repo-main\n  - task: commit-and-tag\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: gitea/gitea # use any image that has the git cli\n      inputs:\n      - name: repo-main\n      outputs:\n      # to pass the commit to the following steps specify\n      # the \"repo-main\" as an output as well\n      - name: repo-main\n      run:\n        path: sh\n        args:\n        - -cx\n        # this is just a bash script\n        - |\n          cd repo-main\n          # edit a file / make a change\n          date +%Y-%m-%d \u003e todays-date\n          git add ./todays-date\n          git commit -m \"Add todays date\"\n          git tag v0.1.6\n  # push commit and tag\n  - put: repo-main\n    params:\n      # specify the \"repo-main\" artifact as the location\n      repository: repo-main\n","depth":5,"section_tag":"creating-commits-and-tags"},"credential-lookup-rules":{"location":"conjur-credential-manager.html#credential-lookup-rules","title":"Credential Lookup Rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* concourse/TEAM_NAME/foo_param\n\n* vaultName/foo_param\n\nThe leading concourse can be changed by specifying --conjur-pipeline-secret-template or --conjur-team-secret-template variables.\n\nThe leading vaultName can be changed by specifying --conjur-secret-template variable.\n\n","depth":5,"section_tag":"credential-lookup-rules"},"credhub-credential-manager":{"location":"credhub-credential-manager.html","title":"The CredHub credential manager","text":"","depth":4,"section_tag":"credhub-credential-manager"},"creds":{"location":"creds.html","title":"Credential Management","text":"Going beyond Encryption, explicit credential management will provide credentials to your builds for a brief amount of time, without being persisted anywhere. It also allows for credentials to be rotated and managed external to the pipeline or team, and prevents them from being revealed by fly get-pipeline.\n\nCredential management works by replacing the credentials with ((vars)) in your pipeline or task config. When the Concourse is about to run the step or check that is configured with vars, it will resolve them by fetching the values from the credential manager. If the values are not present, the action will error.\n\nThe following configurations can be parameterized with a credential manager:\n\n* resource.source under pipeline.resources\n\n* resource_type.source under pipeline.resource_types\n\n* resource.webhook_token under pipeline.resources\n\n* task step params on a task step in a pipeline\n\n* Tasks in their entirety - whether from task step file or task step config in a pipeline, or a config executed with Running tasks with fly execute\n\nWhere these values are looked up and how the credential manager is configured depends on the backend. Consult the relevant section below for whichever backend you want to use.\n\n","depth":3,"section_tag":"creds"},"creds-caching":{"location":"creds-caching.html","title":"Caching credentials","text":"By default, credentials are fetched each time they're used. When many pipelines are configured this can result in a ton of requests to the credential server.\n\nTo reduce load on your credential server you may want to enable caching by setting the following env on the Running a web node:\n\nCONCOURSE_SECRET_CACHE_ENABLED=true\nEnabling secret caching will cache secrets from both credential managers and from var sources.\n\nBy default, credentials will be cached for one minute at a time. This value can be increased to further reduce load on the server like so:\n\nCONCOURSE_SECRET_CACHE_DURATION=5m # increase from 1m default\nCredential cache duration can also determined by the credential manager itself - for example, if Vault returns a lease duration for a credential, the shorter value between the configured cache duration and the credential's lease duration will be used.\n\nBy default, the absence of a credential is also cached for 10 seconds so that Concourse doesn't keep looking for a misconfigured credential. This duration can be configured like so:\n\nCONCOURSE_SECRET_CACHE_DURATION_NOTFOUND=1s # decrease from 10s default\n","depth":4,"section_tag":"creds-caching"},"creds-redacting":{"location":"creds-redacting.html","title":"Redacting credentials","text":"Concourse can be configured to automatically redact credentials from build output like so:\n\nCONCOURSE_ENABLE_REDACT_SECRETS=true\nThis behavior is off by default as there is likely a CPU performance overhead on the Running a web nodes involved with enabling it. It will be on by default once we've confirmed that it performs well enough at large scale.\n\nWhen enabled, Concourse will keep track of the credential values which were used in a build. When writing build logs to the database, it will replace any occurrence of these values with the text ((redacted)).\n\nSay you're running a task which runs the following script:\n\nset -e -u -x\n\necho $SECRET \u003e some-file\nsha1sum some-file\n(Note the set -x - the root cause of many accidental credential leaks.)\n\nLet's say you have a job which runs this task, providing the $SECRET parameter using a credential manager ((var)):\n\nplan:\n- task: use-secret\n  file: # ...\n  params: {SECRET: ((some-var))}\nWith hello in some-var, this will result in the following build output:\n\n+ echo ((redacted))\n+ sha1sum some-file\nf572d396fae9206628714fb2ce00f72e94f2258f  some-file\nGoing a step further, what happens when that var has multiple lines of output, like \"hello\\ngoodbye\"?\n\n+ echo ((redacted)) ((redacted))\n+ sha1sum some-file\n638e5ebcd06a5208906960aa5fbe1d4ebd022771  some-file\nWhat happened here? Well, because we didn't quote the $SECRET var arg to echo, it squashed the lines together into arguments. This could have confused our redacting logic and resulted in leaking the credential, but because Concourse redacts secret values line-by-line, we're still OK. This will also help with JSON marshalled credential values, which get interspersed with \\n in a string literal.\n\nAlthough Concourse tries to be thorough in its redacting of credentials, the best way to prevent credential leakage is to not accidentally print them in the first place. Think of this as an airbag, not a seatbelt!\n\n","depth":4,"section_tag":"creds-redacting"},"creds-retry-logic":{"location":"creds-retry-logic.html","title":"Retrying failed fetches","text":"When a request to the credential manager fails due to an intermittent error (e.g. a timeout or connection refused), Concourse will automatically try the request again up to 5 times before giving up. After all attempts fail, the error will be surfaced in the UI for the resource check or build step that initiated the request.\n\nThe retry logic can be configured by specifying the following env on the Running a web node:\n\nCONCOURSE_SECRET_RETRY_ATTEMPTS=5   # how many times to try\nCONCOURSE_SECRET_RETRY_INTERVAL=10s # how long to wait between attempts\n","depth":4,"section_tag":"creds-retry-logic"},"current-caveats-with-rerunning":{"location":"builds.html#current-caveats-with-rerunning","title":"Current caveats with rerunning","text":"The current implementation of rerunning is an early iteration with one key limitation: a rerun build will use the current state of the job config, instead of running the exact build plan the original build ran with.\n\nThis means that if the job.plan has changed in a way that is backwards-incompatible, the rerun build may error. For example, if a new input is added, its version will not be available as the original build did not use it.\n\nThere are future plans to have reruns execute the exact build plan from the original build. If you are interested in tracking the progress for the second pass at rerunning builds - or contributing yourself! - the project epic is called Build Lifecycle View.\n\n","depth":4,"section_tag":"current-caveats-with-rerunning"},"darwin-and-windows-workers":{"location":"upgrading-concourse.html#darwin-and-windows-workers","title":"Darwin and Windows Workers","text":"There are no additional steps for upgrading Darwin and Windows workers.\n\n","depth":5,"section_tag":"darwin-and-windows-workers"},"dashboard":{"location":"observation.html#dashboard","title":"The Dashboard","text":"The dashboard, available at the default route (/), provides a bird's-eye view of the Concourse cluster. All visible pipelines across all teams are listed here. A high-density (HD) view is available at /hd.\n\n","depth":3,"section_tag":"dashboard"},"db-prerequisites":{"location":"postgresql-node.html#db-prerequisites","title":"Prerequisites","text":"PostgreSQL 9.5 or above is required, though the latest available version is recommended.\n\n","depth":4,"section_tag":"db-prerequisites"},"db-resource-utilization":{"location":"postgresql-node.html#db-resource-utilization","title":"Resource utilization","text":"CPU usage: this is one of the most volatile metrics, and one we try pretty hard to keep down. There will be near-constant database queries running, and while we try to keep them very simple, there is always more work to do. Expect to feed your database with at least a couple cores, ideally four to eight. Monitor this closely as the size of your deployment and the amount of traffic it's handling increases, and scale accordingly.\n\nMemory usage: similar to CPU usage, but not quite as volatile.\n\nDisk usage: pipeline configurations and various bookkeeping metadata for keeping track of jobs, builds, resources, containers, and volumes. In addition, all build logs are stored in the database. This is the primary source of disk usage. To mitigate this, users can configure job.build_logs_to_retain on a job, but currently there is no operator control for this setting. As a result, disk usage on the database can grow arbitrarily large.\n\nBandwidth usage: well, it's a database, so it most definitely uses the network. Something important to consider here is the number of simultaneous connections that the database server itself will allow. Postgres exposes a max_connections configuration variable, and depending on how many web nodes you are running and the size of their connection pool, you may need to tune these two numbers against each other.\n\nHighly available: Up to you. Clustered PostgreSQL is kind of new and probably tricky to deploy, but there are various cloud solutions for this.\n\nOutbound traffic: None\n\nInbound traffic: Only ever from the web node\n\n","depth":4,"section_tag":"db-resource-utilization"},"db-running":{"location":"postgresql-node.html#db-running","title":"Running PostgreSQL","text":"How this node is managed is up to you; Concourse doesn't actually have much of an opinion on it, it just needs a database. By default Concourse will try connecting to a database named atc.\n\nHow to install PostgreSQL is really dependent on your platform. Please refer to your Linux distribution or operating system's documentation.\n\nFor the most part, the instruction on Linux should look something like this:\n\nsudo apt install postgresql\nsudo su postgres -c \"createuser $(whoami)\"\nsudo su postgres -c \"createdb --owner=$(whoami) atc\"\nThis will install PostgreSQL (assuming your distro uses apt), create a user, and create a database that the current UNIX user can access, assuming this same user is going to be running the Running a web node. This is a reasonable default for distros like Ubuntu and Debian which default PostgreSQL to peer auth.\n\n","depth":4,"section_tag":"db-running"},"defining-pipeline-resources":{"location":"building-and-pushing-an-image.html#defining-pipeline-resources","title":"Defining Pipeline Resources","text":"Now we can start building out our pipeline. Let's declare our Resources first. We will need one resource to pull in the repo where our Dockerfile is located, and a second resource pointing to where we want to push the built container image to.\n\nThere are some Variables in this file that we will fill out when setting the pipeline.\n\nresources:\n# The repo with our Dockerfile\n- name: concourse-examples\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/concourse/examples.git\n    branch: master\n\n# Where we will push the image to\n- name: simple-image\n  type: registry-image\n  icon: docker\n  source:\n    repository: ((image-repo-name))/simple-image\n    username: ((registry-username))\n    password: ((registry-password))\n","depth":5,"section_tag":"defining-pipeline-resources"},"defining-the-build-context":{"location":"building-and-pushing-an-image.html#defining-the-build-context","title":"Defining the Build Context","text":"Next we need to tell the oci-build-task what the build context of our Dockerfile is. The README goes over a few other methods of creating your build context. We are going to use the simplest use-case. By specifying CONTEXT the oci-build-task assumes a Dockerfile and its build context are in the same directory.\n\nresources: ... # omitting resource section from above\n\njobs:\n- name: build-and-push\n  plan:\n  - get: concourse-examples\n  - task: build-image\n    privileged: true # oci-build-task must run in a privileged container\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: concourse/oci-build-task\n      inputs:\n      - name: concourse-examples\n      outputs:\n      - name: image\n      params:\n        CONTEXT: concourse-examples/Dockerfiles/simple\n      run: # binary used to build the image\n        path: build\n","depth":5,"section_tag":"defining-the-build-context"},"disable-version":{"location":"resource-versions.html#disable-version","title":"Disabling a Version","text":"A resource version can also be disabled through the web UI on the resource version history page. These disabled versions will not be used to schedule any further builds for any jobs that use the resource as an input.\n\nDisabled versions can also be re-enabled through the resource version history page.\n\nDisabling a version is useful for cases where you know that the version is broken or incompatible.\n\n","depth":4,"section_tag":"disable-version"},"disabling-encryption":{"location":"encryption.html#disabling-encryption","title":"Disabling Encryption","text":"To opt out of encryption entirely (I'm sure you have your reasons), simply pass --old-encryption-key (or old_encryption_key) alone. With no new encryption key, the Running a web node will decrypt all existing data on start.\n\n","depth":4,"section_tag":"disabling-encryption"},"do-step":{"location":"jobs.html#do-step","title":"do step","text":"","depth":3,"section_tag":"steps"},"docker-compose-concourse":{"location":"quick-start.html#docker-compose-concourse","title":"Docker Compose Concourse","text":"Concourse is distributed as a single concourse binary, making it easy to run just about anywhere, especially with Docker.\n\nIf you'd like to get Concourse running somewhere quickly so you can start to kick the tires, the easiest way is to use our docker-compose.yml:\n\n$ curl -O https://concourse-ci.org/docker-compose.yml\n$ docker-compose up -d\nCreating docs_concourse-db_1 ...\nCreating docs_concourse-db_1 ... done\nCreating docs_concourse_1 ...\nCreating docs_concourse_1 ... done\nConcourse will be running at localhost:8080 on your machine. You can log in with the username/password as test/test.\n\n","depth":4,"section_tag":"docker-compose-concourse"},"docs":{"location":"docs.html","title":"Docs","text":"Concourse is a pipeline-based continuous thing-doer.\n\nThe word \"pipeline\" is all the rage in CI these days, so being more specific about this term is kind of important; Concourse's pipelines are significantly different from the rest.\n\nPipelines are built around Resources, which represent all external state, and Jobs, which interact with them. Concourse pipelines represent a dependency flow, kind of like distributed Makefiles. Pipelines are designed to be self-contained so as to minimize server-wide configuration. Maximizing portability also mitigates risk, making it easier for projects to recover from CI disasters.\n\nResources like the git resource and s3 resource are used to express source code, dependencies, deployments, and any other external state. This interface is also used to model more abstract things like scheduled or interval triggers, via the time resource.\n\nResource Types are defined as part of the pipeline itself, making the pipelines more self-contained and keeping Concourse itself small and generic without resorting to a complicated plugin system.\n\nJobs are sequences of get, put, and task steps to execute. These steps determine the job's inputs and outputs. Jobs are designed to be idempotent and loosely coupled, allowing the pipeline to grow with the project's needs without requiring engineers to keep too much in their head at a time.\n\nEverything in Concourse runs in a container. Instead of modifying workers to install build tools, Tasks describe their own container image (typically using Docker images via the registry-image resource).\n\n...What?Concourse admittedly has a steeper learning curve at first, and depending on your background it might be a lot to take in. A core goal of this project is for the curve to flatten out shortly after and result in higher productivity and less stress over time.\n\nIf this all sounds like gobbeldigook, that's OK - you may want to just continue on, start kicking the tires a bit, and use the above as a quick reference of the \"big picture\" as the mental model sets in.\n\n","depth":1,"section_tag":"docs"},"downgrading":{"location":"concourse-web.html#downgrading","title":"Downgrading","text":"If you're stuck in a pinch and need to downgrade from one version of Concourse to another, you can use the concourse migrate command.\n\nFirst, grab the desired migration version by running the following:\n\n# make sure this is the *old* Concourse binary\n$ concourse migrate --supported-db-version\n1551110547\nThat number (yours will be different) is the expected migration version for that version of Concourse.\n\nNext, run the following with the new Concourse binary:\n\n$ concourse migrate --migrate-db-to-version=1551110547\nThis will need the same CONCOURSE_POSTGRES_* configuration described in Running concourse web.\n\nOnce this completes, switch all web nodes back to the older concourse binary and you should be good to go.\n\n","depth":5,"section_tag":"downgrading"},"dummy-var-source":{"location":"vars.html#dummy-var-source","title":"dummy var source","text":"","depth":4,"section_tag":"var-sources"},"dynamic-vars":{"location":"vars.html#dynamic-vars","title":"Dynamic vars","text":"Concourse can read values from \"var sources\" - typically credential managers like Vault - at runtime. This keeps them out of your configuration and prevents sensitive values from being stored in your database. Values will be read from the var source and optionally cached to reduce load on the var source.\n\nThe following attributes can be parameterized through a var source:\n\n* resource.source under pipeline.resources\n\n* resource_type.source under pipeline.resources\n\n* resource.webhook_token under pipeline.resources\n\n* task step params on a task step in a pipeline\n\n* Tasks in their entirety - whether from task step file or task step config in a pipeline, or a config executed with Running tasks with fly execute\n\nConcourse will fetch values for vars as late as possible - i.e. when a step using them is about to execute. This allows the credentials to have limited lifetime and rapid rotation policies.\n\n","depth":3,"section_tag":"dynamic-vars"},"ecosystem":{"location":"ecosystem.html","title":"Ecosystem","text":"Concourse is used by a wide variety of businesses, governments, open source projects and non-profit organisations. The uses of Concourse are as diverse as its user base, and include CI/CD for apps, continuous delivery of infrastructure, release integration, automation of tests, and many more!\n\nIf you use Concourse, or your business offers Concourse-related services, we'd love to hear from you. Please raise a pull request adding your organisation's name in lexicographical order to the list below, and help us let the world know how many people do things continuously with Concourse.\n\n","depth":1,"section_tag":"ecosystem"},"enabling-encryption":{"location":"encryption.html#enabling-encryption","title":"Enabling Encryption","text":"To enable encryption, you'll just need to come up with a 16 or 32-byte random character sequence and configure it as --encryption-key flag to the web command. For BOSH, this is the encryption_key property.\n\nOn startup, the Running a web node will encrypt all existing plaintext data, and any new data being written will be encrypted before it's sent over the network to the database.\n\nThe initial bulk encryption shouldn't take too long, but it will scale linearly with the amount of data that you have, and if another ATC is running it'll suddenly not be able to read the data until it's also given the key. So, expect some downtime.\n\n","depth":4,"section_tag":"enabling-encryption"},"encryption":{"location":"encryption.html","title":"Encryption","text":"Automating everything means authorizing something to automate many things. This makes CI systems a high-risk target for security leaks.\n\nConcourse pipelines are loaded with credentials: resources are configured with private keys, tasks are given credentials to servers they integrate via credential manager variables, task step vars, or task step params, etc. If someone gets their hands on your config, they have access to everything.\n\nTo mitigate this, Concourse supports encrypting sensitive information before it reaches the database. This way the plaintext credentials only exist in memory for as long as they need to, and if someone gains access to your database, they can't so easily gain the keys to the kingdom.\n\nWe strongly encourage anyone running Concourse to configure encryption. Going further, it's best to have Concourse not store the credentials in the first place, in which case you may want to configure credential management as well.\n\n","depth":3,"section_tag":"encryption"},"examples":{"location":"examples.html","title":"Examples","text":"Configuring self-contained Concourse pipelines is a great way to try things out before diving into the deeper content.\n\nEach example contains a pipeline YAML snippet which can be copy-pasted to a local file and configured on your instance via fly set-pipeline. From there you may want to poke around and try changing parts of the configuration to learn how things work. All the available knobs to turn are covered in the Docs.\n\nFor a real-world example, check out Concourse's own pipeline (and its config):\n\n","depth":1,"section_tag":"examples"},"exploring-task-input-and-output-scenarios":{"location":"exploring-task-input-and-output-scenarios.html","title":"Exploring Task Input and Output Scenarios","text":"Understanding how task inputs and outputs work in Concourse can be a little confusing initially. This guide will walk you through a few example pipelines to show you how inputs and outputs work within a single Concourse job. By the end you should understand how inputs and outputs work within the context of a single job.\n\nTo run the pipelines in the following examples yourself you can get your own Concourse running locally by following the Quick Start guide. Then use fly set-pipeline to see the pipelines in action.\n\n","depth":4,"section_tag":"exploring-task-input-and-output-scenarios"},"exposing":{"location":"exposing.html","title":"Pipeline \u0026 Build Visibility","text":"Every newly configured pipeline is hidden to anyone but the pipeline's team. To make a pipeline publicly viewable, both by other teams and unauthenticated users, see fly expose-pipeline.\n\nEven with a pipeline exposed, all build logs are hidden by default. This is because CI jobs are prone to leaking credentials and other...unsavory information. After you've determined that a job's builds should be safe for public consumption, you can set public: true on the job in your pipeline.\n\n","depth":3,"section_tag":"exposing"},"fetching-a-repository":{"location":"basic-git-operations.html#fetching-a-repository","title":"Fetching a Repository","text":"Here is how you fetch the contents of a git repository and use it in a task.\n\nresources:\n- name: concourse-examples\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/concourse/examples\n\njobs:\n- name: read-the-readme\n  plan:\n  - get: concourse-examples\n  - task: cat-readme\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      inputs: # pass concourse-examples into this task step\n      - name: concourse-examples\n      run:\n        path: cat\n        args: [\"concourse-examples/README.md\"]\n","depth":5,"section_tag":"fetching-a-repository"},"fewer-resource-checks-to-perform":{"location":"global-resources.html#fewer-resource-checks-to-perform","title":"Fewer resource checks to perform","text":"With global resources, all resources that have the same configuration will share the same version history and share only one checking interval. This reduces load on the worker and on the external services that the resources point to.\n\nFor example, prior to global resources if there were three resources with the same configuration between three team's pipelines it would result in three check containers performing three resource checks every minute to fetch the versions.\n\nWith global resources, this configuration will result in only one check container and one resource check every minute to fetch versions for all the resources.\n\nSince there will be only one resource check for all resources that have the same configuration, the resource that has the shortest resource.check_every configured will result in its pipeline running the checks for that resource configuration.\n\n","depth":5,"section_tag":"fewer-resource-checks-to-perform"},"fewest-build-containers-strategy":{"location":"container-placement.html#fewest-build-containers-strategy","title":"The fewest-build-containers strategy","text":"When using the fewest-build-containers strategy, step containers are placed on the worker that has the fewest build containers (i.e. containers for other steps of other builds).\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=fewest-build-containers\n","depth":4,"section_tag":"fewest-build-containers-strategy"},"fill-in-the-task-config":{"location":"tutorial-hello-world.html#fill-in-the-task-config","title":"Fill in the Task Config","text":"Let's answer the previous three questions for our hello-world-task:\n\n* What type of worker to run the task on (linux/windows/darwin)\n\n  Linux, because our docker-composed Concourse only has one linux worker. You can verify this by running fly -t tutorial workers\n\n* What container image to use (Linux only)\n\n  We'll use the super small busybox image\n\n* What command to run inside the container\n\n  echo \"Hello world!\"\n\nYou can view the task documentation to see all configurable options for tasks. For now, you can add the following task config to the step.\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      # Tells Concourse which type of worker this task should run on\n      platform: linux\n      # This is one way of telling Concourse which container image to use for a\n      # task. We'll explain this more when talking about resources\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox # images are pulled from docker hub by default\n      # The command Concourse will run inside the container\n      # echo \"Hello world!\"\n      run:\n        path: echo\n        args: [\"Hello world!\"]\n","depth":4,"section_tag":"fill-in-the-task-config"},"fly":{"location":"fly.html","title":"The fly CLI","text":"The first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-abort-build":{"location":"builds.html#fly-abort-build","title":"fly abort-build","text":"To abort a build of a job, run:\n\n$ fly -t example abort-build --job my-pipeline/my-job --build 3\nThis will cancel build 3 of the my-job job in the my-pipeline pipeline.\n\n","depth":3,"section_tag":"fly-abort-build"},"fly-active-users":{"location":"managing-teams.html#fly-active-users","title":"fly active-users","text":"To list all users that have logged into your instance in the last two months, run:\n\n$ fly -t example active-users\nThe output will include the username, connector (which method they used to authenticate) and the date of their last login.\n\nYou can list users whose last login was within a different range by using:\n\n$ fly -t example active-users --since yyyy-MM-dd\nThis can be helpful to get a sense of how active your cluster is. \n\n","depth":4,"section_tag":"fly-active-users"},"fly-archive-pipeline":{"location":"managing-pipelines.html#fly-archive-pipeline","title":"fly archive-pipeline","text":"A pipeline can be archived via fly. This means that the pipeline will be paused and hidden from the web UI. The pipeline config will be deleted (so any secrets or interpolated Vars will be removed) while the build logs will be retained.\n\n$ fly -t example archive-pipeline -p pipeline-1\nTo unarchive a pipeline, simply set the pipeline again with the same name using fly set-pipeline. If a job in the new pipeline has the same name as a job in the archived pipeline, the old build logs for that job will be restored.\n\nNote that because the config is deleted, fly get-pipeline will no longer work for archived pipelines.\n\n","depth":4,"section_tag":"fly-archive-pipeline"},"fly-builds":{"location":"builds.html#fly-builds","title":"fly builds","text":"To list the most recent builds, run:\n\n$ fly -t example builds\nTo list the builds of a job, run:\n\n$ fly -t example builds -j pipeline-name/job-name\nThis can be useful for periodically monitoring the state of a job. The output also works well with tools like awk and grep.\n\nBy default the most recent 50 builds are shown. To see more builds, use the -c flag, like so:\n\n$ fly -t example builds -c 100\n","depth":3,"section_tag":"fly-builds"},"fly-check-resource":{"location":"managing-resources.html#fly-check-resource","title":"fly check-resource","text":"To force immediate checking for new versions of a resource, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource\nTo check from a particular version, including the given version, append the --from flag like so:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource \\\n    --from ref:abcdef\nThis can be useful for collecting versions that are older than the current ones, given that a newly configured resource will only start from the latest version.\n\nNote the ref: prefix is resource-dependent. For example, the bosh-io-release resource might use version:11.2 in place of ref:abcdef.\n\n","depth":4,"section_tag":"fly-check-resource"},"fly-check-resource-type":{"location":"managing-resource-types.html#fly-check-resource-type","title":"fly check-resource-type","text":"To force immediate checking for new versions of a resource type, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource-type --resource-type my-pipeline/my-resource-type\nThis can be useful for forcing an update if you're iterating on your own resource type implementation.\n\n","depth":4,"section_tag":"fly-check-resource-type"},"fly-clear-resource-cache":{"location":"managing-resources.html#fly-clear-resource-cache","title":"fly clear-resource-cache","text":"If you've got a resource cache that you need to clear out for whatever reason, this can be done like so:\n\n$ fly -t example clear-resource-cache -r my-pipeline/my-resource\nThis will immediately invalidate all the caches related to that resource - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.\n\nYou can also clear out a particular version for the given resource cache, using -v:\n\n$ fly -t example clear-resource-cache \\\n    -r my-pipeline/my-resource \\\n    -v ref:abcdef\nIf -v is not specified, all caches for the given resource will be cleared.\n\n","depth":4,"section_tag":"fly-clear-resource-cache"},"fly-clear-task-cache":{"location":"jobs.html#fly-clear-task-cache","title":"fly clear-task-cache","text":"If you've got a task cache that you need to clear out for whatever reason, this can be done like so:\n\n$ fly -t example clear-task-cache --job my-pipeline/my-job --step my-step-name\nThis will immediately invalidate the caches - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.\n\nYou can also clear out a particular path for the given step's cache, using --cache-path:\n\n$ fly -t example clear-task-cache \\\n    --job my-pipeline/my-job \\\n    --step my-step-name \\\n    --cache-path go/pkg\nIf --cache-path is not specified, all caches for the given step will be cleared.\n\n","depth":4,"section_tag":"fly-clear-task-cache"},"fly-cli":{"location":"fly.html","title":"The fly CLI","text":"The first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-completion":{"location":"fly.html#fly-completion","title":"fly completion","text":"Fly can output autocomplete configuration for some shells. For example, you can add an entry to your .bashrc like this:\n\nsource \u003c(fly completion --shell bash)\nor, using the /etc/bash_completion.d directory:\n\n$ fly completion --shell bash \u003e /etc/bash_completion.d/fly\nNote that, unlike other fly commands, this command does not interact with a remote server so you do not need to provide the -t or --target flag.\n\n","depth":3,"section_tag":"fly-completion"},"fly-containers":{"location":"administration.html#fly-containers","title":"fly containers","text":"To list the active containers across all your workers, run:\n\n$ fly -t example containers\nThis can be useful when discovering the containers available for fly intercepting.\n\n","depth":4,"section_tag":"fly-containers"},"fly-curl":{"location":"administration.html#fly-curl","title":"fly curl","text":"To execute an arbirary API request, you can run something like the following:\n\n$ fly -t example curl /api/v1/info\nThis command is just a shim that runs curl under the hood. To pass flags to curl, pass a -- argument after the path so that fly can distinguish them from its own flags:\n\n$ fly -t example curl /api/v1/builds -- \\\n    -X PUT \\\n    -H \"Content-type: application/json\" \\\n    -d @plan.json\nNote: if you use this command the assumption is that you know what you're doing. If you find yourself using this command often, let us know - perhaps there's a missing command!\n\n","depth":4,"section_tag":"fly-curl"},"fly-delete-target":{"location":"fly.html#fly-delete-target","title":"fly delete-target","text":"When logging out just isn't enough, a target can be completely removed from ~/.flyrc by running:\n\n$ fly -t example delete-target\nTo delete all targets, run:\n\n$ fly delete-target -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-delete-target"},"fly-destroy-pipeline":{"location":"managing-pipelines.html#fly-destroy-pipeline","title":"fly destroy-pipeline","text":"Every now and then you just don't want a pipeline to be around anymore. Running fly destroy-pipeline will stop the pipeline activity and remove all data collected by the pipeline, including build history and collected versions.\n\nFor example, to destroy the my-pipeline pipeline, you would run:\n\n$ fly -t example destroy-pipeline --pipeline my-pipeline\n","depth":4,"section_tag":"fly-destroy-pipeline"},"fly-destroy-team":{"location":"managing-teams.html#fly-destroy-team","title":"fly destroy-team","text":"To remove a team, including all of its pipelines and one-off builds, first log in as the The main team, and then run:\n\n$ fly -t example destroy-team --team-name my-team\nCurrently, if there were any workers assigned specifically to this team, they'll be orphaned, without having their containers or volumes cleaned up.\n\n","depth":4,"section_tag":"fly-destroy-team"},"fly-disable-resource-version":{"location":"managing-resources.html#fly-disable-resource-version","title":"fly disable-resource-version","text":"To disable a specific version of a resource, run:\n\n$ fly -t example disable-resource-version --resource my-pipeline/my-resource \\\n    --version ref:bceaf\nNote that the version needs to be provided as a key-value pair. For the git resource the ref: prefix is used while the registry  resource might use digest as a prefix like digest:sha256:94be7d7b.\n\nThis command is idempotent. Disabling an already disabled resource version will do nothing.\n\nYou can also disable a resource version via the UI by clicking on the check mark button next to the desired version on the resource page.\n\n","depth":4,"section_tag":"fly-disable-resource-version"},"fly-edit-target":{"location":"fly.html#fly-edit-target","title":"fly edit-target","text":"To modify a target's name, team, or URL, run:\n\n$ fly -t example edit-target \\\n    --target-name new-name \\\n    --concourse-url https://ci.example.com \\\n    --team-name my-team\nEach flag is optional - only the specified flags will be changed.\n\n","depth":3,"section_tag":"fly-edit-target"},"fly-enable-resource-version":{"location":"managing-resources.html#fly-enable-resource-version","title":"fly enable-resource-version","text":"To enable a specific version of a resource, run:\n\n$ fly -t example enable-resource-version --resource my-pipeline/my-resource \\\n    --version ref:bceaf\nNote that the version needs to be provided as a key-value pair. For the git resource the ref: prefix is used while the registry  resource might use digest as a prefix like digest:sha256:94be7d7b.\n\nThis command is idempotent. Enabling an already enabled resource version will do nothing.\n\nYou can also enable a resource version via the UI by clicking on the check mark button next to the desired version on the resource page.\n\n","depth":4,"section_tag":"fly-enable-resource-version"},"fly-execute":{"location":"tasks.html#running-tasks","title":"Running tasks with fly execute","text":"One of the most common use cases of fly is taking a local project on your computer and setting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.\n\nYou can execute a task like this:\n\n$ fly -t example execute --config tests.yml\nYour files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify -i name=. instead, where name is the input name from the task configuration.\n\nFly will automatically capture SIGINT and SIGTERM and abort the build when received. This allows it to be transparently composed with other toolchains.\n\nBy default, Running tasks with fly execute will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the --include-ignored flag in order to send ignored files to Concourse along with those that are not ignored.\n\nIf your task needs to run as root, then you can specify the -p or --privileged flag.\n\nTasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current working directory) that has the same name as the directory.\n\nTasks must specify the inputs that they require as task.inputs. For fly to upload these inputs you can use the -i or --input arguments with name and path pairs. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\nThis would work together with a build-stemcell.yml if its inputs: section was as follows:\n\ninputs:\n- name: code\n- name: stemcells\nIf you specify an input, then the default input will no longer be added automatically, and you will need to explicitly list it (as with the code input above).\n\nThis feature can be used to mimic other resources and try out input combinations that would normally not be possible in a pipeline.\n\nIf the --inputs-from flag is given, the specified job will be looked up in the pipeline, and the one-off build will base its inputs on those currently configured for the job.\n\nIf any --input flags are given (see above), they will override the base set of inputs.\n\nFor example:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\nThis will trigger a one-off-build using the task.yml task config, basing its inputs on the latest candidates for the integration job in the main pipeline, with the foo input overridden to specify local code to run.\n\nThis can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local development so that you don't have to upload every single resource from your local machine.\n\nWhen using --inputs-from as above, you can additionally specify which input to use as the task's image by passing --image input-name.\n\nFor example, the following pipeline fetches an image via a get step and uses it for task step image:\n\nresources:\n- name: my-repo\n  type: git\n  source: {uri: https://example.com}\n\n- name: some-image\n  type: registry-image\n  source: {repository: ubuntu}\n\njobs:\n- name: integration\n  plan:\n  - get: my-repo\n  - get: some-image\n  - task: my-task\n    file: my-repo/task.yml\n    image: some-image\n...so to run the same task with the same image in a one-off build, you would run:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --image some-image\nIf a task specifies outputs, then you're able to extract these back out of the build and back to your local system. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\nThis would work together with a build-stemcell.yml, if its outputs: section was as follows:\n\noutputs:\n- name: stemcell\nThis feature is useful to farm work out to your Concourse server to build things in a repeatable manner.\n\nAny params listed in the task configuration can be specified by using environment variables.\n\nSo, if you have a task with the following params:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\n...and you run:\n\nBAR=hello fly execute\nThe task would then run with BAR as \"hello\", and FOO as \"fizzbuzz\" (its default value).\n\nTask config files can contain Vars which can can be set during fly execute by using the -v, -y and -l flags:\n\nfly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\nAny variables not satisfied via the above flags will be deferred to the configured credential manager.\n\nTo satisfy these vars when running the task in a pipeline, see task step vars.\n\nIf you want to execute a task on a worker that has a specific tag, you can do so by passing --tag:\n\nfly -t example execute --config task.yml --tag bar\nThis will execute the task specified by task.yml on a worker that has been tagged bar.\n\n","depth":3,"section_tag":"running-tasks"},"fly-expose-pipeline":{"location":"managing-pipelines.html#fly-expose-pipeline","title":"fly expose-pipeline","text":"By default, newly configured pipelines are only visible to the pipeline's team. To make a pipeline viewable by other teams and unauthenticated users, run:\n\n$ fly -t example expose-pipeline --pipeline my-pipeline\nThis feature is useful if you're using Concourse for an open source project and you'd like your community to be able to see into your build pipeline.\n\nTo undo this change, see fly hide-pipeline.\n\nExposing a pipeline reveals basically everything except for build output and resource metadata.\n\nTo expose a resource's metadata, resource.public must be set to true.\n\nTo expose a job's build output, job.public must be set to true. This will also reveal resource metadata for any get step or put steps in the build output.\n\n","depth":4,"section_tag":"fly-expose-pipeline"},"fly-format-pipeline":{"location":"setting-pipelines.html#fly-format-pipeline","title":"fly format-pipeline","text":"To format a pipeline config in a \"canonical\" form (i.e. keys are in normal order, with name first for example), run:\n\n$ fly format-pipeline --config pipeline.yml\nThis will print the formatted pipeline config to stdout. To update the file in-place, pass --write/-w.\n\n","depth":4,"section_tag":"fly-format-pipeline"},"fly-get-pipeline":{"location":"managing-pipelines.html#fly-get-pipeline","title":"fly get-pipeline","text":"Fly can be used to fetch and update the configuration for your pipelines. This is achieved by using the fly get-pipeline and fly set-pipeline commands. For example, to fetch the current configuration of your my-pipeline Concourse pipeline and print it on STDOUT run the following:\n\n$ fly -t example get-pipeline --pipeline my-pipeline\nTo get JSON instead of YAML you can use the -j or --json argument. This can be useful when inspecting your config with jq.\n\n","depth":4,"section_tag":"fly-get-pipeline"},"fly-get-team":{"location":"managing-teams.html#fly-get-team","title":"fly get-team","text":"To show a team's configuration, run: $ fly -t example get-team -n some-team\n\n\n","depth":4,"section_tag":"fly-get-team"},"fly-hide-pipeline":{"location":"managing-pipelines.html#fly-hide-pipeline","title":"fly hide-pipeline","text":"If you realize that you've made a terrible mistake in exposing your pipeline, you can run:\n\n$ fly -t example hide-pipeline --pipeline my-pipeline\nIf you're panicking you can run the command's short form, hp, instead.\n\n","depth":4,"section_tag":"fly-hide-pipeline"},"fly-intercept":{"location":"builds.html#fly-intercept","title":"fly intercept","text":"Sometimes it's helpful to connect to the machine where tasks run. This way you can either profile or inspect tasks, or see the state of the machine at the end of a run. Due to Concourse running tasks in containers on remote machines this would typically be hard to access.\n\nTo this end, there is a fly intercept command that will give you an interactive shell inside the specified container. Containers are identified by a few things, so you may need to specify a few flags to hone down the results. If there are multiple containers that the flags could refer to, an interactive prompt will show up allowing you to disambiguate.\n\nFor example, running the following will run a task and then enter the finished task's container:\n\n$ fly -t example execute\n$ fly -t example intercept --step build\nWhen intercepting a task running on a Windows worker, you will need to specifically tell fly to to run powershell:\n\n$ fly -t example intercept powershell\nContainers are around for a short time after a build finishes in order to allow people to intercept them.\n\nYou can also intercept builds that were run in your pipeline. By using --job, --build, and --step you can intercept a specific step from a build of a job in your pipeline. These flags also have short forms, like so:\n\n$ fly -t example intercept -j some-pipeline/some-job -b some-build -s some-step\nNote that --build can be omitted, and will default to the most recent build of the job. One-off builds can be reached by passing in their build ID to --build which can be found on the build list page.\n\nThe --step flag can also be omitted; this will let you pick the step interactively, if you don't know the exact name.\n\nResource checking containers can also be intercepted with --check or -c:\n\n$ fly -t example intercept --check some-pipeline/some-resource\nA specific command can also be given, e.g. fly intercept ps auxf or fly intercept htop. This allows for patterns such as watch fly intercept ps auxf, which will continuously show the process tree of the current build's task, even as the \"current build\" changes.\n\nThe working directory and any relevant environment variables (e.g. those having come from task step params) used by the original process will also be used for the process run by intercept.\n\n","depth":3,"section_tag":"fly-intercept"},"fly-jobs":{"location":"jobs.html#fly-jobs","title":"fly jobs","text":"To list the jobs configured in a pipeline, run:\n\n$ fly -t example jobs -p my-pipeline\n","depth":4,"section_tag":"fly-jobs"},"fly-land-worker":{"location":"administration.html#fly-land-worker","title":"fly land-worker","text":"To initiate landing of a worker and eventually (after draining) cause it to exit, run:\n\n$ fly -t example land-worker --worker worker-name\n","depth":4,"section_tag":"fly-land-worker"},"fly-login":{"location":"fly.html#fly-login","title":"fly login","text":"The first thing you'll want to do is authenticate with your target. This is done with the fly login command. This is also useful to save targets under a more convenient alias, so you don't have to type out the URL all the time:\n\nThe login command serves double duty: it authenticates with a given endpoint, and saves it under a more convenient name. The name and token are stored in ~/.flyrc (though you shouldn't really edit the file manually).\n\nConcourse deployments can be occupied by multiple teams. To specify the team to which to log in, specify the --team-name or -n flag. If not specified, this defaults to the The main team.\n\nSo, to log in to a team my-team an endpoint served at https://ci.example.com and save it as the more convenient name example, you would run:\n\n$ fly --target example login --team-name my-team \\\n    --concourse-url https://ci.example.com\nThe login command will see which authentication methods are available for the specified team and prompt you to choose one. For basic auth, it will ask your username and password and use them to acquire a token. For OAuth, it will give you a link to click, and after you've gone through the OAuth flow it will print an OAuth token on the page that you can then copy and paste into the prompt.\n\nNote that if no authentication methods are configured, fly will acquire a token without any prompting. You can then use the alias like normal.\n\nIn any case, a token is saved in your ~/.flyrc, which will expire after one day.\n\nIf your Concourse uses SSL but does not have a certificate signed by a trusted CA, you can use the --ca-cert flag so that fly can trust the connection, like so:\n\n$ fly -t example login -c https://ci.example.com --ca-cert ./ca.crt\nThis will read the value out of the file ./ca.crt and save it into ~/.flyrc so you don't have to pass it on every login invocation.\n\nIf your Concourse instance is protected by a proxy server requiring client certificates, you can use --client-cert and --client-key to point to where your certificate is stored. These paths will be stored in .flyrc and the certificate will by attached to every request made to that target.\n\n$ fly -t example login -c https://ci-example.com \\\n    --client-cert ./client.pem \\\n    --client-key ./client.key\nAfter you've logged in you can use --target example (or -t example for short) to run a command against the saved target example. For example, fly -t example builds will list the last few builds on the example Concourse instance.\n\nThe -t flag is intentionally stateless and must be explicitly added to each command. This reduces the risk of accidentally running a command against the wrong environment when you have multiple targets defined.\n\n","depth":3,"section_tag":"fly-login"},"fly-logout":{"location":"fly.html#fly-logout","title":"fly logout","text":"To clear out your token for a given target, run:\n\n$ fly -t example logout\nTo clear out your token for all targets, run:\n\n$ fly logout -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-logout"},"fly-order-instanced-pipelines":{"location":"instanced-pipelines.html#fly-order-instanced-pipelines","title":"fly order-instanced-pipelines","text":"To configure the ordering of instanced pipelines within an individual instance group, run:\n\n$ fly -t example order-instanced-pipelines \\\n    --group group \\\n    --pipeline key1:value1 \\\n    --pipeline key2:value2 \\\n    --pipeline key3:value3\nNote that this command only ensures that the given pipelines are in the given order. If there are other pipelines that you haven't included in the command, they may appear in-between, before, or after the given set.\n\nIf you want to reorder pipelines outside of an individual instance group, you should use the fly order-pipelines command.\n\n","depth":5,"section_tag":"fly-order-instanced-pipelines"},"fly-order-pipelines":{"location":"managing-pipelines.html#fly-order-pipelines","title":"fly order-pipelines","text":"To configure the ordering of pipelines, run:\n\n$ fly -t example order-pipelines \\\n    --pipeline pipeline-1 \\\n    --pipeline pipeline-2 \\\n    --pipeline pipeline-3\nNote that this command only ensures that the given pipelines are in the given order. If there are other pipelines that you haven't included in the command, they may appear in-between, before, or after the given set.\n\nIf you want to reorder instanced pipelines within an individual instance group, you should use the fly order-instanced-pipelines command.\n\n","depth":4,"section_tag":"fly-order-pipelines"},"fly-pause-job":{"location":"jobs.html#fly-pause-job","title":"fly pause-job","text":"To prevent scheduling and running builds of a job, run:\n\n$ fly -t example pause-job --job my-pipeline/my-job\nThis will prevent pending builds of the job from being scheduled, though builds that are in-flight will still run, and pending builds will still be created as normal.\n\n","depth":4,"section_tag":"fly-pause-job"},"fly-pause-pipeline":{"location":"managing-pipelines.html#fly-pause-pipeline","title":"fly pause-pipeline","text":"To pause a pipeline, run:\n\n$ fly -t example pause-pipeline --pipeline my-pipeline\nThis will prevent jobs from being scheduled and stop the periodic checking for new versions of resources. Builds that are in-flight will still finish.\n\n","depth":4,"section_tag":"fly-pause-pipeline"},"fly-pin-resource":{"location":"managing-resources.html#fly-pin-resource","title":"fly pin-resource","text":"To pin a resource to a specific version of that resource, run:\n\n$ fly -t example pin-resource --resource my-pipeline/my-resource \\\n    --version ref:bceaf\nNote that the version needs to be provided as a key-value pair. For the git resource the ref: prefix is used while the registry  resource might use digest as a prefix like digest:sha256:94be7d7b.\n\nA comment can be provided using the --comment flag, which is then also visible in the UI :\n\n$ fly -t example pin-resource --resource my-pipeline/my-resource \\\n    --version ref:abcdef \\\n    --comment \"Some reason\"\nThis can, for example, be used to pull in a fixed version of an external dependency which might break your build in a new release. After the problem has been resolved, the pin can be removed. Another example could be running a build with a set of older inputs when needed.\n\nTo remove the pin on a resource use:\n\n$ fly -t example unpin-resource --resource my-pipeline/my-resource\nYou can also pin a resource via the UI by clicking on the pin button next to the desired version on the resource page. A default comment is automatically generated containing your username and a timestamp. This comment can be edited.\n\n","depth":4,"section_tag":"fly-pin-resource"},"fly-pipelines":{"location":"managing-pipelines.html#fly-pipelines","title":"fly pipelines","text":"To list the currently-configured pipelines and their paused state, run:\n\n$ fly -t example pipelines\nBy default, archived pipelines are not included in the output of this command. To view archived pipelines, provide --include-archived flag.\n\n","depth":4,"section_tag":"fly-pipelines"},"fly-prune-worker":{"location":"administration.html#fly-prune-worker","title":"fly prune-worker","text":"To remove a stalled, landing, landed, or retiring worker, run:\n\n$ fly -t example prune-worker --worker worker-name\nTo prune all stalled workers, run:\n\n$ fly -t example prune-worker --all-stalled\nThis is for those cases where you know a worker is not coming back. Note that running workers cannot be pruned, since they'll just re-register themselves anyway.\n\n","depth":4,"section_tag":"fly-prune-worker"},"fly-rename-pipeline":{"location":"managing-pipelines.html#fly-rename-pipeline","title":"fly rename-pipeline","text":"To rename a pipeline, run:\n\n$ fly -t example rename-pipeline \\\n    --old-name my-pipeline \\\n    --new-name my-cool-pipeline\n","depth":4,"section_tag":"fly-rename-pipeline"},"fly-rename-team":{"location":"managing-teams.html#fly-rename-team","title":"fly rename-team","text":"To rename a team, run:\n\n$ fly -t example rename-team --old-name my-team --new-name cool-team\nThis can only be run by the main team.\n\n","depth":4,"section_tag":"fly-rename-team"},"fly-rerun-build":{"location":"jobs.html#fly-rerun-build","title":"fly rerun-build","text":"To queue a new build of a job with exactly the same inputs as a given build of the same job, run:\n\n$ fly -t example rerun-build --job my-pipeline/my-job --build 4\nThis will enqueue a new build of the my-job job in the my-pipeline pipeline, using the same input versions as build number 4.\n\nTo start watching the newly created build, append the --watch flag like so:\n\n$ fly -t example rerun-build --job my-pipeline/my-job --build 4 --watch\nYou can also rerun builds by visiting the build page for the build in question in the web UI and clicking the rerun button.\n\n","depth":4,"section_tag":"fly-rerun-build"},"fly-set-pipeline":{"location":"setting-pipelines.html#fly-set-pipeline","title":"fly set-pipeline","text":"To submit a pipeline configuration to Concourse from a file on your local disk you can use the -c or --config flag, like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml\nThis will present a diff of the changes and ask you to confirm the changes. If you accept then Concourse's pipeline configuration will switch to the pipeline definition in the YAML file specified.\n\nThe -c or --config flag can also take in the value - to indicate reading from stdin:\n\n$ cat pipeline.yml | fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config -\nNote that reading from stdin disables the confirmation prompt - the pipeline will be set automatically.\n\n","depth":4,"section_tag":"fly-set-pipeline"},"fly-set-team":{"location":"managing-teams.html#fly-set-team","title":"fly set-team","text":"Once you've logged in as the main team with fly, you can run fly set-team to create or update other teams. Users with a owner role can also update their own configuration with the same command.\n\nFor example, to create a new team that authorizes the local foo user, you would run:\n\nfly -t example set-team --team-name my-team \\\n  --local-user foo\nNote that each time set-team is run, the team's authorization config is set as a whole - it is not a stateful operation.\n\nThere are many different ways to configure team auth; see Configuring Auth for more information.\n\nOnce the team has been created, you can use fly login to log in:\n\n$ fly -t example login -n my-team\nAny newly configured pipelines (via fly set-pipeline) and one-off builds (via Running tasks with fly execute) will be owned by the authorized team. Commands that list content will be scoped to the current team by default, such as fly pipelines and fly builds. The web UI will reflect the same state.\n\nNewly configured pipelines are hidden by default, meaning other teams and unauthorized visitors cannot view them. To make them publicly viewable, see Pipeline \u0026 Build Visibility.\n\n","depth":4,"section_tag":"fly-set-team"},"fly-status":{"location":"fly.html#fly-status","title":"fly status","text":"To check your current authentication status with a given target, run:\n\n$ fly -t example status\nThis will let you know if the token has expired.\n\n","depth":3,"section_tag":"fly-status"},"fly-sync":{"location":"fly.html#fly-sync","title":"fly sync","text":"Occasionally we add additional features to fly or make changes to the communication between it and Concourse's API server. To make sure you're running the latest and greatest version that works with the Concourse you are targeting we provide a command called sync that will update your local fly. It can be used like so:\n\n$ fly -t example sync\nThe fly command will also warn you if it notices that your CLI version is out of sync with the server.\n\n","depth":3,"section_tag":"fly-sync"},"fly-targets":{"location":"fly.html#fly-targets","title":"fly targets","text":"To see what targets are currently known to fly, run:\n\n$ fly targets\nThis will show each target's name, URL, and when its token expires.\n\n","depth":3,"section_tag":"fly-targets"},"fly-teams":{"location":"managing-teams.html#fly-teams","title":"fly teams","text":"To list all the teams, run:\n\n$ fly -t example teams\nThis can be useful if you've forgotten your team name.\n\nfly teams -d: With Details\n\nTo list all the teams with authentication details and members, run:\n\n$ fly -t example teams -d\nThis can be helpful when debugging OAuth, OIDC groups or listing all individual members.\n\n","depth":4,"section_tag":"fly-teams"},"fly-trigger-job":{"location":"jobs.html#fly-trigger-job","title":"fly trigger-job","text":"To immediately queue a new build of a job, run:\n\n$ fly -t example trigger-job --job my-pipeline/my-job\nThis will enqueue a new build of the my-job job in the my-pipeline pipeline.\n\nTo start watching the newly created build, append the --watch flag like so:\n\n$ fly -t example trigger-job --job my-pipeline/my-job --watch\nYou can also queue new builds by clicking the + button on the job or build pages in the web UI.\n\n","depth":4,"section_tag":"fly-trigger-job"},"fly-unpause-job":{"location":"jobs.html#fly-unpause-job","title":"fly unpause-job","text":"To resume scheduling of a job, run:\n\n$ fly -t example unpause-job --job my-pipeline/my-job\nThis will resume scheduling of builds queued for the job.\n\n","depth":4,"section_tag":"fly-unpause-job"},"fly-unpause-pipeline":{"location":"managing-pipelines.html#fly-unpause-pipeline","title":"fly unpause-pipeline","text":"To unpause a pipeline, run:\n\n$ fly -t example unpause-pipeline --pipeline my-pipeline\nThis will resume job scheduling and resource checking.\n\n","depth":4,"section_tag":"fly-unpause-pipeline"},"fly-userinfo":{"location":"fly.html#fly-userinfo","title":"fly userinfo","text":"To check what user you're logged in as, as well as which teams you are currently authenticated to and which roles within each team you have, run:\n\n$ fly -t example userinfo\n","depth":3,"section_tag":"fly-userinfo"},"fly-validate-pipeline":{"location":"setting-pipelines.html#fly-validate-pipeline","title":"fly validate-pipeline","text":"To validate a local pipeline configuration without submitting it to Concourse, run validate-pipeline:\n\n$ fly validate-pipeline --config pipeline.yml\nBy default, pipeline errors will cause validate-pipeline to fail, but warnings won't. To fail on both errors and warnings, pass the `--strict` flag.\n\n","depth":4,"section_tag":"fly-validate-pipeline"},"fly-volumes":{"location":"administration.html#fly-volumes","title":"fly volumes","text":"To list the active volumes across all your workers, run:\n\n$ fly -t example volumes\nThis can be useful to observe the caches warming across your cluster, and could be a good indicator of disk use.\n\n","depth":4,"section_tag":"fly-volumes"},"fly-watch":{"location":"builds.html#fly-watch","title":"fly watch","text":"Concourse emits streaming colored logs on the website, but it can be helpful to have the logs available to the command line (e.g. so that they can be processed by other commands).\n\nThe watch command can be used to do just this. You can also view builds that are running in your pipeline, or builds that have already finished.\n\nNote that unlike Running tasks with fly execute, killing fly watch via SIGINT or SIGTERM will not abort the build.\n\nTo watch the most recent one-off build, just run fly watch with no arguments. To watch a specific build (one-off or no), pass --build with the ID of the build to watch. This ID is available at the start of Running tasks with fly execute's output or by browsing to the builds list in the web UI.\n\nBy using the --job and --build flags you can pick out a specific build of a job to watch. For example, the following command will either show the archived logs for an old build, if it has finished running, or it will stream the current logs, if the build is still in progress.\n\n$ fly -t example watch --job my-pipeline/tests --build 52\nIf the --job flag is specified and --build is omitted, the most recent build of the specified job will be selected.\n\nIf there is a mismatch between the fly and web versions, it is possible to run into failed to parse next event: unknown event type error. The --ignore-event-parsing-errors flag can be passed to ignore such errors. \n\n","depth":3,"section_tag":"fly-watch"},"fly-workers":{"location":"administration.html#fly-workers","title":"fly workers","text":"To list the currently registered workers, including additional metadata, run:\n\n$ fly -t example workers\nThis can be useful for monitoring the status of your workers, if you suspect that one keeps dropping out of the pool or getting tasked with too many containers, etc.\n\n","depth":4,"section_tag":"fly-workers"},"further-readings":{"location":"building-and-pushing-an-image.html#further-readings","title":"Further Readings","text":"Understanding what the build context is is important when building container images. You can read Dockerfile Best Practices for more details about build contexts.\n\nThe inputs section of the oci-build-task's README has examples on how to create a build context with multiple inputs and other complex build scenarios.\n\nRead the README's in the oci-build-task and registry-image resource to learn more about their other configuration options.\n\n","depth":5,"section_tag":"further-readings"},"garbage-collector":{"location":"garbage-collector.html","title":"Garbage Collector","text":"Concourse runs everything in isolated environments by creating fresh containers and volumes to ensure things can safely run in a repeatable environment, isolated from other workloads running on the same worker.\n\nThis introduces a new problem of knowing when Concourse should remove these containers and volumes. Safely identifying things for removal and then getting rid of them, releasing their resources, is the process of garbage collection.\n\n","depth":4,"section_tag":"garbage-collector"},"gated-pipeline-patterns":{"location":"gated-pipeline-patterns.html","title":"Gated Pipeline Patterns","text":"Gated pipelines provide control for administrators and release managers on when a given software release is deployed to a tightly protected environment (e.g. production).\n\nThe execution of jobs that perform certain tasks (e.g. deployment) targeting the downstream environment beyond the \"gate\" step is done only upon either an approval coming from an external Change Control system or an explicit manual trigger of such step.\n\n","depth":4,"section_tag":"gated-pipeline-patterns"},"generating-keys":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"","depth":3,"section_tag":"concourse-generate-key"},"generating-the-keys":{"location":"concourse-generate-key.html#generating-the-keys","title":"Generating the Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nSession Signing Key: Used by the Running a web node for signing and verifying user session tokens.\n\n\nTSA Host Key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nWorker Key: Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized worker keys file in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\nor use ssh-keygen:\n\nssh-keygen -t rsa -b 4096 -m PEM -f ./session_signing_key\nssh-keygen -t rsa -b 4096 -m PEM -f ./tsa_host_key\nssh-keygen -t rsa -b 4096 -m PEM -f ./worker_key\nAt this point you should have the following files:\n\n* session_signing_key\n\n* tsa_host_key\n\n* tsa_host_key.pub\n\n* worker_key\n\n* worker_key.pub\n\nYou can remove the session_signing_key.pub file if you have one, it is not needed by any process in Concourse.\n\n","depth":4,"section_tag":"generating-the-keys"},"generic-oauth":{"location":"generic-oauth.html","title":"Generic oAuth","text":"A Concourse server can authenticate against any valid OAuth auth provider, though it's a bit \"closer to the metal\" as you'll need to explicitly configure the auth, token, and user-info URLs. You may want to see if you can use Generic OIDC auth if your auth provider is compatible with OIDC.\n\n","depth":4,"section_tag":"generic-oauth"},"generic-oauth-authentication":{"location":"generic-oauth.html#generic-oauth-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your oAuth provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nThe Generic oAuth provider has many values to set - for a full list consult concourse web --help.\n\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OAUTH_DISPLAY_NAME=Acme\nCONCOURSE_OAUTH_CLIENT_ID=myclientid\nCONCOURSE_OAUTH_CLIENT_SECRET=myclientsecret\nCONCOURSE_OAUTH_AUTH_URL=https://oauth.example.com/oauth2/auth\nCONCOURSE_OAUTH_TOKEN_URL=https://oauth.example.com/oauth2/token\nCONCOURSE_OAUTH_USERINFO_URL=https://oauth.example.com/oauth2/userinfo\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oauth-authentication"},"generic-oauth-authorization":{"location":"generic-oauth.html#generic-oauth-authorization","title":"Authorization","text":"OAuth users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oauth-user=USERNAME: Authorize an individual user.\n\n\n--oauth-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OAUTH_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oauth-user my-username \\\n    --oauth-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oauth:\n    users: [\"my-username\"]\n    groups: [\"my-group\"]\n","depth":5,"section_tag":"generic-oauth-authorization"},"generic-oidc-auth":{"location":"generic-oidc-auth.html","title":"Generic OIDC auth","text":"A Concourse server can authenticate against any valid OIDC auth provider. This provider is similar to Generic oAuth except it only requires an issuer URL rather than auth/token/userinfo URLs.\n\n","depth":4,"section_tag":"generic-oidc-auth"},"generic-oidc-authentication":{"location":"generic-oidc-auth.html#generic-oidc-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your OIDC provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OIDC_DISPLAY_NAME=Acme\nCONCOURSE_OIDC_CLIENT_ID=myclientid\nCONCOURSE_OIDC_CLIENT_SECRET=myclientsecret\nCONCOURSE_OIDC_ISSUER=https://oidc.example.com\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oidc-authentication"},"generic-oidc-authorization":{"location":"generic-oidc-auth.html#generic-oidc-authorization","title":"Authorization","text":"When authorizing individual users, it's up to you to ensure that the preferred_username claim and/or the claim specified by CONCOURSE_OIDC_USER_NAME_KEY is unique. If they're not, then it's possible for users to impersonate each other\n\nOIDC users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oidc-user=USERNAME: Authorize an individual user.\n\n\n--oidc-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OIDC_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oidc-user my-username \\\n    --oidc-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oidc:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"generic-oidc-authorization"},"generic-saml-auth":{"location":"generic-saml-auth.html","title":"Generic SAML auth","text":"A Concourse server can authenticate against any valid SAML auth provider.\n\n","depth":4,"section_tag":"generic-saml-auth"},"generic-saml-authentication":{"location":"generic-saml-auth.html#generic-saml-authentication","title":"Authentication","text":"First you'll need to create an application with your SAML provider. Note that the terminology used for configuring an application may vary between SAML providers - this document uses Okta's terminology.\n\nSAML Assertion Consumer Service (ACS) URL must be the URL of your Concourse server with /sky/issuer/callback appended.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nAudience URI (SP Entity ID) must match CONCOURSE_SAML_ENTITY_ISSUER, which defaults to the URL of your Concourse server with /sky/issuer/callback appended.\n\nAttribute statements that you define in the SAML provider can be remapped in Concourse:\n\nCONCOURSE_SAML_USERNAME_ATTR=name   # default\nCONCOURSE_SAML_EMAIL_ATTR=email     # default\nCONCOURSE_SAML_GROUPS_ATTR=groups   # default\nFinally, the SAML provider will generate a SSO URL, a CA certificate, and an Identity Provider Issuer. These values correspond with CONCOURSE_SAML_SSO_URL, CONCOURSE_SAML_CA_CERT, and CONCOURSE_SAML_SSO_ISSUER respectively.\n\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_SAML_DISPLAY_NAME=Okta\nCONCOURSE_SAML_SSO_URL=https://acme.okta.com/app/Y/Z/sso/saml\nCONCOURSE_SAML_CA_CERT=/path/to/ca_cert\nCONCOURSE_SAML_SSO_ISSUER=http://www.okta.com/X\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-saml-authentication"},"generic-saml-authorization":{"location":"generic-saml-auth.html#generic-saml-authorization","title":"Authorization","text":"SAML users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--saml-user=USERNAME: Authorize an individual user.\n\n\n--saml-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which attribute points to the groups information by specifying CONCOURSE_SAML_GROUPS_ATTR on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --saml-user my-username \\\n    --saml-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  saml:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"generic-saml-authorization"},"get-images-for-tasks-instead-of-using-anonymous-image-resources":{"location":"common-pipeline-practices.html#get-images-for-tasks-instead-of-using-anonymous-image-resources","title":"Get Images for Tasks Instead of using Anonymous Image Resources","text":"It is easy to let Concourse fetch images for tasks right when they are needed by using the task.image_resource field in a task config. It's the easy out-of-the-box solution. Another way is to pass the image for a task as an input to the job by setting the task step image field. This also allows you to track the version of the image being used by the task and also avoid getting rate-limited by configuring the resource with credentials.\n\nBefore - Anonymous Image Fetching for Tasks\n\njobs:\n- name: job\n  plan:\n  - task: simple-task\n    config:\n      platform: linux\n      image_resource: # anonymous image resource\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello world!\"]\nAfter - Passing Task Image as Job Inputs\n\nresources:\n- name: busybox\n  type: registry-image\n  source:\n    repository: busybox\n    username: ((docker.user))\n    password: ((docker.password))\n\njobs:\n- name: job\n  plan:\n  - get: busybox # pass image into job\n  - task: simple-task\n    image: busybox # use image for task. Overrides anonymous image\n    config:\n      platform: linux\n      run:\n        path: echo\n        args: [\"Hello world!\"]\n","depth":5,"section_tag":"get-images-for-tasks-instead-of-using-anonymous-image-resources"},"get-step":{"location":"jobs.html#get-step","title":"get step","text":"","depth":3,"section_tag":"steps"},"get-steps":{"location":"tutorial-resources.html#get-steps","title":"Get Steps","text":"Let's add some automation to our hello-world.yml pipeline and have it trigger on every new commit from some git repo. We will use the git resource to accomplish this.\n\nBefore proceeding, please have a git repository ready that you can push code to. The rest of the tutorial will point to github.com/concourse/examples which you should replace with your own repository or your own fork of concourse/examples.\n\nTo start, let's go back to the one step version of the pipeline.\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello World!\"]\nNow let's add the git resource to the pipeline. First we'll add the resources key to the top-level of our yaml.\n\nresources:\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello World!\"]\nThe resources key takes a list of resources. In our case, we're going to add one item to that list. We will name the resource repo. This is the name we'll use to refer to this specific instance of the resource within our pipeline. We'll define the type as git, which we got from the output of fly -t tutorial workers --details.\n\nresources:\n- name: repo\n  type: git\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello World!\"]\nThere is one last field we need to fill out, the source field.\n\nThe source field contains the configuration for the resource. By convention, documentation for each resource's configuration is found in the README of its git repository. The source field will be different for every resource type, so you'll need to refer to that resource's documentation to determine how to fill out the source field.\n\nIf we look at the README for the git resource we'll find that it requires we set the uri field. This should point to the location of the git repository. We can specify it in https:// or ssh@ format.  Let's add the uri field and set it to your repository.\n\nIf you use the ssh@ format for the uri then you'll likely need to set the private_key field as well.\n\nIf you use the https:// format and your repo is public then you don't need to set anything else. If you repository is private then you'll need to set the username and password fields to allow the git resource to authenticate. private_key can also be used for https:// uri's.\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello World!\"]\nNext we need to connect the repo resource to the hello-world-job. We connect the two objects by adding a get step to the job.\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\n\njobs:\n- name: hello-world-job\n  plan:\n  # Add a get step referencing the resource\n  - get: repo\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello World!\"]\nThe final piece is telling Concourse to trigger the hello-world-job whenever the repo resource emits a new version. We can do that by setting trigger to true for the get step.\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\n\njobs:\n- name: hello-world-job\n  plan:\n  - get: repo\n    trigger: true   # tell Concourse to trigger this job when new versions are emitted\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      run:\n        path: echo\n        args: [\"Hello World!\"]\nLet's set our pipeline and watch it automatically trigger from the web UI.\n\n$ fly -t tutorial set-pipeline -p hello-world -c hello-world.yml\n{image: images/tutorial/hello-world-trigger-job.png}\n\nIf you click the resource you'll see a list of the versions that it has emitted so far. You can expand each version to see which build it was used in and some metadata if the resource fetched any. Metadata is only populated after a version is fetched by a get step.\n\nBy default, resources will only fetch the most recent version available. At this point you can make commits to your repo and watch Concourse find them and trigger the job. If you want to populate the version history with older versions you can use fly check-resource with the --from flag.\n\n","depth":4,"section_tag":"get-steps"},"get-steps-and-inputs":{"location":"tutorial-resources.html#get-steps-and-inputs","title":"Get Steps and Inputs","text":"Let's tie together two concepts that we've learned so far. Get steps and task inputs.\n\nIn the previous section we learned that task steps can specify outputs that other task steps can then consume as inputs. Get steps generate one output that can then be consumed by tasks as an input. Get steps always generate output artifacts based on their name.\n\nTo find out the structure of the artifact generated by a get step you will have to refer to the resource's documentation. The documentation for the git resource tells us that the in script will clone the git repository.\n\nLet's add repo as an input to the hello-world-task. Let's also update the command the task runs to instead print the contents of some file in the repo.\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: https://github.com/concourse/examples.git\n\njobs:\n- name: hello-world-job\n  plan:\n  - get: repo\n    trigger: true\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      inputs: # add the get step as an input to this task\n      - name: repo\n      run: # read the file from the get step\n        path: cat\n        args: [\"repo/README.md\"]\nSet the pipeline and we'll manually trigger the job.\n\n$ fly -t tutorial set-pipeline -p hello-world -c hello-world.yml\n$ fly -t tutorial trigger-job --job hello-world/hello-world-job --watch\nstarted hello-world/hello-world-job #44\n\ninitializing\nselected worker: d032d4471e67\nrunning cat repo/README.md\n# examples\nExamples of Concourse workflows\nsucceeded\nThe job should finish successfully and print out the contents of repo/README.md or whichever file you selected from your repo.\n\n","depth":4,"section_tag":"get-steps-and-inputs"},"getting-started":{"location":"getting-started.html","title":"Getting Started","text":"This tutorial will guide you through the basics of creating Concourse pipelines. You will use a local instance of Concourse running on your machine to run pipelines.\n\nBefore getting started you should have the following installed:\n\n* Docker\n\n* Docker-compose\n\nThis tutorial assumes you understand what Linux containers are and how to work with them. If you know what a Dockfile is and how to make your own then you're probably good to jump into this tutorial. If you're not familiar with Linux containers then you may want to get started with Docker first before diving into this tutorial.\n\nIt will also help if you know how to read YAML. We have a quick Intro to YAML if you're not familiar with the syntax.\n\nIf you have any feedback for this tutorial please share it in this Github discussion\n\n","depth":2,"section_tag":"getting-started"},"git-guides":{"location":"git-guides.html","title":"Git Guides","text":"","depth":3,"section_tag":"git-guides"},"git-trigger-example":{"location":"git-trigger-example.html","title":"git-triggered job example","text":"The git resource can be used to trigger a job.\n\n","depth":2,"section_tag":"git-trigger-example"},"github-auth":{"location":"github-auth.html","title":"GitHub auth","text":"A Concourse server can authenticate against GitHub to leverage their permission model and other security improvements in their infrastructure.\n\n","depth":4,"section_tag":"github-auth"},"github-authentication":{"location":"github-auth.html#github-authentication","title":"Authentication","text":"First, you'll need to create an OAuth application on GitHub.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server. This address must be reachable by GitHub - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITHUB_CLIENT_ID=myclientid\nCONCOURSE_GITHUB_CLIENT_SECRET=myclientsecret\nNote that the client must be created under an organization if you want to authorize users based on organization/team membership. In addition, the GitHub application must have at least read access on the organization's members. If the client is created under a personal account, only individual users can be authorized.\n\nIf you're configuring GitHub Enterprise, you'll also need to set the following env:\n\nCONCOURSE_GITHUB_HOST=github.example.com\nCONCOURSE_GITHUB_CA_CERT=/path/to/ca_cert\nThe GitHub Enterprise host must not contain a scheme, or a trailing slash.\n\n","depth":5,"section_tag":"github-authentication"},"github-authorization":{"location":"github-auth.html#github-authorization","title":"Authorization","text":"Users, teams, and entire organizations can be authorized for a team by passing the following flags to fly set-team:\n\n--github-user=LOGIN: Authorize an individual user.\n\n\n--github-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--github-team=ORG_NAME:TEAM_NAME: Authorize a team's members within an organization.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --github-user my-github-login \\\n    --github-org my-org \\\n    --github-team my-other-org:my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  github:\n    users: [\"my-github-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"github-authorization"},"gitlab-auth":{"location":"gitlab-auth.html","title":"GitLab auth","text":"A Concourse server can authenticate against GitLab to leverage their permission model.\n\n","depth":4,"section_tag":"gitlab-auth"},"gitlab-authentication":{"location":"gitlab-auth.html#gitlab-authentication","title":"Authentication","text":"First you need to create an OAuth application on GitLab.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitLab - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITLAB_CLIENT_ID=myclientid\nCONCOURSE_GITLAB_CLIENT_SECRET=myclientsecret\nIf you're configuring a self hosted GitLab instance, you'll also need to set the following flag:\n\nCONCOURSE_GITLAB_HOST=https://gitlab.example.com\nThe GitLab host must contain a scheme and not a trailing slash.\n\n","depth":5,"section_tag":"gitlab-authentication"},"gitlab-authorization":{"location":"gitlab-auth.html#gitlab-authorization","title":"Authorization","text":"Users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--gitlab-user=USERNAME: Authorize an individual user.\n\n\n--gitlab-group=GROUP_NAME: Authorize an entire groups's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --gitlab-user my-gitlab-user \\\n    --gitlab-team my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  gitlab:\n    users: [\"my-gitlab-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"gitlab-authorization"},"giving-your-cluster-a-name":{"location":"concourse-web.html#giving-your-cluster-a-name","title":"Giving your cluster a name","text":"If you've got many Concourse clusters that you switch between, you can make it slightly easier to notice which one you're on by giving each cluster a name:\n\nCONCOURSE_CLUSTER_NAME=production\nWhen set, this name will be shown in the top bar when viewing the dashboard.\n\n","depth":5,"section_tag":"giving-your-cluster-a-name"},"global-resources":{"location":"global-resources.html","title":"Global Resources (experimental)","text":"Concourse v5.0 contains an experimental feature known as \"global resources\". It is enabled by passing the --enable-global-resources flag to the concourse web command.\n\nThe basic concept of global resources is to share detected resource versions between all resources that have the same resource.type and resource.source configuration.\n\nBefore v5.0.0, each pipeline resource had its own version history, associated to the resource by name. This meant that multiple pipelines with the same resource configs would redundantly collect the same version and metadata information.\n\nWith v5.0.0's experimental 'global resources' feature, resource versions are instead associated to an anonymous 'resource config' i.e. its resource.type and resource.source.\n\n","depth":3,"section_tag":"global-resources"},"goals":{"location":"garbage-collector.html#goals","title":"Goals","text":"Let's define our metrics for success:\n\n* Safe. There should never be a case where a build is running and a container or volume is removed out from under it, causing the build to fail. Resource checking should also never result in errors from check containers being removed. No one should even know garbage collection is happening.\n\n* Airtight. Everything Concourse creates, whether it's a container or volume on a worker or an entry in the database, should never leak. Each object should have a fully defined lifecycle such that there is a clear end to its use. The ATC should be interruptible at any point in time and at the very least be able to remove any state it had created beforehand.\n\n* Resilient. Garbage collection should never be outpaced by the workload. A single misbehaving worker should not prevent garbage collection from being performed on other workers. A slow delete of a volume should not prevent garbage collecting of other things on the same worker.\n\n","depth":5,"section_tag":"goals"},"golang-library-example":{"location":"golang-library-example.html","title":"Golang library testing example","text":"You can run the tests for a Golang library across any specified versions.\n\nThis example shows how to have multiple versions of a language, environment, or dependency fetched and integrated in to a Pipeline.\n\nFor these Docker images, defining them as Resources has two advantages for this use case. First, this enables the pipeline to be triggered when there are new versions of those images available. Second, referencing them in the task's task step image param is helpful as it will ensure consistency between the image versions fetched by the Resource and the image version running in the job.\n\n","depth":2,"section_tag":"golang-library-example"},"gracefully-removing-a-worker":{"location":"concourse-worker.html#gracefully-removing-a-worker","title":"Gracefully Removing a Worker","text":"When a worker machine is going away, it should be retired. This is similar to landing, except at the end the worker is completely unregistered, along with its volumes and containers. This should be done when a worker's VM or container is being destroyed.\n\nTo retire a worker, send SIGUSR2 to the worker process. This will switch the worker to retiring state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the worker will be removed and the worker process will exit.\n\nJust like with landing, you may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR2/300/TERM/15/KILL\nThis will send SIGUSR2, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\n","depth":5,"section_tag":"gracefully-removing-a-worker"},"guardian-runtime":{"location":"concourse-worker.html#guardian-runtime","title":"Guardian runtime","text":"Guardian is currently the default runtime for Concourse. It can also be set by setting the --runtime flag to guardian on the concourse worker command.\n\nThe concourse worker command automatically configures and runs Guardian using the gdn binary, but depending on the environment you're running Concourse in, you may need to pop open the hood and configure a few things.\n\nThe gdn server can be configured in two ways:\n\n1. By creating a config.ini file and passing it as --garden-config (or CONCOURSE_GARDEN_CONFIG).\n\n  The .ini file should look something like this:\n\n  [server]\n  flag-name=flag-value\n  To learn which flags can be set, consult gdn server --help. Each flag listed can be set under the [server] heading.\n\n2. By setting CONCOURSE_GARDEN_* environment variables.\n\n  This is primarily supported for backwards compatibility, and these variables are not present in concourse worker --help. They are translated to flags passed to gdn server by lower-casing the * portion and replacing underscores with hyphens.\n\n","depth":6,"section_tag":"guardian-runtime"},"hello-world-example":{"location":"hello-world-example.html","title":"Hello World pipeline","text":"A single job is the simplest form of pipeline.\n\nWhile this is less of an example pipeline, this is a simple introduction to a critical primitive to form pipelines.\n\nAlso, due to the fact that there are minimal external factors (Resources) for the system to check and resolve. This is often used overall test system health.\n\n","depth":2,"section_tag":"hello-world-example"},"hooks-example":{"location":"hooks-example.html","title":"Job \u0026 task hooks example","text":"Job Hooks and Step hooks are available to perform actions based on the success, failure, or abortion of a job.\n\n","depth":2,"section_tag":"hooks-example"},"how-can-i-help":{"location":"project.html#how-can-i-help","title":"How can I help?","text":"Concourse is a free and Open Source software project that relies on the contributions of sponsors and volunteers from around the world.\n\nIf you're interested in helping out, head on over to GitHub and check out the contributing docs!\n\n","depth":2,"section_tag":"how-can-i-help"},"how-does-concourse-track-artifacts":{"location":"tutorial-inputs-outputs.html#how-does-concourse-track-artifacts","title":"How does Concourse track artifacts?","text":"As Concourse is running the steps in your job, it is creating a list of named artifacts. Let's see what that looks like for the pipeline we just ran.\n\n* Concourse runs the task step hello-world-task with output the-artifact\n\n  Concourse creates an empty artifact, assigns it the name the-artifact, and mounts it inside the task container.\n\n* Concourse runs the task step read-the-artifact with input the-artifact\n\n  Concourse looks up, in its list of artifacts for the build, for an artifact named the-artifact, and mounts it inside the task container. If no input with that name is found then the build would fail.\n\nThe next section will introduce you to the concept of Resources.\n\nIf you have any feedback for this tutorial please share it in this Github discussion\n\n","depth":4,"section_tag":"how-does-concourse-track-artifacts"},"how-it-works":{"location":"garbage-collector.html#how-it-works","title":"How it Works","text":"The garbage collector is a batch operation that runs on an interval with a default of 30 seconds. It's important to note that the collector must be able to run frequently enough to not be outpaced by the workload producing things, and so the batch operation should be able to complete pretty quickly.\n\nThe batch operation first performs garbage collection within the database alone, removing rows that are no longer needed. The removal of rows from one stage will often result in removals in a later stage. There are individual collectors for each object, such as the volume collector or the container collector, and they are all run asynchronously.\n\nAfter the initial pass of garbage collection in the database, there should now be a set of containers and volumes that meet criteria for garbage collection. These two are a bit more complicated to garbage-collect; they both require talking to a worker, and waiting on a potentially slow delete.\n\nContainers and volumes are the costliest resources consumed by Concourse. There are also many of them created over time as builds execute and pipelines perform their resource checking. Therefore it is important to parallelize this aspect of garbage collection so that one slow delete or one slow worker does not cause them to pile up.\n\n","depth":5,"section_tag":"how-it-works"},"how-to-guides":{"location":"how-to-guides.html","title":"How-To Guides","text":"The following pages are guides that show how to accomplish certain workflows within Concourse. Most of the guides will use specific images but you are in no way limited to or forced to use these images to accomplish the same task. There are many ways to accomplish the same thing in Concourse, so don't let these guides limit you in what you think is possible with Concourse.\n\n","depth":2,"section_tag":"how-to-guides"},"iam-permissions":{"location":"aws-ssm-credential-manager.html#iam-permissions","title":"IAM Permissions","text":"The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the Resource section can contain a wildcard to a parameter or be restricted to an individual parameter.\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Sid\": \"AllowAccessToSsmParameters\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ssm:GetParameter\",\n            \"ssm:GetParametersByPath\"\n        ],\n        \"Resource\": [\n            \"arn:aws:ssm:::parameter/concourse/*\",\n            \"arn:aws:ssm:::parameter/concourse/TEAM_NAME/*\",\n            \"arn:aws:ssm:::parameter/concourse/TEAM_NAME/PIPELINE_NAME/*\"\n        ]\n    },\n    {\n        \"Sid\": \"AllowAccessToDecryptSsmParameters\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"kms:Decrypt\",\n            \"kms:DescribeKey\"\n        ],\n        \"Resource\": \"arn:aws:kms:::key/KMS_KEY_ID\"\n    },\n    {\n        \"Sid\": \"AllowListKeys\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"kms:ListAliases\",\n            \"kms:ListKeys\"\n        ],\n        \"Resource\": \"*\"\n    }\n]\n}\nNote that the TEAM_NAME, PIPELINE_NAME, and KMS_KEY_ID text above should be replaced to fit your Concourse setup.\n\nFor more information on how to use IAM roles to restrict access to SSM parameters, review the official documentation.\n\n","depth":5,"section_tag":"iam-permissions"},"implementing-resource-types":{"location":"implementing-resource-types.html","title":"Implementing a Resource Type","text":"A resource type is implemented by a container image with three scripts:\n\n* /opt/resource/check for checking for new versions of the resource\n\n* /opt/resource/in for pulling a version of the resource down\n\n* /opt/resource/out for idempotently pushing a version up\n\nDistributing resource types as containers allows them to package their own dependencies. For example, the git resource comes with the git binary pre-installed.\n\nAll resources must implement all three actions, though the actions can just be no-ops (which still must be correctly implemented as detailed below).\n\nResources can emit logs to the user by writing to stderr. ANSI escape codes (coloring, cursor movement, etc.) will be interpreted properly by the web UI, so you should make your output pretty.\n\n","depth":3,"section_tag":"implementing-resource-types"},"in-parallel-step":{"location":"jobs.html#in-parallel-step","title":"in_parallel step","text":"","depth":3,"section_tag":"steps"},"index":{"location":"index.html","title":"Concourse","text":"Concourse is an open-source continuous thing-doer.Built on the simple mechanics of resources, tasks, and jobs, Concourse presents a general approach to automation that makes it great for CI/CD.\n\nQuick Start\n\nConcourse's RFC process and governance model invite anyone to become a contributor, developing the project roadmap by collaborating in the open.\n\nRFC: ProjectsRFC: ServicesRFC: `across` stepRFC: Concourse Kubernetes OperatorAdd manual stepsRFC: Configurable Build Event StoresRFC: k8s storagePrototypes rev 1.1: inputs/outputsRFC: Pipeline Kubernetes CRDWIP: worker poolRFC: Propose static configuration for Concourse instancesHelp shape Concourse into a tool that fits your needs by submitting feedback on the RFCs listed above!\n\nConcourse is designed to be expressive, versatile, and safe, remaining intuitive as the complexity of your project grows.\n\nConfigure as coderesources:\n- name: booklit\n  type: git\n  source: {uri: \"https://github.com/vito/booklit\"}\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - task: test\n    file: booklit/ci/test.yml\nVisualize to verifyA Concourse pipeline is like a distributed, continuous Makefile.\n\nEach job has a build plan declaring the job's input resources and what to run with them when they change.\n\nYour pipeline is then visualized in the web UI, taking only one click to get from a failed job to seeing why it failed.\n\nThe visualization provides a \"gut check\" feedback loop: if it looks wrong, it probably is wrong.\n\nA more complicated example...Jobs can depend on other jobs by configuring passed constraints. The resulting chain of jobs and resources is a dependency graph that continuously pushes your project forward, from source code to production.\n\nThis particular pipeline can be found in the Booklit repository.\n\nAll configuration and administration is done using the fly CLI.\n\nThe fly set-pipeline command pushes the config up to Concourse. Once it looks good, you can then check the file in to source control. This makes it easy to recover your project if the Concourse server burns down.\n\nEverything runs in containers, ensuring a clean environment on every run.\n\nEach task specifies its own image, giving it full control over its dependencies, rather than managing packages and state on your workers.\n\nThe fly intercept command will pop you right into one of your build's containers, making it easy to troubleshoot flaky builds.\n\nThe Running tasks with fly execute command lets you run a build with local changes.\n\nThis build runs in exactly the same way as it would run in your pipeline, without having to push broken commits until it works.\n\nWhen a build in the pipeline fails, you can run Running tasks with fly execute with the -j flag to run a one-off build with the same inputs as the failed build. You can then replace an input with your local changes with -i to see if your fix is valid.\n\nBring your own integrationsresource_types:\n- name: rubygem\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-rubygems-resource\n\nresources:\n- name: rspec-gem\n  type: rubygem\n  source: {gem: rspec}\n\njobs:\n- name: bundle\n  plan:\n  - get: rspec-gem\n    trigger: true\n  - # ...\nConcourse does not have a complex plugin system. Instead, it focuses on a single strong abstraction: resource, which are implemented by resource types.\n\nThe pipeline.resources field configures external artifacts that your pipeline will monitor for changes, fetch from, and push to.\n\nFor example, a resource with type git refers to a git repository, which will be cloned in a get step and pushed to using a put step. Behind the scenes, Concourse will continuously run git fetch to look for new commits that jobs may want to trigger on.\n\nAt its core, Concourse knows nothing about git. It comes with a git resource type out of the box, but you could just as easily bring your own into your pipeline by setting the pipeline.resource_types field.\n\nTo see what resource types are available, check out the Resource Types catalog!\n\n","depth":0,"section_tag":"index"},"install":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThere are many ways to deploy Concourse, depending on your personal preference. The Quick Start guide shows how to get Concourse up and running quickly via Docker Compose, and there is also an official Concourse Helm chart.\n\nThe documentation found here will primarily focus on the concourse CLI, which is the lowest common denominator, and can also be directly used if you want to just run Concourse yourself on real hardware or your own managed VMs.\n\nThe high-level steps to follow for installing Concourse are:\n\n1. Setup a Postgres database\n\n2. Generate Secrets for the web and worker nodes\n\n3. Install the web node\n\n4. Install the worker node\n\nWe don't document every configuration option for the web and worker commands. To view all flags you can run the following docker commands.\n\ndocker run -t concourse/concourse web --help\ndocker run -t concourse/concourse worker --help\n","depth":2,"section_tag":"install"},"install-fly":{"location":"quick-start.html#install-fly","title":"Install Fly","text":"Next, install the The fly CLI by downloading it from the web UI and login to your local Concourse as the test user:\n\n# MacOS\n% curl 'http://localhost:8080/api/v1/cli?arch=amd64\u0026platform=darwin' -o fly \\\n    \u0026\u0026 chmod +x ./fly \u0026\u0026 mv ./fly /usr/local/bin/\n# Linux\n% curl 'http://localhost:8080/api/v1/cli?arch=amd64\u0026platform=linux' -o fly \\\n    \u0026\u0026 chmod +x ./fly \u0026\u0026 mv ./fly /usr/local/bin/\n\n$ fly -t tutorial login -c http://localhost:8080 -u test -p test\nlogging in to team 'main'\n\ntarget saved\n# Windows (Powershell)\n% $concoursePath = 'C:\\concourse\\'; mkdir $concoursePath; `\n[Environment]::SetEnvironmentVariable('PATH', \"$ENV:PATH;${concoursePath}\", 'USER'); `\n$concourseURL = 'http://localhost:8080/api/v1/cli?arch=amd64\u0026platform=windows'; `\nInvoke-WebRequest $concourseURL -OutFile \"${concoursePath}\\fly.exe\"\n# -- Desired fly.exe location -----------\n$concoursePath = 'C:\\concourse\\'\nmkdir $concoursePath\n\n# -- Sets User Env Variable -------------\n[Environment]::SetEnvironmentVariable('PATH', \"$ENV:PATH;${concoursePath}\", 'USER')\n\n# -- Sets System Env Variable -----------\n# [Environment]::SetEnvironmentVariable(\"PATH\", \"$ENV:PATH;${concoursePath}\", \"MACHINE\")\n\n# -- Download fly.exe -------------------\n$concourseURL = 'http://localhost:8080/api/v1/cli?arch=amd64\u0026platform=windows'\nInvoke-WebRequest $concourseURL -OutFile \"${concoursePath}\\fly.exe\"\nYou'll notice that every fly command in this tutorial has to have the target (-t tutorial) specified. This is annoying when you only have one Concourse to target, but it helps ensure you don't trigger a job on the wrong Concourse instance. It will save you from hurting yourself!\n\nOnce you've confirmed everything is up and running by logging in through fly and the web UI, you can move onto the next section. If you refresh this page you should see the web UI in the frame below.\n\nIf you have any feedback for this tutorial please share it in this Github discussion\n\n","depth":4,"section_tag":"install-fly"},"installing-or-upgrading-bundled-resource-types":{"location":"concourse-worker.html#installing-or-upgrading-bundled-resource-types","title":"Installing or Upgrading Bundled Resource Types","text":"You may want to upgrade the bundled resource types outside of Concourse upgrades or even install additional resource types on your workers to reduce the polling on some external image repository like Docker Hub.\n\nWe will use the git resource as our example. We will assume your Concourse installation is at /usr/local/concourse.\n\nFirst, pull and create a container of the resource you're installing/upgrading. Grab the ID of the container that Docker creates.\n\n$ docker run -d concourse/git-resource\nb253417142565cd5eb43902e94a2cf355d5354b583fbc686488c9a153584c6ba\nExport the containers file system into a gzip compressed tar archive named rootfs.tgz\n\ndocker export b253417142 | gzip \u003e rootfs.tgz\nCreate a file called resource_metadata.json and populate it with the following contents. Make sure the type does not conflict with an existing resource type when you're installing a new resource type. In our example here we're calling the type gitv2 to avoid conflicting with the pre-existing git resource.\n\n{\n  \"type\": \"gitv2\",\n  \"version\": \"1.13.0\",\n  \"privileged\": false,\n  \"unique_version_history\": false\n}\nAt this point you should have two files: rootfs.tgz and resource_metadata.json.\n\nCreate a new directory under the resource-types folder in your Concourse installation directory. By convention it should be the same name as the type.\n\nmkdir /usr/local/concourse/resource-types/gitv2\nPlace the rootfs.tgz and resource_metadata.json inside the folder. Restart your worker and verify the new resource type is on there by running one of the following commands:\n\nfly workers --details\n# or\nfly curl api/v1/workers\nYou can also verify that Concourse can create a container with the rootfs.tgz you made by running a simple pipeline:\n\nresources:\n- name: some-resource\n  type: gitv2 #change to your resource type\n  source:\n    uri: https://github.com/concourse/git-resource.git\n\njobs:\n- name: simple-job\n  plan:\n  - get: some-resource\n","depth":6,"section_tag":"installing-or-upgrading-bundled-resource-types"},"instanced-pipelines":{"location":"instanced-pipelines.html","title":"Grouping Pipelines","text":"Instanced Pipelines/Instance Groups are currently experimental, and are subject to change.\n\nTo experiment with Instanced Pipelines on your deployment, you need to set the feature flag --enable-pipeline-instances (CONCOURSE_ENABLE_PIPELINE_INSTANCES=true)\n\nAlthough pipelines operate independently of one another, it's not uncommon to have several pipelines that are highly related, and possibly derived from the same pipeline template. It's useful to be able to group these pipelines to reduce clutter and improve navigation. For this, Concourse has the concept of Instanced Pipelines and Instance Groups, where an Instance Group composes several related Instanced Pipelines.\n\nFor instance, suppose you support multiple version lines of your software (v1.0.x and v2.0.x, say), and want a pipeline for each version line in order to facilitate delivering patch releases. You create a common pipeline template that uses Vars to specialize each pipeline:\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: git@...\n    # The only difference between the pipelines is the git branch to use\n    branch: release/v((version))\n\njobs:\n- name: test\n  plan: [...]\n\n- name: deploy-to-staging\n  plan: [...]\n\n- name: release\n  plan: [...]\nBefore Concourse v7.0.0, you might set multiple pipelines with the version information encoded in the pipeline name, e.g.:\n\n$ fly -t example set-pipeline \\\n    --pipeline release-1.0.x \\\n    --config template.yml \\\n    --var version=1.0.x\n$ fly -t example set-pipeline \\\n    --pipeline release-2.0.x \\\n    --config template.yml \\\n    --var version=2.0.x\nThe downside to this approach is that things can get disorganized quickly as the number of pipelines increases, which can make the UI cluttered and hard to navigate. Additionally, not everything can easily be encoded into the pipeline name, especially with the restrictions on identifiers - while it's readable in this case, it can get unwieldy as the number of variables in the template grows.\n\nThe recommended approach is to construct an Instance Group where each version has its own Instanced Pipeline:\n\n$ fly -t example set-pipeline \\\n    --pipeline release \\\n    --config template.yml \\\n    --instance-var version=1.0.x\n$ fly -t example set-pipeline \\\n    --pipeline release \\\n    --config template.yml \\\n    --instance-var version=2.0.x\nThere are only a few differences from the previous approach in terms of creating the pipelines:\n\n1. We give each Instanced Pipeline the same name (in this case, release), and\n\n2. We use the --instance-var flag instead of --var. Doing so makes the variable name and value a part of the pipeline's identifier (Managing Instanced Pipelines describes how to work with Instanced Pipelines in fly)\n\nThe -i or --instance-var flag behaves like the -y or --yaml-var, meaning instance vars can hold arbitrary YAML/JSON data. The v or --var flag, on the other hand, only defines strings. See Static vars to learn the difference between the flags\n\nNote that there are no fly commands for constructing an Instance Group - Concourse logically groups all Instanced Pipelines with the same name into a single Instance Group. Instanced Pipelines have the same pipeline semantics as other pipelines - they are just organized and identified in a different way.\n\n","depth":3,"section_tag":"instanced-pipelines"},"intercept-admin-only":{"location":"global-resources.html#intercept-admin-only","title":"Intercepting check containers is no longer safe","text":"Now that check containers are shared across teams, it would be dangerous to allow anyone to fly intercept to check containers. For this reason, this capability is limited to admin users.\n\nWe recognize that this will make it a bit more difficult for end users to debug things like failing checks. We plan to improve this by introducing a way to provision a new check container to facilitate debugging. See 3344 for more information.\n\n","depth":5,"section_tag":"intercept-admin-only"},"internals":{"location":"internals.html","title":"Internals","text":"This section provides a deeper understanding of some of the concepts surrounding Concourse.\n\nAn understanding of the basics of Concourse concepts, such as pipelines, jobs, etc, is recommended as parts of this section might assume a level of knowledge from them. This section is not necessary for using Concourse but are more for experienced users that want to dig deeper into how Concourse works.\n\n","depth":2,"section_tag":"internals"},"intro-to-yaml":{"location":"config-basics.html#intro-to-yaml","title":"Intro to YAML","text":"YAML is a human-friendly syntax for writing structured documents. You can think of it as JSON without the sharp edges.\n\nHere's a quick example demonstrating common YAML syntax:\n\n# commented lines are prefixed with the '#' character\n\n# strings\nquoted_string: \"bar\"\nunquoted_string: hello world!\nmultiline_string: |\n  hello, world!\n  this is one big string with a trailing linebreak!\n\n# arrays\narray: [hello, world]\nmultiline_array:\n- hello\n- world\n\n# objects\nobject: {one: uno, two: dos}\nmultiline_object:\n  one: uno\n  two: dos\n\n# boolean values\nbooleans: [true, false]\n\n# numeric values\nnumeric: [1234, 12.34]\n","depth":3,"section_tag":"intro-to-yaml"},"java-example":{"location":"java-example.html","title":"Java application testing example","text":"You can run the tests for a Java application.\n\n","depth":2,"section_tag":"java-example"},"jobs":{"location":"jobs.html","title":"Jobs","text":"Jobs determine the actions of your pipeline. They determine how resources progress through it, and how the pipeline is visualized.\n\nThe most important attribute of a job is its build plan, configured as job.plan. This determines the sequence of Steps to execute in any builds of the job.\n\nA pipeline's jobs are listed under pipeline.jobs with the following schema:\n\n","depth":2,"section_tag":"jobs"},"jobs-and-resources-instanced-pipelines":{"location":"instanced-pipelines.html#jobs-and-resources-instanced-pipelines","title":"Managing Jobs and Resources","text":"Managing Jobs and Managing Resources walk you through some of the commands you can use to manage jobs and resources within pipelines. For Instanced Pipelines, we need to encode the instance vars in the --job and --resource flags. These flags now take the form:\n\n--job group/var1:value1,var2:value2/job\nand:\n\n--resource group/var1:value1,var2:value2/resource\nFor instance, to trigger the test job of release/version:1.0.x, we issue the following command:\n\n$ fly -t example trigger-job --job release/version:1.0.x/test\nTo check the repo resource of release/version:1.0.x, we issue the following command:\n\n$ fly -t example check-resource --resource release/version:1.0.x/repo\n","depth":5,"section_tag":"jobs-and-resources-instanced-pipelines"},"kubernetes-credential-lookup-rules":{"location":"kubernetes-credential-manager.html#kubernetes-credential-lookup-rules","title":"Credential lookup rules","text":"When resolving a parameter such as ((foo)), Concourse will lookup for it in the following order in the namespace configured for that team:\n\n* Name:         PIPELINE_NAME.foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  value:        32 bytes\n\n* Name:         foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  value:        32 bytes\n\nYou can also have nested fields, which can be accessed using . syntax, e.g. ((foo.bar)):\n\n* Name:         PIPELINE_NAME.foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  bar:          32 bytes\n\n* Name:         foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  bar:          32 bytes\n\nThe prefix prepended to the namespace used by Concourse to search for secrets (in the examples above, concourse-) can be changed by configuring the following in the Running a web node:\n\nCONCOURSE_KUBERNETES_NAMESPACE_PREFIX=some-other-prefix-\nIf an action is being run in a one-off build, Concourse will not include the pipeline name in the secret that it looks for.\n\n","depth":5,"section_tag":"kubernetes-credential-lookup-rules"},"kubernetes-credential-manager":{"location":"kubernetes-credential-manager.html","title":"Kubernetes Credential Manager","text":"Concourse can be configured to pull credentials from Kubernetes secret objects.\n\nTo configure it, either enable the in-cluster client by setting the following environment variable on the Running a web node:\n\nCONCOURSE_KUBERNETES_IN_CLUSTER=true\nor set the path to a kubeconfig file:\n\nCONCOURSE_KUBERNETES_CONFIG_PATH=~/.kube/config\n","depth":4,"section_tag":"kubernetes-credential-manager"},"ldap-auth":{"location":"ldap-auth.html","title":"LDAP auth","text":"The LDAP provider can be used for operators who wish to authenticate their users against an LDAP server.\n\n","depth":4,"section_tag":"ldap-auth"},"ldap-authentication":{"location":"ldap-auth.html#ldap-authentication","title":"Authentication","text":"The LDAP provider is configured by pointing it to an LDAP host with a read-only bind DN and password. This bind DN and password is used for authenticating with the LDAP host and querying the users.\n\nAdditionally, the base DN under which users are searched as well as the attribute of the users to associate to 'usernames' must also be configured.\n\nThese can be specified via env to the Running a web node like so:\n\nCONCOURSE_LDAP_DISPLAY_NAME=Acme # optional; default \"LDAP\"\nCONCOURSE_LDAP_HOST=ldap.example.com # port defaults to 389 or 636\nCONCOURSE_LDAP_BIND_DN='cn=read-only-admin,dc=example,dc=com'\nCONCOURSE_LDAP_BIND_PW=read-only-admin-password\nCONCOURSE_LDAP_USER_SEARCH_BASE_DN='cn=users,dc=example,dc=com'\nCONCOURSE_LDAP_USER_SEARCH_USERNAME=uid\nTo configure TLS, you may need to set a CA cert:\n\nCONCOURSE_LDAP_CA_CERT=/path/to/ca_cert\nIf your LDAP host does not use TLS, you must set:\n\nCONCOURSE_LDAP_INSECURE_NO_SSL=true\nTo fine-tune which users are queried, you can specify a user search filter like so:\n\nCONCOURSE_LDAP_USER_SEARCH_FILTER='(objectClass=person)'\nTo set which user attributes map to the token claims, you can set the following:\n\nCONCOURSE_LDAP_USER_SEARCH_ID_ATTR=uid         # default\nCONCOURSE_LDAP_USER_SEARCH_EMAIL_ATTR=mail     # default\nCONCOURSE_LDAP_USER_SEARCH_NAME_ATTR=some-attr # no default\n","depth":5,"section_tag":"ldap-authentication"},"ldap-authorization":{"location":"ldap-auth.html#ldap-authorization","title":"Authorization","text":"LDAP users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--ldap-user=USERNAME: Authorize an individual user.\n\n\n--ldap-group=GROUP_NAME: Authorize anyone from the group.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --ldap-user my-username \\\n    --ldap-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  ldap:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"ldap-authorization"},"lets-encrypt":{"location":"concourse-web.html#lets-encrypt","title":"TLS via Let's Encrypt","text":"Concourse can be configured to automatically acquire a TLS certificate via Let's Encrypt:\n\n# Enable TLS\nCONCOURSE_TLS_BIND_PORT=443\n\n# Enable Let's Encrypt\nCONCOURSE_ENABLE_LETS_ENCRYPT=true\nConcourse's Let's Encrypt integration works by storing the TLS certificate and key in the database, so it is imperative that you enable database encryption as well.\n\nBy default, Concourse will reach out to Let's Encrypt's ACME CA directory. An alernative URL can be configured like so:\n\nCONCOURSE_LETS_ENCRYPT_ACME_URL=https://acme.example.com/directory\nIn order to negotiate the certificate, your web node must be reachable by the ACME server. There are intentionally no publicly listed IP addresses to whitelist, so this typically means just making your web node publicly reachable.\n\n","depth":5,"section_tag":"lets-encrypt"},"limit-active-containers-strategy":{"location":"container-placement.html#limit-active-containers-strategy","title":"The limit-active-containers strategy","text":"limit-active-containers is an experimental feature.\n\nThe limit-active-containers placement strategy rejects workers that already have too many containers. It makes no effort to find the worker with the fewest number of containers present, and is therefore most useful when combined with other placement stragies by Chaining Placement Strategies.\n\nmax-active-containers-per-worker can be set to an integer of 1 or more, in which case a worker will not execute more than that amount of containers. If unset (or set to a value of 0), the limit-active-containers strategy has no effect - if this is your only placement strategy, workers will be chosen at random.\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-containers\nCONCOURSE_MAX_ACTIVE_CONTAINERS_PER_WORKER=200\n","depth":4,"section_tag":"limit-active-containers-strategy"},"limit-active-tasks-strategy":{"location":"container-placement.html#limit-active-tasks-strategy","title":"The limit-active-tasks strategy","text":"limit-active-tasks is an experimental feature.\n\nWhen selecting the limit-active-tasks placement strategy, each task executed on a worker will increase the number of \"active tasks\" on that worker by one. When the task completes the number is decreased by one. The Running a web node then places get, put and task containers on the worker that currently has the least amount of active tasks.\n\nAdditionally max-active-tasks-per-worker can be set to an integer of 1 or more, in which case a worker will not execute more than that amount of tasks. A value of 0 means that there is no limit on the maximum number of active tasks on the workers. If no worker can be selected because all of them already have max-active-tasks-per-worker active tasks, then the task will wait for a free worker, periodically polling the pool. The metric concourse_steps_waiting{type=\"task\"} is emitted to monitor these events. Note that the parameter does not apply to get and put steps which will always be scheduled on the worker with the fewest active tasks.\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-tasks\n and, optionally CONCOURSE_MAX_ACTIVE_TASKS_PER_WORKER=1\n\n\n","depth":4,"section_tag":"limit-active-tasks-strategy"},"limit-active-volumes-strategy":{"location":"container-placement.html#limit-active-volumes-strategy","title":"The limit-active-volumes strategy","text":"limit-active-volumes is an experimental feature.\n\nThe limit-active-volumes placement strategy rejects workers that already have too many volumes. It makes no effort to find the worker with the fewest number of volumes present, and is therefore most useful when combined with other placement stragies by Chaining Placement Strategies.\n\nmax-active-volumes-per-worker can be set to be an integer of 1 or more, in which case a worker will not execute more than that amount of volumes. If unset (or set to a value of 0), the limit-active-volumes strategy has no effect - if this is your only placement strategy, workers will be chosen at random.\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-volumes\nCONCOURSE_MAX_ACTIVE_VOLUMES_PER_WORKER=200\n","depth":4,"section_tag":"limit-active-volumes-strategy"},"linux-workers":{"location":"upgrading-concourse.html#linux-workers","title":"Linux Workers","text":"The Linux tarball from the Github release page contains extra assets that you will want to ensure are also upgraded at the same time. Make sure you overwrite the contents of the following directories:\n\n* concourse/bin/... - Other binaries like gdn, runc, and containerd are in this directory\n\n* concourse/resource-types/... - The location of the default resource-types included with each Concourse release\n\n","depth":5,"section_tag":"linux-workers"},"load-var-step":{"location":"jobs.html#load-var-step","title":"load_var step","text":"","depth":3,"section_tag":"steps"},"local-auth":{"location":"local-auth.html","title":"Local User auth","text":"Local User auth is a primitive username/password-based auth mechanism. All users and passwords are configured statically.\n\nIn general, we recommend configuring one of the other providers instead, but for small deployments with only a few users, local user auth may be all you need.\n\n","depth":4,"section_tag":"local-auth"},"local-authentication":{"location":"local-auth.html#local-authentication","title":"Authentication","text":"Local users are configured on the Running a web node by setting the following env:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass,anotheruser:anotherpass\nThis configures two users, myuser and anotheruser, with their corresponding passwords.\n\nWhen local users are configured, the log-in page in the web UI will show a username/password prompt.\n\nLocal users can also log in via fly login with the --username and --password flags.\n\n","depth":5,"section_tag":"local-authentication"},"local-authorization":{"location":"local-auth.html#local-authorization","title":"Authorization","text":"Local users are granted access to teams via fly set-team, using the --local-user flag:\n\n$ fly set-team -n my-team \\\n    --local-user some_username\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  local:\n    users: [\"some_username\"]\n","depth":5,"section_tag":"local-authorization"},"local-vars":{"location":"vars.html#local-vars","title":"The \".\" var source","text":"The special var source name . refers to a \"local var source.\"\n\nThe precise scope for these \"local vars\" depends on where they're being used. Currently the only mechanism that uses the local var source is the load_var step, which sets a var in a local var source provided to all steps executed in the build.\n\n","depth":3,"section_tag":"local-vars"},"main-team":{"location":"main-team.html","title":"The main team","text":"Out of the box, Concourse comes with a single team called main.\n\nThe main team is an admin team, meaning members (specifically, users with the owner role) can create and update other teams. Currently there is no way to promote a team to become an admin team, so main is a special-case.\n\nThe main team is different in that all flags normally passed to fly set-team are instead passed to the concourse web command, prefixed with --main-team-. The values set in these flags take effect whenever the web node starts up. This is done so that you can't get locked out.\n\nTo learn how to configure your main team, continue on to the appropriate section for your auth provider of choice under Configuring Auth.\n\n","depth":3,"section_tag":"main-team"},"managing-instanced-pipelines":{"location":"instanced-pipelines.html#managing-instanced-pipelines","title":"Managing Instanced Pipelines","text":"Instanced Pipelines can be managed via fly as described in Managing Pipelines, with one important distinction - since instance vars are a part of the pipeline's identifier, the --pipeline flag must include both the name of the Instance Group as well as the instance vars. The --pipeline flag takes the form:\n\n--pipeline group/var1:value1,var2:value2\nAs a concrete example, to pause the release Instanced Pipeline with version:1.0.x, you would issue the following command:\n\n$ fly -t example pause-pipeline --pipeline release/version:1.0.x\nLet's look at a more complicated example - suppose you have an Instanced Pipeline that was set using one of the following commands:\n\n$ fly -t example set-pipeline \\\n    --pipeline upgrade \\\n    --config template.yml \\\n    --instance-var version.from=1.0.0 \\\n    --instance-var version.to=2.0.0 \\\n    --instance-var branch=feature/foo\n# ...or equivalently\n$ fly -t example set-pipeline \\\n    --pipeline upgrade \\\n    --config template.yml \\\n    --instance-var 'version={from: 1.0.0, to: 2.0.0}' \\\n    --instance-var branch=feature/foo\nUsing dot-notation here (as in the first command) is recommended since YAML is finicky about spaces.\n\nFor instance, had we used --instance-var 'version={from:1.0.0, to:2.0.0}' (no spaces between keys and values), we would end up with the following object (represented as JSON):\n\n{\"from:1.0.0\": null, \"to:2.0.0\": null}\n\nSpecifying each field individually using dot-notation is harder to mess up.\n\nHere, there are two instance vars: version, that contains the object {\"from\": \"1.0.0\", \"to\": \"2.0.0\"}, and branch, that contains the string \"feature/foo\". In order to pause this pipeline, you could issue one of the following commands:\n\n$ fly -t example pause-pipeline \\\n    --pipeline 'upgrade/version.from:1.0.0,version.to:2.0.0,branch:\"feature/foo\"'\n# ... or equivalently\n$ fly -t example pause-pipeline \\\n    --pipeline 'upgrade/version:{from: 1.0.0, to: 2.0.0},branch:\"feature/foo\"'\n For accessing sub-fields of an object, we can either use dot-notation as described in Providing static values for vars, or we can define the object in full as valid YAML.\n\nIf the instance var name or value contains a \"special character\" (., ,, /, {, }, or whitespace), it must be surrounded by double quotes \".  Depending on your shell, this usually means the entire flag must be quoted, since otherwise your shell will try to expand the quotes.\n\n","depth":4,"section_tag":"managing-instanced-pipelines"},"managing-jobs":{"location":"jobs.html#managing-jobs","title":"Managing Jobs","text":"","depth":3,"section_tag":"managing-jobs"},"managing-pipelines":{"location":"managing-pipelines.html","title":"Managing Pipelines","text":"","depth":3,"section_tag":"managing-pipelines"},"managing-resource-types":{"location":"managing-resource-types.html","title":"Managing Resource Types","text":"","depth":3,"section_tag":"managing-resource-types"},"managing-resources":{"location":"managing-resources.html","title":"Managing Resources","text":"","depth":3,"section_tag":"managing-resources"},"managing-teams":{"location":"managing-teams.html","title":"Managing Teams","text":"","depth":3,"section_tag":"managing-teams"},"manual-trigger-example":{"location":"manual-trigger-example.html","title":"Manually triggered job example","text":"A job can be triggered by a resource. After it's complete, the next job can run automatically or manually.\n\n","depth":2,"section_tag":"manual-trigger-example"},"merging-branches":{"location":"basic-git-operations.html#merging-branches","title":"Merging Branches","text":"Here is how you can merge two branches. Common if you are using gitflow and need to merge a dev branch into main every so often.\n\nresources:\n- name: repo-main\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/user/my-repo\n    branch: main\n\n- name: repo-dev\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/user/my-repo\n    branch: dev\n\njobs:\n- name: merge-dev-into-main\n  plan:\n  - get: repo-dev\n  - put: repo-main\n    params:\n      repository: repo-dev\n      merge: true\n","depth":5,"section_tag":"merging-branches"},"metrics":{"location":"metrics.html","title":"Metrics","text":"Metrics are essential in understanding how any large system is behaving and performing. Concourse can emit metrics about both the system health itself and about the builds that it is running. Operators can tap into these metrics in order to observe the health of the system.\n\nIn the spirit of openness, the metrics from our deployment are public. We consider it a bug to emit anything sensitive or secret into our metrics pipeline.\n\n","depth":3,"section_tag":"metrics"},"microsoft-auth":{"location":"microsoft-auth.html","title":"Microsoft auth","text":"A Concourse server can authenticate against Microsoft Azure AD to leverage its permission model.\n\n","depth":4,"section_tag":"microsoft-auth"},"microsoft-authentication":{"location":"microsoft-auth.html#microsoft-authentication","title":"Authentication","text":"You'll need to register a new application on Azure.\n\nThe \"Callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by Microsoft - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_MICROSOFT_CLIENT_ID=myclientid\nCONCOURSE_MICROSOFT_CLIENT_SECRET=myclientsecret\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"microsoft-authentication"},"multi-branch-cleaning-up":{"location":"multi-branch-workflows.html#multi-branch-cleaning-up","title":"Cleaning Up Old Workspaces","text":"With the setup described in Tracking Branches, Concourse will automatically archive any pipelines for branches that get removed. However, Concourse doesn't know that it should destroy Terraform workspaces when a branch is removed. To accomplish this, we can yet again make use of the Terraform resource to destroy these workspaces. We'll add another job to the tracker pipeline that figures out which workspaces don't belong to an active branch and destroy them.\n\nexamples/pipelines/multi-branch/tracker.yml resource_types:\n- name: git-branches\n  ...\n\n- name: terraform\n  type: registry-image\n  source:\n    repository: ljfranklin/terraform-resource\n\nresources:\n- name: feature-branches\n  ...\n\n- name: examples\n  ...\n\n- name: staging-env\n  type: terraform\n  source:\n    backend_type: gcs\n    backend_config: \u0026terraform_backend_config\n      bucket: concourse-examples\n      prefix: multi-branch/terraform\n      credentials: ((gcp_service_account_key))\n\njobs:\n- name: set-feature-pipelines\n  ...\n\n- name: cleanup-inactive-workspaces\n  plan:\n  - in_parallel:\n    - get: feature-branches\n      passed: [set-feature-pipelines]\n      trigger: true\n    - get: examples\n  - task: find-inactive-workspaces\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: {repository: hashicorp/terraform}\n      inputs:\n      - name: feature-branches\n      outputs:\n      - name: extra-workspaces\n      params:\n        TERRAFORM_BACKEND_CONFIG:\n          gcs: *terraform_backend_config\n      run:\n        path: sh\n        args:\n        - -c\n        - |\n          set -euo pipefail\n\n          apk add -q jq\n\n          active_features=\"$(jq '[.[].groups.feature]' feature-branches/branches.json)\"\n\n          jq -n \"{terraform: {backend: $TERRAFORM_BACKEND_CONFIG}}\" \u003e backend.tf.json\n          terraform init\n\n          # List all active workspaces, ignoring the default workspace\n          active_workspaces=\"$(terraform workspace list | grep -v '^[*]' | tr -d ' ' | jq --raw-input --slurp 'split(\"\\n\") | map(select(. != \"\"))')\"\n\n          jq -n \"$active_workspaces - $active_features\" \u003e extra-workspaces/workspaces.json\n  - load_var: extra_workspaces\n    file: extra-workspaces/workspaces.json\n  - across:\n    - var: workspace\n      values: ((.:extra_workspaces))\n    put: staging-env\n    params:\n      terraform_source: examples/terraform/staging\n      env_name: ((.:workspace))\n      action: destroy\n    get_params:\n      action: destroy\n\n\n","depth":5,"section_tag":"multi-branch-cleaning-up"},"multi-branch-test-build-deploy":{"location":"multi-branch-workflows.html#multi-branch-test-build-deploy","title":"Test, Build \u0026 Deploy","text":"We'll start out by defining the pipeline that should run for each active branch. For this example, we'll be working with the following sample Go application.\n\nOur pipeline will have three stages:\n\n1. Run unit tests\n\n2. Build and upload a binary to a blobstore (in our case, we'll use Google Cloud Storage)\n\n3. Trigger a terraform apply to deploy our app to a staging environment. The Terraform module we'll use here doesn't actually provision any infrastructure, and is just used as an example\n\nSince the pipeline config is intended to be used as a template for multiple different branches, we can use Vars to parameterize the config. In particular, we'll use the vars ((feature)) and ((branch)), which represent the name of the feature and the name of the branch, respectively.\n\nBelow is the full pipeline config:\n\nexamples/pipelines/multi-branch/template.yml resource_types:\n- name: terraform\n  type: registry-image\n  source:\n    repository: ljfranklin/terraform-resource\n\n- name: gcs\n  type: registry-image\n  source:\n    repository: frodenas/gcs-resource\n\nresources:\n- name: branch\n  type: git\n  source:\n    uri: https://github.com/concourse/examples\n    branch: ((branch))\n\n- name: examples\n  type: git\n  source:\n    uri: https://github.com/concourse/examples\n\n- name: build-artifact\n  type: gcs\n  source:\n    bucket: concourse-examples\n    json_key: ((gcp_service_account_key))\n    regexp: multi-branch/features/((feature))/my-app-(.+)\\.tgz\n\n- name: staging-env\n  type: terraform\n  source:\n    env_name: ((feature))\n    backend_type: gcs\n    backend_config:\n      bucket: concourse-examples\n      prefix: multi-branch/terraform\n      credentials: ((gcp_service_account_key))\n\njobs:\n- name: test\n  plan:\n  - in_parallel:\n    - get: branch\n      trigger: true\n    - get: examples\n  - task: unit\n    file: examples/tasks/go-test.yml\n    input_mapping: {repo: branch}\n    params: {MODULE: apps/golang}\n\n- name: build\n  plan:\n  - in_parallel:\n    - get: branch\n      passed: [test]\n      trigger: true\n    - get: examples\n  - task: build\n    file: examples/tasks/go-build.yml\n    params:\n      MODULE: apps/golang\n      BINARY_NAME: my-app\n    input_mapping: {repo: branch}\n  - put: build-artifact\n    params: {file: \"binary/my-app-*.tgz\"}\n\n- name: deploy\n  plan:\n  - in_parallel:\n    - get: build-artifact\n      passed: [build]\n      trigger: true\n    - get: examples\n  - load_var: bundle_url\n    file: build-artifact/url\n  - put: staging-env\n    params:\n      terraform_source: examples/terraform/staging\n      vars: {bundle_url: ((.:bundle_url))}\n\n\n","depth":5,"section_tag":"multi-branch-test-build-deploy"},"multi-branch-tracking-branches":{"location":"multi-branch-workflows.html#multi-branch-tracking-branches","title":"Tracking Branches","text":"In addition to the branch pipeline template, we'll also need a pipeline to track the list of branches and set a pipeline for each one.\n\nTo track the list of branches in a repository, we can use aoldershaw/git-branches-resource. This resource_type emits a new resource version whenever a branch is created or deleted. It also lets us filter the list of branches by a regular expression. In this case, let's assume our feature branches match the regular expression feature/.*.\n\nBelow is the full pipeline config for this tracker pipeline:\n\nexamples/pipelines/multi-branch/tracker.yml resource_types:\n- name: git-branches\n  type: registry-image\n  source:\n    repository: aoldershaw/git-branches-resource\n\nresources:\n- name: feature-branches\n  type: git-branches\n  source:\n    uri: https://github.com/concourse/examples\n    # The \"(?P\u003cname\u003epattern)\" syntax defines a named capture group.\n    # aoldershaw/git-branches-resource emits the value of each named capture\n    # group under the `groups` key.\n    #\n    # e.g. feature/some-feature ==\u003e {\"groups\": {\"feature\": \"some-feature\"}}\n    branch_regex: 'feature/(?P\u003cfeature\u003e.*)'\n\n- name: examples\n  type: git\n  source:\n    uri: https://github.com/concourse/examples\n\njobs:\n- name: set-feature-pipelines\n  plan:\n  - in_parallel:\n    - get: feature-branches\n      trigger: true\n    - get: examples\n  - load_var: branches\n    file: feature-branches/branches.json\n  - across:\n    - var: branch\n      values: ((.:branches))\n    set_pipeline: dev\n    file: examples/pipelines/multi-branch/template.yml\n    instance_vars: {feature: ((.:branch.groups.feature))}\n    vars: {branch: ((.:branch.name))}\n\n\nWe set each pipeline as an instanced pipeline - this will result in Concourse grouping all of the related dev pipelines in the UI.\n\n","depth":5,"section_tag":"multi-branch-tracking-branches"},"multi-branch-workflows":{"location":"multi-branch-workflows.html","title":"Multi-Branch Workflows","text":"Teams may make use of multiple branches for their development. For instance, some teams create feature branches while working on new functionality - once this functionality is ready, the branch will be merged into the main branch and the feature branch will be deleted.\n\nWhile a feature is under development, you'll often want to run tests against the feature branch and possibly deploy to a staging environment. To model this in Concourse, you'll need to have a pipeline for each active feature branch. Manually setting (and eventually archiving) a pipeline for each feature branch would be quite a burden. For this type of workflow, Concourse has a few important tools to help you out: the set_pipeline step, step.across, and instanced pipelines.\n\nstep.across and instanced pipelines are both experimental features, and must be enabled with the feature flags CONCOURSE_ENABLE_ACROSS_STEP and CONCOURSE_ENABLE_PIPELINE_INSTANCES, respectively.\n\nIn this guide, we'll cover:\n\n1. Writing a pipeline to Test, Build \u0026 Deploy a branch to a staging environment. We'll use Terraform for our deployment\n\n2. Tracking Branches in a repository; for each branch, we'll set a pipeline (using the set_pipeline step and step.across)\n\n3. Automatically Cleaning Up Old Workspaces after branches get merged or deleted\n\n","depth":4,"section_tag":"multi-branch-workflows"},"multiple-worker-keys":{"location":"concourse-generate-key.html#multiple-worker-keys","title":"Multiple Worker Keys","text":"Currently you have one worker_key. You can use this one key-pair with multiple Running a worker nodes. Another good strategy is to have each worker or group of workers use a key that's unique to that one worker or group of workers.\n\nIn the second case you will end up with multiple private and public worker keys. The Running a web node needs to know about all of the public worker keys.  To pass all public worker keys to the Running a web node create a file that contains all of the worker public keys. A common name for this file is authorized_worker_keys. The file should look like this, with one public key per line.\n\n$ cat authorized_worker_keys\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCgKtVnbGRJ7Y63QKoO+loS...\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDU6lA4gSRYIc4MXzphJ2l5...\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDgNU7KBz/QQusPO52pNcea...\nYou should now have all the necessary keys needed to deploy Web and Worker nodes.\n\n","depth":4,"section_tag":"multiple-worker-keys"},"nodejs-example":{"location":"nodejs-example.html","title":"Nodejs application testing example","text":"You can run the tests for a Nodejs application.\n\n","depth":2,"section_tag":"nodejs-example"},"note-on-allow-host-access":{"location":"concourse-worker.html#note-on-allow-host-access","title":"A note on allowing host access and DNS proxy","text":"Setting allow-host-access will, well, allow containers to access your host VM's network. If you don't trust your container workloads, you may not want to allow this.  With host network access, containers will be able to reach out to any other locally running network processes running on the worker including the garden and baggageclaim servers which would allow them to issue commands and manipulate other containers and volumes on the same worker.\n\nSetting dns-proxy-enable will also enable allow-host-access (since the dns proxy will be run on the host, therefore requiring host access be enabled).\n\n","depth":7,"section_tag":"note-on-allow-host-access"},"observation":{"location":"observation.html","title":"Observation","text":"This section outlines everything you need to know for observing the state of your pipelines.\n\n","depth":2,"section_tag":"observation"},"opa":{"location":"opa.html","title":"Open Policy Agent Integration","text":"The Open Policy Agent (OPA, pronounced “oh-pa”) is an open source, general-purpose policy engine that unifies policy enforcement across the stack.\n\nOPA allows you to create arbitrary rules within Concourse without having to add a new feature to Concourse. You could even recreate Concourse's RBAC system using OPA.\n\nMore likely use-cases are to enforce rules your organization may have, such as not using certain container images or disallowing the use of privileged workloads. With OPA you can be as general or fine-grained as you want, enforcing these rules at the team or pipeline level.\n\nThe next few sections explain how to configure Concourse to talk to an OPA server and how to write OPA rules for Concourse.\n\n","depth":3,"section_tag":"opa"},"operation":{"location":"operation.html","title":"Operation","text":"The following sections describes operator-focused features and tools that Concourse provides, such as monitoring and credential management.\n\nThese concepts are not required to operate Concourse, but are for users that are looking to extend the capabilities of managing a Concourse deployment. For users that are new to these concepts, we do recommend learning how to set up Credential Management and Encryption.\n\n","depth":2,"section_tag":"operation"},"overview":{"location":"tutorial-inputs-outputs.html#overview","title":"Overview","text":"This section is going to go over how to pass data between different steps in a job. We'll continue building on our hello-world.yml pipeline.\n\nIn the previous section we learned that steps are where we tell Concourse what to run (i.e. run my tests, run this bash script, build this image, etc.). We are going to expand on the concept of steps and show you how to pass artifacts/files between tasks.\n\n","depth":4,"section_tag":"overview"},"parallelizing-get-steps-in-jobs":{"location":"common-pipeline-practices.html#parallelizing-get-steps-in-jobs","title":"Parallelizing Get Steps in Jobs","text":"All jobs usually have get steps as their first set of steps.\n\njobs:\n- name: awesome-job\n  plan:\n  - get: cool-code\n  - get: funny-binary\n  - get: the-weather\n  - task: business-stuff\nTo reduce the waiting time to the length of longest running get step, put all get steps under an in_parallel step.\n\njobs:\n- name: awesome-job\n  plan:\n  - in_parallel:\n    - get: cool-code\n    - get: funny-binary\n    - get: the-weather\n  - task: business-stuff\n","depth":5,"section_tag":"parallelizing-get-steps-in-jobs"},"passing-outputs-to-another-task":{"location":"tutorial-inputs-outputs.html#passing-outputs-to-another-task","title":"Passing outputs to another task","text":"To pass artifacts from one task to another, the first task must declare the artifact as an output. The second task must then declare the same artifact as an input. Let's update the pipeline to do the following:\n\n* Have the first task create a file inside the-artifact\n\n* Create a second task to read the file inside the-artifact from the previous step\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      outputs:\n      - name: the-artifact\n      run:\n        # This is a neat way of embedding a script into a task\n        path: sh\n        args:\n        - -cx\n        - |\n          ls -l .\n          echo \"hello from another step!\" \u003e the-artifact/message\n  # Add a second task that reads the contents of the-artifact/message\n  - task: read-the-artifact\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      # To recieve \"the-artifact\", specify it as an input\n      inputs:\n      - name: the-artifact\n      run:\n        path: sh\n        args:\n        - -cx\n        - |\n          ls -l .\n          cat the-artifact/message\nUpdate the pipeline and trigger the job: $ fly -t tutorial set-pipeline -p hello-world -c hello-world.yml\n$ fly -t tutorial trigger-job --job hello-world/hello-world-job --watch\n\ninitializing\nselected worker: 57d7419112ca\nrunning sh -cx ls -l .\necho \"hello from another step!\" \u003e the-artifact/message\n\n+ ls -l .\ntotal 4\ndrwxr-xr-x    2 root     root          4096 Feb 26 19:09 the-artifact\n+ echo 'hello from another step!'\ninitializing\nselected worker: 57d7419112ca\nrunning sh -cx ls -l .\ncat the-artifact/message\n\n+ ls -l .\ntotal 4\ndrwxr-xr-x    1 root     root          4096 Feb 26 19:09 the-artifact\n+ cat the-artifact/message\nhello from another step!\nsucceeded\n\n\nWith the above pipeline we can see that the file made in the first step is made available in the second step. You can view the same build output from the web UI. Click on the job tile to see the individual steps. Click on each step to expand it and see the build logs.\n\n","depth":4,"section_tag":"passing-outputs-to-another-task"},"php-example":{"location":"php-example.html","title":"PHP application testing example","text":"You can run the tests for a PHP application.\n\n","depth":2,"section_tag":"php-example"},"pipeline-guides":{"location":"pipeline-guides.html","title":"Pipeline Guides","text":"","depth":3,"section_tag":"pipeline-guides"},"pipeline-static-vars":{"location":"setting-pipelines.html#pipeline-static-vars","title":"Providing static values for vars","text":"The pipeline configuration can contain Vars which may be replaced with static values or loaded at runtime. This allows for credentials to be extracted from a pipeline config, making it safe to check in to a public repository or pass around.\n\nFor example, if you have a pipeline.yml as follows:\n\nresources:\n- name: private-repo\n  type: git\n  source:\n    uri: git@...\n    branch: master\n    private_key: ((private-repo-key))\n...you could then configure this pipeline like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"private-repo-key=$(cat id_rsa)\"\nOr, if you had a vars.yml as follows:\n\nprivate-repo-key: |\n  -----BEGIN RSA PRIVATE KEY-----\n  ...\n  -----END RSA PRIVATE KEY-----\n...you could configure it like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from vars.yml\nYou can use nested fields in your pipeline.yml as follows: resources:\n- name: private-repo\n  type: git\n  source:\n    uri: git@((repo.uri))\n    branch: ((repo.branch))\n    private_key: ((\"github.com\".private-repo-key))\n\n\n...you could configure it by --load-vars-from with a vars.yml as follows:\n\nrepo:\n  uri: github.com/...\n  branch: master\ngithub.com:\n  private-repo-key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    ...\n    -----END RSA PRIVATE KEY-----\n...or you could also configure it by passing the vars as flags:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"repo.uri=github.com\" \\\n    --var \"repo.branch=master\" \\\n    --var \"\\\"github.com\\\".private-repo-key=$(cat id_rsa)\"\nWhen configuring a pipeline, any vars not provided statically will be left to resolve at runtime. To check that all vars are resolveable, you can pass the --check-creds flag:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from vars.yml \\\n    --check-creds\nThis will fill in all statically-provided vars and then attempt to resolve all remanining vars server-side. If any fail to resolve, configuring the pipeline will fail.\n\n","depth":5,"section_tag":"pipeline-static-vars"},"pipeline-vars-example":{"location":"pipeline-vars-example.html","title":"Pipeline ((vars)) example","text":"You can use params in a pipelines configuration file.\n\n","depth":2,"section_tag":"pipeline-vars-example"},"pipeline-visibility":{"location":"observation.html#pipeline-visibility","title":"Pipeline Visibility","text":"Pipelines may be exposed so that they can be monitored without having to authenticate. For more information, see Pipeline \u0026 Build Visibility.\n\n","depth":3,"section_tag":"pipeline-visibility"},"pipelines":{"location":"pipelines.html","title":"Pipelines","text":"A pipeline is the result of configuring Jobs and Resources together. When you configure a pipeline, it takes on a life of its own, to continuously detect resource versions and automatically queue new builds for jobs as they have new available inputs.\n\nPipelines are configured via fly set-pipeline or the set_pipeline step as declarative YAML files which conform to the following schema:\n\n","depth":2,"section_tag":"pipelines"},"pointing-to-external-dns-servers":{"location":"concourse-worker.html#pointing-to-external-dns-servers","title":"Pointing to external DNS servers","text":"If you have no need for special DNS resolution within your Concourse containers, you can configure your containers to use specific DNS server addresses external to the VM.\n\nThe Guardian and containerd runtimes can have their DNS servers configured with flags or envs vars.\n\nconcourse worker --containerd-dns-server=\"1.1.1.1\" --containerd-dns-server=\"8.8.8.8\"\n# containerd runtime\nCONCOURSE_CONTAINERD_DNS_SERVER=\"1.1.1.1,8.8.8.8\"\n# Guardian runtime\nCONCOURSE_GARDEN_DNS_SERVER=\"1.1.1.1,8.8.8.8\"\n[server]\n; configure Google DNS\ndns-server=8.8.8.8\ndns-server=8.8.4.4\nTo verify this solves your problem you can fly intercept into a container and check which nameservers are in /etc/resolv.conf:\n\n$ fly -t ci intercept -j my-pipeline/the-job\nbash-5.0$ cat /etc/resolv.conf\nnameserver 1.1.1.1\nnameserver 8.8.8.8\nbash-5.0$ ping google.com\nPING google.com (108.177.111.139): 56 data bytes\n64 bytes from 108.177.111.139: seq=0 ttl=47 time=2.672 ms\n64 bytes from 108.177.111.139: seq=1 ttl=47 time=0.911 ms\n","depth":7,"section_tag":"pointing-to-external-dns-servers"},"postgresql-node":{"location":"postgresql-node.html","title":"Running a PostgreSQL node","text":"Concourse uses PostgreSQL for storing all data and coordinating work in a multi-Running a web node installation.\n\n","depth":3,"section_tag":"postgresql-node"},"project":{"location":"project.html","title":"Project","text":"Concourse started as a side-project by @vito (hi!) and @xoebus in 2014. Over time Concourse has grown into a dedicated community with an open governance model and contributors from all around the world.\n\n","depth":1,"section_tag":"project"},"publish-the-container-image":{"location":"building-and-pushing-an-image.html#publish-the-container-image","title":"Publish the Container Image","text":"To push the container image add a put step to our job plan and tell the regstry-image resource where the tarball of the container image is.\n\nThe put step will push the container image using the information defined previously in the resource's source.\n\nresources: ... # omitting resource section from above\n\njobs:\n- name: build-and-push\n  plan:\n  - get: concourse-examples\n  - task: build-image\n    privileged: true # oci-build-task must run in a privileged container\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: concourse/oci-build-task\n      inputs:\n      - name: concourse-examples\n      outputs:\n      - name: image\n      params:\n        CONTEXT: concourse-examples/Dockerfiles/simple\n      run: # binary used to build the image\n        path: build\n  - put: simple-image\n    params:\n      image: image/image.tar\n","depth":5,"section_tag":"publish-the-container-image"},"put-step":{"location":"jobs.html#put-step","title":"put step","text":"","depth":3,"section_tag":"steps"},"put-steps":{"location":"tutorial-resources.html#put-steps","title":"Put Steps","text":"The last piece of the resource interface is put steps. Put steps are generally for pushing some change to the external system or object the resource represents. What this means will vary by resource, so again, you should carefully read the documentation for the specific resource you are using. Each resource is basically its own little application.\n\nIn order to \"put\" something, in the context of the git resource, we'll need to make a commit that can then be \"put\" to the repo.\n\nIf you haven't already, you'll probably need to provide the git resource with your private_key or a username and password, depending on the format of uri you provided.\n\nIf you're used to pushing code from your computer you probably have a private key in your ~/.ssh/ directory that you can use.\n\nresources:\n- name: repo\n  type: git\n  source:\n    # changed uri to ssh format\n    uri: git@github.com:concourse/examples.git\n    # specify branch because it's required for the put step\n    branch: master\n    private_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n\njobs:\n- name: hello-world-job\n  plan:\n  - get: repo\n    trigger: true\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      inputs:\n      - name: repo\n      run:\n        path: cat\n        args: [\"repo/README.md\"]\nNext let's change the hello-world-task to instead create a commit. Since we want the commit to propagate to future steps, specifically the put step we'll be adding soon, we will also need to add the repo as an output of the task.\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: git@github.com:concourse/examples.git\n    branch: master\n    private_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n\njobs:\n- name: hello-world-job\n  plan:\n  - get: repo\n    trigger: true\n  - task: create-commit # renamed task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: gitea/gitea # use any image that has the git cli\n      inputs:\n      - name: repo\n      outputs: # Add repo as an output\n      - name: repo\n      params: # Change these to match your ssh key info\n        EMAIL: person@example.com\n        GIT_AUTHOR_NAME: Person Doe\n      run:\n        path: sh\n        args:\n        - -cx\n        # this is just a bash script\n        - |\n          cd repo\n          date +%Y-%m-%d \u003e todays-date\n          git add ./todays-date\n          git config --global user.email $EMAIL\n          git config --global user.name $GIT_AUTHOR_NAME\n          git commit -m \"Update todays date\"\nLastly we can add a put step to push the commit made by the task. Put steps usually have various params that you'll need to set in order run the step correctly. If we read the documentation for the git resource we'll see that we need to specify repository field.\n\nIf you're wondering why we don't do git push from the task, that's for two reasons: 1. The newly made commit represents a new version and Concourse won't capture it correctly if it's pushed from the task, which is outside of the resource interface (check, get, put).\n\n2. The private_key is not available in the task so you would get an authentication error if you git pushed.\n\n\n\nresources:\n- name: repo\n  type: git\n  source:\n    uri: git@github.com:concourse/examples.git\n    branch: master\n    private_key: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n\njobs:\n- name: hello-world-job\n  plan:\n  - get: repo\n    trigger: true\n  - task: create-commit\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: gitea/gitea\n      inputs:\n      - name: repo\n      outputs:\n      - name: repo\n      params:\n        EMAIL: person@example.com\n        GIT_AUTHOR_NAME: Person Doe\n      run:\n        path: sh\n        args:\n        - -cx\n        - |\n          cd repo\n          date +%Y-%m-%d \u003e todays-date\n          git add ./todays-date\n          git config --global user.email $EMAIL\n          git config --global user.name $GIT_AUTHOR_NAME\n          git commit -m \"Update todays date\"\n  - put: repo\n    params:\n      repository: repo\nSet the pipeline and let's watch it trigger and run from the web UI.\n\n$ fly -t tutorial set-pipeline -p hello-world -c hello-world.yml\nThe job now has the repo resource to the left and right of the hello-world-job. This is how Concourse visually represents the inputs (get steps) and outputs (put steps) of a job.\n\nThat covers the basics of resources. It is best of think of each resource as a little application, so carefully read any documentation the resource author has provided. Every resource behaves a little differently based on the external system it is representing.\n\n","depth":4,"section_tag":"put-steps"},"putting-task-configs-in-files":{"location":"common-pipeline-practices.html#putting-task-configs-in-files","title":"Putting Task Configs in Files","text":"A lot of the pipeline examples that you will find on this site and in resource repos will embed a task step config directly in the pipeline. This is a nice way of clearly seeing what inputs/outputs the task uses. Tasks are usually designed to be used in multiple places, maybe with slightly different configuration. To support this scenario, most users store task configs in files instead of embedding the config directly in the pipeline.\n\nHere's what this looks like in practice:\n\nplatform: linux\n\nimage_resource: # define a default image for the task to use\n  type: registry-image\n  source:\n    repository: busybox\n\nrun:\n  path: date\n  args: [\"+%Y-%m-%d\"]\n \n\nUsing the task in a pipeline:\n\nresources:\n- name: ci\n  type: git\n  source:\n    uri: http://github.com/username/my-ci-repo\n\njobs:\n- name: the-best-job\n  plan:\n  - get: ci\n  - task: what-day-is-it\n    file: ci/tasks/todays-date.yml\n","depth":5,"section_tag":"putting-task-configs-in-files"},"quick-start":{"location":"quick-start.html","title":"Quick Start","text":"","depth":3,"section_tag":"quick-start"},"rails-example":{"location":"rails-example.html","title":"Rails application testing example","text":"You can run the tests for a Rails that requires a specific version of ruby and relies on a Postgres database.\n\n","depth":2,"section_tag":"rails-example"},"random-strategy":{"location":"container-placement.html#random-strategy","title":"The random strategy","text":"With the random strategy, the Running a web node places get, put and task containers on any worker, ignoring any affinity.\n\nAs this is truly random, this will be fine until one day it's not fine.\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=random\n","depth":4,"section_tag":"random-strategy"},"reducing-redundant-data":{"location":"global-resources.html#reducing-redundant-data","title":"Reducing redundant data","text":"The majority of Concourse resources will benefit from having versions shared globally because most resource versions have an external source of truth.\n\nFor example, a check for the git resource that pulls in the concourse/concourse repository will always return the same set of versions as an equivalent resource pointing to the same repository. By consolidating the checks and the versions, there will essentially only be one set of versions collected from the repository and saved into the database.\n\n","depth":5,"section_tag":"reducing-redundant-data"},"references":{"location":"php-example.html#references","title":"References","text":"* Jobs\n\n* Steps\n\n* Tasks\n\n","depth":3,"section_tag":"references"},"reliable-resource-version-history":{"location":"global-resources.html#reliable-resource-version-history","title":"Reliable Resource Version History","text":"Prior to global resources, a resource's version history was directly associated to the resource name. This meant that any changes to a resource's configuration without changing its name would basically append the versions from the new configuration after the old versions, which are no longer accurate to the current configuration.\n\nGlobal resources instead associates the resource versions to the resource's resource.type and resource.source. Therefore, whenever a resource definition changes, the versions will \"reset\" and change along with it, resulting in truthful and reliable version histories.\n\n","depth":5,"section_tag":"reliable-resource-version-history"},"reloading-worker-authorized-key":{"location":"concourse-web.html#reloading-worker-authorized-key","title":"Reloading worker authorized key","text":"While Running concourse web, the authorized worker key file, which contains all public keys for the workers, is loaded at startup. During the lifecycle of a Running a web node new worker keys might be added or old ones removed. To perform a live reload of this file you can send a SIGHUP signal to the concourse web process. The process will remain running and Concourse will reload the authorized worker key file.\n\n","depth":5,"section_tag":"reloading-worker-authorized-key"},"resource-certs":{"location":"implementing-resource-types.html#resource-certs","title":"Certificate Propagation","text":"Certificates can be automatically propagated into each resource container, if the worker is configured to do so. The BOSH release configures this automatically, while the concourse binary must be given a --certs-dir flag pointing to the path containing the CA certificate bundle.\n\nThe worker's certificate directory will then be always mounted at /etc/ssl/certs, read-only, in each resource container created on the worker. There's no single standard path for this, so we picked one that would work out of the box in most cases.\n\nThis approach to certificate configuration is similar in mindset to the propagation of http_proxy/https_proxy - certs are kind of a baseline assumption when deploying software, so Concourse should do its best to respect it out-of-the-box, especially as they're often used in tandem with a man-in-the-middle corporate SSL proxy. (In this way it doesn't feel too much like the anti-pattern of hand-tuning workers.)\n\n","depth":4,"section_tag":"resource-certs"},"resource-check":{"location":"implementing-resource-types.html#resource-check","title":"check: Check for new versions.","text":"A resource type's check script is invoked to detect new versions of the resource. It is given the configured source and current version on stdin, and must print the array of new versions, in chronological order, to stdout, including the requested version if it's still valid.\n\nThe request body will have the following fields:\n\n* source is an arbitrary JSON object which specifies the location of the resource, including any credentials. This is passed verbatim from the resource configuration.\n\n  For the git resource this would be the repo URI, the branch, and the private key, if necessary.\n\n* version is a JSON object with string fields, used to uniquely identify an instance of the resource. For git this would be the commit's SHA.\n\n  This will be omitted from the first request, in which case the resource should return the current version (not every version since the resource's inception).\n\nFor example, here's what the input for the git resource may look like:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cbef\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\n[ -d /tmp/repo ] || git clone git://some-uri /tmp/repo\ncd /tmp/repo\ngit pull \u0026\u0026 git log 61cbef..HEAD\nNote that it conditionally clones; the container for checking versions is reused between checks, so that it can efficiently pull rather than cloning every time.\n\nAnd the output, assuming d74e01 is the commit immediately after 61cbef:\n\n[\n  { \"ref\": \"61cbef\" },\n  { \"ref\": \"d74e01\" },\n  { \"ref\": \"7154fe\" }\n]\nThe list may be empty, if there are no versions available at the source. If the given version is already the latest, an array with that version as the sole entry should be listed.\n\nIf your resource is unable to determine which versions are newer than the given version (e.g. if it's a git commit that was push -fed over), then the current version of your resource should be returned (i.e. the new HEAD).\n\n","depth":4,"section_tag":"resource-check"},"resource-defaults":{"location":"concourse-web.html#resource-defaults","title":"Configuring defaults for resource types","text":"Defaults for the \"core\" resource types (those that show up under the Concourse org) that comes with Concourse can be set cluster-wide by passing in a configuration file. The format of the file is the name of the resource type followed by an arbitrary configuration.\n\nDocumentation for each resource type's configuration is in each implementation's README.\n\nCONCOURSE_BASE_RESOURCE_TYPE_DEFAULTS=./defaults.yml\nFor example, a defaults.yml that configures the entire cluster to use a registry mirror would have: registry-image:\n  registry_mirror:\n    host: https://registry.mirror.example.com\n\n\n","depth":5,"section_tag":"resource-defaults"},"resource-in":{"location":"implementing-resource-types.html#resource-in","title":"in: Fetch a given resource.","text":"The in script is passed a destination directory as command line argument $1, and is given on stdin the configured source and a precise version of the resource to fetch.\n\nThe script must fetch the resource and place it in the given directory.\n\nIf the desired resource version is unavailable (for example, if it was deleted), the script must exit with error.\n\nThe script must emit the fetched version, and may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* version is the same type of value passed to check: Check for new versions., and specifies the version to fetch.\n\n* params is an arbitrary JSON object passed along verbatim from get step params on a get step.\n\nExample request, in this case for the git resource:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ngit clone --branch develop git://some-uri $1\ncd $1\ngit checkout 61cebf\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Hulk Hogan\" }\n  ]\n}\n","depth":4,"section_tag":"resource-in"},"resource-interface":{"location":"tutorial-resources.html#resource-interface","title":"Resource Interface","text":"Resources are container images that contain three executables. Each executable is run by a different type of step within a pipeline:\n\n* /opt/resource/check - implicitly ran when a job contains a get step. Should return the latest version from the external system or object. Its responsibility is to find new versions.\n\n* /opt/resource/in - run in a get step. in is given a specific version (generated by a check or put step) and retrieves the files representing that version from the external system or object.\n\n* /opt/resource/out - run in a put step. Generates a new version, usually based on some input generated by another step in the job. Depending on the resource, this may mean sending something to the external system. For the git resource, this means pushing commits to the external git repository.\n\nThat's a high-level overview of the resource interface. Next, we will learn how to add resources to a pipeline.\n\n","depth":4,"section_tag":"resource-interface"},"resource-metadata":{"location":"implementing-resource-types.html#resource-metadata","title":"Metadata","text":"When used in a get step or a put step, metadata about the running build is made available via the following environment variables:\n\n$BUILD_ID: The internal identifier for the build. Right now this is numeric, but it may become a UUID in the future. Treat it as an absolute reference to the build.\n\n\n$BUILD_NAME: The build number within the build's job.\n\n\n$BUILD_JOB_NAME: The name of the build's job.\n\n\n$BUILD_PIPELINE_NAME: The name of the pipeline that the build's job lives in.\n\n\n$BUILD_PIPELINE_INSTANCE_VARS: The instance vars of the instanced pipeline that the build's job lives in, serialized as JSON. See Grouping Pipelines for a definition of instanced pipelines.\n\n\n$BUILD_TEAM_NAME: The team that the build belongs to.\n\n\n$ATC_EXTERNAL_URL: The public URL for your ATC; useful for debugging.\n\n\n\nIf the build is a one-off, $BUILD_NAME, $BUILD_JOB_NAME, $BUILD_PIPELINE_NAME, and $BUILD_PIPELINE_INSTANCE_VARS will not be set.\n\nAdditionally, $BUILD_PIPELINE_INSTANCE_VARS will not be set if the build's pipeline has no instance vars (i.e. is not an instanced pipeline).\n\nNone of these variables are available to check: Check for new versions..\n\nThese variables should be used solely for annotating things with metadata for traceability, i.e. for linking to the build in an alert or annotating an automated commit to facilitate its origin discovery.\n\nThey should not be used to emulate versioning (e.g. by using the increasing build number). They are not provided to task steps to avoid this anti-pattern.\n\n","depth":4,"section_tag":"resource-metadata"},"resource-out":{"location":"implementing-resource-types.html#resource-out","title":"out: Update a resource.","text":"The out script is passed a path to the directory containing the build's full set of sources as command line argument $1, and is given on stdin the configured params and the resource's source configuration.  \n\nThe script must emit the resulting version of the resource. For example, the git resource emits the SHA of the commit that it has just pushed.\n\nAdditionally, the script may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* params is an arbitrary JSON object passed along verbatim from get step params on a put step.\n\nExample request, in this case for the git resource:\n\n{\n  \"params\": {\n    \"branch\": \"develop\",\n    \"repo\": \"some-repo\"\n  },\n  \"source\": {\n    \"uri\": \"git@...\",\n    \"private_key\": \"...\"\n  }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ncd $1/some-repo\ngit push origin develop\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Mick Foley\" }\n  ]\n}\n","depth":4,"section_tag":"resource-out"},"resource-types":{"location":"resource-types.html","title":"Resource Types","text":"Each resource in a pipeline has a type. The resource's type determines what versions are detected, the bits that are fetched when the resource's get step runs, and the side effect that occurs when the resource's put step runs.\n\nConcourse comes with a few \"core\" resource types to cover common use cases like git and s3 - the rest are developed and supported by the Concourse community. An exhaustive list of all resource types is available in the Resource Types catalog.\n\nA pipeline's resource types are listed under pipeline.resource_types with the following schema:\n\nResource Types can be used to extend the functionality of your pipeline and provide deeper integrations. This example uses one to trigger a job whenever a new Dinosaur Comic is out.\n\n---\nresource_types:\n- name: rss\n  type: registry-image\n  source:\n    repository: suhlig/concourse-rss-resource\n    tag: latest\n\nresources:\n- name: booklit-releases\n  type: rss\n  source:\n    url: http://www.qwantz.com/rssfeed.php\n\njobs:\n- name: announce\n  plan:\n  - get: booklit-releases\n    trigger: true\n","depth":2,"section_tag":"resource-types"},"resource-versions":{"location":"resource-versions.html","title":"Resource Versions","text":"As you may know, resources represent external state that changes over time. But how do we track those changes in a generic way that will properly represent all the different resource types? That is where resource versions are introduced. Concourse uses versions to represent the exact changes of a resource over time.\n\nThe versions of a resource are directly dependent on its resource configuration and resource type. Each resource type has its own definition of what its versions should be. For example, the versions of a git resource would be the commits of the github repository and the versions of a docker image resource are the image digests.\n\nIf you want to figure out what determines the version of a resource type, it is typically outlined in the `check` behavior for the resource type. For example, the git resource uses commits as versions git resource type check behavior.\n\n","depth":3,"section_tag":"resource-versions"},"resources":{"location":"resources.html","title":"Resources","text":"Resources are the heart and soul of Concourse. They represent all external inputs to and outputs of jobs in the pipeline.\n\nEach resource represents a versioned artifact with an external source of truth. Configuring the same resource in any pipeline on any Concourse cluster will behave the exact same way. Concourse will continuously check each configured resource to discover new versions. These versions then flow through the pipeline via get steps configured on Jobs.\n\nA pipeline's resources are listed under pipeline.resources with the following schema.\n\n","depth":2,"section_tag":"resources"},"restarting-a-worker":{"location":"concourse-worker.html#restarting-a-worker","title":"Restarting a Worker","text":"Workers can be restarted in-place by sending SIGTERM to the worker process and starting it back up. Containers will remain running and Concourse will reattach to builds that were in flight.\n\nThis is a pretty aggressive way to restart a worker, and may result in errored builds - there are a few moving parts involved and we're still working on making this airtight.\n\nA safer way to restart a worker is to land it by sending SIGUSR1 to the worker process. This will switch the worker to the landing state and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the process will exit.\n\nYou may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR1/300/TERM/15/KILL\nThis will send SIGUSR1, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\nOnce the timeout is enforced, there's still a chance that builds that were running will continue when the worker comes back.\n\n","depth":5,"section_tag":"restarting-a-worker"},"restarting-and-upgrading":{"location":"concourse-web.html#restarting-and-upgrading","title":"Restarting \u0026 Upgrading","text":"The web nodes can be killed and restarted willy-nilly. No draining is necessary; if the web node was orchestrating a build it will continue where it left off when it comes back, or the build will be picked up by one of the other web nodes.\n\nTo upgrade a web node, stop its process and start a new one using the newly installed concourse. Any database migrations will be run automatically on start. If web nodes are started in parallel, only one will run the migrations.\n\nWe don't currently guarantee a lack of funny-business if you're running mixed Concourse versions - database migrations can perform modifications that confuse other web nodes. So there may be some turbulence during a rolling upgrade, but everything should stabilize once all web nodes are running the latest version.\n\nIf you want more control over when the database migrations happen and know if they were successful you can use the concourse migrate command. The migrate command accepts the same CONCOURSE_POSTGRES_* env vars as the concourse web command.\n\n","depth":5,"section_tag":"restarting-and-upgrading"},"risks-and-side-effects":{"location":"global-resources.html#risks-and-side-effects","title":"Risks and Side Effects","text":"","depth":4,"section_tag":"risks-and-side-effects"},"rotating-the-encryption-key":{"location":"encryption.html#rotating-the-encryption-key","title":"Rotating the Encryption Key","text":"To swap out the encryption key, you'll need to pass the previous key as --old-encryption-key (or old_encryption_key), and the new key as --encryption-key (or encryption_key).\n\nOn startup, the Running a web node will decrypt all existing data and re-encrypt it with the new key, in one go. If it encounters a row which is already encrypted with the new key, it will continue on (as may be the case when restarting with the flags again, or if the ATC died in the middle of rotating).\n\nIf the ATC encounters a row which cannot be decrypted with neither the old key nor the new one, it will log loudly and fail to start, telling you which row it choked on. This data must be dealt with in some way, either by re-configuring the key the row was encrypted with as the old key, or manually performing database surgery to remove the offending row. Hopefully this doesn't happen to you!\n\n","depth":4,"section_tag":"rotating-the-encryption-key"},"run-the-pipeline":{"location":"tutorial-hello-world.html#run-the-pipeline","title":"Run the pipeline","text":"That's the whole pipeline! You can now set it, unpause, and trigger it using the fly cli. You can then view it from the web ui.\n\n$ fly -t tutorial set-pipeline -p hello-world -c hello-world.yml\n# pipelines are paused when first created\n$ fly -t tutorial unpause-pipeline -p hello-world\n# trigger the job and watch it run to completion\n$ fly -t tutorial trigger-job --job hello-world/hello-world-job --watch\nYou'll see extra output than what we're showing below (the busybox image being downloaded) but the last four lines will be the task executing.\n\nselected worker: 701785fa43a1\nrunning echo Hello world!\nHello world!\nsucceeded\nThe following is an iframe to http://localhost:8080/teams/main/pipelines/hello-world \n\nCongratulations on building your first Concourse pipeline!\n\nIn the next section we will build upon what we have learned about tasks and introduce inputs and outputs, which allow you to pass files between tasks.\n\nIf you have any feedback for this tutorial please share it in this Github discussion\n\n","depth":4,"section_tag":"run-the-pipeline"},"running-tasks":{"location":"tasks.html#running-tasks","title":"Running tasks with fly execute","text":"One of the most common use cases of fly is taking a local project on your computer and setting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.\n\nYou can execute a task like this:\n\n$ fly -t example execute --config tests.yml\nYour files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify -i name=. instead, where name is the input name from the task configuration.\n\nFly will automatically capture SIGINT and SIGTERM and abort the build when received. This allows it to be transparently composed with other toolchains.\n\nBy default, Running tasks with fly execute will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the --include-ignored flag in order to send ignored files to Concourse along with those that are not ignored.\n\nIf your task needs to run as root, then you can specify the -p or --privileged flag.\n\nTasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current working directory) that has the same name as the directory.\n\nTasks must specify the inputs that they require as task.inputs. For fly to upload these inputs you can use the -i or --input arguments with name and path pairs. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\nThis would work together with a build-stemcell.yml if its inputs: section was as follows:\n\ninputs:\n- name: code\n- name: stemcells\nIf you specify an input, then the default input will no longer be added automatically, and you will need to explicitly list it (as with the code input above).\n\nThis feature can be used to mimic other resources and try out input combinations that would normally not be possible in a pipeline.\n\nIf the --inputs-from flag is given, the specified job will be looked up in the pipeline, and the one-off build will base its inputs on those currently configured for the job.\n\nIf any --input flags are given (see above), they will override the base set of inputs.\n\nFor example:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\nThis will trigger a one-off-build using the task.yml task config, basing its inputs on the latest candidates for the integration job in the main pipeline, with the foo input overridden to specify local code to run.\n\nThis can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local development so that you don't have to upload every single resource from your local machine.\n\nWhen using --inputs-from as above, you can additionally specify which input to use as the task's image by passing --image input-name.\n\nFor example, the following pipeline fetches an image via a get step and uses it for task step image:\n\nresources:\n- name: my-repo\n  type: git\n  source: {uri: https://example.com}\n\n- name: some-image\n  type: registry-image\n  source: {repository: ubuntu}\n\njobs:\n- name: integration\n  plan:\n  - get: my-repo\n  - get: some-image\n  - task: my-task\n    file: my-repo/task.yml\n    image: some-image\n...so to run the same task with the same image in a one-off build, you would run:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --image some-image\nIf a task specifies outputs, then you're able to extract these back out of the build and back to your local system. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\nThis would work together with a build-stemcell.yml, if its outputs: section was as follows:\n\noutputs:\n- name: stemcell\nThis feature is useful to farm work out to your Concourse server to build things in a repeatable manner.\n\nAny params listed in the task configuration can be specified by using environment variables.\n\nSo, if you have a task with the following params:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\n...and you run:\n\nBAR=hello fly execute\nThe task would then run with BAR as \"hello\", and FOO as \"fizzbuzz\" (its default value).\n\nTask config files can contain Vars which can can be set during fly execute by using the -v, -y and -l flags:\n\nfly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\nAny variables not satisfied via the above flags will be deferred to the configured credential manager.\n\nTo satisfy these vars when running the task in a pipeline, see task step vars.\n\nIf you want to execute a task on a worker that has a specific tag, you can do so by passing --tag:\n\nfly -t example execute --config task.yml --tag bar\nThis will execute the task specified by task.yml on a worker that has been tagged bar.\n\n","depth":3,"section_tag":"running-tasks"},"saving-credentials-in-aws":{"location":"aws-asm-credential-manager.html#saving-credentials-in-aws","title":"Saving credentials in AWS","text":"It seems to be best to use the 'other type of secret' option and the 'plaintext' entry (otherwise your secrets will be interpolated as JSON) for best results. Make sure your secret locations match the lookup templates exactly; include the leading /, for example.\n\n","depth":5,"section_tag":"saving-credentials-in-aws"},"scaling":{"location":"concourse-web.html#scaling","title":"Scaling","text":"The Running a web node can be scaled up for high availability. They'll also roughly share their scheduling workloads, using the database to synchronize. This is done by just running more web commands on different machines, and optionally putting them behind a load balancer.\n\nTo run a cluster of Running a web nodes, you'll first need to ensure they're all pointing to the same PostgreSQL server.\n\nNext, you'll need to configure a peer address. This is a DNS or IP address that can be used to reach this web node from other web nodes. Typically this uses a private IP, like so:\n\nCONCOURSE_PEER_ADDRESS=10.10.0.1\nThis address will be used for forwarded worker connections, which listen on the ephemeral port range.\n\nFinally, if all of these nodes are going to be accessed through a load balancer, you'll need to configure the external URL that will be used to reach your Concourse cluster:\n\nCONCOURSE_EXTERNAL_URL=https://ci.example.com\nAside from the peer URL, all configuration must be consistent across all web nodes in the cluster to ensure consistent results.\n\n","depth":5,"section_tag":"scaling"},"scaling-workers":{"location":"concourse-worker.html#scaling-workers","title":"Scaling Workers","text":"More workers should be added to accomodate more pipelines. To know when this is necessary you should probably set up Metrics and keep an eye on container counts. If it's starting to approach 200 or so, you should probably add another worker. Load average is another metric to keep an eye on.\n\nTo add a worker, just create another machine for the worker and follow the Running concourse worker instructions again.\n\nNote: it doesn't make sense to run multiple workers on one machine since they'll both be contending for the same physical resources. Workers should be given their own VMs or physical machines to maximize resource usage.\n\n","depth":5,"section_tag":"scaling-workers"},"scheduler":{"location":"scheduler.html","title":"Build Scheduler","text":"As of the v6.0.0 release, there have been many changes to the scheduler so it would be advisable to assume that this documentation should only be used for Concourse deployments v6.0.0 and above.\n\nBuilds represent each execution of a job. Figuring out when to schedule a new job build is the responsibility of the build scheduler. The scheduling of new job builds can be dependent on many different factors such as when a new version of a resource is discovered, when a dependent upstream build finishes, or when a user manually triggers a build.\n\nThe build scheduler is a global component, where it deals with all the jobs within a deployment. It runs on an interval with a default of 10 seconds. If there are multiple ATCs, only one of the ATC's scheduler component will run per interval tick in order to ensure that there will be no duplicated work between ATC nodes.\n\nThe subcomponent used to figure out whether a build can be scheduled is called the algorithm.\n\n","depth":4,"section_tag":"scheduler"},"scheduling-behavior":{"location":"scheduler.html#scheduling-behavior","title":"Scheduling behavior","text":"The scheduler will schedule a new build if any of the versions produced by the algorithm for trigger: true resources is not equal to the version used by the previous build of the job.\n\nWhat this means is if the algorithm runs and computes an input version, the scheduler will create a new build as long as that version is different than the previous build's version for that same input. Even if that version has been used by a build 2 months ago, the scheduler will schedule a new build as long as it has not been used by the previous build.\n\nIf there are any input versions that are different than the previous build, it will trigger a new build. This can affect some users that are using the pinning functionality to run older versions.\n\nLet's say you have a job that takes a resource as an input. It currently has two builds, build 1 has ran with version v1 of the resource and build 2 has ran with v2.\n\nimages/new-pinning-behavior-1.pngIf I pin the input to an older version, a new build is produced because the pinned version v1 is not equivalent to the version of the previous build v2.\n\nimages/new-pinning-behavior-3.pngThen if I unpin the version, another build would be triggered again using the latest version.\n\nimages/new-pinning-behavior-4.pngThis is because after unpinning the resource, the input version for the next build becomes the latest version v2 which is not equal to the version v1 used by the previous build.\n\nThis is to allow the current state of the builds to always reflect the current state of the versions. That being said, this behavior can be undesirable if the goal is to only run a job using an old version. In this case, build rerunning is the better choice. If you would like to learn more about build rerunning, you can read more about it in the build rerunning section.\n\n","depth":5,"section_tag":"scheduling-behavior"},"scheduling-on-demand":{"location":"scheduler.html#scheduling-on-demand","title":"Scheduling on demand","text":"The scheduler runs on an interval, but rather than scheduling all the jobs within a deployment on every tick, it only schedules the jobs that need to be scheduled.\n\nFirst, the scheduler determines which jobs need to be scheduled. Below are all the reasons why Concourse will think a job needs to be scheduled: * Detecting new versions of a resource through a check\n\n* Saving a new version through a put\n\n* A build finishes for an upstream job (through passed constraints)\n\n* Enabling/Disabling a resource version\n\n* Pinning/Unpinning a resource version\n\n* Setting a pipeline\n\n* Updating a resource's resource_config\n\n* Manually triggering a build\n\n* Rerunning a build\n\n* Multiple versions available for a version every constraint\n\n\n\nEach job that is scheduled will use the algorithm to determine what inputs its next build should have. Then the build is scheduled and picked up by the Build Tracker.\n\n","depth":5,"section_tag":"scheduling-on-demand"},"schema.across_var":{"location":"jobs.html#schema.across_var","title":"across_var schema","text":"","depth":3,"section_tag":"steps"},"schema.across_var.fail_fast":{"location":"jobs.html#schema.across_var.fail_fast","title":"across_var.fail_fast","text":"Default false. When enabled, the across step will fail fast by returning as soon as any sub-step fails. This means that running steps will be interrupted and pending steps will no longer be scheduled.\n\n","depth":3,"section_tag":"steps"},"schema.across_var.max_in_flight":{"location":"jobs.html#schema.across_var.max_in_flight","title":"across_var.max_in_flight","text":"Default 1. If set to all, the substep will run with all combinations of the current var in parallel. If set to a number schema, only that number of substeps may run in parallel.\n\nIf multiple vars are configured, the effective max_in_flight is multiplicative. For instance:\n\nplan:\n- across:\n  - var: var1\n    values: [a, b, c]\n    max_in_flight: all\n  - var: var2\n    values: [1, 2]\n  - var: var3\n    values: [foo, bar]\n    max_in_flight: 2\nHere, 6 substeps will run in parallel, since all 3 of var1's values can run in parallel, and 2 of var3's values can run in parallel.\n\n","depth":3,"section_tag":"steps"},"schema.across_var.values":{"location":"jobs.html#schema.across_var.values","title":"across_var.values","text":"The list of values that the var will iterate over when running the substep. If multiple vars are configured, all combinations of values across all vars will run.\n\nThe list of values may also be interpolated. For instance, you may use the load_var step to first load a list of value schema into a local var, and then iterate across that dynamic list of values.\n\nThe following step.across will run the task foo/build.yml for each package defined in foo/packages-to-build.json with Go 1.15 and 1.16.\n\nplan:\n- get: foo\n- load_var: packages\n  file: foo/packages-to-build.json\n- across:\n  - var: package\n    values: ((.:packages))\n  - var: go_version\n    values: ['1.15', '1.16']\n  task: build\n  file: foo/build.yml\n  vars:\n    go_version: ((.:go_version))\n    package: ((.:package))\nSupposing foo/packages-to-build.json had the following content: [\"./cmd/first\", \"./cmd/second\", \"./cmd/third\"]\n\n\n...then the task foo/build.yml would be run with the following var combinations:\n\n* {package: \"./cmd/first\", go_version: \"1.15\"}\n\n* {package: \"./cmd/first\", go_version: \"1.16\"}\n\n* {package: \"./cmd/second\", go_version: \"1.15\"}\n\n* {package: \"./cmd/second\", go_version: \"1.16\"}\n\n* {package: \"./cmd/third\", go_version: \"1.15\"}\n\n* {package: \"./cmd/third\", go_version: \"1.16\"}\n\n","depth":3,"section_tag":"steps"},"schema.across_var.var":{"location":"jobs.html#schema.across_var.var","title":"across_var.var","text":"The name of the variable that will be added to the \".\" var source. This variable will only be accessible in the scope of the step - each iteration of the step gets its own scope.\n\nIf a variable of the same name already exists in the parent scope, a warning will be printed.\n\n","depth":3,"section_tag":"steps"},"schema.anonymous_resource":{"location":"tasks.html#schema.anonymous_resource","title":"anonymous_resource schema","text":"","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.params":{"location":"tasks.html#schema.anonymous_resource.params","title":"anonymous_resource.params","text":"A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.source":{"location":"tasks.html#schema.anonymous_resource.source","title":"anonymous_resource.source","text":"The configuration for the resource; see resource.source.\n\n","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.type":{"location":"tasks.html#schema.anonymous_resource.type","title":"anonymous_resource.type","text":"The type of the resource. Usually registry-image.\n\nYou can use any resource type that returns a filesystem in the correct format: a /rootfs directory containing a full filesystem, and a metadata.json file containing.\n\n","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.version":{"location":"tasks.html#schema.anonymous_resource.version","title":"anonymous_resource.version","text":"A specific version of the resource to fetch. This should be a map with string keys and values. If not specified, the latest version will be fetched.\n\n","depth":2,"section_tag":"tasks"},"schema.boolean":{"location":"config-basics.html#schema.boolean","title":"boolean schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.build_log_retention_policy":{"location":"jobs.html#schema.build_log_retention_policy","title":"build_log_retention_policy schema","text":"","depth":2,"section_tag":"jobs"},"schema.build_log_retention_policy.builds":{"location":"jobs.html#schema.build_log_retention_policy.builds","title":"build_log_retention_policy.builds","text":"Keep logs for the last specified number of builds.\n\n","depth":2,"section_tag":"jobs"},"schema.build_log_retention_policy.days":{"location":"jobs.html#schema.build_log_retention_policy.days","title":"build_log_retention_policy.days","text":"Keep logs for builds which have finished within the specified number of days.\n\n","depth":2,"section_tag":"jobs"},"schema.build_log_retention_policy.minimum_succeeded_builds":{"location":"jobs.html#schema.build_log_retention_policy.minimum_succeeded_builds","title":"build_log_retention_policy.minimum_succeeded_builds","text":"Keep a minimum number of successful build logs that would normally be reaped.\n\nRequires builds to be set to an integer higher than 0 in order to work. For example, if builds is set to 5, and this attribute to 1, say a job has the following build history: 7(f), 6(f), 5(f), 4(f), 3(f), 2(f), 1(s), where f means failed and s means succeeded, then builds 2 and 3 will be reaped, because it retains 5 build logs, and at least 1 succeeded build log. Default is 0.\n\n","depth":2,"section_tag":"jobs"},"schema.cache":{"location":"tasks.html#schema.cache","title":"cache schema","text":"","depth":2,"section_tag":"tasks"},"schema.cache.path":{"location":"tasks.html#schema.cache.path","title":"cache.path","text":"The path to a directory to be cached.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"schema.command":{"location":"tasks.html#schema.command","title":"command schema","text":"","depth":2,"section_tag":"tasks"},"schema.command.args":{"location":"tasks.html#schema.command.args","title":"command.args","text":"Arguments to pass to the command. Note that when executed with Fly, any arguments passed to Running tasks with fly execute are appended to this array.\n\n","depth":2,"section_tag":"tasks"},"schema.command.dir":{"location":"tasks.html#schema.command.dir","title":"command.dir","text":"A directory, relative to the initial working directory, to set as the working directory when running the script.\n\n","depth":2,"section_tag":"tasks"},"schema.command.path":{"location":"tasks.html#schema.command.path","title":"command.path","text":"The name of or path to the executable to run.\n\npath is relative to the working directory. If dir is specified to set the working directory, then path is relative to it.\n\nThis is commonly a path to a script provided by one of the task's inputs, e.g. my-resource/scripts/test. It could also be a command like bash (respecting standard $PATH lookup rules), or an absolute path to a file to execute, e.g. /bin/bash.\n\n","depth":2,"section_tag":"tasks"},"schema.command.user":{"location":"tasks.html#schema.command.user","title":"command.user","text":"Explicitly set the user to run as. If not specified, this defaults to the user configured by the task's image. If not specified there, it's up to the Garden backend, and may be e.g. root on Linux.\n\n","depth":2,"section_tag":"tasks"},"schema.config":{"location":"config-basics.html#schema.config","title":"config schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.container_limits":{"location":"tasks.html#schema.container_limits","title":"container_limits schema","text":"","depth":2,"section_tag":"tasks"},"schema.container_limits.cpu":{"location":"tasks.html#schema.container_limits.cpu","title":"container_limits.cpu","text":"The maximum amount of CPU available to the task container, measured in shares. 0 means unlimited.\n\n","depth":2,"section_tag":"tasks"},"schema.container_limits.memory":{"location":"tasks.html#schema.container_limits.memory","title":"container_limits.memory","text":"The maximum amount of memory available to the task container, measured in bytes. 0 means unlimited.\n\n","depth":2,"section_tag":"tasks"},"schema.dir-path":{"location":"config-basics.html#schema.dir-path","title":"dir-path schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.display_config":{"location":"pipelines.html#schema.display_config","title":"display_config schema","text":"","depth":2,"section_tag":"pipelines"},"schema.display_config.background_image":{"location":"pipelines.html#schema.display_config.background_image","title":"display_config.background_image","text":"Allow users to specify a custom background image which is put at 30% opacity, grayscaled and blended into existing background. Must be an http, https, or relative URL.\n\n","depth":2,"section_tag":"pipelines"},"schema.dummy_config":{"location":"vars.html#schema.dummy_config","title":"dummy_config schema","text":"","depth":4,"section_tag":"var-sources"},"schema.dummy_config.vars":{"location":"vars.html#schema.dummy_config.vars","title":"dummy var source vars","text":"A mapping of var name to var value.\n\n","depth":4,"section_tag":"var-sources"},"schema.duration":{"location":"config-basics.html#schema.duration","title":"duration schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.env-vars":{"location":"config-basics.html#schema.env-vars","title":"env-vars schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.file-path":{"location":"config-basics.html#schema.file-path","title":"file-path schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.group_config":{"location":"pipelines.html#schema.group_config","title":"group_config schema","text":"","depth":2,"section_tag":"pipelines"},"schema.group_config.jobs":{"location":"pipelines.html#schema.group_config.jobs","title":"group_config.jobs","text":"A list of jobs that should appear in this group. A job may appear in multiple groups. Neighbours of jobs in the current group will also appear on the same page in order to give context of the location of the group in the pipeline.\n\nYou may also use any valid glob to represent several jobs, e.g.:\n\ngroups:\n- name: develop\n  jobs:\n  - terraform-*\n  - test\n  - deploy-{dev,staging}\n- name: ship\n  jobs:\n  - deploy-prod\n- name: all\n  jobs:\n  - \"*\"\nIn this example, the develop group will match terraform-apply, terraform-destroy, test, deploy-dev, deploy-staging. The ship group will only match deploy-prod. The all group will match all jobs in the pipeline.\n\nNote that depending on how it's used, *, {, and } have special meaning in YAML, and may need to be quoted (as was done in the all job above)\n\n","depth":2,"section_tag":"pipelines"},"schema.group_config.name":{"location":"pipelines.html#schema.group_config.name","title":"group_config.name","text":"A unique name for the group. This should be short and simple as it will be used as the tab name for navigation.\n\n","depth":2,"section_tag":"pipelines"},"schema.identifier":{"location":"config-basics.html#schema.identifier","title":"identifier schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.in_parallel_config":{"location":"jobs.html#schema.in_parallel_config","title":"in_parallel_config schema","text":"","depth":3,"section_tag":"steps"},"schema.in_parallel_config.fail_fast":{"location":"jobs.html#schema.in_parallel_config.fail_fast","title":"in_parallel step fail_fast","text":"Default false. When enabled the parallel step will fail fast by returning as soon as any sub-step fails. This means that running steps will be interrupted and pending steps will no longer be scheduled.\n\n","depth":3,"section_tag":"steps"},"schema.in_parallel_config.limit":{"location":"jobs.html#schema.in_parallel_config.limit","title":"in_parallel step limit","text":"Default unlimited. A sempahore which limits the parallelism when executing the steps in a in_parallel step. When set, the number of running steps will not exceed the limit.\n\nWhen not specified, in_parallel will execute all steps immediately.\n\n","depth":3,"section_tag":"steps"},"schema.in_parallel_config.steps":{"location":"jobs.html#schema.in_parallel_config.steps","title":"in_parallel step steps","text":"The steps to perform in parallel.\n\n","depth":3,"section_tag":"steps"},"schema.input":{"location":"tasks.html#schema.input","title":"input schema","text":"","depth":2,"section_tag":"tasks"},"schema.input.name":{"location":"tasks.html#schema.input.name","title":"input.name","text":"The name of the input.\n\n","depth":2,"section_tag":"tasks"},"schema.input.optional":{"location":"tasks.html#schema.input.optional","title":"input.optional","text":"Default false. If true, then the input is not required by the task. The task may run even if this input is missing.\n\nAn optional input that is missing will not appear in the current directory of the running task.\n\n","depth":2,"section_tag":"tasks"},"schema.input.path":{"location":"tasks.html#schema.input.path","title":"input.path","text":"The path where the input will be placed. If not specified, the input's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"schema.job":{"location":"jobs.html#schema.job","title":"job schema","text":"","depth":2,"section_tag":"jobs"},"schema.job.build_log_retention":{"location":"jobs.html#schema.job.build_log_retention","title":"job.build_log_retention","text":"Configures the retention policy for build logs. This is useful if you have a job that runs often but after some amount of time the logs aren't worth keeping around.\n\nBuilds which are not retained by the configured policy will have their logs reaped. If this configuration is omitted, logs are kept forever (unless Build log retention is configured globally).\n\nThe following example will keep logs for any builds that have completed in the last 2 days, while also keeping the last 1000 builds and at least 1 succeeded build.\n\njobs:\n- name: smoke-tests\n  build_log_retention:\n    days: 2\n    builds: 1000\n    minimum_succeeded_builds: 1\n  plan:\n  - get: 10m\n  - task: smoke-tests\n    # ...\nIf more than 1000 builds finish in the past 2 days, all of them will be retained thanks to the days configuration. Similarly, if there are 1000 builds spanning more than 2 days, they will also be kept thanks to the builds configuration. And if they all happened to have failed, the minimum_succeeded_builds will keep around at least one successful build. All policies operate independently.\n\n","depth":2,"section_tag":"jobs"},"schema.job.build_logs_to_retain":{"location":"jobs.html#schema.job.build_logs_to_retain","title":"job.build_logs_to_retain","text":"Deprecated. Equivalent to setting job.build_log_retention.builds.\n\n","depth":2,"section_tag":"jobs"},"schema.job.disable_manual_trigger":{"location":"jobs.html#schema.job.disable_manual_trigger","title":"job.disable_manual_trigger","text":"Default false. If set to true, manual triggering of the job (via the web UI or fly trigger-job) will be disabled.\n\n","depth":2,"section_tag":"jobs"},"schema.job.ensure":{"location":"jobs.html#schema.job.ensure","title":"job.ensure","text":"Step to execute regardless of whether the job succeeds, fails, errors, or aborts. Equivalent to the step.ensure hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.interruptible":{"location":"jobs.html#schema.job.interruptible","title":"job.interruptible","text":"Default false. Normally, when a worker is shutting down it will wait for builds with containers running on that worker to finish before exiting. If this value is set to true, the worker will not wait on the builds of this job. You may want this if e.g. you have a self-deploying Concourse or long-running-but-low-importance jobs.\n\n","depth":2,"section_tag":"jobs"},"schema.job.max_in_flight":{"location":"jobs.html#schema.job.max_in_flight","title":"job.max_in_flight","text":"If set, specifies a maximum number of builds to run at a time. If serial or serial_groups are set, they take precedence and force this value to be 1.\n\n","depth":2,"section_tag":"jobs"},"schema.job.name":{"location":"jobs.html#schema.job.name","title":"job.name","text":"The name of the job. This should be short; it will show up in URLs.\n\n","depth":2,"section_tag":"jobs"},"schema.job.old_name":{"location":"jobs.html#schema.job.old_name","title":"job.old_name","text":"The old name of the job. If configured, the history of old job will be inherited to the new one. Once the pipeline is set, this field can be removed as the builds have been transfered.\n\nThis can be used to rename a job without losing its history, like so:\n\njobs:\n- name: new-name\n  old_name: current-name\n  plan: [get: 10m]\nAfter the pipeline is set, because the builds have been inherited, the job can have the field removed:\n\njobs:\n- name: new-name\n  plan: [get: 10m]\n","depth":2,"section_tag":"jobs"},"schema.job.on_abort":{"location":"jobs.html#schema.job.on_abort","title":"job.on_abort","text":"Step to execute when the job aborts. Equivalent to the step.on_abort hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.on_error":{"location":"jobs.html#schema.job.on_error","title":"job.on_error","text":"Step to execute when the job errors. Equivalent to the step.on_error hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.on_failure":{"location":"jobs.html#schema.job.on_failure","title":"job.on_failure","text":"Step to execute when the job fails. Equivalent to the step.on_failure hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.on_success":{"location":"jobs.html#schema.job.on_success","title":"job.on_success","text":"Step to execute when the job succeeds. Equivalent to the step.on_success hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.plan":{"location":"jobs.html#schema.job.plan","title":"job.plan","text":"The sequence of steps to execute.\n\n","depth":2,"section_tag":"jobs"},"schema.job.public":{"location":"jobs.html#schema.job.public","title":"job.public","text":"Default false. If set to true, the build log of this job will be viewable by unauthenticated users. Unauthenticated users will always be able to see the inputs, outputs, and build status history of a job. This is useful if you would like to expose your pipeline publicly without showing sensitive information in the build log.\n\nNote: when this is set to true, any get step and put steps will show the metadata for their resource version, regardless of whether the resource itself has set resource.public to true.\n\n","depth":2,"section_tag":"jobs"},"schema.job.serial":{"location":"jobs.html#schema.job.serial","title":"job.serial","text":"Default false. If set to true, builds will queue up and execute one-by-one, rather than executing in parallel.\n\n","depth":2,"section_tag":"jobs"},"schema.job.serial_groups":{"location":"jobs.html#schema.job.serial_groups","title":"job.serial_groups","text":"Default []. When set to an array of arbitrary tag-like strings, builds of this job and other jobs referencing the same tags will be serialized.\n\nThis can be used to ensure that certain jobs do not run at the same time, like so:\n\njobs:\n- name: job-a\n  serial_groups: [some-tag]\n- name: job-b\n  serial_groups: [some-tag, some-other-tag]\n- name: job-c\n  serial_groups: [some-other-tag]\nIn this example, job-a and job-c can run concurrently, but neither job can run builds at the same time as job-b.\n\nThe builds are executed in their order of creation, across all jobs with common tags.\n\n","depth":2,"section_tag":"jobs"},"schema.number":{"location":"config-basics.html#schema.number","title":"number schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.output":{"location":"tasks.html#schema.output","title":"output schema","text":"","depth":2,"section_tag":"tasks"},"schema.output.name":{"location":"tasks.html#schema.output.name","title":"output.name","text":"The name of the output. The contents under path will be made available to the rest of the plan under this name.\n\n","depth":2,"section_tag":"tasks"},"schema.output.path":{"location":"tasks.html#schema.output.path","title":"output.path","text":"The path to a directory where the output will be taken from. If not specified, the output's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"schema.pipeline":{"location":"pipelines.html#schema.pipeline","title":"pipeline schema","text":"","depth":2,"section_tag":"pipelines"},"schema.pipeline.display":{"location":"pipelines.html#schema.pipeline.display","title":"pipeline.display","text":"display was introduced in Concourse v6.6.0. It is considered an experimental feature.\n\nVisual configurations for personalizing your pipeline.\n\nThe following example will display an image in the background of the pipeline it is configured on.\n\ndisplay:\n  background_image: https://avatars1.githubusercontent.com/u/7809479?s=400\u0026v=4\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.groups":{"location":"pipelines.html#schema.pipeline.groups","title":"pipeline.groups","text":"A list of job groups to use for organizing jobs in the web UI.\n\nGroups have no functional effect on your pipeline. They are purely for making it easier to grok large pipelines in the web UI.\n\nNote: once you have added groups to your pipeline, all jobs must be in a group.\n\nThe following example will make the \"tests\" group the default view (since it's listed first), separating the later jobs into a \"publish\" group:\n\ngroups:\n- name: test\n  jobs:\n  - unit\n  - integration\n- name: publish\n  jobs:\n  - deploy\n  - shipit\nThis would display two tabs at the top of the home page: \"test\" and \"publish\".\n\nFor a real world example of how groups can be used to simplify navigation and provide logical grouping, see the groups used at the top of the page in the Concourse pipeline.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.jobs":{"location":"pipelines.html#schema.pipeline.jobs","title":"pipeline.jobs","text":"A set of jobs for the pipeline to continuously schedule. At least one job is required for a pipeline to be valid.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.resource_types":{"location":"pipelines.html#schema.pipeline.resource_types","title":"pipeline.resource_types","text":"A set of resource types for resources within the pipeline to use.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.resources":{"location":"pipelines.html#schema.pipeline.resources","title":"pipeline.resources","text":"A set of resources for the pipeline to continuously check.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.var_sources":{"location":"pipelines.html#schema.pipeline.var_sources","title":"pipeline.var_sources","text":"A set of Var sources (experimental) for the pipeline to use.\n\n","depth":2,"section_tag":"pipelines"},"schema.resource":{"location":"resources.html#schema.resource","title":"resource schema","text":"","depth":2,"section_tag":"resources"},"schema.resource.check_every":{"location":"resources.html#schema.resource.check_every","title":"resource.check_every","text":"Default 1m. The interval on which to check for new versions of the resource. Acceptable interval options are defined by the time.ParseDuration function.\n\nIf set to never the resource will not be automatically checked. The resource can still be checked manually via the web UI, fly, or webhooks.\n\n","depth":2,"section_tag":"resources"},"schema.resource.icon":{"location":"resources.html#schema.resource.icon","title":"resource.icon","text":"The name of a Material Design icon to show next to the resource name in the web UI. For example, github.\n\n","depth":2,"section_tag":"resources"},"schema.resource.name":{"location":"resources.html#schema.resource.name","title":"resource.name","text":"The name of the resource. This should be short and simple. This name will be referenced by build plans of jobs in the pipeline.\n\n","depth":2,"section_tag":"resources"},"schema.resource.old_name":{"location":"resources.html#schema.resource.old_name","title":"resource.old_name","text":"The old name of the resource. If configured, the history of the old resource will be inherited to the new one. Once the pipeline is set, this field can be removed as the history has been transferred.\n\nThis can be used to rename a resource without losing its history, like so:\n\nresources:\n- name: new-name\n  old_name: current-name\n  type: git\n  source: uri: \"https://github.com/vito/booklit\"\nAfter the pipeline is set, the resource was successfully renamed, so the `old_name` field can be removed from the resource:\n\nresources:\n- name: new-name\n  type: git\n  source: uri: \"https://github.com/vito/booklit\"\n","depth":2,"section_tag":"resources"},"schema.resource.public":{"location":"resources.html#schema.resource.public","title":"resource.public","text":"Default false. If set to true, the metadata for each version of the resource will be viewable by unauthenticated users (assuming the pipeline has been exposed).\n\nResource metadata should never contain credentials or secret information, but this is off by default just to be safe in case users don't want to show things like commit messages and authors to the public.\n\nNote: even when set to false, the versions identifiers will be visible. In addition, if a resource is fetched in a build whose job is marked job.public, metadata will be visible in the build output.\n\n","depth":2,"section_tag":"resources"},"schema.resource.source":{"location":"resources.html#schema.resource.source","title":"resource.source","text":"The configuration for the resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use git as an example, the source may contain the repo URI, the branch of the repo to track, and a private key to use when pushing/pulling.\n\nBy convention, documentation for each resource type's configuration is in each implementation's README.\n\nYou can find the source for the resource types provided with Concourse at the Concourse GitHub organization.\n\n","depth":2,"section_tag":"resources"},"schema.resource.tags":{"location":"resources.html#schema.resource.tags","title":"resource.tags","text":"Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example.\n\nThis does not apply tags to all get steps or put steps that use the resource. If you want these steps to use tags, you must set step.tags for each step.\n\n","depth":2,"section_tag":"resources"},"schema.resource.type":{"location":"resources.html#schema.resource.type","title":"resource.type","text":"The resource type implementing the resource.\n\n","depth":2,"section_tag":"resources"},"schema.resource.version":{"location":"resources.html#schema.resource.version","title":"resource.version","text":"A version to pin the resource to across the pipeline. This has the same effect as setting get step version on every get step referencing the resource.\n\nResources can also be temporarily pinned to a version via the API and web UI. However this functionality is disabled if the resource is pinned via configuration, and if a pipeline is configured to have a version pinned while also pinned in the web UI, the configuration takes precedence and will clear out the temporary pin.\n\n","depth":2,"section_tag":"resources"},"schema.resource.webhook_token":{"location":"resources.html#schema.resource.webhook_token","title":"resource.webhook_token","text":"If specified, web hooks can be sent to trigger an immediate check of the resource, specifying this value as a primitive form of authentication via query params.\n\nAfter configuring this value, you would then configure your hook sender with the following painfully long path appended to your external URL:\n\n/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN\nFor instance pipelines you will need to include the pipeline vars for a single pipeline instance. Currently you can not have webhooks for all instances of a pipeline.\n\nThe pipeline vars should be added to the webhook URL as URL parameters with the format vars.MY-VAR=\"SOME-VALUE\". A webhook URL for a pipeline instance may look like this:\n\n/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN\u0026vars.my-var=\"some-value\"\u0026vars.second-var=\"two\"\nNote that the request payload sent to this API endpoint is entirely ignored.  You should configure the resource as if you're not using web hooks, as the resource config is still the \"source of truth.\"\n\n","depth":2,"section_tag":"resources"},"schema.resource_type":{"location":"resource-types.html#schema.resource_type","title":"resource_type schema","text":"","depth":2,"section_tag":"resource-types"},"schema.resource_type.check_every":{"location":"resource-types.html#schema.resource_type.check_every","title":"resource_type.check_every","text":"Default 1m. The interval on which to check for new versions of the resource type. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.defaults":{"location":"resource-types.html#schema.resource_type.defaults","title":"resource_type.defaults","text":"The default configuration for the resource type. This varies by resource type, and is a black box to Concourse; it is merged with (duplicate fields are overwritten by) resource.source and passed to the resource at runtime.\n\nresource_types:\n- name: gcs\n  type: registry-image\n  source:\n    repository: frodenas/gcs-resource\n  defaults:\n    json_key: ((default_key))\n\nresources:\n- name: bucket-a\n  type: gcs\n  source:\n    bucket: a\n\n- name: bucket-b\n  type: gcs\n  source:\n    bucket: b\n\n- name: bucket-c\n  type: gcs\n  source:\n    bucket: c\n    json_key: ((different_key))\nSince it's possible to overwrite the base resource types, it can be used to give defaults to resources at the pipeline level.\n\nresource_types:\n- name: registry-image\n  type: registry-image\n  source:\n    repository: concourse/registry-image-resource\n  defaults:\n    registry_mirror:\n      host: https://registry.mirror.example.com\n\nresources:\n- name: mirrored-image\n  type: registry-image\n  source:\n    repository: busybox\nAlternatively, the web node can be configured to use defaults for base resource types\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.name":{"location":"resource-types.html#schema.resource_type.name","title":"resource_type.name","text":"The name of the resource type. This should be short and simple. This name will be referenced by pipeline.resources defined within the same pipeline, and task.image_resources used by tasks running in the pipeline.\n\nPipeline-provided resource types can override the core resource types by specifying the same name.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.params":{"location":"resource-types.html#schema.resource_type.params","title":"resource_type.params","text":"Arbitrary config to pass when running the get to fetch the resource type's image.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.privileged":{"location":"resource-types.html#schema.resource_type.privileged","title":"resource_type.privileged","text":"Default false. If set to true, the resource's containers will be run with full capabilities, as determined by the worker backend the task runs on.\n\nFor Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the root user is \"actual\" root (if set to true) or a user namespaced root (if set to false, the default).\n\nThis is a gaping security hole; only configure it if the resource type needs it (which should be called out in its documentation). This is not up to the resource type to decide dynamically, so as to prevent privilege escalation via third-party resource type exploits.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.source":{"location":"resource-types.html#schema.resource_type.source","title":"resource_type.source","text":"The configuration for the resource type's resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use registry-image as an example, the source would contain something like repository: username/reponame. See the Registry Image resource (or whatever resource type your resource type uses) for more information.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.tags":{"location":"resource-types.html#schema.resource_type.tags","title":"resource_type.tags","text":"Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also step.tags.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.type":{"location":"resource-types.html#schema.resource_type.type","title":"resource_type.type","text":"The type of the resource used to provide the resource type's container image.\n\nThis is a bit meta. Usually this value will be registry-image as the resource type must result in a container image.\n\nA resource type's type can refer to other resource types, and can also use the core type that it's overriding. This is useful for bringing in a newer or forked registry-image resource.\n\n","depth":2,"section_tag":"resource-types"},"schema.step":{"location":"jobs.html#schema.step","title":"step schema","text":"","depth":3,"section_tag":"steps"},"schema.step.across":{"location":"jobs.html#schema.step.across","title":"step.across","text":"Run a step multiple times with different combinations of variable values.\n\nacross is considered an experimental feature, and its syntax/semantics may change. To enable across for your deployment, you must set the feature flag CONCOURSE_ENABLE_ACROSS_STEP.\n\nThe across step can be combined with the load_var step, the set_pipeline step, and instanced pipelines to maintain a dynamically sized group of related pipelines.\n\nYou can use the across step to set a pipeline for each branch in a git repository.\n\nplan:\n- get: release-branches\n  trigger: true\n- get: ci\n- load_var: branches\n  file: release-branches/branches.json\n- across:\n  - var: branch\n    values: ((.:branches))\n  set_pipeline: release\n  file: ci/pipelines/release.yml\n  instance_vars: {branch: ((.:branch.name))}\nWhen a new branch is added, a new pipeline will be created. When a branch is deleted, the pipeline will be automatically archived as described in the set_pipeline step.\n\nFor a more complete example, refer to Multi-Branch Workflows.\n\n","depth":3,"section_tag":"steps"},"schema.step.attempts":{"location":"jobs.html#schema.step.attempts","title":"step.attempts","text":"The total number of times a step should be tried before it should fail, e.g. 5 will run the step up to 5 times before giving up.\n\nAttempts will retry on a Concourse error as well as build failure. When the number of attempts is reached and the step has still not succeeded then the step will fail.\n\nThe following will run the task and retry it up to 9 times (for a total of 10 attempts) if it fails:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  attempts: 10\nWhen used in combination with timeout, the timeout applies to each step.\n\nThis semi-arbitary decision was made because often things either succeed in a reasonable amount of time or fail due to hanging/flakiness. In this case it seems more useful to allow each attempt the allotted timeout rather than have one very long attempt prevent more attempts.\n\nplan:\n- get: flake\n- task: flaky-tests\n  file: flake/integration.yml\n  timeout: 10m\n  attempts: 3\n","depth":3,"section_tag":"steps"},"schema.step.do-step.do":{"location":"jobs.html#schema.step.do-step.do","title":"do step do","text":"Simply performs the given steps serially, with the same semantics as if they were at the top level step listing.\n\nThis can be used to perform multiple steps serially in an step.on_failure:\n\nplan:\n- get: my-repo\n- task: unit\n  file: my-repo/ci/unit.yml\n  on_failure:\n    do:\n    - put: alert\n    - put: email\n","depth":3,"section_tag":"steps"},"schema.step.ensure":{"location":"jobs.html#schema.step.ensure","title":"step.ensure","text":"A hook step to execute after the parent step regardless of whether the parent step succeeds, fails, or errors. The step will also be executed if the build was aborted and its parent step was interrupted.\n\nIf the parent step succeeds and the ensured step fails, the overall step fails.\n\nThe following build plan acquires a lock and then ensures that the lock is released.\n\nplan:\n- put: some-lock\n  params: {acquire: true}\n- task: integration\n  file: foo/integration.yml\n  ensure:\n    put: some-lock\n    params: {release: some-lock}\n","depth":3,"section_tag":"steps"},"schema.step.get-step.get":{"location":"jobs.html#schema.step.get-step.get","title":"get step get","text":"Fetches a version of a resource.\n\nThe fetched bits will be registered in the build's artifact namespace under the given identifier. Subsequent task step and put step which list the identifier as an input will have a copy of the bits in their working directory.\n\nAlmost every simple unit test job will look something like this: fetch my code with a get step and run its tests with a task step.\n\nplan:\n- get: my-repo\n  trigger: true\n- task: unit\n  file: my-repo/ci/unit.yml\n","depth":3,"section_tag":"steps"},"schema.step.get-step.params":{"location":"jobs.html#schema.step.get-step.params","title":"get step params","text":"Arbitrary configuration to pass to the resource. Refer to the resource type's documentation to see what it supports.\n\nThe following plan fetches a version number via the semver resource, bumps it to the next release candidate, and puts it back.\n\nplan:\n- get: version\n  params:\n    bump: minor\n    rc: true\n- put: version\n  params: {version: version/number}\n","depth":3,"section_tag":"steps"},"schema.step.get-step.passed":{"location":"jobs.html#schema.step.get-step.passed","title":"get step passed","text":"When specified, only the versions of the resource that made it through the given list of jobs (AND-ed together) will be considered when triggering and fetching.\n\nIf multiple gets are configured with passed constraints, all of the mentioned jobs are correlated. That is, with the following set of inputs:\n\nplan:\n- get: a\n  passed: [a-unit, integration]\n- get: b\n  passed: [b-unit, integration]\n- get: x\n  passed: [integration]\nThis means \"give me the versions of a, b, and x that have passed the same build of integration, with the same version of a passing a-unit and the same version of b passing b-unit.\"\n\nThis is crucial to being able to implement safe \"fan-in\" semantics as things progress through a pipeline.\n\n","depth":3,"section_tag":"steps"},"schema.step.get-step.resource":{"location":"jobs.html#schema.step.get-step.resource","title":"get step resource","text":"Defaults to the value of get. The resource to fetch, as configured in pipeline.resources.\n\nUse this attribute to rename a resource from the overall pipeline context into the job-specific context.\n\n","depth":3,"section_tag":"steps"},"schema.step.get-step.trigger":{"location":"jobs.html#schema.step.get-step.trigger","title":"get step trigger","text":"Default false. If set to true, new builds of the job will be automatically created when a new version for this input becomes available.\n\nNote: if none of a job's get steps are set to true, the job can only be manually triggered.\n\n","depth":3,"section_tag":"steps"},"schema.step.get-step.version":{"location":"jobs.html#schema.step.get-step.version","title":"get step version","text":"Default latest. The version of the resource to fetch.\n\nIf set to latest, scheduling will just find the latest available version of a resource and use it, allowing versions to be skipped.  This is usually what you want, e.g. if someone pushes 100 git commits.\n\nIf set to every, builds will walk through all available versions of the resource. Note that if passed is also configured, it will only step through the versions satisfying the constraints.\n\nIf set to a specific version (e.g. {ref: abcdef123}), only that version will be used. Note that the version must be available and detected by the resource, otherwise the input will never be satisfied. You may want to use fly check-resource to force detection of resource versions, if you need to use an older one that was never detected (as all newly configured resources start from the latest version).\n\n","depth":3,"section_tag":"steps"},"schema.step.in-parallel-step.in_parallel":{"location":"jobs.html#schema.step.in-parallel-step.in_parallel","title":"in_parallel step in_parallel","text":"Performs the given steps in parallel.\n\nIf any sub-steps (or task) in a parallel result in a failure or error, the parallel step as a whole is considered to have failed or errored.\n\nSteps are either configured as a array or within a in_parallel_config schema.\n\nUsing the in_parallel step where possible is the easiest way to speeding up a builds.\n\nIt is often used to fetch all dependent resources together at the start of a build plan:\n\nplan:\n- in_parallel:\n  - get: component-a\n  - get: component-b\n  - get: integration-suite\n- task: integration\n  file: integration-suite/task.yml\nIf any step in the in_parallel fails, the build will fail, making it useful for build matrices:\n\nplan:\n- get: some-repo\n- in_parallel:\n  - task: unit-windows\n    file: some-repo/ci/windows.yml\n  - task: unit-linux\n    file: some-repo/ci/linux.yml\n  - task: unit-darwin\n    file: some-repo/ci/darwin.yml\nUsing limit is useful for performing parallel execution of a growing number of tasks without overloading your workers. In the example below, two tasks will be run in parallel and in order until all steps have been executed:\n\nplan:\n- get: some-repo\n- in_parallel:\n    limit: 2\n    fail_fast: false\n    steps:\n      - task: unit-windows\n        file: some-repo/ci/windows.yml\n      - task: unit-linux\n        file: some-repo/ci/linux.yml\n      - task: unit-darwin\n        file: some-repo/ci/darwin.yml\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.file":{"location":"jobs.html#schema.step.load-var-step.file","title":"load_var step file","text":"The path to a file whose content shall be read and used as the var's value.\n\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.format":{"location":"jobs.html#schema.step.load-var-step.format","title":"load_var step format","text":"The format of the file's content.\n\nIf unset, Concourse will try to detect the format from the file extension. If the file format cannot be determined, Concourse will fallback to trim.\n\nIf set to json, yaml, or yml, the file content will be parsed accordingly and the resulting structure will be the value of the var.\n\nIf set to trim, the var will be set to the content of the file with any trailing and leading whitespace removed.\n\nIf set to raw, the var will be set to the content of the file without modification (i.e. with any existing whitespace).\n\nLet's say we have a task, generate-creds, which produces a generated-user output containing a user.json file like so:\n\n{\n  \"username\": \"some-user\",\n  \"password\": \"some-password\"\n}\nWe could pass these credentials to subsequent steps by loading it into a var with load_var, which will detect that it is in JSON format based on the file extension:\n\nplan:\n- task: generate-creds\n- load_var: user\n  file: generated-user/user.json\n- task: use-creds\n  params:\n    USERNAME: ((.:user.username))\n    PASSWORD: ((.:user.password))\nIf the use-creds task were to print these values, they would be automatically redacted unless reveal: true is set.\n\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.load_var":{"location":"jobs.html#schema.step.load-var-step.load_var","title":"load_var step load_var","text":"Load the value for a var at runtime, making it available to subsequent steps as a build-local var named after the given identifier.\n\nThe following build plan uses a version produced by the semver resource as a tag:\n\nplan:\n- get: version\n- load_var: version-tag\n  file: version/version\n- put: image\n  params: {tag: ((.:version-tag))}\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.reveal":{"location":"jobs.html#schema.step.load-var-step.reveal","title":"load_var step reveal","text":"Default false. If set to true, allow the var's content to be printed in the build output even with secret redaction enabled.\n\n","depth":3,"section_tag":"steps"},"schema.step.on_abort":{"location":"jobs.html#schema.step.on_abort","title":"step.on_abort","text":"A hook step to execute if the build is aborted and the parent step is terminated.\n\nThe following will perform the cleanup task only if the build is aborted while the unit task was running:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  on_abort:\n    task: cleanup\n    file: foo/cleanup.yml\n","depth":3,"section_tag":"steps"},"schema.step.on_error":{"location":"jobs.html#schema.step.on_error","title":"step.on_error","text":"A hook step to execute after the parent step if the parent step terminates abnormally in any way other than those handled by the step.on_abort or step.on_failure. This covers scenarios as broad as configuration mistakes, temporary network issues with the workers, or running longer than a step.timeout.\n\nUntil notifications become first-class (RFC #28, this step can be used to notify folks if their builds errored out:\n\nplan:\n- do:\n  - get: ci\n  - task: unit\n    file: ci/unit.yml\n  on_error:\n    put: slack\n","depth":3,"section_tag":"steps"},"schema.step.on_failure":{"location":"jobs.html#schema.step.on_failure","title":"step.on_failure","text":"A hook step to execute if the parent step fails.\n\nThis does not \"recover\" the failure - it will still fail even if the hook step succeeds.\n\nThe following will perform the alert task only if the unit task fails:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  on_failure:\n    task: alert\n    file: foo/alert.yml\n","depth":3,"section_tag":"steps"},"schema.step.on_success":{"location":"jobs.html#schema.step.on_success","title":"step.on_success","text":"A hook step to execute if the parent step succeeds.\n\nThe following will perform the second task only if the first one succeeds:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  on_success:\n    task: alert\n    file: foo/alert.yml\nNote that this is semantically equivalent to the following:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n- task: alert\n  file: foo/alert.yml\nThe on_success hook is provided mainly for cases where there is an equivalent step.on_failure, and having them next to each other is more clear.\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.get_params":{"location":"jobs.html#schema.step.put-step.get_params","title":"put step get_params","text":"Arbitrary configuration to get to the resource during the implicit get step. Refer to the resource type's documentation to see what it supports.\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.inputs":{"location":"jobs.html#schema.step.put-step.inputs","title":"put step inputs","text":"Default all. When not set, or set to all, all artifacts will be provided. This can result in slow performance if the prior steps in the build plan register a bunch of large artifacts before this step, so you may want to consider being explicit.\n\nIf configured as a list of identifiers, only the listed artifacts will be provided to the container.\n\nIf set to detect, the artifacts are detected based on the configured put step params by looking for all string values and using the first path segment as an identifier. (This may become the default in the future.)\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.params":{"location":"jobs.html#schema.step.put-step.params","title":"put step params","text":"Arbitrary configuration to pass to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.put":{"location":"jobs.html#schema.step.put-step.put","title":"put step put","text":"Pushes to the given resource.\n\nWhen the step succeeds, the version by the step will be immediately fetched via an additional implicit get step. This is so that later steps in your plan can use the artifact that was produced. The artifact will be available under the identifier put specifies.\n\nThe following plan fetches a repo using get and pushes it to another repo (assuming repo-develop and repo-master are defined as git resources):\n\nplan:\n- get: repo-develop\n- put: repo-master\n  params:\n    repository: repo-develop\nIf the logical name (whatever put specifies) differs from the concrete resource, you would specify resource as well, like so:\n\nplan:\n- put: resource-image\n  resource: registry-image-resource\nAdditionally, you can control the settings of the implicit get step by setting get_params. For example, if you did not want a put step utilizing the  registry-image resource type to download the image, you would implement your put step as such:\n\nplan:\n- put: docker-build\n  params: {build: git-resource}\n  get_params: {skip_download: true}\n","depth":3,"section_tag":"steps"},"schema.step.put-step.resource":{"location":"jobs.html#schema.step.put-step.resource","title":"put step resource","text":"Defaults to the value of put. The resource to update, as configured in pipeline.resources.\n\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.file":{"location":"jobs.html#schema.step.set-pipeline-step.file","title":"set_pipeline step file","text":"The path to the pipeline's configuration file.\n\nfile points at a .yml file containing the pipeline configuration, which allows this to be tracked with your resources or generated by a task step.\n\nThe first segment in the path should refer to another artifact from the plan, and the rest of the path is relative to that artifact.\n\nThe get step can be used to fetch your configuration from a git repo and auto-configure it using a set_pipeline step:\n\n- get: ci\n- set_pipeline: my-pipeline\n  file: ci/pipelines/my-pipeline.yml\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.instance_vars":{"location":"jobs.html#schema.step.set-pipeline-step.instance_vars","title":"set_pipeline step instance_vars","text":"A map of instance vars used to identify instanced pipelines. These vars will also be interpolated into the pipeline config.\n\nNote that variables set with this field will not propagate to tasks configured via task step file. If you want those variables to be determined at the time the pipeline is set, use task step vars as well.\n\nBoth instance_vars and vars may be statically passed like so:\n\nplan:\n- get: my-repo\n- set_pipeline: release\n  file: my-repo/ci/pipeline.yml\n  instance_vars:\n    version: 1.0.x\n  vars:\n    text: \"Hello World\"\nAny Vars in the pipeline config will be filled in statically using this field.\n\nFor example, if my-repo/ci/pipeline.yml looks like...:\n\nresources:\n- name: task-image\n  type: registry-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\njobs:\n- name: job\n  plan:\n  - get: task-image\n  - task: do-stuff\n    image: task-image\n    config:\n      platform: linux\n      run:\n        path: echo\n        args: [\"((text)) from version ((version))!\"]\n...this will resolve \"((text)) from version ((version))\" to \"Hello World from version 1.0.x!\", while ((myuser)) and ((mypass)) will be left in the pipeline to be fetched at runtime.\n\nAdditionally, as per Managing Instanced Pipelines, this pipeline would be referred to in fly using the flag --pipeline release/version:1.0.x\n\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.set_pipeline":{"location":"jobs.html#schema.step.set-pipeline-step.set_pipeline","title":"set_pipeline step set_pipeline","text":"Configures a pipeline.\n\nThe identifier specifies the name of the pipeline to configure. Unless set_pipeline step team is set, it will be configured within the current team and be created unpaused. If set to self, the current pipeline will update its own config.\n\nset_pipeline: self was introduced in Concourse v6.5.0. It is considered an experimental feature and may be removed at any time. Contribute to the associated discussion with feedback.\n\nPipelines configured with the set_pipeline step are tied to the job that configured them, and will be automatically archived in the following scenarios: * When the job runs a successful build which did not configure the pipeline (i.e. the set_pipeline step was removed).\n\n* When the job is removed from its pipeline configuration (see job.old_name for renaming instead of removing).\n\n* When the job's pipeline is archived or destroyed.\n\n This means any job that uses set_pipeline should set all still-desired pipelines in each build, rather than setting them one-by-one through many builds.\n\nSee fly archive-pipeline for what happens when a pipeline is archived.\n\nThis is a way to ensure a pipeline stays up to date with its definition in a source code repository, eliminating the need to manually run fly set-pipeline.\n\nresources:\n- name: booklit\n  type: git\n  source: {uri: https://github.com/vito/booklit}\njobs:\n- name: reconfigure\n  plan:\n  - get: booklit\n    trigger: true\n  - set_pipeline: booklit\n    file: booklit/ci/pipeline.yml\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.team":{"location":"jobs.html#schema.step.set-pipeline-step.team","title":"set_pipeline step team","text":"By default, the set_pipeline step sets the pipeline for the same team that is running the build.\n\nThe team attribute can be used to specify another team.\n\nOnly the The main team is allowed to set another team's pipeline.  Any team other than the The main team using the team attribute will error, unless they reference their own team.\n\nThe team attribute was introduced in Concourse v6.4.0. It is considered an experimental feature and may be removed at any time. Contribute to the associated discussion with feedback.\n\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.var_files":{"location":"jobs.html#schema.step.set-pipeline-step.var_files","title":"set_pipeline step var_files","text":"A list of paths to .yml files that will be passed to the pipeline config in the same manner as the --load-vars-from flag to fly set-pipeline. This means that if a variable appears in multiple files, the value from a file that is passed later in the list will override the values from files earlier in the list.\n\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.vars":{"location":"jobs.html#schema.step.set-pipeline-step.vars","title":"set_pipeline step vars","text":"A map of template variables to pass to the pipeline config. Unlike instance_vars, vars are solely used to for interpolation, and do not become a part of the pipeline's identifier.\n\nNote that variables set with this field will not propagate to tasks configured via task step file. If you want those variables to be determined at the time the pipeline is set, use task step vars as well.\n\nA var may be statically passed like so:\n\nplan:\n- get: my-repo\n- set_pipeline: configure-the-pipeline\n  file: my-repo/ci/pipeline.yml\n  vars:\n    text: \"Hello World!\"\nAny Vars in the pipeline config will be filled in statically using this field.\n\nFor example, if my-repo/ci/pipeline.yml looks like...:\n\nresources:\n- name: task-image\n  type: registry-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\njobs:\n- name: job\n  plan:\n  - get: task-image\n  - task: do-stuff\n    image: task-image\n    config:\n      platform: linux\n      run:\n        path: echo\n        args: [\"((text))\"]\n...this will resolve \"((text))\" to \"Hello World!\", while ((myuser)) and ((mypass)) will be left in the pipeline to be fetched at runtime.\n\n","depth":3,"section_tag":"steps"},"schema.step.tags":{"location":"jobs.html#schema.step.tags","title":"step.tags","text":"Default []. The tags by which to match workers.\n\nThe step will be placed within the a pool of workers that match all of the given set of tags.\n\nFor example, if [a, b] is specified, only workers advertising the a and b tags (in addition to any others) will be used for running the step.\n\nYou may have a private cluster only reachable by special workers running on-premises. To run steps against those workers, just provide a matching tag:\n\nplan:\n- get: my-repo\n- put: my-site\n  tags: [private]\n  params: {path: my-repo}\n- task: acceptance-tests\n  tags: [private]\n  file: my-repo/ci/acceptance.yml\n","depth":3,"section_tag":"steps"},"schema.step.task-step.config":{"location":"jobs.html#schema.step.task-step.config","title":"task step config","text":"The task config to execute.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.container_limits":{"location":"jobs.html#schema.step.task-step.container_limits","title":"task step container_limits","text":"CPU and memory limits to enforce on the task container.\n\nNote that these values, when specified, will override any limits set by passing the --default-task-cpu-limit or --default-task-memory-limit flags to the concourse web command.\n\nThese values will also override any configuration set on task.container_limits.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.container_limits.cpu":{"location":"jobs.html#schema.step.task-step.container_limits.cpu","title":"task step cpu","text":"The maximum amount of CPU available to the task container, measured in shares. 0 means unlimited.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.container_limits.memory":{"location":"jobs.html#schema.step.task-step.container_limits.memory","title":"task step memory","text":"The maximum amount of memory available to the task container, measured in bytes. 0 means unlimited.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.file":{"location":"jobs.html#schema.step.task-step.file","title":"task step file","text":"A dynamic alternative to task step config.\n\nfile points at a .yml file containing the task config, which allows this to be tracked with your resources.\n\nThe first segment in the path should refer to another source from the plan, and the rest of the path is relative to that source.\n\nThe content of the config file may contain template ((vars)), which will be filled in using task step vars or a configured credential manager.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.image":{"location":"jobs.html#schema.step.task-step.image","title":"task step image","text":"Specifies an artifact source containing an image to use for the task. This overrides any task.image_resource configuration present in the task configuration.\n\nThis is very useful when part of your pipeline involves building an image, possibly with dependencies pre-baked. You can then propagate that image through the rest of your pipeline, guaranteeing that the correct version (and thus a consistent set of dependencies) is used throughout your pipeline.\n\nThis can be used in to explicitly keep track of dependent images:\n\nresources:\n- name: my-image\n  type: registry-image\n  source: {repository: golang, tag: \"1.13\"}\n\n- name: my-repo\n  type: git\n  source: # ...\n\njobs:\n- name: use-image\n  plan:\n  - get: my-image\n  - get: my-repo\n  - task: unit\n    file: my-repo/ci/unit.yml\n    image: my-image\nHere's a pipeline which builds an image in one job and then propagates it to the next:\n\nresources:\n- name: my-project\n  type: git\n  source: {uri: https://github.com/my-user/my-project}\n\n- name: my-task-image\n  type: registry-image\n  source: {repository: my-user/my-repo}\n\njobs:\n- name: build-task-image\n  plan:\n  - get: my-project\n  - put: my-task-image\n    params: {build: my-project/ci/images/my-task}\n\n- name: use-task-image\n  plan:\n  - get: my-task-image\n    passed: [build-task-image]\n  - get: my-project\n    passed: [build-task-image]\n  - task: use-task-image\n    image: my-task-image\n    file: my-project/ci/tasks/my-task.yml\n","depth":3,"section_tag":"steps"},"schema.step.task-step.input_mapping":{"location":"jobs.html#schema.step.task-step.input_mapping","title":"task step input_mapping","text":"A map from task input names to concrete names in the build plan. This allows a task with generic input names to be used multiple times in the same plan, mapping its inputs to specific resources within the plan.\n\nThe following example demonstrates a task with a generic release-repo input being mapped to more specific artifact names:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: audit-diego-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: diego-release}\n- task: audit-cf-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: cf-release}\n","depth":3,"section_tag":"steps"},"schema.step.task-step.output_mapping":{"location":"jobs.html#schema.step.task-step.output_mapping","title":"task step output_mapping","text":"A map from task output names to concrete names to register in the build plan. This allows a task with generic output names to be used multiple times in the same plan.\n\nThis is often used together with task step input_mapping:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: create-diego-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: diego-release}\n  output_mapping: {release-tarball: diego-release-tarball}\n- task: create-cf-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: cf-release}\n  output_mapping: {release-tarball: cf-release-tarball}\n","depth":3,"section_tag":"steps"},"schema.step.task-step.params":{"location":"jobs.html#schema.step.task-step.params","title":"task step params","text":"A map of task environment variable parameters to set, overriding those configured in the task's config or file.\n\nThe difference between params and vars is that vars allows you to interpolate any template variable in an external task, while params can be used to overwrite task parameters (i.e. env variables) specifically. Also, params can have default values declared in the task.\n\nLet's say we have a task config in intgration.yml like so:\n\nplatform: linux\nimage_resource: # ...\nparams:\n  REMOTE_SERVER: https://example.com\n  USERNAME:\n  PASSWORD:\nThis indicates that there are three params which can be set: REMOTE_SERVER, which has a default, and USERNAME and PASSWORD.\n\nA pipeline could run the task with credentials passed in like so:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    USERNAME: my-user\n    PASSWORD: my-pass\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    REMOTE_SERVER: 10.20.30.40:8080\n    USERNAME: ((integration-username))\n    PASSWORD: ((integration-password))\n","depth":3,"section_tag":"steps"},"schema.step.task-step.privileged":{"location":"jobs.html#schema.step.task-step.privileged","title":"task step privileged","text":"Default false. If set to true, the task will run with escalated capabilities available on the task's platform.\n\nSetting privileged: true is a gaping security hole; use wisely and only if necessary. This is not part of the task configuration in order to prevent privilege escalation via pull requests changing the task file.\n\nFor the linux platform, this determines whether or not the container will run in a separate user namespace. When set to true, the container's root user is actual root, i.e. not in a user namespace. This is not recommended, and should never be used with code you do not trust - e.g. pull requests.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.task":{"location":"jobs.html#schema.step.task-step.task","title":"task step task","text":"Executes a task.\n\nWhen a task completes, the artifacts specified by task.outputs will be registered in the build's artifact namespace. This allows subsequent task steps and put steps to access the result of a task.\n\nThe identifier value is just a name - short and sweet. The value is shown in the web UI but otherwise has no affect on anything. This may change in the future; RFC #32 proposes that the name be used to reference a file within the project.\n\nThe following plan pulls down a repo, makes a commit to it, and pushes the commit to another repo (the task must have an output called repo-with-commit):\n\nplan:\n- get: my-repo\n- task: commit\n  file: my-repo/commit.yml\n- put: other-repo\n  params:\n    repository: repo-with-commit\nThe following plan fetches a single repository and executes multiple tasks, using the in_parallel step, in a build matrix style configuration:\n\nplan:\n- get: my-repo\n- in_parallel:\n  - task: go-1.3\n    file: my-repo/go-1.3.yml\n  - task: go-1.4\n    file: my-repo/ci/go-1.4.yml\nOnly if both tasks succeed will the overall step succeed. See also in_parallel step.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.vars":{"location":"jobs.html#schema.step.task-step.vars","title":"task step vars","text":"A map of template variables to pass to an external task. Not to be confused with task.params, which provides environment variables to the task.\n\nThis is to be used with external tasks defined in task step file.\n\nA var may be statically passed like so:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: \"Hello World!\"\nThis is often used in combination with Vars in the pipeline (note the replacement of the string literal with the ((text)) pipeline var):\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: ((text))\nWhen run with the following task.yml:\n\n---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\nrun:\n  path: echo\n  args: [\"((text))\"]\n...this will resolve \"((text))\" to \"Hello World!\", while ((myuser)) and ((mypass))  will be resolved in runtime via a credential manager, if it has been configured.\n\n","depth":3,"section_tag":"steps"},"schema.step.timeout":{"location":"jobs.html#schema.step.timeout","title":"step.timeout","text":"The amount of time to limit the step's execution to, e.g. 30m for 30 minutes.\n\nWhen exceeded, the step will be interrupted, with the same semantics as aborting the build (except the build will be failed, not aborted, to distinguish between human intervention and timeouts being enforced).\n\nThe following will run the unit task and cancel it if it takes longer than 1 hour and 30 minutes:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  timeout: 1h30m\n","depth":3,"section_tag":"steps"},"schema.step.try-step.try":{"location":"jobs.html#schema.step.try-step.try","title":"try step try","text":"Performs the given step, ignoring any failure and masking it with success.\n\nThis can be used when you want to perform some side-effect, but you don't really want the whole build to fail if it doesn't work.\n\nWhen emitting logs somewhere for analyzing later, if the destination flakes out it may not really be critical, so we may want to just swallow the error:\n\nplan:\n- task: run-tests\n  config: # ...\n  on_success:\n    try:\n      put: test-logs\n      params:\n        from: run-tests/*.log\n- task: do-something-else\n  config: # ...\n","depth":3,"section_tag":"steps"},"schema.string":{"location":"config-basics.html#schema.string","title":"string schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.task":{"location":"tasks.html#schema.task","title":"task schema","text":"","depth":2,"section_tag":"tasks"},"schema.task.caches":{"location":"tasks.html#schema.task.caches","title":"task.caches","text":"The cached directories shared between task runs.\n\nOn the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those artifacts.\n\nCaches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.\n\nCaches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off builds.\n\n","depth":2,"section_tag":"tasks"},"schema.task.container_limits":{"location":"tasks.html#schema.task.container_limits","title":"task.container_limits","text":"CPU and memory limits to enforce on the task container.\n\nNote that these values, when specified, will override any limits set by passing the --default-task-cpu-limit or --default-task-memory-limit flags to the concourse web command.\n\n","depth":2,"section_tag":"tasks"},"schema.task.image_resource":{"location":"tasks.html#schema.task.image_resource","title":"task.image_resource","text":"The container image to run with, as provided by an anonymous resource definition.\n\nWhenever the task runs, the anonymous resource will be checked to discover the latest version available. The image will then be fetched onto the worker, if necessary, just prior to running the task.\n\nTo use an image provided by a previous step within your build plan, set task step image on the task step instead.\n\nNOTE: This field is only required for tasks targeting the Linux platform. This field will be ignored for Windows and Darwin workers. Windows containers are currently not supported and Darwin does not have native containers. The task will run inside a clean temporary directory on the Windows/Darwin worker with any inputs and outputs copied into the same directory. Any dependencies should be pre-installed on the worker.\n\nThe following task config will use the golang Docker image to run go version:\n\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source: {repository: golang}\n\nrun:\n  path: go\n  args: [version]\n","depth":2,"section_tag":"tasks"},"schema.task.inputs":{"location":"tasks.html#schema.task.inputs","title":"task.inputs","text":"The set of artifacts used by task, determining which artifacts will be available in the current directory when the task runs.\n\nThese are satisfied by get steps or task.outputs of a previous task. These can also be provided by -i with Running tasks with fly execute.\n\nIf any required inputs are missing at run-time, then the task will error immediately.\n\n","depth":2,"section_tag":"tasks"},"schema.task.outputs":{"location":"tasks.html#schema.task.outputs","title":"task.outputs","text":"The artifacts produced by the task.\n\nEach output configures a directory to make available to later steps in the build plan. The directory will be automatically created before the task runs, and the task should place any artifacts it wants to export in the directory.\n\n","depth":2,"section_tag":"tasks"},"schema.task.params":{"location":"tasks.html#schema.task.params","title":"task.params","text":"A key-value mapping of string keys and values that are exposed to the task via environment variables.\n\nPipelines can override these params by setting task step params on the task step. This is a common method of providing credentials to a task.\n\n","depth":2,"section_tag":"tasks"},"schema.task.platform":{"location":"tasks.html#schema.task.platform","title":"task.platform","text":"The platform the task should run on. This determines the pool of workers that the task can run against.\n\nTechnically any string value is allowed so long as a worker advertises the same platform, but in practice only linux, darwin, and windows are in use.\n\n","depth":2,"section_tag":"tasks"},"schema.task.rootfs_uri":{"location":"tasks.html#schema.task.rootfs_uri","title":"task.rootfs_uri","text":"A string specifying the rootfs uri of the container, as interpreted by your worker's Garden backend.\n\ntask.image_resource is the preferred way to specify base image. You should only use this if you have no other option and you really know what you're doing.\n\n","depth":2,"section_tag":"tasks"},"schema.task.run":{"location":"tasks.html#schema.task.run","title":"task.run","text":"The command to execute in the container.\n\nNote that this is not provided as a script blob, but explicit path and args values; this allows fly to forward arguments to the script, and forces your config .yml to stay fairly small.\n\n","depth":2,"section_tag":"tasks"},"schema.value":{"location":"config-basics.html#schema.value","title":"value schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.var_source":{"location":"vars.html#schema.var_source","title":"var_source schema","text":"","depth":4,"section_tag":"var-sources"},"schema.var_source.dummy-var-source.config":{"location":"vars.html#schema.var_source.dummy-var-source.config","title":"dummy var source config","text":"","depth":4,"section_tag":"var-sources"},"schema.var_source.dummy-var-source.type":{"location":"vars.html#schema.var_source.dummy-var-source.type","title":"dummy var source type","text":"The dummy type supports configuring a static map of vars to values.\n\nThis is really only useful if you have no better alternative for credential management but still have sensitive values that you would like to redact them from build output.\n\n","depth":4,"section_tag":"var-sources"},"schema.var_source.name":{"location":"vars.html#schema.var_source.name","title":"var_source.name","text":"The name of the ((var)) source. This should be short and simple. This name will be referenced ((var)) syntax throughout the config.\n\n","depth":4,"section_tag":"var-sources"},"schema.var_source.vault-var-source.config":{"location":"vars.html#schema.var_source.vault-var-source.config","title":"vault var source config","text":"Configuration for the Vault server has the following schema:\n\n","depth":4,"section_tag":"var-sources"},"schema.var_source.vault-var-source.type":{"location":"vars.html#schema.var_source.vault-var-source.type","title":"vault var source type","text":"The vault type supports configuring a Vault server as a ((var)) source.\n\n","depth":4,"section_tag":"var-sources"},"schema.vars":{"location":"config-basics.html#schema.vars","title":"vars schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.vault_config":{"location":"vars.html#schema.vault_config","title":"vault_config schema","text":"","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_backend":{"location":"vars.html#schema.vault_config.auth_backend","title":"vault var source auth_backend","text":"Authenticate using an auth backend, e.g. cert or approle.\n\nSee Using the approle auth backend or Using the cert auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_max_ttl":{"location":"vars.html#schema.vault_config.auth_max_ttl","title":"vault var source auth_max_ttl","text":"Maximum duration to elapse before forcing the client to log in again.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_params":{"location":"vars.html#schema.vault_config.auth_params","title":"vault var source auth_params","text":"A key-value map of parameters to pass during authentication.\n\nSee Using the approle auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_retry_initial":{"location":"vars.html#schema.vault_config.auth_retry_initial","title":"vault var source auth_retry_initial","text":"When retrying during authentication, start with this retry interval. The interval will increase exponentially until auth_retry_max is reached.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_retry_max":{"location":"vars.html#schema.vault_config.auth_retry_max","title":"vault var source auth_retry_max","text":"When failing to authenticate, give up after this amount of time.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.ca_cert":{"location":"vars.html#schema.vault_config.ca_cert","title":"vault var source ca_cert","text":"The PEM encoded contents of a CA certificate to use when connecting to the API.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.client_cert":{"location":"vars.html#schema.vault_config.client_cert","title":"vault var source client_cert","text":"A PEM encoded client certificate, for use with TLS based auth.\n\nSee Using the cert auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.client_key":{"location":"vars.html#schema.vault_config.client_key","title":"vault var source client_key","text":"A PEM encoded client key, for use with TLS based auth.\n\nSee Using the cert auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.client_token":{"location":"vars.html#schema.vault_config.client_token","title":"vault var source client_token","text":"Authenticate via a periodic client token.\n\nSee Using a periodic token for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.insecure_skip_verify":{"location":"vars.html#schema.vault_config.insecure_skip_verify","title":"vault var source insecure_skip_verify","text":"Skip TLS validation. Not recommended. Don't do it. No really, don't.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.lookup_templates":{"location":"vars.html#schema.vault_config.lookup_templates","title":"vault var source lookup_templates","text":"Default [\"/{{.Team}}/{{.Pipeline}}/{{.Secret}}\", \"/{{.Team}}/{{.Secret}}\"].\n\nA list of path templates to be expanded in a team and pipeline context subject to the path_prefix and namespace.\n\nSee Changing the path templates for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.namespace":{"location":"vars.html#schema.vault_config.namespace","title":"vault var source namespace","text":"A Vault namespace to operate under.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.path_prefix":{"location":"vars.html#schema.vault_config.path_prefix","title":"vault var source path_prefix","text":"Default /concourse. A prefix under which to look for all credential values.\n\nSee Changing the path prefix for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.server_name":{"location":"vars.html#schema.vault_config.server_name","title":"vault var source server_name","text":"The expected name of the server when connecting through TLS.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.shared_path":{"location":"vars.html#schema.vault_config.shared_path","title":"vault var source shared_path","text":"An additional path under which credentials will be looked up.\n\nSee Configuring a shared path for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.url":{"location":"vars.html#schema.vault_config.url","title":"vault var source url","text":"The URL of the Vault API.\n\n","depth":4,"section_tag":"var-sources"},"schema.version":{"location":"config-basics.html#schema.version","title":"version schema","text":"","depth":3,"section_tag":"basic-schemas"},"security-contact":{"location":"project.html#security-contact","title":"Report a security issue","text":"To report a security issue, please send an email to security@concourse-ci.org.\n\nSecurity advisories will be published as concourse/concourse GitHub Security Advisories and subsequently cross-posted to the Concourse Security mailing list which you may join to subscribe to security updates.\n\n","depth":2,"section_tag":"security-contact"},"serial-job-example":{"location":"serial-job-example.html","title":"Serial job example","text":"Setting the job.serial flag restricts a job to run one build at a time.\n\nBy default, jobs are run in parallel. For some use cases this might be ideal (ex. testing all incoming commits from a repository). For other use cases this might be less ideal (ex. deploying an application).\n\nYou can also set the job.max_in_flight value to 1 to disable parallel job runs.\n\n","depth":2,"section_tag":"serial-job-example"},"service-providers":{"location":"ecosystem.html#service-providers","title":"Service Providers","text":"The following organisations offer a range of Concourse-related services, such as training, consultancy, support and managed offerings.\n\n* Altoros\n\n* anynines\n\n* Armakuni\n\n* EngineerBetter\n\n* Gstack\n\n* Novatec\n\n* Resilient Scale\n\n* Stark \u0026 Wayne\n\n* SuperOrbital\n\n* TRULLLA Software\n\n* VMware Tanzu Labs\n\n","depth":2,"section_tag":"service-providers"},"set-pipeline-step":{"location":"jobs.html#set-pipeline-step","title":"set_pipeline step","text":"","depth":3,"section_tag":"steps"},"set-pipelines-example":{"location":"set-pipelines-example.html","title":"Set Pipelines Example","text":"You can set a static set of pipelines from another pipeline on the same team.\n\n","depth":2,"section_tag":"set-pipelines-example"},"setpipeline":{"location":"opa.html#setpipeline","title":"SetPipeline","text":"This action occurs whenever a set_pipeline step is run. The JSON payload for this action will contain the pipeline config in JSON format under the data key:\n\n{\n  \"input\": {\n    \"service\": \"concourse\",\n    \"cluster_name\": \"dev\",\n    \"cluster_version\": \"7.4.0\",\n    \"action\": \"SetPipeline\",\n    \"team\": \"main\",\n    \"pipeline\": \"simple\",\n    \"data\": {\n      \"jobs\": [\n        {\n          \"name\": \"test\",\n          \"plan\": [\n            {\n              \"get\": \"tiny\"\n            },\n            {\n              \"config\": {\n                \"image_resource\": {\n                  \"source\": {\n                    \"repository\": \"busybox\"\n                  },\n                  \"type\": \"registry-image\"\n                },\n                \"platform\": \"linux\",\n                \"run\": {\n                  \"args\": [\n                    \"-exc\",\n                    \"echo hello\"\n                  ],\n                  \"path\": \"sh\"\n                }\n              },\n              \"task\": \"a-task\" }\n] } ] } } }\n","depth":5,"section_tag":"setpipeline"},"setting-pipelines":{"location":"setting-pipelines.html","title":"Setting Pipelines","text":"Pipelines are configured entirely via the The fly CLI. There is no GUI.\n\n","depth":3,"section_tag":"setting-pipelines"},"setting-roles":{"location":"managing-teams.html#setting-roles","title":"Setting User Roles","text":"By default, authorization config passed to set-team configures the owner role.\n\nMore advanced roles configuration can be specified through the --config or -c flag.\n\nThe -c flag expects a .yml file with a single field, roles:, pointing to a list of role authorization configs.\n\nAll of the attributes in each config will vary by provider. Consult the appropriate section for your provider under Configuring Auth for specifics.\n\nFor example, the following config sets three roles with different auth config for each role's provider:\n\nroles:\n- name: owner\n  github:\n    users: [\"admin\"]\n- name: member\n  github:\n    teams: [\"org:team\"]\n- name: viewer\n  github:\n    orgs: [\"org\"]\n  local:\n    users: [\"visitor\"]\n","depth":5,"section_tag":"setting-roles"},"sharing-versions-doesnt-work-well-for-all-resource-types":{"location":"global-resources.html#sharing-versions-doesnt-work-well-for-all-resource-types","title":"Sharing versions doesn't work well for all resource types","text":"Sharing versions isn't always a good idea. For example, the time resource is often used to generate versions on an interval so that jobs can fire periodically. If version history were to be shared for all users with e.g. a 10 minute interval, that would lead to a thundering herd of builds storming your workers, leading to load spikes and a lot of unhappy builds.\n\nWe are working toward a solution to the time resource's thundering herd problem - namely, to not model time as a resource, and instead model it as a var_source. We are tracking progress toward this goal in 5815.\n\nAnother case where version history shouldn't be shared is when resources \"automagically\" learn their auth credentials using things like IAM roles. In these cases, the credentials aren't in the resource.source. If version history were to be shared, anyone could configure the same source:, not specifying any credentials, and see the version history discovered by some other pipeline that ran its checks on workers that had access via IAM roles.\n\nFor this reason, any resource types that acquire credentials outside of source: should not share version history. Granted, the user won't be able to fetch these versions, but it's still an information leak.\n\nIAM roles are a bit of a thorn in our side when it comes to designing features like this. We're planning on introducing support for them in a way that doesn't have this problem in 3023.\n\n","depth":5,"section_tag":"sharing-versions-doesnt-work-well-for-all-resource-types"},"special-actions":{"location":"opa.html#special-actions","title":"Special Actions","text":"Most of the actions you can filter for come directly from the list of API actions. There are currently two special actions you can also filter on.\n\n","depth":4,"section_tag":"special-actions"},"specify-inputs-for-put-steps":{"location":"common-pipeline-practices.html#specify-inputs-for-put-steps","title":"Specify Inputs for Put Steps","text":"By default put step's have all artifacts from a job mounted in their resource container. This can result in long initialization times for put steps. It's likely that a put step only needs a subset of all available artifacts generated throughout the job.\n\nThere are two ways to specify which artifacts to send to a put step. You can specify detect as the put step inputs or you can pass in an exact list of all artifacts the put step needs.\n\nUsing detect: jobs:\n- name: the-job\n  plan: # Get some artifacts\n  - in_parallel:\n    - get: apples\n    - get: apple-basket\n    - get: monkeys\n  # using detect will result in \"apples-location\" and \"basket\" being passed in\n  # \"monkeys\" will not be passed in\n  - put: apples-in-basket\n    input: detect\n    params:\n      apples-location: apples/location # matches the first get step\n      basket: apple-basket # matches the second get step\n\n\nSpecifying the exact inputs needed for the put step: jobs:\n- name: the-job\n  plan: # Get some artifacts\n  - in_parallel:\n    - get: apples\n    - get: apple-basket\n    - get: monkeys\n  - put: apples-in-basket\n    input: [apples, apple-basket] # specify the exact inputs needed\n    params:\n      apples-location: apples/location\n      basket: apple-basket\n\n\n","depth":5,"section_tag":"specify-inputs-for-put-steps"},"static-vars":{"location":"vars.html#static-vars","title":"Static vars","text":"Var values may also be specified statically using the set_pipeline step and task step.\n\nWhen running the The fly CLI equivalent commands (fly set-pipeline and Running tasks with fly execute), var values may be provided using the following flags:\n\n* -v or --var NAME=VALUE sets the string VALUE as the value for the var NAME.\n\n* -y or --yaml-var NAME=VALUE parses VALUE as YAML and sets it as the value for the var NAME.\n\n* -i or --instance-var NAME=VALUE parses VALUE as YAML and sets it as the value for the instance var NAME. See Grouping Pipelines to learn more about instance vars.\n\n* -l or --load-vars-from FILE loads FILE, a YAML document containing mapping var names to values, and sets them all.\n\nWhen used in combination with -l, the -y and -v flags take precedence. This way a vars file may be re-used, overriding individual values by hand.\n\nLet's say we have a task config like so:\n\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: golang\n    tag: ((tag))\n\ninputs:\n- name: booklit\n\nrun:\n  path: booklit/ci/unit\nWe could use vars to run this task against different versions of Go:\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - task: unit-1.13\n    file: booklit/ci/unit.yml\n    vars: {tag: 1.13}\n  - task: unit-1.8\n    file: booklit/ci/unit.yml\n    vars: {tag: 1.8}\nWith a pipeline template like so:\n\nresources:\n- name: booklit\n  type: booklit\n  source:\n    uri: https://github.com/vito/booklit\n    branch: ((branch))\n    private_key: ((\"github.com\".private_key))\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: ((trigger))\n  - task: unit\n    file: booklit/ci/unit.yml\nLet's say we have a private key in a file called private_key.\n\nThe fly validate-pipeline command may be used to test how interpolation is applied, by passing the --output flag.\n\n$ fly validate-pipeline \\\n  -c pipeline.yml \\\n  -y trigger=true \\\n  -v \\\"github.com\\\".private_key=\"$(cat private_key)\" \\\n  -v branch=master \\\n  --output\nThe above incantation should print the following:\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - file: booklit/ci/unit.yml\n    task: unit\nresources:\n- name: booklit\n  type: booklit\n  source:\n    branch: master\n    private_key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      # ... snipped ...\n      -----END RSA PRIVATE KEY-----\n    uri: https://github.com/vito/booklit\nNote that we had to use -y so that the trigger: true ends up with a boolean value instead of the string \"true\".\n\nWith a pipeline template like so:\n\nresources:\n- name: booklit\n  type: booklit\n  source:\n    uri: https://github.com/vito/booklit\n    branch: ((branch))\n    private_key: ((\"github.com\".private_key))\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: ((trigger))\n  - task: unit\n    file: booklit/ci/unit.yml\nLet's say I've put the private_key var in a file called vars.yml, since it's quite large and hard to pass through flags:\n\ngithub.com:\n  private_key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    # ... snipped ...\n    -----END RSA PRIVATE KEY-----\nThe fly validate-pipeline command may be used to test how interpolation is applied, by passing the --output flag.\n\n$ fly validate-pipeline \\\n  -c pipeline.yml \\\n  -l vars.yml \\\n  -y trigger=true \\\n  -v branch=master \\\n  --output\nThe above incantation should print the following:\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - task: unit\n    file: booklit/ci/unit.yml\nresources:\n- name: booklit\n  type: booklit\n  source:\n    branch: master\n    private_key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      # ... snipped ...\n      -----END RSA PRIVATE KEY-----\n    uri: https://github.com/vito/booklit\nNote that we had to use -y so that the trigger: true ends up with a boolean value instead of the string \"true\".\n\n","depth":3,"section_tag":"static-vars"},"steps":{"location":"jobs.html#steps","title":"Steps","text":"Each job has a single build plan configured as job.plan. A build plan is a recipe for what to run when a build of the job is created.\n\nA build plan is a sequence of steps:\n\n* the task step runs a task\n\n* the get step fetches a resource\n\n* the put step updates a resource\n\n* the set_pipeline step configures a pipeline\n\n* the load_var step loads a value into a local var\n\n* the in_parallel step runs steps in parallel\n\n* the do step runs steps in sequence\n\n* the across step modifier runs a step multiple times; once for each combination of variable values\n\n* the try step attempts to run a step and succeeds even if the step fails\n\nWhen a new version is available for a get step with trigger: true configured, a new build of the job will be created from the build plan.\n\nWhen viewing the job in the pipeline, resources that are used as get steps appear as inputs, and resources that are used in put steps appear as outputs. Jobs are rendered downstream of any jobs they reference in passed constraints, connected by the resource.\n\nIf any step in the build plan fails, the build will fail and subsequent steps will not be executed. Additional steps may be configured to run after failure by configuring step.on_failure or step.ensure (or the job equivalents, job.on_failure and job.ensure).\n\n","depth":3,"section_tag":"steps"},"tagging-workers":{"location":"concourse-worker.html#tagging-workers","title":"Tagging Workers","text":"If there's something special about your worker and you'd like to target builds at it specifically, you can configure tags like so:\n\nCONCOURSE_TAG=\"tag-1,tag-2\"\nA tagged worker is taken out of the default placement logic. Tagged workers will not be used for any untagged Steps.\n\nTo run build steps on a tagged worker, specify the step.tags on any particular step in your job.\n\nTo perform resource checks on on a tagged worker, specify tags on the resource declaration.\n\n","depth":5,"section_tag":"tagging-workers"},"tags-and-team-workers":{"location":"concourse-worker.html#tags-and-team-workers","title":"Tags and Team Workers","text":"When you have a worker configured with tag(s) and a team like so:\n\nCONCOURSE_TAG=\"tag-1,tag-2\"\nCONCOURSE_TEAM=\"lightweavers\"\nOnly steps that are tagged and from the specified team will be scheduled on such a worker. Any untagged work the team has will land on either:\n\n1. Untagged team workers belonging to the team, or\n\n2. Untagged workers not configured to a specific team\n\n","depth":6,"section_tag":"tags-and-team-workers"},"task-environment":{"location":"tasks.html#task-environment","title":"Task runtime environment","text":"A task runs in a new container every time, using the image provided by task.image_resource as its base filesystem (i.e. /).\n\nThe command specified by task.run will be executed in a working directory containing each of the task.inputs. If any input is missing, the task will not run (and the container will not even be created).\n\nThe working directory will also contain empty directories for each of the task.outputs. The task must place artifacts in the output directories for them to be exported. This meshes well with build tools with configurable destination paths.\n\nIf your build tools don't support output paths, you can configure an input and output with the same path. The directory will be populated by the input, and any changes made to the directory will propagate downstream as an output.\n\nAny task step params configured will be set in the environment for the task's command, along with any environment variables provided by the task's image (i.e. ENV rules from your Dockerfile).\n\nThe user the command runs as is determined by the image. If you're using a Docker image, this will be the user set by a USER rule in your Dockerfile, or root, if not specified.\n\nAnother relevant bit of configuration is task step privileged, which determines whether the user the task runs as will have full privileges (primarily when running as root). This is intentionally not configurable by the task itself, to prevent privilege escalation by submitting pull requests to repositories that contain task configs.\n\nPutting all this together, the following task config:\n\n---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: golang\n    tag: '1.6'\n\nparams:\n  SOME_PARAM: some-default-value\n\ninputs:\n- name: some-input\n- name: some-input-with-custom-path\n  path: some/custom/path\n\noutputs:\n- name: some-output\n\nrun:\n  path: sh\n  args:\n  - -exc\n  - |\n    whoami\n    env\n    go version\n    find .\n    touch some-output/my-built-artifact\n...will produce the following output:\n\n+ whoami\nroot\n+ env\nUSER=root\nHOME=/root\nGOLANG_DOWNLOAD_SHA256=5470eac05d273c74ff8bac7bef5bad0b5abbd1c4052efbdbc8db45332e836b0b\nPATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nGOPATH=/go\nPWD=/tmp/build/e55deab7\nGOLANG_DOWNLOAD_URL=https://golang.org/dl/go1.6.linux-amd64.tar.gz\nGOLANG_VERSION=1.6\nSOME_PARAM=some-default-value\n+ go version\ngo version go1.6 linux/amd64\n+ find .\n.\n./some-input\n./some-input/foo\n./some\n./some/custom\n./some/custom/path\n./some/custom/path/bar\n./some-output\n+ touch some-output/my-built-artifact\n...and propagate my-built-artifact to any later task steps or put steps that reference the some-output artifact, in the same way that this task had some-input as an input.\n\n","depth":3,"section_tag":"task-environment"},"task-inputs-outputs-example":{"location":"task-inputs-outputs-example.html","title":"Task inputs and outputs example","text":"A task can pass an artifacts to another task in the same job.\n\nTasks within a job have the ability to pass artifacts directly inbetween them to allow you to process artifacts in many ways.\n\nWhile you are free to create as many jobs as you'd like for your pipeline, you have to use resources to pass artifacts inbetween them.\n\nThese constructs give you the ability to design a pipeline that can process artifacts in many different ways via Tasks, and then store those processed artifacts externally via Resources.\n\n","depth":2,"section_tag":"task-inputs-outputs-example"},"task-step":{"location":"jobs.html#task-step","title":"task step","text":"","depth":3,"section_tag":"steps"},"tasks":{"location":"tasks.html","title":"Tasks","text":"The smallest configurable unit in a Concourse pipeline is a single task. A task can be thought of as a function from task.inputs to task.outputs that can either succeed or fail.\n\nGoing a bit further, ideally tasks are pure functions: given the same set of inputs, it should either always succeed with the same outputs or always fail. This is entirely up to your script's level of discipline, however. Flaky tests or dependencies on the internet are the most common source of impurity.\n\nOnce you have a running Concourse deployment, you can start configuring your tasks and executing them interactively from your terminal with the Fly commandline tool.\n\nOnce you've figured out your task's configuration, you can reuse it for a Job in your Pipeline.\n\nConventionally a task's configuration is placed in the same repository as the code it's testing, possibly under some ci directory.\n\nA task's configuration specifies the following:\n\nThis configuration specifies that the task must run with the ruby:2.1 Docker image with a my-app input, and when the task is executed it will run the scripts/test script in the same repo.\n\n---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: ruby\n    tag: '2.1'\n\ninputs:\n- name: my-app\n\nrun:\n  path: my-app/scripts/test\nA task can configure task.outputs to produce artifacts that can then be propagated to a put step or another task step in the same plan. They can also be downloaded with Running tasks with fly execute by passing -o.\n\n---\nplatform: linux\n\nimage_resource: # ...\n\ninputs:\n- name: project-src\n\noutputs:\n- name: built-project\n\nrun:\n  path: project-src/ci/build\n...assuming project-src/ci/build looks something like:\n\n#!/bin/bash\n\nset -e -u -x\n\nexport GOPATH=$PWD/project-src\n\ngo build -o built-project/my-project \\\n  github.com/concourse/my-project\n...this task could then be used in a build plan like so:\n\nplan:\n- get: project-src\n- task: build-bin\n  file: project-src/ci/build.yml\n- put: project-bin\n  params: file: built-project/my-project\nThe following task and script could be used by a Node project to cache the node_modules directory:\n\n---\nplatform: linux\n\nimage_resource: # ...\n\ninputs:\n- name: project-src\n\ncaches:\n- path: project-src/node_modules\n\nrun:\n  path: project-src/ci/build\n...assuming project-src/ci/build looks something like:\n\n#!/bin/bash\n\nset -e -u -x\n\ncd project-src\nnpm install\n\n# ...\n...this task would cache the contents of project-src/node_modules between runs of this task on the same worker.\n\nThe following external task uses an image from a private registry. Assuming the CA is configured properly on the workers, SSL should Just Work™.\n\nExternal tasks are now fully interpolated using credential manager variables and task step vars, so you can use template variables in an external task:\n\n---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\ninputs:\n- name: my-app\n\nrun:\n  path: my-app/scripts/test\n  args: [\"Hello, world!\", \"((myparam))\"]\n","depth":2,"section_tag":"tasks"},"team-member-role":{"location":"user-roles.html#team-member-role","title":"member role","text":"Team members can operate within their team in a read \u0026 write fashion, but they can not change the configuration of their team.\n\nActions assigned to the member role by default:\n\nmember:\n- SaveConfig\n- CreateBuild\n- DeletePipeline\n- OrderPipelines\n- OrderPipelinesWithinGroup\n- ExposePipeline\n- HidePipeline\n- RenamePipeline\n- ArchivePipeline\n- CreatePipelineBuild\n- RegisterWorker\n- LandWorker\n- RetireWorker\n- PruneWorker\n- HeartbeatWorker\n- DeleteWorker\n- HijackContainer\n- ReportWorkerContainers\n- ReportWorkerVolumes\n- CreateArtifact\n- GetArtifact\n","depth":4,"section_tag":"team-member-role"},"team-owner-role":{"location":"user-roles.html#team-owner-role","title":"owner role","text":"Team owners have read, write and auth management capabilities within the scope of their team, and can rename or destroy the team.\n\nActions assigned to the owner role by default:\n\nowner:\n- SetTeam\n- RenameTeam\n- DestroyTeam\n","depth":4,"section_tag":"team-owner-role"},"team-pipeline-operator-role":{"location":"user-roles.html#team-pipeline-operator-role","title":"pipeline-operator role","text":"Team pipeline operators can perform pipeline operations such as triggering builds and pinning resources, however they cannot update pipeline configurations.\n\nActions assigned to the pipeline-operator role by default:\n\npipeline-operator:\n- AbortBuild\n- RerunJobBuild\n- CreateJobBuild\n- PauseJob\n- UnpauseJob\n- ClearTaskCache\n- UnpinResource\n- SetPinCommentOnResource\n- CheckResource\n- CheckResourceWebHook\n- CheckResourceType\n- EnableResourceVersion\n- DisableResourceVersion\n- PinResourceVersion\n- PausePipeline\n- UnpausePipeline\n- ClearResourceCache\n","depth":4,"section_tag":"team-pipeline-operator-role"},"team-viewer-role":{"location":"user-roles.html#team-viewer-role","title":"viewer role","text":"Team viewers have \"read-only\" access to a team and its pipelines. This locks everything down, preventing users from doing a fly set-pipeline or fly intercept.\n\nActions assigned to the viewer role by default:\n\nviewer:\n- GetConfig\n- GetCC\n- GetBuild\n- GetCheck\n- GetBuildPlan\n- ListBuilds\n- BuildEvents\n- BuildResources\n- GetBuildPreparation\n- GetJob\n- ListAllJobs\n- ListJobs\n- ListJobBuilds\n- ListJobInputs\n- GetJobBuild\n- GetVersionsDB\n- JobBadge\n- MainJobBadge\n- ListAllResources\n- ListResources\n- ListResourceTypes\n- GetResource\n- ListResourceVersions\n- GetResourceVersion\n- ListBuildsWithVersionAsInput\n- ListBuildsWithVersionAsOutput\n- GetResourceCausality\n- ListAllPipelines\n- ListPipelines\n- GetPipeline\n- ListPipelineBuilds\n- PipelineBadge\n- ListWorkers\n- DownloadCLI\n- GetInfo\n- ListContainers\n- GetContainer\n- ListDestroyingContainers\n- ListVolumes\n- ListDestroyingVolumes\n- ListTeams\n- GetTeam\n- ListTeamBuilds\n- ListBuildArtifacts\n","depth":4,"section_tag":"team-viewer-role"},"team-workers":{"location":"concourse-worker.html#team-workers","title":"Team Workers","text":"If you want to isolate all workloads for a team then you can configure a worker to belong to a single team like so:\n\nCONCOURSE_TEAM=\"lightweavers\"\nOnce an untagged team worker is registered Concourse will schedule all untagged builds for that team on its team worker(s). Builds for this team will no longer be scheduled on any untagged, non-team workers.\n\nIt is possible to have a Concourse cluster made up of only team workers and have zero non-team workers, though this is not a common setup because resource utilization across all workers ends up underutilized. It is useful though if you have a particular team with heavy workloads that usually bothers other teams pipelines.\n\n","depth":5,"section_tag":"team-workers"},"teams":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"teams-caveats":{"location":"teams-caveats.html","title":"Security Caveats","text":"At present, teams only provide trusted multi-tenancy. This means it should be used for cases where you know and trust who you're allowing access into your Concourse cluster.\n\nThere are a few reasons it'd be a bad idea to do otherwise:\n\n* Any team can run builds with task step privileged tasks. A bad actor in the mix could easily use this to harm your workers and your cluster.\n\n  In the future, we'll probably have this as a flag on a team, indicating whether they're permitted to run privileged builds.\n\n* There are no networking restrictions in place, and traffic to and from the workers is currently unencrypted and unauthorized. Anyone could run a task that does horrible things to your worker's containers, possibly stealing sensitive information.\n\n  This can be remedied with configuration specified on Garden to restrict access to the internal network, but this is not detailed in our docs, and we'll probably want to find a better answer than configuration in the future.\n\n","depth":3,"section_tag":"teams-caveats"},"thanks":{"location":"project.html#thanks","title":"Thanks!","text":"It's been a long journey and we've got a lot of people to thank for our continued success. We are deeply indebted to any and all who help keep this project going, but the heroic effort of the following organizations is worth giving special props.\n\nPivotal: Concourse wouldn't be what it is today without Pivotal. This goes beyond the sponsorship, which began in early 2015 - without the experiences we had and the practices we learned while working on Cloud Foundry and BOSH, we would have neither the technical experience nor the strong opinions that led to Concourse being born.\n\n\nStark \u0026 Wayne: The Concourse Tutorial by Stark \u0026 Wayne  is probably the only reason a lot of people were able to learn Concourse. It's a great asset and does its job so well that we've decided to basically just kill our tutorials and delegate to theirs. Thanks to Dr. Nic for writing and maintaining it, and everyone else that has helped out!\n\n\n\n","depth":2,"section_tag":"thanks"},"the-dockerfile":{"location":"building-and-pushing-an-image.html#the-dockerfile","title":"The Dockerfile","text":"FROM busybox\n\nRUN echo \"I'm simple!\"\nCOPY ./stranger /stranger\nRUN cat /stranger\n","depth":5,"section_tag":"the-dockerfile"},"the-entire-pipeline":{"location":"building-and-pushing-an-image.html#the-entire-pipeline","title":"The Entire Pipeline","text":"Putting all the pieces together, here is our pipeline that builds and pushes a container image.\n\nresources:\n# The repo with our Dockerfile\n- name: concourse-examples\n  type: git\n  icon: github\n  source:\n    uri: https://github.com/concourse/examples.git\n    branch: master\n\n# Where we will push the image\n- name: simple-image\n  type: registry-image\n  icon: docker\n  source:\n    repository: ((image-repo-name))/simple-image\n    username: ((registry-username))\n    password: ((registry-password))\n\njobs:\n- name: build-and-push\n  plan:\n  - get: concourse-examples\n  - task: build-task-image\n    privileged: true\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: concourse/oci-build-task\n      inputs:\n      - name: concourse-examples\n      outputs:\n      - name: image\n      params:\n        CONTEXT: concourse-examples/Dockerfiles/simple\n      run:\n        path: build\n  - put: simple-image\n    params:\n      image: image/image.tar\n \n\nYou can set the pipeline with the following fly command, updating the variable values with real values the pipeline can use to run.\n\nfly -t \u003ctarget\u003e set-pipeline -p build-and-push-image \\\n  -c ./examples/pipelines/build-and-push-simple-image.yml \\\n  --var image-repo-name=\u003crepo-name\u003e \\\n  --var registry-username=\u003cuser\u003e \\\n  --var registry-password=\u003cpassword\u003e\n","depth":5,"section_tag":"the-entire-pipeline"},"the-heart-of-concourse":{"location":"tutorial-resources.html#the-heart-of-concourse","title":"The Heart of Concourse","text":"Resources are the heart of Concourse. Resources make Concourse tick and are the source of automation within all Concourse pipelines. Resources are how Concourse interacts with the outside world. Here's a short list of some things that resources can do:\n\n* You want something to run every five minutes? Time resource.\n\n* You want to run tests on every new commit to the main branch? Git resource.\n\n* Run unit tests on new PR's? Github-PR resource.\n\n* Fetch or push the latest image of your app? Registry-image resource\n\nResources can do a lot of things! The main goal of resources is to represent some external system or object in your pipeline. That external thing can then be used as a trigger for your Jobs or your Jobs can push back and modify the external system or object. It all depends on the resource you use and what features its author has implemented.\n\nResources are also how Concourse tries to stay as technology agnostic as possible. For example, Concourse doesn't care what version control system you store your code in, if you deploy apps with Helm or Terraform, or what language your apps are built in.  If you can put your latest and hottest tech behind the resource interface then Concourse can understand your workflow.\n\nThe Concourse team bundles a few basic resource types that come with the Linux tarball you can download from Github. You'll notice that the Linux tarball is much larger than the macOS or Windows tarball because of all the bundled resources.\n\nYou can find out which resources a worker has by running: $ fly -t tutorial workers --details\n\n\nResources only run on Linux workers because resources are distributed as Linux container images. There are currently no resources for macOS or Windows. Only task steps can run on macOS or Windows workers.\n\n","depth":4,"section_tag":"the-heart-of-concourse"},"time-to-takeoff":{"location":"tutorial-resources.html#time-to-takeoff","title":"Time For Takeoff ✈️","text":"This brings us to the end of the tutorial. You should have a basic understanding about how to read Concourse pipelines and start creating your own. Here are some other parts of the site to help you take off with Concourse:\n\n* How-To Guides - Contains tips for writing pipelines and examples of common pipeline workflows\n\n* Check out all the reference documentation: * Jobs\n\n  * Tasks\n\n  * Resources\n\n  * Resource Types\n\n* Implement your own resource type\n\n* Find other resources at resource-types.concourse-ci.org or put something concourse resource into your favorite search engine.\n\nBest of luck on your automation journey!\n\nIf you have any feedback for this tutorial please share it in this Github discussion\n\n","depth":4,"section_tag":"time-to-takeoff"},"time-trigger-example":{"location":"time-trigger-example.html","title":"time-triggered job example","text":"The time resource can be used to trigger a job.\n\n","depth":2,"section_tag":"time-trigger-example"},"tracing":{"location":"tracing.html","title":"Tracing","text":"This is an experimental feature.\n\nTracing in Concourse enables the delivery of traces related to the internal processes that go into running builds, and other internal operations, breaking them down by time, and component.\n\nIt leverages the (OpenTelemetry) SDK to allow support for many platforms. Currently tracing can be configured to integrates with:\n\n* Jaeger\n\n* Google Cloud Trace (Stackdriver)\n\n* Honeycomb.io\n\n* OpenTelemetry Protocol Exporter\n\n","depth":3,"section_tag":"tracing"},"transitioning-from-guardian-to-containerd":{"location":"concourse-worker.html#transitioning-from-guardian-to-containerd","title":"Transitioning from Guardian to containerd","text":"If you are transitioning from Guardian to containerd you will need to convert any --garden-* (CONCOURSE_GARDEN_*) flags to their containerd (CONCOURSE_CONTAINERD_*) counterparts:\n\n| Guardian Flags | Containerd Flags |\n| --garden-request-timeout CONCOURSE_GARDEN_REQUEST_TIMEOUT\n\n | --containerd-request-timeout CONCOURSE_CONTAINERD_REQUEST_TIMEOUT\n\n |\n| --garden-dns-proxy-enable CONCOURSE_GARDEN_DNS_PROXY_ENABLE\n\n | --containerd-dns-proxy-enable CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE\n\n |\n| No equivalent CLI flag CONCOURSE_GARDEN_ALLOW_HOST_ACCESS\n\n | --containerd-allow-host-access CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS\n\n |\n| --garden-network-pool CONCOURSE_GARDEN_NETWORK_POOL\n\n | --containerd-network-pool CONCOURSE_CONTAINERD_NETWORK_POOL\n\n |\n| --garden-max-containers CONCOURSE_GARDEN_MAX_CONTAINERS\n\n | --containerd-max-containers CONCOURSE_CONTAINERD_MAX_CONTAINERS\n\n |\n| No equivalent CLI flag CONCOURSE_GARDEN_DENY_NETWORKS\n\n | --containerd-restricted-network CONCOURSE_CONTAINERD_RESTRICTED_NETWORK\n\n |\n| No equivalent CLI flag CONCOURSE_GARDEN_DNS_SERVER\n\n | --containerd-dns-server CONCOURSE_CONTAINERD_DNS_SERVER\n\n |\n| No equivalent CLI flag CONCOURSE_GARDEN_EXTERNAL_IP\n\n | --containerd-external-ip CONCOURSE_CONTAINERD_EXTERNAL_IP\n\n |\n| No equivalent CLI flag CONCOURSE_GARDEN_MTU\n\n | --containerd-mtu CONCOURSE_CONTAINERD_MTU\n\n |\n\n","depth":6,"section_tag":"transitioning-from-guardian-to-containerd"},"troubleshooting-and-fixing-dns-resolution":{"location":"concourse-worker.html#troubleshooting-and-fixing-dns-resolution","title":"Troubleshooting and fixing DNS resolution","text":"Note: The Guardian runtime took care of a lot of container creation operations for Concourse in the past. It was very user-friendly for the project to use as a container runtime. While implementing the containerd runtime most reported bugs were actually a difference in containerd's default behaviour compared to Guardian's. Currently Concourse's containerd runtime mostly behaves like the Guardian runtime did. Most of the following DNS section should apply to both runtimes.\n\nBy default, containers created by the Guardian or containerd (will refer to both as runtime) runtime will carry over the /etc/resolv.conf from the host into the container. This is often fine, but some Linux distributions configure a special 127.x.x.x DNS resolver (e.g. systemd-resolved).\n\nWhen the runtime copies the resolv.conf over, it removes these entries as they won't be reachable from the container's network namespace. As a result, your containers may not have any valid nameservers configured.\n\nTo diagnose this problem you can fly intercept into a failing container and check which nameservers are in /etc/resolv.conf:\n\n$ fly -t ci intercept -j concourse/concourse\nbash-5.0$ grep nameserver /etc/resolv.conf\nbash-5.0$\nIn this case it is empty, as the host only listed a single 127.0.0.53 address which was then stripped out. To fix this you'll need to explicitly configure DNS instead of relying on the default runtime behavior.\n\n","depth":6,"section_tag":"troubleshooting-and-fixing-dns-resolution"},"try-step":{"location":"jobs.html#try-step","title":"try step","text":"","depth":3,"section_tag":"steps"},"tutorial-hello-world":{"location":"tutorial-hello-world.html","title":"Hello World Pipeline","text":"","depth":3,"section_tag":"tutorial-hello-world"},"tutorial-inputs-outputs":{"location":"tutorial-inputs-outputs.html","title":"Inputs and Outputs","text":"","depth":3,"section_tag":"tutorial-inputs-outputs"},"tutorial-resources":{"location":"tutorial-resources.html","title":"Resources","text":"","depth":3,"section_tag":"tutorial-resources"},"upgrading-concourse":{"location":"upgrading-concourse.html","title":"Upgrading Concourse","text":"Be careful to check the \"Breaking Changes\" in the release notes - in particular, you'll want to look for any flags that have changed.\n\n","depth":3,"section_tag":"upgrading-concourse"},"upgrading-the-web-node":{"location":"upgrading-concourse.html#upgrading-the-web-node","title":"Upgrading the Web Node","text":"The web node is upgraded by stopping the Concourse process, swapping out the concourse binary with the new one, and re-starting it.\n\nEach Running a web node will automatically run database migrations on start-up and lock via the database to ensure only one of the web nodes runs the migrations. We currently do not guarantee zero-downtime upgrades, as migrations may make changes that confuse the older web nodes. This should resolve as each web node is upgraded, and shouldn't result in any inconsistent state.\n\nTypically, Concourse can be upgraded from any version to any other version, though around 3.x and 4.x we made some changes to how migrations are run, and as a result the following upgrade paths must be followed:\n\n| Current Version | Upgrade Path |\n| \u003c v3.6.0 | v3.6.0 -\u003e v4.0.0 -\u003e latest |\n| = v3.6.0 | v4.0.0 -\u003e latest |\n\nWe'll try to minimize this kind of thing in the future.\n\nLastly, you will want to overwrite the contents of concourse/fly-assets with the contents from the Github release tarball so users can fly sync to the correct version.\n\n","depth":4,"section_tag":"upgrading-the-web-node"},"upgrading-the-worker-node":{"location":"upgrading-concourse.html#upgrading-the-worker-node","title":"Upgrading the Worker Node","text":"The worker node is upgraded by stopping the Concourse process, swapping out the concourse binary with the new one, and re-starting it.\n\n","depth":4,"section_tag":"upgrading-the-worker-node"},"use-the-image":{"location":"building-an-image-and-using-it-in-a-task.html#use-the-image","title":"Use the Image","text":"Next we want to add a second task to this job which will use the image generated from the first task as its container image. To use the image from the previous step add the top-level image key to the task step.\n\nresources: ...  # omitting for brevity\n\njobs:\n- name: build-and-run\n  plan:\n  - get: concourse-examples\n  - task: build-image\n    privileged: true\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: concourse/oci-build-task\n      inputs:\n      - name: concourse-examples\n      outputs:\n      - name: image\n      params:\n        CONTEXT: concourse-examples/Dockerfiles/simple\n        UNPACK_ROOTFS: true\n      run:\n        path: build\n  - task: use-built-image-in-task  # add a new task step\n    image: image  # using the image built in the previous step\n    config:\n      platform: linux\n      run:\n        path: cat\n        args: [\"/stranger\"]\n \n\nYou can set the pipeline with the following fly command.\n\nfly -t \u003ctarget\u003e set-pipeline -p build-and-use-image \\\n  -c ./build-and-use-image.yml \\\n","depth":5,"section_tag":"use-the-image"},"useimage":{"location":"opa.html#useimage","title":"UseImage","text":"Before Concourse starts a container you can check what image it is going to use to create the container. Depending on the image_type the image_source field may contain other fields. The JSON payload for this action will look similar to the following example:\n\n{\n  \"input\": {\n    \"service\": \"concourse\",\n    \"cluster_name\": \"dev\",\n    \"cluster_version\": \"7.4.0\",\n    \"action\": \"UseImage\",\n    \"team\": \"main\",\n    \"pipeline\": \"simple\",\n    \"data\": {\n      \"image_type\": \"registry-image\",\n      \"privileged\": true,\n      \"image_source\": {\n        \"repository\": \"alpine\",\n        \"tag\": \"latest\"\n      }\n    }\n  }\n}\n","depth":5,"section_tag":"useimage"},"user-roles":{"location":"user-roles.html","title":"User Roles \u0026 Permissions","text":"Concourse comes with five roles:\n\n1. Concourse Admin\n\n2. Team Owner\n\n3. Team Member\n\n4. Pipeline Operator\n\n5. Team Viewer\n\nThese roles are strictly ordered, so that each role always has all the permissions of any other role lower on the list. This means that a Pipeline Operator can always do anything a Team Viewer can, and so on.\n\nIn this document we say an action is assigned to a role if that role is capable of performing the action, but any less-privileged role is not. For example, the SaveConfig action is assigned to the member role, so owners and members can set a pipeline config, but pipeline operators and viewers cannot.\n\n","depth":3,"section_tag":"user-roles"},"using-a-local-dns-server":{"location":"concourse-worker.html#using-a-local-dns-server","title":"Using a local DNS server","text":"If you would like to use Consul, dnsmasq, or some other DNS server running on the worker VM, you'll have to configure the LAN address of the VM as the DNS server and allow the containers to reach the address, like so:\n\nconcourse worker --containerd-dns-server=\"10.0.1.3\" --containerd-allow-host-access=\"true\"\n# containerd runtime\nCONCOURSE_CONTAINERD_DNS_SERVER=\"10.0.1.3\"\nCONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS=\"true\"\n# Guardian runtime\nCONCOURSE_GARDEN_DNS_SERVER=\"10.0.1.3\"\nCONCOURSE_GARDEN_ALLOW_HOST_ACCESS=\"true\"\n[server]\n; internal IP of the worker machine\ndns-server=10.0.1.3\n\n; allow containers to reach the above IP\nallow-host-access=true\nMake sure to read A note on allowing host access and DNS proxy to understand the implications of using allow-host-access\n\nTo validate whether the changes have taken effect, you can fly intercept into any container and check /etc/resolv.conf once again:\n\n$ fly -t ci intercept -j my-pipeline/the-job\nbash-5.0$ cat /etc/resolv.conf\nnameserver 10.1.2.3\nbash-5.0$ nslookup concourse-ci.org\nServer:         10.1.2.3\nAddress:        10.1.2.3#53\n\nNon-authoritative answer:\nName:   concourse-ci.org\nAddress: 185.199.108.153\nName:   concourse-ci.org\nAddress: 185.199.109.153\nName:   concourse-ci.org\nAddress: 185.199.110.153\nName:   concourse-ci.org\nAddress: 185.199.111.153\nIf nslookup times out or fails, you may need to open up firewalls or security group configuration so that the worker VM can send UDP/TCP packets to itself.\n\n","depth":7,"section_tag":"using-a-local-dns-server"},"using-external-resource-types":{"location":"tutorial-resources.html#using-external-resource-types","title":"Using External Resource Types","text":"Concourse comes bundled with a lot of resources that are enough for most people to start using Concourse with. However, users will want to extend Concourse to work with all sorts of systems and that means bringing your own Resource Types.\n\nAdding a resource type to your pipeline looks very similar to adding a resource. You can even override the bundled resource types by re-declaring them in your pipeline.\n\nRemember, a resource is a container image. So to pull in a new resource type you need to tell Concourse where to pull the image from. This is done by using the built-in registry-image resource. The process of adding a resource type is just like adding a regular resource.\n\nHere's an example of using an external resource type to read an rss feed.\n\nresource_types:\n# declare the new resource type\n- name: rss\n  type: registry-image\n  source:\n    repository: suhlig/concourse-rss-resource\n    tag: latest\n\nresources:\n# use the resource as usual\n- name: dino-feed\n  type: rss\n  source:\n    url: http://www.qwantz.com/rssfeed.php\n\njobs:\n- name: announce\n  plan:\n  - get: dino-feed\n    trigger: true\nThat's how you add external resource types to your pipeline. If you're looking for more resource types there's a catalog of them at resource-types.concourse-ci.org.\n\n","depth":4,"section_tag":"using-external-resource-types"},"var-interpolation":{"location":"vars.html#var-interpolation","title":"Interpolation","text":"Values for vars are substituted structurally. That is, if you have foo: ((bar)), whatever value ((bar)) resolves to will become the value of the foo field in the object. This can be a value of any type and structure: a boolean, a simple string, a multiline credential like a certificate, or a complicated data structure like an array of objects.\n\nThis differs from text-based substitution in that it's impossible for a value to result in broken YAML syntax, and it relieves the template author from having to worry about things like whitespace alignment.\n\nWhen a ((var)) appears adjacent to additional string content, e.g. foo: hello-((bar))-goodbye, its value will be concatenated with the surrounding content. If the ((var)) resolves to a non-string value, an error will be raised.\n\n","depth":3,"section_tag":"var-interpolation"},"var-sources":{"location":"vars.html#var-sources","title":"Var sources (experimental)","text":"var_sources was introduced in Concourse v5.8.0. It is considered an experimental feature until its associated RFC is resolved.\n\nVar sources can be configured for a pipeline via pipeline.var_sources.\n\nEach var source has a name which is then referenced as the source-name in var syntax, e.g. ((my-vault:test-user.username)) to fetch the test-user var from the my-vault var source. See ((var)) syntax for a detailed explanation of this syntax.\n\nCurrently, only two types are supported: vault and dummy. In the future we want to make use of something like the Prototypes (RFC #37) so that third-party credential managers can be used just like resource types.\n\n","depth":4,"section_tag":"var-sources"},"var-syntax":{"location":"vars.html#var-syntax","title":"((var)) syntax","text":"The full syntax for vars is ((source-name:secret-path.secret-field)).\n\nThe optional source-name identifies the var source from which the value will be read. If omitted (along with the : delimiter), the cluster-wide credential manager will be used, or the value may be provided statically. The special name . refers to the local var source, while any other name refers to a var source.\n\nThe required secret-path identifies the location of the credential. The interpretation of this value depends on the var source type. For example, with Vault this may be a path like path/to/cred. For the Kubernetes secret manager this may just be the name of a secret. For credential managers which support path-based lookup, a secret-path without a leading / may be queried relative to a predefined set of path prefixes. This is how the Vault credential manager currently works; foo will be queried under /concourse/(team name)/(pipeline name)/foo.\n\nThe optional secret-field specifies a field on the fetched secret to read. If omitted, the credential manager may choose to read a 'default field' from the fetched credential if the field exists. For example, the Vault credential manager will return the value of the value field if present. This is useful for simple single-value credentials where typing ((foo.value)) would feel verbose.\n\nThe secret-path and secret-field may be surrounded by double quotes \"...\" if they contain special characters like . and :. For instance, ((source:\"my.secret\".\"field:1\")) will set the secret-path to my.secret and the secret-field to field:1.\n\n","depth":3,"section_tag":"var-syntax"},"variables":{"location":"pipeline-vars-example.html#variables","title":"Variables","text":"---\nfirst: initial\nnumber: \"9000\"\nhello: HAL\n","depth":3,"section_tag":"variables"},"vars":{"location":"vars.html","title":"Vars","text":"Concourse supports value substitution in YAML configuration by way of ((vars)).\n\nAutomation entails the use of all kinds of credentials. It's important to keep these values separate from the rest of your configuration by using vars instead of hardcoding values. This allows your configuration to be placed under source control and allows credentials to be tucked safely away into a secure credential manager like Vault instead of the Concourse database.\n\nAside from credentials, vars may also be used for generic parameterization of pipeline configuration templates, allowing a single pipeline config file to be configured multiple times with different parameters - e.g. ((branch_name)).\n\n","depth":2,"section_tag":"vars"},"vault-approle-auth":{"location":"vault-credential-manager.html#vault-approle-auth","title":"Using the approle auth backend","text":"The approle backend allows for an app (in this case, Concourse) to authenticate with a role pre-configured in Vault.\n\nWith this backend, the Running a web node is configured with a role_id corresponding to a pre-configured role, and a secret_id which is used to authenticate and acquire a token.\n\nThe approle backend must first be configured in Vault. Vault's approle backend allows for a few parameters which you may want to set to determine the permissions and lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\ntoken_ttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\ntoken_max_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and token_max_ttl as the max TTL will be ignored.\n\n\ntoken_num_uses=count: This sets a limit on how often a token can be used. We do not recommend setting this value, as it will effectively hamstring Concourse after a few credential acquisitions. The web node does not currently know to re-acquire a token when this limit is reached.\n\n\nsecret_id_ttl=duration and secret_id_num_uses=count: These two configurations will result in the secret ID expiring after the configured time or configured number of log-ins, respectively.\n\nYou should only set these if you have something periodically re-generating secret IDs and re-configuring your web nodes accordingly.\n\n\n\nGiven all that, a typical configuration may look something like this:\n\n$ vault auth enable approle\nSuccess! Enabled approle auth method at: approle/\n$ vault write auth/approle/role/concourse policies=concourse period=1h\nSuccess! Data written to: auth/approle/role/concourse\nNow that the backend is configured, we'll need to obtain the role_id and generate a secret_id:\n\n$ vault read auth/approle/role/concourse/role-id\nKey        Value\n---        -----\nrole_id    5f3420cd-3c66-2eff-8bcc-0e8e258a7d18\n$ vault write -f auth/approle/role/concourse/secret-id\nKey                   Value\n---                   -----\nsecret_id             f7ec2ac8-ad07-026a-3e1c-4c9781423155\nsecret_id_accessor    1bd17fc6-dae1-0c82-d325-3b8f9b5654ee\nThese should then be set on the Running a web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"approle\"\nCONCOURSE_VAULT_AUTH_PARAM=\"role_id:5f3420cd-3c66-2eff-8bcc-0e8e258a7d18,secret_id:f7ec2ac8-ad07-026a-3e1c-4c9781423155\"\n","depth":6,"section_tag":"vault-approle-auth"},"vault-cert-auth":{"location":"vault-credential-manager.html#vault-cert-auth","title":"Using the cert auth backend","text":"The cert auth method allows authentication using SSL/TLS client certificates.\n\nWith this backend, the Running a web node is configured with a client cert and a client key. Vault must be configured with TLS, which you should be almost certainly be doing anyway.\n\nThe cert backend must first be configured in Vault. The backend is associated to a policy and a CA cert used to verify the client certificate. It may also be given the client certificate itself.\n\nThe cert backend must first be configured in Vault. Vault's cert backend allows for a few parameters which you may want to set to determine the lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\nttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\nmax_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and max_ttl as the max TTL will be ignored.\n\n\n\n$ vault auth enable cert\nSuccess! Enabled cert auth method at: cert/\n$ vault write auth/cert/certs/concourse policies=concourse certificate=@out/vault-ca.crt ttl=1h\nSuccess! Data written to: auth/cert/certs/concourse\nOnce that's all set up, you'll just need to configure the client cert and key on the web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"cert\"\nCONCOURSE_VAULT_CLIENT_CERT=vault-certs/concourse.crt\nCONCOURSE_VAULT_CLIENT_KEY=vault-certs/concourse.key\nIn this case no additional auth params are necessary, as the Vault's TLS auth backend will check the certificate against all roles if no name is specified.\n\n","depth":6,"section_tag":"vault-cert-auth"},"vault-credential-lookup-rules":{"location":"vault-credential-manager.html#vault-credential-lookup-rules","title":"Credential lookup rules","text":"Vault lets you organize secrets into hierarchies, which is useful for when they should be accessible for particular pipelines or teams. When you have a parameter like ((foo)) in a pipeline definition, Concourse will (by default) look for it in the following paths, in order:\n\n* /concourse/TEAM_NAME/PIPELINE_NAME/foo\n\n* /concourse/TEAM_NAME/foo\n\nVault credentials are actually key-value, so for ((foo)) Concourse will default to the field name value. You can specify the field to grab via . syntax, e.g. ((foo.bar)).\n\nIf you have multiple, intermediate levels in your path, you can use the / separator to reach your intended field, e.g. ((foo/bar/baz.qux)).\n\nWhen executing a one-off task, there is no pipeline: so in this case, only the team path /concourse/TEAM_NAME/foo is searched.\n\nThere are several ways to customize the lookup logic:\n\n1. Add a \"shared path\", for secrets common to all teams.\n\n2. Change the team- and pipeline-dependent path templates.\n\n3. Change the path prefix from /concourse to something else.\n\n4. Set a Vault namespace for isolation within a Vault Enterprise installation.\n\nEach of these can be controlled by Concourse command line flags, or environment variables.\n\n","depth":5,"section_tag":"vault-credential-lookup-rules"},"vault-credential-manager":{"location":"vault-credential-manager.html","title":"The Vault credential manager","text":"Concourse can be configured to pull credentials from a Vault instance.\n\nTo configure this, first configure the URL of your Vault server by setting the following env on the Running a web node:\n\nCONCOURSE_VAULT_URL=https://vault.example.com:8200\nYou may also need to configure the CA cert for Vault:\n\nCONCOURSE_VAULT_CA_CERT=path/to/ca.crt\nYou'll also need to configure how the web node authenticates with Vault - see Authenticating with Vault for more details as that step is quite involved.\n\n","depth":4,"section_tag":"vault-credential-manager"},"vault-lookup-templates":{"location":"vault-credential-manager.html#vault-lookup-templates","title":"Changing the path templates","text":"You can choose your own list of templates, which will expand to team- or pipeline-specific paths. These are subject to the path prefix. By default, the templates used are:\n\nCONCOURSE_VAULT_LOOKUP_TEMPLATES=/{{.Team}}/{{.Pipeline}}/{{.Secret}},/{{.Team}}/{{.Secret}}\nWhen secrets are to be looked up, these are evaluated subject to the configured path prefix, where {{.Team}} expands to the current team, {{.Pipeline}} to the current pipeline (if any), and {{.Secret}} to the name of the secret. So if the settings are:\n\nCONCOURSE_VAULT_PATH_PREFIX=/secrets\nCONCOURSE_VAULT_LOOKUP_TEMPLATES=/{{.Team}}/concourse/{{.Pipeline}}/{{.Secret}},/{{.Team}}/concourse/{{.Secret}},/common/{{.Secret}}\nand ((password)) is used in team myteam and pipeline mypipeline, Concourse will look for the following, in order:\n\n1. /secrets/myteam/concourse/mypipeline/password\n\n2. /secrets/myteam/concourse/password\n\n3. /secrets/common/password\n\n","depth":6,"section_tag":"vault-lookup-templates"},"vault-namespace":{"location":"vault-credential-manager.html#vault-namespace","title":"Using a Vault namespace","text":"If you are using Vault Enterprise, you can make secret lookups and authentication happen under a namespace.\n\nCONCOURSE_VAULT_NAMESPACE=chosen/namespace/path\nThis setting applies to all teams equally.\n\n","depth":6,"section_tag":"vault-namespace"},"vault-path-prefix":{"location":"vault-credential-manager.html#vault-path-prefix","title":"Changing the path prefix","text":"The leading /concourse can be changed by specifying the following:\n\nCONCOURSE_VAULT_PATH_PREFIX=/some-other-prefix\n","depth":6,"section_tag":"vault-path-prefix"},"vault-periodic-token":{"location":"vault-credential-manager.html#vault-periodic-token","title":"Using a periodic token","text":"The simplest way to authenticate is by generating a periodic token:\n\n$ vault token create --policy concourse --period 1h\nKey                Value\n---                -----\ntoken              s.mSNnbhGAqxK2ZbMasOQ91rIA\ntoken_accessor     0qsib5YcYvROm86cT08IFxIT\ntoken_duration     1h\ntoken_renewable    true\ntoken_policies     [concourse default]\nChoose your --period wisely, as the timer starts counting down as soon as the token is created. You should also use a duration long enough to account for any planned web node downtime.\n\nOnce you have the token, just set the following env on the web node:\n\nCONCOURSE_VAULT_CLIENT_TOKEN=s.mSNnbhGAqxK2ZbMasOQ91rIA\nPeriodic tokens are the quickest way to get started, but they have one fatal flaw: if the web node is down for longer than the token's configured period, the token will expire and a new one will have to be created and configured. This can be avoided by using the approle auth backend.\n\n","depth":6,"section_tag":"vault-periodic-token"},"vault-shared-path":{"location":"vault-credential-manager.html#vault-shared-path","title":"Configuring a shared path","text":"A \"shared path\" can also be configured for credentials that you would like to share across all teams and pipelines, foregoing the default team/pipeline namespacing. Use with care!\n\nCONCOURSE_VAULT_SHARED_PATH=some-shared-path\nThis path must exist under the configured path prefix. The above configuration would correspond to /concourse/some-shared-path with the default /concourse prefix.\n\n","depth":6,"section_tag":"vault-shared-path"},"vault-var-source":{"location":"vars.html#vault-var-source","title":"vault var source","text":"","depth":4,"section_tag":"var-sources"},"version-pinning":{"location":"resource-versions.html#version-pinning","title":"Version Pinning","text":"A common job workflow is to use the latest version of a resource in order to trigger new builds. This works most of the time until you run into a situation where you need to run the job using an old version of a resource. Concourse provides a solution to this, which is called resource pinning.\n\nThere are two different ways to pin a resource: through the pipeline config and through the web UI. Within the pipeline config, you can either pin the resource to a version through the resource configuration or through a get step version configuration. If you would like to pin through the web UI, the functionality can be found in the resource version history page which is accessed through clicking into the resource within the pipeline page.\n\nPinning through the pipeline config is useful for a more permanent pinned state. If a resource is pinned through the pipeline config, it cannot be modified through the web UI and can only be changed through modifiying and resetting the pipeline config.\n\nPinning through the web UI is useful for reactionary pinning of a resource. For example, it can be used in the event of a broken upstream dependency.\n\nIf you had a version pinned in the web UI and then pinned it through the pipeline config, the pipeline config pinned version will take precendence.\n\nA pinned version is associated to a resource and can be viewed in the resource page (excluding the case that the version was pinned on a get step). This pinned version will be propagated throughout the pipeline and used by the jobs that take that pinned resource as an input. If there is a job that has a passed constraint on a pinned resource, this means that the input is only valid if that pinned version has been used by the passed constraint job.\n\nLet's say we have a pipeline with two jobs and one resource that is being used as a passed constraint between the two jobs. If that resource is pinned to a version, the first job will produce a build using the pinned version of the resource. After that build succeeds, the second job that has a passed constraint on the first will then be able to trigger off a build because the pinned version has been successfully used by the first job.\n\n","depth":4,"section_tag":"version-pinning"},"version-unpinning":{"location":"resource-versions.html#version-unpinning","title":"Unpinning","text":"When a version is unpinned, Concourse will go back to using the latest available version. This means a new build will be queued up if the most recent build used the old pinned version and the input has trigger: true.\n\nIf you would like to learn more about how version pinning and unpinning works with the build scheduler, you can read more about it in the scheduling behavior section.\n\n","depth":5,"section_tag":"version-unpinning"},"versions":{"location":"tutorial-resources.html#versions","title":"Versions","text":"Resources represent the external system or object to Concourse by emitting versions. When a new version is emitted by a resource, that is how Concourse knows to start trigger jobs.\n\nA version is any way that a resource can uniquely identify the state of the external system or object.\n\nFor example, the git resource emits versions based on the SHA of new commits it finds. A single version from the git resource will look like this to Concourse.\n\n{ \"ref\": \"04194bfc880c457a5b00f07db78d0532620414cc\" }\nLet's start digging into resources a bit more by going over the resource interface.\n\n","depth":4,"section_tag":"versions"},"volume-locality-strategy":{"location":"container-placement.html#volume-locality-strategy","title":"The volume-locality strategy","text":"When using volume-locality, the Running a web node places task step and put step containers on workers where a majority of their inputs are already present. This is the default strategy.\n\nThe advantage of this approach is that it reduces the likelihood that large artifacts will have to be streamed from one Running a worker node, through the Running a web node, and to the target Running a worker node. For large artifacts, this can result in quite a bit of overhead.\n\nThe disadvantage of this approach is that it can sometimes result in builds \"gravitating\" to a particular worker and overloading it, at least until the resource caches warm across the worker pool. This disadvantage can be partially mitigated using the (currently experimental) The limit-active-volumes strategy in conjunction with Chaining Placement Strategies.\n\nIf your builds tend to be light on artifacts and heavy on task execution, you may want to try the The fewest-build-containers strategy or the (currently experimental) The limit-active-tasks strategy.\n\n","depth":4,"section_tag":"volume-locality-strategy"},"web-configuration":{"location":"concourse-web.html#web-configuration","title":"Configuring the web node","text":"","depth":4,"section_tag":"web-configuration"},"web-connection-pooling":{"location":"concourse-web.html#web-connection-pooling","title":"Database connection pooling","text":"You may wish to configure the max number of parallel database connections that each node makes. There are two pools to configure: one for serving API requests, and one used for all the backend work such as pipeline scheduling.\n\nCONCOURSE_API_MAX_CONNS=10     # default\nCONCOURSE_BACKEND_MAX_CONNS=50 # default\nThere are some non-configurable connection pools. They take up the following number of connections per pool: * Garbage Collection: 5\n\n* Lock: 1\n\n* Worker Registration: 1\n\n\n\nThe sum of these numbers across all web nodes should not be greater than the maximum number of simultaneous connections your Postgres server will allow. See db node resource utilization for more information.\n\nFor example, if 3 web nodes are configured with the values shown above then your PostgreSQL server should be configured with a connection limit of at least 201: (10 + 50 + 5 + 1 + 1) * 3 web nodes.\n\n","depth":6,"section_tag":"web-connection-pooling"},"web-ingress":{"location":"concourse-web.html#web-ingress","title":"Configuring ingress traffic","text":"If your web nodes are going to be accessed multiple network layers, you will need to set CONCOURSE_EXTERNAL_URL to a URL accessible by your Concourse users. If you don't set this property, logging in will incorrectly redirect to its default value of 127.0.0.1.\n\nIf your web node(s) will be behind a load balancer or reverse proxy then you will need to ensure connctions made by fly intercept are properly handled by upgrading the connection. Here is a sample nginx configuration that upgrades connections made by fly intercept.\n\nserver {\n  server_name ci.example.com;\n\n  add_header Strict-Transport-Security \"max-age=31536000\" always;\n  ssl_stapling on;\n  ssl_stapling_verify on;\n\n  # Proxy main concourse traffic\n  location / {\n      proxy_pass http://concourse.local:8080/;\n      proxy_set_header Host $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Protocol $scheme;\n      proxy_set_header X-Forwarded-Host $http_host;\n  }\n\n  # Proxy fly intercept traffic\n  location ~ /hijack$ {\n      proxy_pass http://concourse.local:8080;\n      proxy_set_header Host $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Protocol $scheme;\n      proxy_set_header X-Forwarded-Host $http_host;\n      # Upgrade connection\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection \"upgrade\";\n  }\n}\n","depth":5,"section_tag":"web-ingress"},"web-node":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"web-operation":{"location":"concourse-web.html#web-operation","title":"Operating a web node","text":"The web nodes themselves are stateless - they don't store anything on disk, and coordinate entirely using the database.\n\n","depth":4,"section_tag":"web-operation"},"web-prerequisites":{"location":"concourse-web.html#web-prerequisites","title":"Prerequisites","text":"Nothing special - the web node is a pretty simple Go application that can be run like a 12-factor app.\n\n","depth":4,"section_tag":"web-prerequisites"},"web-resource-utilization":{"location":"concourse-web.html#web-resource-utilization","title":"Resource utilization","text":"CPU usage: peaks during pipeline scheduling, primarily when scheduling Jobs. Mitigated by adding more web nodes. In this regard, web nodes can be considered compute-heavy more than anything else at large scale.\n\nMemory usage: not very well classified at the moment as it's not generally a concern. Give it a few gigabytes and keep an eye on it.\n\nDisk usage: none\n\nBandwidth usage: aside from handling external traffic, the web node will at times have to stream bits out from one worker and into another while executing Steps.\n\nHighly available: yes; web nodes can all be configured the same (aside from --peer-address) and placed behind a load balancer. Periodic tasks like garbage-collection will not be duplicated for each node.\n\nHorizontally scalable: yes; they will coordinate workloads using the database, resulting in less work for each node and thus lower CPU usage.\n\nOutbound traffic:\n\n* db on its configured port for persistence\n\n* db on its configured port for locking and coordinating in a multi-web node deployment\n\n* other web nodes (possibly itself) on an ephemeral port when a worker is forwarded through the web node's TSA\n\nInbound traffic:\n\n* worker connects to the TSA on port 2222 for registration\n\n* worker downloads inputs from the ATC during Running tasks with fly execute via its external URL\n\n* external traffic to the ATC API via the web UI and The fly CLI\n\n","depth":5,"section_tag":"web-resource-utilization"},"web-running":{"location":"concourse-web.html#web-running","title":"Running concourse web","text":"The concourse CLI can run as a web node via the web subcommand.\n\nBefore running it, let's configure a local user so we can log in:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass\nCONCOURSE_MAIN_TEAM_LOCAL_USER=myuser\nThis will configure a single user, myuser, with the password mypass. You'll probably want to change those to sensible values, and later you may want to configure a proper auth provider - check out Auth \u0026 Teams whenever you're ready.\n\nNext, you'll need to configure the session signing key, the SSH key for the worker gateway, and the authorized worker key. Check Generating Keys to learn what these are and how they are created.\n\nCONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key\nCONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key\nCONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys\nFinally, web needs to know how to reach your Postgres database. This can be set like so:\n\nCONCOURSE_POSTGRES_HOST=127.0.0.1 # default\nCONCOURSE_POSTGRES_PORT=5432      # default\nCONCOURSE_POSTGRES_DATABASE=atc   # default\nCONCOURSE_POSTGRES_USER=my-user\nCONCOURSE_POSTGRES_PASSWORD=my-password\nIf you're running PostgreSQL locally, you can probably just point it to the socket and rely on the peer auth:\n\nCONCOURSE_POSTGRES_SOCKET=/var/run/postgresql\nNow that everything's set, run:\n\nconcourse web\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"web-running"},"what-are-inputs-and-outputs":{"location":"tutorial-inputs-outputs.html#what-are-inputs-and-outputs","title":"What are inputs and outputs?","text":"The simple answer is that inputs and outputs are directories that get passed between steps. We'll refer to both inputs and outputs as artifacts.\n\nLet's start exploring how artifacts work by adding a task.outputs to our hello-world-task.\n\njobs:\n- name: hello-world-job\n  plan:\n  - task: hello-world-task\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source:\n          repository: busybox\n      # Add \"the-artifact\" to our task\n      outputs:\n      - name: the-artifact\n      run:\n        # Change the command to `ls -lF` to see\n        # what the task sees in its working directory\n        path: ls\n        args: [\"-lF\"]\nUpdate the pipeline and trigger the job: $ fly -t tutorial set-pipeline -p hello-world -c hello-world.yml\n$ fly -t tutorial trigger-job --job hello-world/hello-world-job --watch\n...\nselected worker: 57d7419112ca\nrunning ls -lF\ntotal 4\ndrwxr-xr-x    2 root     root          4096 Apr  8 16:42 the-artifact/\nsucceeded\n\n\nWe can see that in the task's current working directory there is now a folder called the-artifact. Concourse makes output directories for you and will pass any contents inside the folder onto later steps. Let's see how that works next.\n\n","depth":4,"section_tag":"what-are-inputs-and-outputs"},"what-is-a-step":{"location":"tutorial-hello-world.html#what-is-a-step","title":"What is a step?","text":"A step is a single container running on a Concourse worker. Each step in a job plan runs in its own container. You can run anything you want inside the container (i.e. run my tests, run this bash script, build this image, etc.).\n\nSo if you have a job with five steps Concourse will create five containers, one for each step. Therefore, we need to tell Concourse the following about each step:\n\n* What type of worker to run the task on (linux/windows/darwin)\n\n* What container image to use (Linux only)\n\n* What command to run inside the container\n\nConcourse currently only supports Linux containers. Concourse does not yet run Windows containers. Darwin does not have native containers.\n\n","depth":4,"section_tag":"what-is-a-step"},"what-resources-produce":{"location":"checker.html#what-resources-produce","title":"What do resource checks produce?","text":"The whole point of running checks is to produce versions. Concourse's Build Scheduler is centered around the idea of resource versions. It's how Concourse determines that something is new and a new build needs to be triggered.\n\nThe versions produced by each resource are unique to the underlying resource type. For instance, the git resource type uses commit SHAs as versions. The registry-image resource uses the image digest and tag in the version.\n\n","depth":5,"section_tag":"what-resources-produce"},"whats-emitted":{"location":"tracing.html#whats-emitted","title":"What's emitted?","text":"Below is a summary of the various operations that Concourse currently traces. They are arranged like a call tree, so that for each operation described, its sub-operations are described indented immediately below.\n\n* scanner.Run -- An execution of the Resource Checker, responsible for determining which resources need to be checked. * scanner.check -- This operation simply represents inserting the check in the database.\n\n* scheduler.Run -- This represents one tick of the Build Scheduler. * schedule-job -- this is the same operation scoped to a single job. * Algorithm.Compute -- this is where the Algorithm determines inputs for a job. Each of the resolvers below describes a different strategy for determining inputs, depending on the job's config. * individualResolver.Resolve -- This is used to determine versions input to a get step without passed constraints.\n\n      * groupResolver.Resolve -- This is the juicy part of the algorithm, which deals with passed constraints.\n\n      * pinnedResolver.Resolve -- This operation is used to determine inputs when Version Pinning is at play.\n\n    * job.EnsurePendingBuildExists -- This is where a new build, if deemed necessary by scheduling constraints, will be inserted into the database. This operation follows from checker.Run above and will appear under the same trace as the check which produced the resource version responsible for triggering the new build.\n\n* build -- this is the primary operation performed by the Build Tracker. When a build is automatically triggered, this span follows from the job.EnsurePendingBuildExists operation which created the build, appearing in the same trace. * get -- this tracks the execution of a get step.\n\n  * put -- this tracks the execution of a put step.\n\n  * task -- this tracks the execution of a task step.\n\n  * set_pipeline -- this tracks the execution of a set_pipeline step.\n\n  * load_var -- this tracks the execution of a load_var step.\n\n","depth":4,"section_tag":"whats-emitted"},"whats-encrypted":{"location":"encryption.html#whats-encrypted","title":"What's encrypted?","text":"The following values are expected to contain credentials, and so will be encrypted:\n\n* Resource resource.sources, as they often contain private keys and other credentials for writing to (or simply granting access to) the resource.\n\n* Resource type resource_type.sources, for the same reason as above, though this is probably a less common use case.\n\n* Pipeline task step vars and task step params, in case they contain sensitive information such as usernames and/or passwords.\n\n* Put step put step params and get step get step params are also encrypted, even though they rarely should contain credentials (they're usually in resource.source).\n\n* Team auth configurations, as they often contain things like GitHub or other oAuth client secrets.\n\nNote that the actual implementation encrypts things in a more heavy-handed way than the above list implies. For example, pipeline configs are actually encrypted as one large blob.\n\nNotably, the following things are NOT encrypted:\n\n* Build logs. If your jobs are outputting credentials, encryption won't help you. We have chosen not to tackle this initially as it would introduce a performance burden for what is not as much of an obvious win.\n\n* Resource versions. These should never contain credentials, and are often meaningless on their own.\n\n* Resource metadata. These are visible to anyone if your pipeline is exposed, and should never contain credentials.\n\n* Pipeline names, job names, etc. - anything else that is not a high-risk target for credential leakage, as opposed to regular information leaks.\n\n  Resources and jobs in particular exist in their own tables, with their names in plaintext, and only their config encrypted. In this way, names are not protected, even though the pipeline config itself is also stored as one big encrypted blob.\n\n","depth":4,"section_tag":"whats-encrypted"},"when-are-resources-checked":{"location":"checker.html#when-are-resources-checked","title":"When are resources checked?","text":"The component that schedules resource checks is called the resource checker. The rate at which these checks happen is called the check interval (configurable via CONCOURSE_LIDAR_SCANNER_INTERVAL).  There's an obvious tradeoff, whereby the more frequently you poll, the bigger the strain on Concourse (as well as the external source). However, if you want to pick up those new commits as quickly as possible, then you need to poll as often as possible.\n\nThe resource checker uses the resource.check_every interval in order to figure out if a resource needs to be checked. A resource's check_every interval dictates how often it should be checked for new versions, with a default of 1 minute. If that seems like a lot of checking, it is, but it's how Concourse keeps everything snappy. You can configure this interval independently for each resource using check_every.\n\nIf your external service supports it, you can set resource.webhook_token to eliminate the need for periodic checking altogether. If a webhook_token is configured, the external service can notify Concourse when to check for new versions. Note that configuring a webhook_token will not stop Concourse from periodically checking your resource. If you wish to rely solely on webhooks for detecting new versions, you can set check_every to never.\n\nOn every interval tick, the resource checker will see if there are any resources that need to be checked. It does this by first finding resources which are used as inputs to jobs, and then comparing the current time against the last time each resource was checked. If it has been longer than a resource's configured check_every interval, a new check will be scheduled. In practice this means that if a resource has a check_every of 1m, it is not guaranteed to be checked precisely every 60 seconds. check_every simply sets a lower bound on the time between checks.\n\nWhen the resource checker finds a resource to check (either because its check_every interval elapsed, or because its configured webhook_token was triggered), it schedules a new build that invokes the check script of the resource's underlying resource type.\n\n","depth":5,"section_tag":"when-are-resources-checked"},"where-and-what-versions":{"location":"resource-versions.html#where-and-what-versions","title":"Where do they come from and what are they used for?","text":"The resource checker is responsible for checking for new versions of a resource. These versions are then saved to the database and can be viewed from the resource page in the web UI.\n\nResource versions are used by the build scheduler in order to schedule new builds for a job.\n\n","depth":4,"section_tag":"where-and-what-versions"},"where-is-everything":{"location":"project.html#where-is-everything","title":"Where is everything?","text":"* The Concourse repo is where the main code lives, where the backlog is planned, and where bugs are reported. It is managed by the maintainers team. Another high-level view of the backlog is also available at project.concourse-ci.org\n\n* The RFCs repo is where shiny new features are designed through collaborating in the open and collecting feedback. It is managed by the core team.\n\n* The Docs repo is the source for the website you're reading now! It is managed by the community team.\n\n* The Governance repo holds the source of truth for the project's permission structure and lays the foundation for self-governing teams. It is managed jointly by the community team and the infrastructure team.\n\n* GitHub Discussions are used for support, announcements, sharing ideas, and general discussion.\n\n* The concourse/concourse GitHub wiki contains community-curated lists of tools and tutorials and serves as a knowledge base for Concourse users and operators.\n\n* The Concourse blog features tutorials and updates from the development side.\n\n* The Concourse Discord server is a great place to chat with other contributors.\n\n","depth":2,"section_tag":"where-is-everything"},"who-uses-concourse":{"location":"ecosystem.html#who-uses-concourse","title":"Who Uses Concourse?","text":"These organisations have either added themselves to this list, or whose usage of Concourse is in the public domain. There are many more users of Concourse who are unable to publicly share information about their tech stack (typically banks and security companies).\n\n1. Altoros\n\n2. anynines\n\n3. Aptomi\n\n4. Armakuni\n\n5. boclips\n\n6. Cerner\n\n7. cloud.gov\n\n8. Cloud Foundry Foundation\n\n9. Comcast\n\n10. EngineerBetter\n\n11. Express Scripts\n\n12. Fauna\n\n13. Fidelity International\n\n14. Gardener\n\n15. (United Kingdom) Government Digital Services\n\n16. Gstack\n\n17. The Home Depot\n\n18. IBM\n\n19. LeapYear\n\n20. Napoleon Sports \u0026 Casino\n\n21. Nasdaq\n\n22. Nokogiri\n\n23. RabbitMQ\n\n24. Resilient Scale\n\n25. SAP\n\n26. Smarsh\n\n27. Springer Nature\n\n28. Stark \u0026 Wayne\n\n29. SUSE\n\n30. SuperOrbital\n\n31. Unit 2 Games\n\n32. United States Air Force - Kessel Run\n\n33. Varian\n\n34. Verizon\n\n35. VMware\n\n36. Webfleet Solutions\n\n37. Yahoo!\n\n38. Zipcar\n\n","depth":2,"section_tag":"who-uses-concourse"},"why-make-concourse":{"location":"project.html#why-make-concourse","title":"Why make Concourse?","text":"When working on a sizable project, having a pipeline to reliably test, deploy, and publish the product is crucial for rapid iteration.\n\nBut with every CI system we used, we found ourselves constantly dealing with the same old problems: complicated configs hidden in many pages of the web UI, not knowing who changed what \u0026 when, managing dependencies and state on the workers, build pollution, annoying UX...\n\nOur project was growing larger, and with every box we checked and for every worker we hand-tweaked, the anxiety of having to do it all over again if something went wrong grew and grew. We started writing software to manage our CI instead of writing the software for the product we wanted to build.\n\nWe built Concourse to be a CI system that lets you sleep easier at night. A CI that's simple enough to fully grok and easy to manage as your project grows; in both the complexity of the product and the size of your team. We wanted to build a CI with strong abstractions and fewer things to learn, so that it can be easier to understand and so that Concourse can age gracefully.\n\n","depth":2,"section_tag":"why-make-concourse"},"worker-configuration":{"location":"concourse-worker.html#worker-configuration","title":"Configuring the worker node","text":"","depth":4,"section_tag":"worker-configuration"},"worker-healthcheck-endpoint":{"location":"concourse-worker.html#worker-healthcheck-endpoint","title":"Healthcheck Endpoint","text":"The worker will automatically listen on port 8888 as its healthcheck endpoint. It will return a HTTP 200 status code with an empty body on a successful check. A successful check means the worker can reach the Garden and BaggageClaim servers.\n\nThe healthcheck endpoint is configurable through three variables: --healthcheck-bind-ip=\nIP address on which to listen for health checking requests. (default: 0.0.0.0)\n\n--healthcheck-bind-port\nPort on which to listen for health checking requests. (default: 8888)\n\n--healthcheck-timeout\nHTTP timeout for the full duration of health checking. (default: 5s)\n\n\n","depth":5,"section_tag":"worker-healthcheck-endpoint"},"worker-heartbeating-and-stalling":{"location":"concourse-worker.html#worker-heartbeating-and-stalling","title":"Worker Heartbeating \u0026 Stalling","text":"Workers will continuously heartbeat to the Concourse cluster in order to remain registered and healthy. If a worker hasn't checked in after a while, possibly due to a network error, being overloaded, or having crashed, the web node will transition its state to stalled and new workloads will not be scheduled on that worker until it recovers.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":5,"section_tag":"worker-heartbeating-and-stalling"},"worker-lifecycle":{"location":"internals.html#worker-lifecycle","title":"The worker lifecycle","text":"","depth":4,"section_tag":"worker-lifecycle"},"worker-node":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"worker-operation":{"location":"concourse-worker.html#worker-operation","title":"Operating a worker node","text":"The worker nodes are designed to be stateless and as interchangeable as possible. Tasks and Resources bring their own Docker images, so you should never have to install dependencies on the worker. Windows and Darwin workers are the exception to this. Any dependencies should be pre-installed on Windows and Darwin workers.\n\nIn Concourse, all important data is represented by Resources, so the workers themselves are dispensible. Any data in the work-dir is ephemeral and should go away when the worker machine is removed - it should not be persisted between worker VM or container re-creates.\n\n","depth":4,"section_tag":"worker-operation"},"worker-prerequisites":{"location":"concourse-worker.html#worker-prerequisites","title":"Prerequisites","text":"* Linux: We test and support the following distributions. Minimum kernel version tested is 4.4.\n\n  * Ubuntu 16.04 (kernel 4.4)\n\n  * Ubuntu 18.04 (kernel 5.3)\n\n  * Ubuntu 20.04 (kernel 5.4)\n\n  * Debian 10 (kernel 4.19)\n\n  Other Requirements: * User namespaces must be enabled.\n\n  * To enforce memory limits on tasks, memory + swap accounting must be enabled.\n\n  * The Guardian runtime only supports cgroupsV1. Use the containerd runtime if you want to use cgroupsV2 or downgrade to cgroupsV1.\n\n* Windows/Darwin: no special requirements (that we know of).\n\n  NOTE: Windows containers are currently not supported and Darwin does not have native containers. Steps will run inside a temporary directory on the Windows/Darwin worker. Any dependencies needed for your tasks (e.g. git, .NET, golang, ssh) should be pre-installed on the worker. Windows/Darwin workers do not come with any resource types.\n\n","depth":4,"section_tag":"worker-prerequisites"},"worker-resource-types":{"location":"concourse-worker.html#worker-resource-types","title":"Resource Types","text":"The following section only applies to Linux workers. Resource types are simply Linux container images and therefore can't be run on Windows or Darwin workers.\n\n","depth":5,"section_tag":"worker-resource-types"},"worker-resource-utilization":{"location":"concourse-worker.html#worker-resource-utilization","title":"Resource utilization","text":"CPU usage: almost entirely subject to pipeline workloads. More resources configured will result in more checking, and in-flight builds will use as much CPU as they want.\n\nMemory usage: also subject to pipeline workloads. Expect usage to increase with the number of containers on the worker and spike as builds run.\n\nBandwidth usage: again, almost entirely subject to pipeline workloads. Expect spikes from periodic checking, though the intervals should spread out over enough time. Resource fetching and pushing will also use arbitrary bandwidth.\n\nDisk usage: arbitrary data will be written as builds run, and resource caches will be kept and garbage collected on their own life cycle. We suggest going for a larger disk size if it's not too much trouble. All state on disk must not outlive the worker itself; it is all ephemeral. If the worker is re-created (i.e. fresh VM/container and all processes were killed), it should be brought back with an empty disk.\n\nHighly available: not applicable. Workers are inherently singletons, as they're being used as drivers running entirely different workloads.\n\nHorizontally scalable: yes; workers directly correlate to your capacity required by however many pipelines, resources, and in-flight builds you want to run. It makes sense to scale them up and down with demand.\n\nOutbound traffic: * External traffic to arbitrary locations as a result of periodic resource checking and running builds\n\n* External traffic to the web node's configured external URL when downloading the inputs for a Running tasks with fly execute\n\n* External traffic to the web node's TSA port (2222) for registering the worker\n\n* If P2P streaming is enabled there will be traffic to other workers.\n\n\n\nInbound traffic: * From the Running a web node on port 7777 (Garden) and 7788 (BaggageClaim). These ports do not need to be exposed, they are forwarded to the web node via the ssh connection on port 2222.\n\n* If P2P streaming is enabled there will be traffic to other workers.\n\n\n\n","depth":5,"section_tag":"worker-resource-utilization"},"worker-running":{"location":"concourse-worker.html#worker-running","title":"Running concourse worker","text":"The concourse CLI can run as a worker node via the worker subcommand.\n\nFirst, you'll need to configure a directory for the worker to store data:\n\nCONCOURSE_WORK_DIR=/opt/concourse/worker\nThis is where all the builds run, and where all resources are fetched in to, so make sure it's backed by enough storage.\n\nNext, point the worker at your Running a web node like so:\n\nCONCOURSE_TSA_HOST=10.0.2.15:2222\nCONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub\nCONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key\nFinally start the worker:\n\n# run with -E to forward env config, or just set it all as root\nsudo -E concourse worker\nNote that the worker must be run as root because it orchestrates containers.\n\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"worker-running"},"writing-opa-rules":{"location":"opa.html#writing-opa-rules","title":"Writing OPA Rules","text":"On the OPA server you'll need to create a package and policy for Concourse. This should match up with the endpoint provided to Concourse. The OPA documentation has a good guide explaining how to generally write OPA rules and setup an OPA server.\n\nFor any actions that Concourse has been configured to filter it will send a JSON request to the OPA server with the following details. Top-level data directly under the input key will be present for most actions. The information under the data key will differ based on the action being checked.\n\nThis sample JSON payload is what OPA is sent when a user sets a pipeline. The data key contains the pipeline in JSON format. {\n  \"input\": {\n    \"service\": \"concourse\",\n    \"cluster_name\": \"dev\",\n    \"cluster_version\": \"7.4.0\",\n    \"http_method\": \"PUT\",\n    \"action\": \"SaveConfig\",\n    \"user\": \"test\",\n    \"team\": \"main\",\n    \"pipeline\": \"check-pipeline\",\n    \"data\": {\n      \"jobs\": [\n        {\n          \"name\": \"test\",\n          \"plan\": [\n            {\n              \"get\": \"tiny\"\n            },\n            {\n              \"config\": {\n                \"image_resource\": {\n                  \"source\": {\n                    \"repository\": \"busybox\"\n                  },\n                  \"type\": \"registry-image\"\n                },\n                \"platform\": \"linux\",\n                \"run\": {\n                  \"args\": [\n                    \"-exc\",\n                    \"echo hello\"\n                  ],\n                  \"path\": \"sh\"\n                }\n              },\n              \"task\": \"a-task\" }\n] } ] } } }\n\n\nAn OPA rule can respond to Concourse with three fields:\n\n* allowed (required): Boolean type. Setting to False will deny the action unless the block field is False.\n\n* block (optional): Boolean type. If set to False and allowed is True this creates a soft-policy enforcement. The action will be allowed and the reasons will still be printed to the web UI like a warning message.\n\n  Not setting block is the same as setting \"block\": true.\n\n* reasons (optional): List of string type. If an action is denied based on the allowed field then the reason(s) will be displayed in the UI.\n\nHere is an example OPA policy. By default it will allow whatever action it has been sent. It will deny the action if one or more of the three deny rules are true.\n\npackage concourse\n\ndefault decision = {\"allowed\": true}\n\ndecision = {\"allowed\": false, \"reasons\": reasons} {\n  count(deny) \u003e 0\n  reasons := deny\n}\n\ndeny[\"cannot use docker-image types\"] {\n  input.action == \"UseImage\"\n  input.data.image_type == \"docker-image\"\n}\n\ndeny[\"cannot run privileged tasks\"] {\n  input.action == \"SaveConfig\"\n  input.data.jobs[_].plan[_].privileged\n}\n\ndeny[\"cannot use privileged resource types\"] {\n  input.action == \"SaveConfig\"\n  input.data.resource_types[_].privileged\n}\n","depth":4,"section_tag":"writing-opa-rules"},"yaml-quirks":{"location":"config-basics.html#yaml-quirks","title":"YAML Quirks","text":"YAML has some weird parts. For example, all of the following terms are acceptable boolean values: true, false, yes, no, on, off.\n\nYAML is also whitespace-sensitive. For the most part, this is really handy because it keeps you from having to count curly-braces in deeply nested parts of configuration such as job.plan. Sometimes, though, it can be hard to keep track of the correct indentation level.\n\n","depth":4,"section_tag":"yaml-quirks"},"yaml-tips-and-tricks":{"location":"config-basics.html#yaml-tips-and-tricks","title":"YAML Tips \u0026 Tricks","text":"YAML anchor syntax can be used to avoid repetition within configuration.\n\nFor example, the following YAML document...:\n\nlarge_value: \u0026my_anchor\n  do_the_thing: true\n  with_these_values: [1, 2, 3]\n\nduplicate_value: *my_anchor\n...is exactly equivalent to:\n\nlarge_value:\n  do_the_thing: true\n  with_these_values: [1, 2, 3]\n\nduplicate_value:\n  do_the_thing: true\n  with_these_values: [1, 2, 3]\nIf you find yourself repeating configuration throughout your pipeline, it may be a sign that Concourse is missing some kind of abstraction to make your pipeline less verbose. If you have the time and are interested in helping out with Concourse's design, feedback of this sort is welcome in GitHub Discussions!\n\n","depth":4,"section_tag":"yaml-tips-and-tricks"}}
