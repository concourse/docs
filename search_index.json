{"LANDED-table":{"location":"worker-internals.html#LANDED-table","title":"LANDED","text":"A worker in this state has successfully waited for all non-interruptible jobs on it after having concourse land-worker called. It will no longer be used to schedule any new containers or create volumes until it registers as RUNNING again.\n\n","depth":4,"section_tag":"worker-lifecycle"},"LANDING-table":{"location":"worker-internals.html#LANDING-table","title":"LANDING","text":"The concourse land-worker command will put a worker in the LANDING state to safely drain its assignments for temporary downtime.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and transition the worker into LANDED state.\n\n","depth":4,"section_tag":"worker-lifecycle"},"RETIRING-table":{"location":"worker-internals.html#RETIRING-table","title":"RETIRING","text":"The concourse retire-worker command will put a worker in the RETIRING state to remove it from the cluster permanently.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and remove the worker.\n\n","depth":4,"section_tag":"worker-lifecycle"},"RUNNING-table":{"location":"worker-internals.html#RUNNING-table","title":"RUNNING","text":"A worker in this state is registered with the cluster and ready to start running containers and storing volumes.\n\n","depth":4,"section_tag":"worker-lifecycle"},"STALLED-table":{"location":"worker-internals.html#STALLED-table","title":"STALLED","text":"A worker in this state was previously registered with the cluster, but stopped advertising itself for some reason. Ususally this is due to network connectivity issues, or the worker stopping unexpectedly.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":4,"section_tag":"worker-lifecycle"},"abstract-objects":{"location":"database-schema.html#abstract-objects","title":"Abstract Objects","text":"Concourse's database manages the abstractions around resources, resource configuration, resource versioning, and resource types. These tables help make the Resources concept operate as expected, scale across many pipelines, and to many different types.\n\n","depth":4,"section_tag":"abstract-objects"},"adding-cf-users-to-the-main-team":{"location":"cf-uaa-auth.html#adding-cf-users-to-the-main-team","title":"Adding CF Users to the main Team","text":"CloudFoundry users and org/space members can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_CF_USER=username\nCONCOURSE_MAIN_TEAM_CF_SPACE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_GUID=SPACE_GUID\nCONCOURSE_MAIN_TEAM_CF_ORG=org-name\nMultiple users, spaces, etc. may be specified by comma-separating them.\n\n","depth":6,"section_tag":"adding-cf-users-to-the-main-team"},"administration":{"location":"administration.html","title":"Administration","text":"","depth":3,"section_tag":"administration"},"aggregate":{"location":"aggregate-step.html#aggregate","title":"aggregate","text":"Performs the given steps in parallel.\n\nIf any sub-steps in an aggregate result in an error, the aggregate step as a whole is considered to have errored.\n\nSimilarly, when aggregating task steps, if any fail, the aggregate step will fail.\n\n","depth":4,"section_tag":"aggregate-step"},"aggregate-step":{"location":"aggregate-step.html","title":"aggregate step","text":"","depth":4,"section_tag":"aggregate-step"},"architecture":{"location":"architecture.html","title":"Architecture","text":"Concourse is a fairly simple distributed system built up from the following components. You'll see them referenced here and there throughout the documentation, so you may want to skim this page just to get an idea of what they are.\n\n{image: images/concourse_architecture.png}\n\n","depth":2,"section_tag":"architecture"},"architecture-worker":{"location":"architecture.html#architecture-worker","title":"Workers: container runtime \u0026 cache management","text":"Workers are machines running Garden and Baggageclaim servers and registering themselves via the TSA.\n\nWorkers have no important state configured on their machines, as everything runs in a container and thus shouldn't care about what packages are installed on the host (well, except for those that allow it to be a worker in the first place). This is very different from workers in other non-containerized CI solutions, where the state of packages on the worker is crucial to whether your pipeline works or not.\n\nEach worker registers itself with the Concourse cluster via the TSA.\n\nWorkers by default listen on port 7777 for Garden and port 7788 for Baggageclaim. If they are within a private network reachable by the ATC, they'll probably bind on all addresses (0.0.0.0) and register themselves directly. Otherwise they should bind on 127.0.0.1 and forward themselves through the TSA.\n\n","depth":3,"section_tag":"architecture-worker"},"attempts":{"location":"attempts-step-modifier.html#attempts","title":"attempts","text":"The total number of times a step should be tried should it fail, e.g. 5 will try the step up to 5 times before giving up.\n\nWhen the number of attempts is reached and the step has still not succeeded then the step will fail.\n\n","depth":4,"section_tag":"attempts-step-modifier"},"attempts-step-modifier":{"location":"attempts-step-modifier.html","title":"attempts step modifier","text":"Any step can set the number of times it should be attempted by attaching an attempts parameter with the number of times it should be tried.\n\nAttempts will retry on a Concourse error as well as build failure.\n\n","depth":4,"section_tag":"attempts-step-modifier"},"auth":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"authenticating-with-vault":{"location":"vault-credential-manager.html#authenticating-with-vault","title":"Authenticating with Vault","text":"There are many ways to authenticate with a Vault server. The web-node can be configured with either a token or an arbitrary auth backend and arbitrary auth params, so just about all of them should be configurable.\n\nWhen the web node acquires a token, either by logging in with an auth backend or by being given one directly, it will continuously renew the token to ensure it doesn't expire. The renewal interval is half of the token's lease duration.\n\n","depth":5,"section_tag":"authenticating-with-vault"},"aws-asm-credential-manager":{"location":"aws-asm-credential-manager.html","title":"The AWS Secrets Manager credential manager","text":"","depth":4,"section_tag":"aws-asm-credential-manager"},"aws-secretsmanager-access-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-access-key","title":"aws-secretsmanager-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-pipeline-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-pipeline-secret-template","title":"aws-secretsmanager-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-region":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-region","title":"aws-secretsmanager-region","text":"The AWS region that requests to Secrets Manager will be sent to.\n\nEnvironment variable AWS_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-secret-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-secret-key","title":"aws-secretsmanager-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-session-token":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-session-token","title":"aws-secretsmanager-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-team-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-team-secret-template","title":"aws-secretsmanager-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-access-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-access-key","title":"aws-ssm-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SSM_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-credential-manager":{"location":"aws-ssm-credential-manager.html","title":"The AWS SSM credential manager","text":"","depth":4,"section_tag":"aws-ssm-credential-manager"},"aws-ssm-pipeline-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-pipeline-secret-template","title":"aws-ssm-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-region":{"location":"aws-ssm-credential-manager.html#aws-ssm-region","title":"aws-ssm-region","text":"The AWS region that requests to parameter store will be sent to.\n\nEnvironment variable AWS_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-secret-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-secret-key","title":"aws-ssm-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SSM_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-session-token":{"location":"aws-ssm-credential-manager.html#aws-ssm-session-token","title":"aws-ssm-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SSM_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-team-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-team-secret-template","title":"aws-ssm-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"base_resource_types-table":{"location":"database-schema.html#base_resource_types-table","title":"base_resource_types","text":"An entry in the base resource types table represents a resource type which can be provided by any worker. It is used to keep resource_configs as general as possible, and not tied to particular workers. This in turn allows resource_caches to also be general, as is desired for Concourse's higher-level pipeline resource caching semantics.\n\n","depth":4,"section_tag":"abstract-objects"},"basing-inputs-on-a-job-in-your-pipeline-with---inputs-from":{"location":"running-tasks.html#basing-inputs-on-a-job-in-your-pipeline-with---inputs-from","title":"Basing inputs on a job in your pipeline with --inputs-from","text":"If the --inputs-from flag is given, the specified job will be looked up in the pipeline, and the one-off build will base its inputs on those currently configured for the job.\n\nIf any --input flags are given (see above), they will override the base set of inputs.\n\nFor example:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\nThis will trigger a one-off-build using the task.yml task config, basing its inputs on the latest candidates for the integration job in the main pipeline, with the foo input overridden to specify local code to run.\n\nThis can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local development so that you don't have to upload every single resource from your local machine.\n\n","depth":5,"section_tag":"basing-inputs-on-a-job-in-your-pipeline-with---inputs-from"},"benefits-of-global-resources":{"location":"global-resources.html#benefits-of-global-resources","title":"Benefits of Global Resources","text":"","depth":4,"section_tag":"benefits-of-global-resources"},"binaries":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI, which can be downloaded from the latest GitHub release - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"bitbucket-cloud-auth":{"location":"bitbucket-cloud-auth.html","title":"BitBucket Cloud auth","text":"A Concourse server can authenticate against BitBucket Cloud to leverage its permission model.\n\n","depth":4,"section_tag":"bitbucket-cloud-auth"},"bitbucket-cloud-authentication":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authentication","title":"Authentication","text":"First, you'll need to create an OAuth consumer on Bitbucket Cloud.\n\nThe consumer will need the following permissions:\n\n* Account:\n\n  * Email\n\n  * Read\n\n* Team membership:\n\n  * Read\n\nThe \"Callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by BitBucket Cloud - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_ID=myclientid\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_SECRET=myclientsecret\n","depth":5,"section_tag":"bitbucket-cloud-authentication"},"bitbucket-cloud-authorization":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authorization","title":"Authorization","text":"BitBucket users and teams can be authorized for a team by passing the following flags to fly set-team:\n\n--bitbucket-cloud-user=LOGIN: Authorize an individual user.\n\n\n--bitbucket-cloud-team=TEAM_NAME: Authorize an entire organization's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --bitbucket-cloud-user my-bitbucket-login \\\n    --bitbucket-cloud-team my-bitbucket-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  bitbucket-cloud:\n    users: [\"my-bitbucket-login\"]\n    teams: [\"my-bitbucket-team\"]\n","depth":5,"section_tag":"bitbucket-cloud-authorization"},"build finished":{"location":"metrics.html#build finished","title":"build finished","text":"This event is emitted when a build ends. Its value is the duration of the build in milliseconds. You can use this metric in conjunction with build started to annotate your metrics with when builds started and stopped.\n\nAttributes pipeline\n\n: The pipeline which contains the build that finished.\n\n\njob\n\n: The job which configured the build that finished.\n\n\nbuild_name\n\n: The name of the build that finished. (Remember that build numbers in Concourse are actually names and are strings).\n\n\nbuild_id\n\n: The ID of the build that finished.\n\n\nbuild_status\n\n: The resulting status of the build; one of \"succeeded\", \"failed\", \"errored\", or \"aborted\".\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"build started":{"location":"metrics.html#build started","title":"build started","text":"This event is emitted when a build starts. Its value is the build ID of the build. However, it is most useful for annotating your metrics with the start and end of different jobs.\n\nAttributes pipeline\n\n: The pipeline which contains the build being started.\n\n\njob\n\n: The job which configured the build being started.\n\n\nbuild_name\n\n: The name of the build being started. (Remember that build numbers in Concourse are actually names and are strings).\n\n\nbuild_id\n\n: The ID of the build being started.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"build-plans":{"location":"steps.html","title":"Steps","text":"Each Job has a single build plan. When a build of a job is created, the plan determines what happens.\n\nA build plan is a sequence of steps to execute. These steps may fetch down or update Resources, or execute Tasks.\n\nA new build of the job is scheduled whenever get steps with trigger: true have new versions available.\n\nTo visualize the job in the pipeline, resources that appear as get steps are drawn as inputs, and resources that appear in put steps appear as outputs.\n\n","depth":3,"section_tag":"steps"},"build-retention":{"location":"caching-and-retention.html#build-retention","title":"Build Retention","text":"","depth":4,"section_tag":"build-retention"},"build_image_resource_caches-table":{"location":"database-schema.html#build_image_resource_caches-table","title":"build_image_resource_caches","text":"A build image resource cache is a join table between a build and a resource cache.\n\nA build image resource cache is used for keeping caches that were used for an image_resource in a build, as part of the resource retention policy.\n\n","depth":4,"section_tag":"abstract-objects"},"builds":{"location":"builds.html","title":"Builds","text":"A build is an execution of a build plan, which is either configured as a sequence of steps in a job, or submitted directly to Concourse as a one-off build via fly execute.\n\nContainers and volumes are created as get steps, put steps, and task steps run. When a build completes successfully, these containers go away.\n\nA failed build's containers and volumes are kept around so that you can debug the build via fly intercept. If the build belongs to a job, the containers will go away when the next build starts. If the build is a one-off, its containers will be removed immediately, so make sure you intercept while it's running if you want to debug.\n\n","depth":2,"section_tag":"builds"},"builds-table":{"location":"database-schema.html#builds-table","title":"builds","text":"The builds table tracks the details of every run of every job or one-off build, including its name, the status of the build, whether the build has been scheduled or completed, and information about the start and end times of the build.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"cache-path":{"location":"tasks.html#cache-path","title":"caches.path","text":"Required. The path to a directory to be cached.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"caches":{"location":"tasks.html#caches","title":"caches","text":"Where cache is:\n\nOptional. The cached directories shared between task runs.\n\nOn the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those artifacts.\n\nCaches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.\n\nCaches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off builds.\n\n","depth":2,"section_tag":"tasks"},"caching-and-retention":{"location":"caching-and-retention.html","title":"Caching \u0026 Retention","text":"TODO:\n\n* How it relates to garbage-collection\n\n* How it relates to the schema\n\n* Resource cache retention policy\n\n* Container retention policy\n\n","depth":3,"section_tag":"caching-and-retention"},"cf-authentication":{"location":"cf-uaa-auth.html#cf-authentication","title":"Authentication","text":"You'll need to configure your UAA with a concourse client by setting the following under uaa.clients:\n\nconcourse:\n  id: myclientid\n  secret: myclientsecret\n  scope: openid,cloud_controller.read\n  authorized-grant-types: \"authorization_code,refresh_token\"\n  access-token-validity: 3600\n  refresh-token-validity: 3600\n  redirect-uri: https://concourse.example.com/sky/issuer/callback\nThe value for redirect-uri must be the external URL of your Concourse server with /sky/issuer/callback appended.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nNext, you'll need to take the same client ID and secret and configure it on the Running a web node by setting the following env:\n\nCONCOURSE_CF_API_URL=http://mycf.example.com\nCONCOURSE_CF_CLIENT_ID=myclientid\nCONCOURSE_CF_CLIENT_SECRET=myclientsecret\nNote: if you're integrating with Cloud Foundry, you're probably also deploying Concourse via BOSH - in which case you'll want to set the cf_auth.* properties in your manifest instead of setting the above env.\n\n","depth":5,"section_tag":"cf-authentication"},"cf-authorization":{"location":"cf-uaa-auth.html#cf-authorization","title":"Authorization","text":"CloudFoundry users and org/space members can be authorized for a team by passing the following flags to fly set-team:\n\n--cf-user=USERNAME: Authorize an individual user.\n\n\n--cf-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--cf-space=ORG_NAME:SPACE_NAME: Authorize the members of a space within an organization.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --cf-user my-username \\\n    --cf-org my-org \\\n    --cf-space my-other-org:my-space\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  cf:\n    users: [\"my-username\"]\n    orgs: [\"my-org\"]\n    spaces: [\"my-other-org:my-space\"]\n","depth":5,"section_tag":"cf-authorization"},"cf-uaa-auth":{"location":"cf-uaa-auth.html","title":"CF/UAA auth","text":"Cloud Foundry (CF) auth can be used for operators who wish to authenticate their users configured against their Cloud Foundry instance via the UAA auth component.\n\n","depth":4,"section_tag":"cf-uaa-auth"},"complications-with-reusing-containers":{"location":"global-resources.html#complications-with-reusing-containers","title":"Complications with reusing containers","text":"There is an exception to sharing check containers within a deployment, which is workers belonging to a team and workers with tags.\n\nIf a resource has tags configured, and the resource's check interval ends up acquiring the checking lock, a new container will be created on a worker matching the appropriate tags, even if a check container already exists for the same resource config elsewhere.\n\nSimilarly, if a team has its own workers, and their check interval ended up acquiring the lock, a new container will be created on the team's workers, rather than re-using a container from the shared worker pool.\n\nThis is a bit complicated to reason about and we plan to stop re-using check containers to simplify all of this. See 3079 for more information.\n\n","depth":6,"section_tag":"complications-with-reusing-containers"},"component-atc":{"location":"architecture.html#component-atc","title":"ATC: web UI \u0026 build scheduler","text":"The ATC is the heart of Concourse. It runs the web UI and API and is responsible for all pipeline scheduling. It connects to PostgreSQL, which it uses to store pipeline data (including build logs).\n\nMultiple ATCs can be running as one cluster; as long as they're all pointing to the same database, they'll synchronize using basic locking mechanisms and roughly spread work across the cluster.\n\nThe ATC by default listens on port 8080, and is usually colocated with the TSA and sitting behind a load balancer.\n\nNote: for fly intercept to function, make sure your load balancer is configured to do TCP or SSL forwarding, not HTTP or HTTPS.\n\n","depth":3,"section_tag":"component-atc"},"component-tsa":{"location":"architecture.html#component-tsa","title":"TSA: worker registration \u0026 forwarding","text":"The TSA is a custom-built SSH server that is used solely for securely registering workers with the ATC.\n\nThe TSA only supports two commands: register-worker and forward-worker.\n\nThe register-worker command is used to register a worker directly with the ATC. This should be used if the worker is running in the same (private) network as the ATC.\n\nThe forward-worker command is used to reverse-tunnel a worker's addresses through the TSA and register the forwarded connections with the ATC. This allows workers running in arbitrary networks to register securely, so long as they can reach the TSA. This is much safer than opening the worker up to the outside world.\n\nThe TSA by default listens on port 2222, and is usually colocated with the ATC and sitting behind a load balancer.\n\n","depth":3,"section_tag":"component-tsa"},"concourse-admin":{"location":"user-roles.html#concourse-admin","title":"Concourse Admin","text":"Admin is a special user attribute granted only to owners of the The main team.\n\nAdmins have the ability to administrate teams using fly set-team, fly destroy-team, fly rename-team, etc.\n\n","depth":4,"section_tag":"concourse-admin"},"concourse-cli":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI, which can be downloaded from the latest GitHub release - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"concourse-generate-key":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nsession_signing_key: Used by the Running a web node for signing and verifying user session tokens.\n\n\ntsa_host_key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nworker_key (one per worker): Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized keys configuration in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n...and we'll also start on an authorized_keys file, currently listing this initial worker key:\n\ncp worker_key.pub authorized_worker_keys\n","depth":3,"section_tag":"concourse-generate-key"},"concourse-web":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"concourse-worker":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"configuration":{"location":"php-example.html#configuration","title":"Pipeline Configuration","text":"---\nresources:\n  - name: larvel-websockets-git\n    type: git\n    icon: github-circle\n    source:\n      uri: https://github.com/beyondcode/laravel-websockets.git\n\njobs:\n  - name: test\n    public: true\n    plan:\n      - get: larvel-websockets-git\n        trigger: true\n      - task: run-tests\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: php, tag: 7.2-cli }\n          inputs:\n            - name: larvel-websockets-git\n          run:\n            path: /bin/sh\n            args:\n              - -c\n              - |\n                apt-get update\n                apt-get install -y git unzip\n\n                php -r \"copy('https://getcomposer.org/installer', 'composer-setup.php');\"\n                php -r \"if (hash_file('sha384', 'composer-setup.php') === '93b54496392c062774670ac18b134c3b3a95e5a5e5c8f1a9f115f203b75bf9a129d5daa8ba6a13e2cc8a1da0806388a8') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\"\n                php composer-setup.php --filename=composer --install-dir=/usr/bin\n                php -r \"unlink('composer-setup.php');\"\n\n                cd larvel-websockets-git\n\n                composer install\n                vendor/bin/phpunit --coverage-text --coverage-clover=coverage.clover\n","depth":3,"section_tag":"configuration"},"configuring-auth":{"location":"configuring-auth.html","title":"Configuring Auth","text":"The very first thing to configure with Concourse is how users will log in, and what those users should be able to do.\n\nThis is configured in two separate tiers:\n\n* Authentication, how users identify themselves, is configured on the Running a web node.\n\n* Authorization, how user access is determined, is configured on each team.\n\nConcourse currently supports the following auth methods:\n\nAny number of providers can be enabled at any one time. Users will be given a choice when logging in as to which one they would like to use.\n\nConcourse uses a fork of Dex for its authentication. You can find additional documentation on the supported auth providers in the Dex connectors documentation.\n\nAdding a new auth provider to Concourse is as simple as submitting a pull request to our fork of Dex and then adding a bit of configuration to the skymarshal component.\n\n","depth":3,"section_tag":"configuring-auth"},"configuring-credential-caching":{"location":"vault-credential-manager.html#configuring-credential-caching","title":"Configuring credential caching","text":"By default, credentials are fetched each time they're used. This can add up when many pipelines are configured, resulting in a ton of requests to Vault.\n\nTo reduce load on your Vault server you may want to enable caching, by setting the following env on the Running a web node:\n\nCONCOURSE_VAULT_CACHE=true\nWhen enabled, credentials are cached for half of their lease duration.\n\nTo set an upper bound and force cache busting after a certain amount of time, set the following env:\n\nCONCOURSE_VAULT_MAX_LEASE=1m\nWith this set, credentials will only be cached for up to 1 minute.\n\n","depth":5,"section_tag":"configuring-credential-caching"},"configuring-gdn-server":{"location":"concourse-worker.html#configuring-gdn-server","title":"Configuring gdn server","text":"On Linux, the concourse binary is packaged alongside a gdn binary. This binary is used for running Guardian, which is a Garden backend implementation which runs containers via runc (the same technology underlying tools like Docker).\n\nThe concourse worker command automatically configures and runs gdn, but depending on the environment you're running Concourse in, you may need to pop open the hood and configure a few things.\n\nThe gdn server can be configured in two ways:\n\n1. By creating a config.ini file and passing it as --garden-config (or CONCOURSE_GARDEN_CONFIG).\n\n  The .ini file should look something like this:\n\n  [server]\n  flag-name = flag-value\n  To learn which flags can be set, consult gdn server --help. Each flag listed can be set under the [server] heading.\n\n2. By setting CONCOURSE_GARDEN_* environment variables.\n\n  This is primarily supported for backwards compatibility, and these variables are not present in concourse web --help. They are translated to flags passed to gdn server by lower-casing the * portion and replacing underscores with hyphens.\n\n","depth":4,"section_tag":"configuring-gdn-server"},"configuring-ldap-group-search":{"location":"ldap-auth.html#configuring-ldap-group-search","title":"Configuring LDAP group search","text":"The LDAP provider can also be configured with group search configuration, so that users can be configured for team authorization by their 'group' in LDAP.\n\nFor example, to find groups and identify them by their ou attribute, you would configure:\n\nCONCOURSE_LDAP_GROUP_SEARCH_BASE_DN='cn=groups,dc=example,dc=com'\nCONCOURSE_LDAP_GROUP_SEARCH_NAME_ATTR=ou\nThe attributes correlating a user to a group must be specified like so:\n\nCONCOURSE_LDAP_GROUP_SEARCH_USER_ATTR=uid\nCONCOURSE_LDAP_GROUP_SEARCH_GROUP_ATTR=members\nThis specifies that the uid attribute of the user must be present in the members attribute of the group.\n\nAn additional filter may be specified, just like with users:\n\nCONCOURSE_LDAP_GROUP_SEARCH_FILTER='(objectClass=posixGroup)'\n","depth":6,"section_tag":"configuring-ldap-group-search"},"configuring-main-team-authorization":{"location":"generic-oauth.html#configuring-main-team-authorization","title":"Configuring main Team Authorization","text":"OAuth users and groups can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_OAUTH_USER=my-user\nCONCOURSE_MAIN_TEAM_OAUTH_GROUP=my-group\nMultiple users and groups may be specified by comma-separating them.\n\n","depth":6,"section_tag":"configuring-main-team-authorization"},"configuring-metrics":{"location":"metrics.html#configuring-metrics","title":"Configuring Metrics","text":"The Running a web node can be configured to emit metrics on start.\n\nCurrently supported metrics emitters are InfluxDB, NewRelic, Prometheus, Datadog, and Riemann. There is also a dummy emitter that will just spit the metrics out in to the logs at DEBUG level, which can be enabled with the --emit-to-logs flag.\n\nThere are various flags for different emitters; run concourse web --help and look for \"Metric Emitter\" to see what's available.\n\n","depth":4,"section_tag":"configuring-metrics"},"configuring-the-secrets-engine":{"location":"vault-credential-manager.html#configuring-the-secrets-engine","title":"Configuring the secrets engine","text":"Concourse is currently limited to looking under a single path, meaning only one secrets engine is supported: kv, version 1. This may change in the future - we're still collecting ideas in RFC #21.\n\nSo, let's configure the kv secrets engine and mount it at /concourse:\n\n$ vault secrets enable -version=1 -path=concourse kv\nNext, you'll want to create a policy to allow Concourse to read from this path.\n\npath \"concourse/*\" {\n  policy = \"read\"\n}\nSave this to concourse-policy.hcl, and then run:\n\nvault policy write concourse ./concourse-policy.hcl\nThis configuration will allow Concourse to read all credentials under /concourse. This should match your configured path prefix.\n\n","depth":5,"section_tag":"configuring-the-secrets-engine"},"container-collection":{"location":"garbage-collection.html#container-collection","title":"Container Collection","text":"First, a fairly simple query is executed to find containers that meet one of the following conditions:\n\n* If it has a NULL reference for all four dependent columns:\n\n  * containers (build_id)\n\n  * containers (image_check_container_id)\n\n  * containers (image_get_container_id)\n\n  * containers (worker_resource_config_check_session_id)\n\n  This is the simplest case: the things that needed the container are now gone, so it can go away.\n\n* The containers (build_id) referenced by the container is no longer interceptible. See Build Retention.\n\n* The containers (image_check_container_id) or containers (image_get_container_id) referenced by the container is no longer in CREATING state (likely CREATED).\n\nOnce these containers are found, they are all deleted in parallel, with a max-in-flight limit per worker so that the worker doesn't get hammered by a burst of writes.\n\nThe deletion of every container is a careful process to ensure they never leak and are never deleted while a user is hijacked into them:\n\n* If the container is CREATING, we mark it CREATED. This is a bit wonky but makes it easier to just step it through the rest of the lifecycle, since if there was a container being created on the worker, we need to clean it up.\n\n* If the container is CREATED, we first check to see if it was hijacked. If not, we transition it to DESTROYING.\n\n  If the container is hijacked, we try to find the container in the worker.\n\n  If the worker container is found, we set a grace time on it (a period of inactivity after which the container will be reaped by the worker itself), mark the database container as discontinued, and transition the container to DESTROYING.\n\n  If the worker container is not found, we transition the container to DESTROYING, just to funnel it down the same code path as below.\n\n* If the container is DESTROYING, and the container is discontinued, we check if the container has expired yet (via the grace time) by looking for it on the worker. If it's still there, we leave it alone, and leave the container in the database. If it's gone, we reap the container from the database.\n\n  If the container is not discontinued, we destroy the container on the worker and reap the container from the database.\n\nNote that if any point of the above process fails, the container is left in its current state in the database. A container is only ever removed from the database when it's guaranteed that everything has been cleaned up.\n\n","depth":5,"section_tag":"container-collection"},"container-internals":{"location":"container-internals.html","title":"Containers","text":"","depth":3,"section_tag":"container-internals"},"container-lifecycle":{"location":"container-internals.html#container-lifecycle","title":"Lifecycle","text":"Containers can be in one of 3 states; CREATING, CREATED, DESTROYING.\n\nCREATING containers are still being initialized on the worker and are not yet ready to be used. CREATING, containers can only transition to CREATED.\n\nCREATED containers are initialized on the worker and are ready to be used. A CREATED container can only be transitioned to DESTROYING.\n\nDESTROYING containers are marked for removal on the worker, and should no longer be used; they will be removed from the database when they no longer exist on the worker.\n\n","depth":4,"section_tag":"container-lifecycle"},"container-placement":{"location":"container-placement.html","title":"Container Placement","text":"Each step in a build is executed inside a container. The Running a web node distributes containers across the worker cluster depending on the configured strategy.\n\n","depth":3,"section_tag":"container-placement"},"containers-build_id":{"location":"database-schema.html#containers-build_id","title":"build_id","text":"If this container is for a build step, this column refers to the builds (id) this container is related to.\n\n","depth":4,"section_tag":"runtime"},"containers-handle":{"location":"database-schema.html#containers-handle","title":"handle","text":"The unique identifier of the container in Garden.\n\n","depth":4,"section_tag":"runtime"},"containers-image_check_container_id":{"location":"database-schema.html#containers-image_check_container_id","title":"image_check_container_id","text":"This signifies that the container is dependant on another container which is busy checking for the image this container will be based on. This is used in the case of custom resource types, or tasks with image_resource.  This container will be in the CREATING state until the image is fetched later.\n\n","depth":4,"section_tag":"runtime"},"containers-image_get_container_id":{"location":"database-schema.html#containers-image_get_container_id","title":"image_get_container_id","text":"This signifies that the container is dependant on another container which is busy downloading the bits for the image this container will be based on. This is used in the case of custom resource types, or tasks with image_resource. This container will be in the CREATING state until the image is fetched.\n\n","depth":4,"section_tag":"runtime"},"containers-resource_id":{"location":"database-schema.html#containers-resource_id","title":"worker_resource_config_check_session_id","text":"If this container is for running the check script for a resource, this column refers to a worker_resource_config_check_sessions (id).\n\n","depth":4,"section_tag":"runtime"},"containers-state":{"location":"database-schema.html#containers-state","title":"state","text":"The stage in the lifecycle of the container. See Lifecycle\n\n","depth":4,"section_tag":"runtime"},"containers-table":{"location":"database-schema.html#containers-table","title":"containers","text":"The containers table represents the set of all containers across all the workers, and keeps track of their state such that no container is ever left behind on a worker.\n\nContainers have a handful of really important attributes:\n\nContainers can be one of four types, each with individual references to their relating object.\n\n","depth":4,"section_tag":"runtime"},"containers-worker_name":{"location":"database-schema.html#containers-worker_name","title":"worker_name","text":"The name of the worker where the container is located.\n\n","depth":4,"section_tag":"runtime"},"contribute":{"location":"contribute.html","title":"Contribute","text":"Concourse is a free and Open Source software project that relies on the contributions of sponsors and volunteers from around the world. As a growing community of continuous thing-doers, the team is always in need of more people to help out in our community.\n\nEven if you're just getting started with Concourse you can contribute in a few ways:\n\n* Discuss and Share your experiences with Concourse in our Discord forums\n\n* Blog about Concourse. We would love to hear how you got started with Concourse, the type of pipelines you're building, or any tips \u0026 tricks that you can share with the community.\n\n* Answer questions about Concourse in the Support section of our forums or on Stack Overflow\n\nAs you become more comfortable with Concourse, you might be interested in contributing to Concourse's development: * Review issues and report bugs in the concourse/concourse repo. We are by no means experts in every subject area; it sometimes takes us while to understand a problem space well enough to figure out things fit into Concourse's puritanical world. You can help us by: * Voting for issues by adding an emoji reaction to the issue\n\n  * Submitting new bugs when you run into a new problem with Concourse\n\n  * Review GitHub issues submitted by other members of the community; ask for (or provide!) clarification when necessary. A lot of the issues that come in are simply unclear and we end up spending a lot of time on clarifying issues.\n\n* Writing Documentation for the project. You can get started by reviewing the docs code at concourse/docs and making PRs against the project.\n\n* Contributing Code. If you're interested in contributing some work to the core project, you can get started by reviewing the CONTRIBUTING.md getting started guide. You can also get an overview of the Concourse architecture by reviewing some of the documentation under Concourse Architecture. If you're curious about what we're working on you can follow along with the team's progress on our public projects page.\n\n\n\nThe Concourse project is committed to fostering an open and welcoming environment for all project members, maintainers and community members. The Concourse project pledges to make our community  a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. You can read more about our commitment in the project's Contributor Code of Conduct\n\n","depth":2,"section_tag":"contribute"},"credential-lookup-rules":{"location":"aws-asm-credential-manager.html#credential-lookup-rules","title":"Credential Lookup Rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* /concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* /concourse/TEAM_NAME/foo_param\n\nThe leading /concourse can be changed by specifying --aws-secretsmanager-pipeline-secret-template or --aws-secretsmanager-team-secret-template variables.\n\n","depth":5,"section_tag":"credential-lookup-rules"},"credhub-credential-manager":{"location":"credhub-credential-manager.html","title":"The CredHub credential manager","text":"","depth":4,"section_tag":"credhub-credential-manager"},"creds":{"location":"creds.html","title":"Credential Management","text":"Going beyond Encryption, explicit credential management will provide credentials to your builds for a brief amount of time, without being persisted anywhere. It also allows for credentials to be rotated and managed external to the pipeline or team, and prevents them from being revealed by fly get-pipeline.\n\nCredential management works by replacing the credentials with ((vars)) in your pipeline or task config. When the Concourse is about to run the step or check that is configured with vars, it will resolve them by fetching the values from the credential manager. If the values are not present, the action will error.\n\nThe following configurations can be parameterized with a credential manager:\n\n* source under Resources in a pipeline\n\n* source under Resource Types in a pipeline\n\n* webhook_token under Resources in a pipeline\n\n* params on a task step in a pipeline\n\n* Tasks in their entirety - whether from file or config in a pipeline, or a config executed with fly execute\n\nWhere these values are looked up and how the credential manager is configured depends on the backend. Consult the relevant section below for whichever backend you want to use.\n\nConcourse currently supports the following credential managers:\n\nCommon Configuration Parameters\n\nWhen a request to the credential manager fails due to an intermittent error (e.g. a timeout or connection refused), Concourse will automatically try the request again up to 5 times before giving up. After all attempts fail, the error will be surfaced in the UI for the resource check or build step that initiated the request.\n\nThe retry logic can be configured by specifying the following env on the Running a web node:\n\nCONCOURSE_SECRET_RETRY_ATTEMPTS=5   # how many times to try\nCONCOURSE_SECRET_RETRY_INTERVAL=10s # how long to wait between attempts\n","depth":3,"section_tag":"creds"},"database-schema":{"location":"database-schema.html","title":"Database Schema","text":"","depth":3,"section_tag":"database-schema"},"db-prerequisites":{"location":"postgresql-node.html#db-prerequisites","title":"Prerequisites","text":"PostgreSQL 9.5 or above is required, though the latest available version is recommended.\n\n","depth":4,"section_tag":"db-prerequisites"},"db-properties":{"location":"postgresql-node.html#db-properties","title":"Properties","text":"CPU usage: this is one of the most volatile metrics, and one we try pretty hard to keep down. There will be near-constant database queries running, and while we try to keep them very simple, there is always more work to do. Expect to feed your database with at least a couple cores, ideally four to eight. Monitor this closely as the size of your deployment and the amount of traffic it's handling increases, and scale accordingly.\n\nMemory usage: similar to CPU usage, but not quite as volatile.\n\nDisk usage: pipeline configurations and various bookkeeping metadata for keeping track of jobs, builds, resources, containers, and volumes. In addition, all build logs are stored in the database. This is the primary source of disk usage. To mitigate this, users can configure build_logs_to_retain on a job, but currently there is no operator control for this setting. As a result, disk usage on the database can grow arbitrarily large.\n\nBandwidth usage: well, it's a database, so it most definitely uses the network (duh). Not much should stand out here, though build logs can result in an arbitrary amount of data being sent over the network to the database. This should be nothing compared to worker bandwidth, though.\n\nHighly available: up to you. Clustered PostgreSQL is kind of new and probably tricky to deploy, but there are various cloud solutions for this.\n\nHorizontally scalable: I...don't think so?\n\nOutbound traffic:\n\n* none\n\nInbound traffic:\n\n* only ever from the web node\n\n","depth":4,"section_tag":"db-properties"},"db-running":{"location":"postgresql-node.html#db-running","title":"Running","text":"How this node is managed is up to you; Concourse doesn't actually have much of an opinion on it, it just needs a database.\n\nHow to install PostgreSQL is really dependent on your platform. Please refer to your Linux distribution or operating system's documentation.\n\nFor the most part, the instruction on Linux should look something like this:\n\nsudo apt install postgresql\nsudo su postgres -c \"createuser $(whoami)\"\nsudo su postgres -c \"createdb --owner=$(whoami) atc\"\nThis will install PostgreSQL (assuming your distro uses apt), create a user, and create a database that the current UNIX user can access, assuming this same user is going to be running the Running a web node. This is a reasonable default for distros like Ubuntu and Debian which default PostgreSQL to peer auth.\n\n","depth":4,"section_tag":"db-running"},"disabling-encryption":{"location":"encryption.html#disabling-encryption","title":"Disabling Encryption","text":"To opt out of encryption entirely (I'm sure you have your reasons), simply pass --old-encryption-key (or old_encryption_key) alone. With no new encryption key, the Running a web node will decrypt all existing data on start.\n\n","depth":4,"section_tag":"disabling-encryption"},"do":{"location":"do-step.html#do","title":"do","text":"Simply performs the given steps serially, with the same semantics as if they were at the top level step listing.\n\n","depth":4,"section_tag":"do-step"},"do-step":{"location":"do-step.html","title":"do step","text":"","depth":4,"section_tag":"do-step"},"docs":{"location":"docs.html","title":"Docs","text":"Concourse is a pipeline-based continuous thing-doer.\n\nThe word \"pipeline\" is all the rage in CI these days, so being more specific about this term is kind of important; Concourse's pipelines are significantly different from the rest.\n\nPipelines are built around Resources, which represent all external state, and Jobs, which interact with them. Concourse pipelines represent a dependency flow, kind of like distributed Makefiles. Pipelines are designed to be self-contained so as to minimize server-wide configuration. Maximizing portability also mitigates risk, making it easier for projects to recover from CI disasters.\n\nResources like the git resource and s3 resource are used to express source code, dependencies, deployments, and any other external state. This interface is also used to model more abstract things like scheduled or interval triggers, via the time resource.\n\nResource Types are defined as part of the pipeline itself, making the pipelines more self-contained and keeping Concourse itself small and generic without resorting to a complicated plugin system.\n\nJobs are sequences get, put, and task steps to execute. These steps determine the job's inputs and outputs. Jobs are designed to be idempotent and loosely coupled, allowing the pipeline to grow with the project's needs without requiring engineers to keep too much in their head at a time.\n\nEverything in Concourse runs in a container. Instead of modifying workers to install build tools, Tasks describe their own container image (typically using Docker images via the registry-image resource).\n\n...What?\n\nConcourse admittedly has a steeper learning curve at first, and depending on your background it might be a lot to take in. A core goal of this project is for the curve to flatten out shortly after and result in higher productivity and less stress over time.\n\nIf this all sounds like gobbeldigook, that's OK - you may want to just continue on, start kicking the tires a bit, and use the above as a quick reference of the \"big picture\" as the mental model sets in.\n\n","depth":1,"section_tag":"docs"},"downgrading":{"location":"concourse-web.html#downgrading","title":"Downgrading","text":"If you're stuck in a pinch and need to downgrade from one version of Concourse to another, you can use the concourse migrate command.\n\nNote: support for down migrations is a fairly recent addition to Concourse; it is not supported for downgrading to v3.6.0 and below.\n\nFirst, grab the desired migration version by running the following:\n\n# make sure this is the *old* Concourse binary\n$ concourse migrate --supported-db-version\n1551110547\nThat number (yours will be different) is the expected migration version for that version of Concourse.\n\nNext, run the following with the new Concourse binary:\n\n$ concourse migrate --migrate-db-to-version=1551110547\nThis will need the same CONCOURSE_POSTGRES_* configuration described in Running.\n\nOnce this completes, switch all web nodes back to the older concourse binary and you should be good to go.\n\n","depth":5,"section_tag":"downgrading"},"enabling-encryption":{"location":"encryption.html#enabling-encryption","title":"Enabling Encryption","text":"To enable encryption, you'll just need to come up with a 16 or 32-byte random character sequence and configure it as --encryption-key flag to the web command. For BOSH, this is the encryption_key property.\n\nOn startup, the Running a web node will encrypt all existing plaintext data, and any new data being written will be encrypted before it's sent over the network to the database.\n\nThe initial bulk encryption shouldn't take too long, but it will scale linearly with the amount of data that you have, and if another ATC is running it'll suddenly not be able to read the data until it's also given the key. So, expect some downtime.\n\n","depth":4,"section_tag":"enabling-encryption"},"encryption":{"location":"encryption.html","title":"Encryption","text":"Automating everything means authorizing something to automate many things. This makes CI systems a high-risk target for security leaks.\n\nConcourse pipelines are loaded with credentials: resources are configured with private keys, tasks are given credentials to servers they integrate via credential manager variables, vars, or params, etc. If someone gets their hands on your config, they have access to everything.\n\nTo mitigate this, Concourse supports encrypting sensitive information before it reaches the database. This way the plaintext credentials only exist in memory for as long as they need to, and if someone gains access to your database, they can't so easily gain the keys to the kingdom.\n\nWe strongly encourage anyone running Concourse to configure encryption. Going further, it's best to have Concourse not store the credentials in the first place, in which case you may want to configure credential management as well.\n\n","depth":3,"section_tag":"encryption"},"ensure":{"location":"ensure-step-hook.html#ensure","title":"ensure","text":"The step to execute. Regardless of whether the parent step succeeds, fails, or errors, this step will be executed. The step will also be executed if the build was aborted, and its parent step was interrupted.\n\nIf the parent step succeeds and the ensured step fails, the parent step is considered to have failed.\n\nThe ensured step executes after any on_success step hooks or on_failure step hooks.\n\n","depth":4,"section_tag":"ensure-step-hook"},"ensure-step-hook":{"location":"ensure-step-hook.html","title":"ensure step hook","text":"Any step can have ensure tacked onto it, whose value is a second step to execute regardless of the result of the parent step.\n\n","depth":4,"section_tag":"ensure-step-hook"},"examples":{"location":"examples.html","title":"Examples","text":"Configuring self-contained Concourse pipelines is a great way to try things out before diving into the deeper content.\n\nEach example contains a pipeline YAML snippet which can be copy-pasted to a local file and configured on your instance via fly set-pipeline. From there you may want to poke around and try changing parts of the configuration to learn how things work. All the available knobs to turn are covered in the Docs.\n\nFor a real-world example, check out Concourse's own pipeline (and its config):\n\n","depth":1,"section_tag":"examples"},"exposing":{"location":"exposing.html","title":"Pipeline \u0026 Build Visibility","text":"Every newly configured pipeline is hidden to anyone but the pipeline's team. To make a pipeline publicly viewable, both by other teams and unauthenticated users, see fly expose-pipeline.\n\nEven with a pipeline exposed, all build logs are hidden by default. This is because CI jobs are prone to leaking credentials and other...unsavory information. After you've determined that a job's builds should be safe for public consumption, you can set public: true on the job in your pipeline.\n\n","depth":3,"section_tag":"exposing"},"fewer-resource-checks-to-perform":{"location":"global-resources.html#fewer-resource-checks-to-perform","title":"Fewer resource checks to perform","text":"With global resources, all resources that have the same configuration will share the same version history and share only one checking interval. This reduces load on the worker and on the external services that the resources point to.\n\nFor example, prior to global resources if there were three resources with the same configuration between three team's pipelines it would result in three check containers performing three resource checks every minute to fetch the versions.\n\nWith global resources, this configuration will result in only one check container and one resource check every minute to fetch versions for all the resources.\n\nSince there will be only one resource check for all resources that have the same configuration, the resource that has the shortest check_every configured will result in its pipeline running the checks for that resource configuration.\n\n","depth":5,"section_tag":"fewer-resource-checks-to-perform"},"fewest-build-containers-strategy":{"location":"container-placement.html#fewest-build-containers-strategy","title":"The fewest-build-containers strategy","text":"When using the fewest-build-containers strategy, step containers are placed on the worker that has the fewest build containers (i.e. containers for other steps of other builds).\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=fewest-build-containers\n","depth":4,"section_tag":"fewest-build-containers-strategy"},"fly":{"location":"fly.html","title":"The fly CLI","text":"Concourse is primarily driven from the command-line; there is no GUI config wizard.\n\nSo, the first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-abort-build":{"location":"builds.html#fly-abort-build","title":"fly abort-build","text":"To abort a build of a job, run:\n\n$ fly -t example abort-build --job my-pipeline/my-job --build 3\nThis will cancel build 3 of the my-job job in the my-pipeline pipeline.\n\n","depth":3,"section_tag":"fly-abort-build"},"fly-builds":{"location":"builds.html#fly-builds","title":"fly builds","text":"To list the most recent builds, run:\n\n$ fly -t example builds\nTo list the builds of a job, run:\n\n$ fly -t example builds -j pipeline-name/job-name\nThis can be useful for periodically monitoring the state of a job. The output also works well with tools like awk and grep.\n\nBy default the most recent 50 builds are shown. To see more builds, use the -c flag, like so:\n\n$ fly -t example builds -c 100\n","depth":3,"section_tag":"fly-builds"},"fly-check-resource":{"location":"managing-resources.html#fly-check-resource","title":"fly check-resource","text":"To force immediate checking for new versions of a resource, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource\nTo check from a particular version, including the given version, append the --from flag like so:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource \\\n    --from ref:abcdef\nThis can be useful for collecting versions that are older than the current ones, given that a newly configured resource will only start from the latest version.\n\nNote the ref: prefix is resource-dependent. For example, the bosh-io-release resource might use version:11.2 in place of ref:abcdef.\n\n","depth":4,"section_tag":"fly-check-resource"},"fly-check-resource-type":{"location":"managing-resource-types.html#fly-check-resource-type","title":"fly check-resource-type","text":"To force immediate checking for new versions of a resource type, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource-type --resource-type my-pipeline/my-resource-type\nThis can be useful for forcing an update if you're iterating on your own resource type implementation.\n\n","depth":4,"section_tag":"fly-check-resource-type"},"fly-clear-task-cache":{"location":"managing-jobs.html#fly-clear-task-cache","title":"fly clear-task-cache","text":"If you've got a task cache that you need to clear out for whatever reason, this can be done like so:\n\n$ fly -t example clear-task-cache --job my-pipeline/my-job --step my-step-name\nThis will immediately invalidate the caches - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.\n\nYou can also clear out a particular path for the given step's cache, using --cache-path:\n\n$ fly -t example clear-task-cache \\\n    --job my-pipeline/my-job \\\n    --step my-step-name \\\n    --cache-path go/pkg\nIf --cache-path is not specified, all caches for the given step will be cleared.\n\n","depth":4,"section_tag":"fly-clear-task-cache"},"fly-cli":{"location":"fly.html","title":"The fly CLI","text":"Concourse is primarily driven from the command-line; there is no GUI config wizard.\n\nSo, the first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-containers":{"location":"administration.html#fly-containers","title":"fly containers","text":"To list the active containers across all your workers, run:\n\n$ fly -t example containers\nThis can be useful when discovering the containers available for fly intercepting.\n\n","depth":4,"section_tag":"fly-containers"},"fly-curl":{"location":"administration.html#fly-curl","title":"fly curl","text":"To execute an arbirary API request, you can run something like the following:\n\n$ fly -t example curl /api/v1/info\nThis command is just a shim that runs curl under the hood. To pass flags to curl, pass a -- argument after the path so that fly can distinguish them from its own flags:\n\n$ fly -t example curl /api/v1/builds -- \\\n    -X PUT \\\n    -H \"Content-type: application/json\" \\\n    -d @plan.json\nNote: if you use this command the assumption is that you know what you're doing. If you find yourself using this command often, let us know - perhaps there's a missing command!\n\n","depth":4,"section_tag":"fly-curl"},"fly-delete-target":{"location":"fly.html#fly-delete-target","title":"fly delete-target","text":"When logging out just isn't enough, a target can be completely removed from ~/.flyrc by running:\n\n$ fly -t example delete-target\nTo delete all targets, run:\n\n$ fly delete-target -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-delete-target"},"fly-destroy-pipeline":{"location":"managing-pipelines.html#fly-destroy-pipeline","title":"fly destroy-pipeline","text":"Every now and then you just don't want a pipeline to be around anymore. Running fly destroy-pipeline will stop the pipeline activity and remove all data collected by the pipeline, including build history and collected versions.\n\nFor example, to destroy the my-pipeline pipeline, you would run:\n\n$ fly -t example destroy-pipeline --pipeline my-pipeline\n","depth":4,"section_tag":"fly-destroy-pipeline"},"fly-destroy-team":{"location":"managing-teams.html#fly-destroy-team","title":"fly destroy-team","text":"To remove a team, including all of its pipelines and one-off builds, first log in as the The main team, and then run:\n\n$ fly -t example destroy-team --team-name my-team\nCurrently, if there were any workers assigned specifically to this team, they'll be orphaned, without having their containers or volumes cleaned up.\n\n","depth":4,"section_tag":"fly-destroy-team"},"fly-edit-target":{"location":"fly.html#fly-edit-target","title":"fly edit-target","text":"To modify a target's name, team, or URL, run:\n\n$ fly -t example edit-target \\\n    --target-name new-name \\\n    --concourse-url https://ci.example.com \\\n    --team-name my-team\nEach flag is optional - only the specified flags will be changed.\n\n","depth":3,"section_tag":"fly-edit-target"},"fly-execute":{"location":"running-tasks.html#fly-execute","title":"fly execute","text":"You can execute a task like this:\n\n$ fly -t example execute --config tests.yml\nYour files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify -i name=. instead, where name is the input name from the task configuration.\n\nFly will automatically capture SIGINT and SIGTERM and abort the build when received. This allows it to be transparently composed with other toolchains.\n\nBy default, Fly will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the --include-ignored flags in order to send ignored files to Concourse along with those that are not ignored.\n\nIf your task needs to run as root then you can specify the -p or --privileged flag.\n\n","depth":4,"section_tag":"fly-execute"},"fly-execute-vars":{"location":"running-tasks.html#fly-execute-vars","title":"Providing values for ((vars))","text":"Task config files can contain template variables in the form of ((foo-bar)), the same as pipeline ((vars)).\n\nThese vars can be set during fly execute by using the -v, -y and -l flags, the same as fly set-pipeline:\n\nfly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\nAny variables not satisfied via the above flags will be deferred to the configured credential manager.\n\nTo satisfy these vars when running the task in a pipeline, see vars.\n\n","depth":5,"section_tag":"fly-execute-vars"},"fly-expose-pipeline":{"location":"managing-pipelines.html#fly-expose-pipeline","title":"fly expose-pipeline","text":"By default, newly configured pipelines are only visible to the pipeline's team. To make a pipeline viewable by other teams and unauthenticated users, run:\n\n$ fly -t example expose-pipeline --pipeline my-pipeline\nThis feature is useful if you're using Concourse for an open source project and you'd like your community to be able to see into your build pipeline.\n\nTo undo this change, see fly hide-pipeline.\n\nExposing a pipeline reveals basically everything except for build output and resource metadata.\n\nTo expose a resource's metadata, public must be set to true.\n\nTo expose a job's build output, public must be set to true. This will also reveal resource metadata for any get step or put steps in the build output.\n\n","depth":4,"section_tag":"fly-expose-pipeline"},"fly-format-pipeline":{"location":"setting-pipelines.html#fly-format-pipeline","title":"fly format-pipeline","text":"To format a pipeline config in a \"canonical\" form (i.e. keys are in normal order, with name first for example), run:\n\n$ fly format-pipeline --config pipeline.yml\nThis will print the formatted pipeline config to stdout. To update the file in-place, pass --write/-w.\n\n","depth":4,"section_tag":"fly-format-pipeline"},"fly-get-pipeline":{"location":"managing-pipelines.html#fly-get-pipeline","title":"fly get-pipeline","text":"Fly can be used to fetch and update the configuration for your pipelines. This is achieved by using the fly get-pipeline and fly set-pipeline commands. For example, to fetch the current configuration of your my-pipeline Concourse pipeline and print it on STDOUT run the following:\n\n$ fly -t example get-pipeline --pipeline my-pipeline\nTo get JSON instead of YAML you can use the -j or --json argument. This can be useful when inspecting your config with jq.\n\n","depth":4,"section_tag":"fly-get-pipeline"},"fly-hide-pipeline":{"location":"managing-pipelines.html#fly-hide-pipeline","title":"fly hide-pipeline","text":"If you realize that you've made a terrible mistake in exposing your pipeline, you can run:\n\n$ fly -t example hide-pipeline --pipeline my-pipeline\nIf you're panicking you can run the command's short form, hp, instead.\n\n","depth":4,"section_tag":"fly-hide-pipeline"},"fly-intercept":{"location":"builds.html#fly-intercept","title":"fly intercept","text":"Sometimes it's helpful to be on the same machine as your tasks so that you can profile or inspect them as they run or see the state the machine at the end of a run. Due to Concourse running tasks in containers on remote machines this would typically be hard to access.\n\nTo this end, there is a fly intercept command that will give you an interactive shell inside the specified container. Containers are identified by a few things, so you may need to specify a few flags to hone down the results. If there are multiple containers that the flags could refer to, an interactive prompt will show up allowing you to disambiguate.\n\nFor example, running the following will run a task and then enter the finished task's container:\n\n$ fly -t example execute\n$ fly -t example intercept --step build\nWhen intercepting a task running on a Windows worker, you will need to specifically tell fly to to run powershell:\n\n$ fly -t example intercept powershell\nContainers are around for a short time after a build finishes in order to allow people to intercept them.\n\nYou can also intercept builds that were run in your pipeline. By using --job, --build, and --step you can intercept a specific step from a build of a job in your pipeline. These flags also have short forms, like so:\n\n$ fly -t example intercept -j some-pipeline/some-job -b some-build -s some-step\nNote that --build can be omitted, and will default to the most recent build of the job. One-off builds can be reached by passing in their build ID to --build which can be found on the build list page.\n\nThe --step flag can also be omitted; this will let you pick the step interactively if you don't know the exact name.\n\nResource checking containers can also be intercepted with --check or -c:\n\n$ fly -t example intercept --check some-pipeline/some-resource\nA specific command can also be given, e.g. fly intercept ps auxf or fly intercept htop. This allows for patterns such as watch fly intercept ps auxf, which will continuously show the process tree of the current build's task, even as the \"current build\" changes.\n\nThe working directory and any relevant environment variables (e.g. those having come from params) used by the original process will also be used for the process run by intercept.\n\n","depth":3,"section_tag":"fly-intercept"},"fly-jobs":{"location":"managing-jobs.html#fly-jobs","title":"fly jobs","text":"To list the jobs configured in a pipeline, run:\n\n$ fly -t example jobs -p my-pipeline\n","depth":4,"section_tag":"fly-jobs"},"fly-land-worker":{"location":"administration.html#fly-land-worker","title":"fly land-worker","text":"To initiate landing of a worker and eventually (after draining) cause it to exit, run:\n\n$ fly -t example land-worker --worker worker-name\n","depth":4,"section_tag":"fly-land-worker"},"fly-login":{"location":"fly.html#fly-login","title":"fly login","text":"The first thing you'll want to do is authenticate with your target. This is done with the fly login command. This is also useful to save targets under a more convenient alias, so you don't have to type out the URL all the time:\n\nThe login command serves double duty: it authenticates with a given endpoint, and saves it under a more convenient name. The name and token are stored in ~/.flyrc (though you shouldn't really edit the file manually).\n\nConcourse deployments can be occupied by multiple teams. To specify the team to which to log in, specify the --team-name or -n flag. If not specified, this defaults to the The main team.\n\nSo, to log in to a team my-team an endpoint served at https://ci.example.com and save it as the more convenient name example, you would run:\n\n$ fly --target example login --team-name my-team \\\n    --concourse-url https://ci.example.com\nThe login command will see which authentication methods are available for the specified team and prompt you to choose one. For basic auth, it will ask your username and password and use them to acquire a token. For OAuth, it will give you a link to click, and after you've gone through the OAuth flow it will print an OAuth token on the page that you can then copy and paste into the prompt.\n\nNote that if no authentication methods are configured, fly will acquire a token without any prompting. You can then use the alias like normal.\n\nIn any case, a token is saved in your ~/.flyrc, which will expire after one day.\n\nIf your Concourse uses SSL but does not have a certificate signed by a trusted CA, you can use the --ca-cert flag so that fly can trust the connection, like so:\n\n$ fly -t example login -c https://ci.example.com --ca-cert ./ca.crt\nThis will read the value out of the file ./ca.crt and save it into ~/.flyrc so you don't have to pass it on every login invocation.\n\nAfter you've logged in you can use --target example (or -t example for short) to run a command against the saved target example. For eample, fly -t example builds will list the last few builds on the example Concourse instance.\n\nThe -t flag is intentionally stateless and must be explicitly added to each command. This reduces the risk of accidentally running a command against the wrong environment when you have multiple targets defined.\n\n","depth":3,"section_tag":"fly-login"},"fly-logout":{"location":"fly.html#fly-logout","title":"fly logout","text":"To clear out your token for a given target, run:\n\n$ fly -t example logout\nTo clear out your token for all targets, run:\n\n$ fly logout -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-logout"},"fly-order-pipelines":{"location":"managing-pipelines.html#fly-order-pipelines","title":"fly order-pipelines","text":"To configure the ordering of pipelines, run:\n\n$ fly -t example order-pipelines \\\n    --pipeline pipeline-1 \\\n    --pipeline pipeline-2 \\\n    --pipeline pipeline-3\nNote that this command only ensures that the given pipelines are in the given order. If there are other pipelines that you haven't included in the command, they may appear in-between, before, or after the given set.\n\n","depth":4,"section_tag":"fly-order-pipelines"},"fly-pause-job":{"location":"managing-jobs.html#fly-pause-job","title":"fly pause-job","text":"To prevent scheduling and running builds of a job, run:\n\n$ fly -t example pause-job --job my-pipeline/my-job\nThis will prevent pending builds of the job from being scheduled, though builds that are in-flight will still run, and pending builds will still be created as normal.\n\n","depth":4,"section_tag":"fly-pause-job"},"fly-pause-pipeline":{"location":"managing-pipelines.html#fly-pause-pipeline","title":"fly pause-pipeline","text":"To pause a pipeline, run:\n\n$ fly -t example pause-pipeline --pipeline my-pipeline\nThis will prevent jobs from being scheduled and stop the periodic checking for new versions of resources. Builds that are in-flight will still finish.\n\n","depth":4,"section_tag":"fly-pause-pipeline"},"fly-pipelines":{"location":"managing-pipelines.html#fly-pipelines","title":"fly pipelines","text":"To list the currently-configured pipelines and their paused state, run:\n\n$ fly -t example pipelines\n","depth":4,"section_tag":"fly-pipelines"},"fly-prune-worker":{"location":"administration.html#fly-prune-worker","title":"fly prune-worker","text":"To remove a stalled, landing, landed, or retiring worker, run:\n\n$ fly -t example prune-worker --worker worker-name\nTo prune all stalled workers, run:\n\n$ fly -t example prune-worker --all-stalled\nThis is for those cases where you know a worker is not coming back. Note that running workers cannot be pruned, since they'll just re-register themselves anyway.\n\n","depth":4,"section_tag":"fly-prune-worker"},"fly-rename-pipeline":{"location":"managing-pipelines.html#fly-rename-pipeline","title":"fly rename-pipeline","text":"To rename a pipeline, run:\n\n$ fly -t example rename-pipeline \\\n    --old-name my-pipeline \\\n    --new-name my-cool-pipeline\n","depth":4,"section_tag":"fly-rename-pipeline"},"fly-rename-team":{"location":"managing-teams.html#fly-rename-team","title":"fly rename-team","text":"To rename a team, run:\n\n$ fly -t example rename-team --old-name my-team --new-name cool-team\nThis can only be run by the main team.\n\n","depth":4,"section_tag":"fly-rename-team"},"fly-set-pipeline":{"location":"setting-pipelines.html#fly-set-pipeline","title":"fly set-pipeline","text":"To submit a pipeline configuration to Concourse from a file on your local disk you can use the -c or --config flag, like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml\nThis will present a diff of the changes and ask you to confirm the changes. If you accept then Concourse's pipeline configuration will switch to the pipeline definition in the YAML file specified.\n\n","depth":4,"section_tag":"fly-set-pipeline"},"fly-set-team":{"location":"managing-teams.html#fly-set-team","title":"fly set-team","text":"Once you've logged in as the main team with fly, you can run fly set-team to create or update other teams. Users with a Team Owner role can also update their own configuration with the same command.\n\nFor example, to create a new team that authorizes the local foo user, you would run:\n\nfly -t example set-team --team-name my-team \\\n  --local-user foo\nNote that each time set-team is run, the team's authorization config is set as a whole - it is not a stateful operation.\n\nThere are many different ways to configure team auth; see Configuring Auth for more information.\n\nOnce the team has been created, you can use fly login to log in:\n\n$ fly login -n my-team\nAny newly configured pipelines (via fly set-pipeline) and one-off builds (via fly execute) will be owned by the authorized team. Commands that list content will be scoped to the current team by default, such as fly pipelines and fly builds. The web UI will reflect the same state.\n\nNewly configured pipelines are hidden by default, meaning other teams and unauthorized visitors cannot view them. To make them publicly viewable, see Pipeline \u0026 Build Visibility.\n\n","depth":4,"section_tag":"fly-set-team"},"fly-status":{"location":"fly.html#fly-status","title":"fly status","text":"To check your current authentication status with a given target, run:\n\n$ fly -t example status\nThis will let you know if the token has expired.\n\n","depth":3,"section_tag":"fly-status"},"fly-sync":{"location":"fly.html#fly-sync","title":"fly sync","text":"Occasionally we add additional features to fly or make changes to the communiction between it and Concourse's API server. To make sure you're running the latest and greatest version that works with the Concourse you are targeting we provide a command called sync that will update your local fly. It can be used like so:\n\n$ fly -t example sync\nThe fly command will also warn you if it notices that your CLI version is out of sync with the server.\n\n","depth":3,"section_tag":"fly-sync"},"fly-targets":{"location":"fly.html#fly-targets","title":"fly targets","text":"To see what targets are currently known to fly, run:\n\n$ fly targets\nThis will show each target's name, URL, and when its token expires.\n\n","depth":3,"section_tag":"fly-targets"},"fly-teams":{"location":"managing-teams.html#fly-teams","title":"fly teams","text":"To list all the teams, run:\n\n$ fly -t example teams\nThis can be useful if you've forgotten your team name.\n\nfly teams -d: With Details\n\nTo list all the teams with authentication details and members, run:\n\n$ fly -t example teams -d\nThis can be helpful when debugging OAuth, OIDC groups or listing all individual members.\n\n","depth":4,"section_tag":"fly-teams"},"fly-trigger-job":{"location":"managing-jobs.html#fly-trigger-job","title":"fly trigger-job","text":"To immediately queue a new build of a job, run:\n\n$ fly -t example trigger-job --job my-pipeline/my-job\nThis will enqueue a new build of the my-job job in the my-pipeline pipeline.\n\nTo start watching the newly created build, append the --watch flag like so:\n\n$ fly -t example trigger-job --job my-pipeline/my-job --watch\n","depth":4,"section_tag":"fly-trigger-job"},"fly-unpause-job":{"location":"managing-jobs.html#fly-unpause-job","title":"fly unpause-job","text":"To resume scheduling of a job, run:\n\n$ fly -t example unpause-job --job my-pipeline/my-job\nThis will resume scheduling of builds queued for the job.\n\n","depth":4,"section_tag":"fly-unpause-job"},"fly-unpause-pipeline":{"location":"managing-pipelines.html#fly-unpause-pipeline","title":"fly unpause-pipeline","text":"To unpause a pipeline, run:\n\n$ fly -t example unpause-pipeline --pipeline my-pipeline\nThis will resume job scheduling and resource checking.\n\n","depth":4,"section_tag":"fly-unpause-pipeline"},"fly-userinfo":{"location":"fly.html#fly-userinfo","title":"fly userinfo","text":"To check what user you're logged in as, as well as which teams you are currently authenticated to and which roles within each team you have, run:\n\n$ fly -t example userinfo\n","depth":3,"section_tag":"fly-userinfo"},"fly-validate-pipeline":{"location":"setting-pipelines.html#fly-validate-pipeline","title":"fly validate-pipeline","text":"To validate a local pipeline configuration without submitting it to Concourse, run validate-pipeline:\n\n$ fly validate-pipeline --config pipeline.yml\nBy default, pipeline errors will cause validate-pipeline to fail, but warnings won't. To fail on both errors and warnings, pass the `--strict` flag.\n\n","depth":4,"section_tag":"fly-validate-pipeline"},"fly-volumes":{"location":"administration.html#fly-volumes","title":"fly volumes","text":"To list the active volumes across all your workers, run:\n\n$ fly -t example volumes\nThis can be useful to observe the caches warming across your cluster, and could be a good indicator of disk use.\n\n","depth":4,"section_tag":"fly-volumes"},"fly-watch":{"location":"builds.html#fly-watch","title":"fly watch","text":"Concourse emits streaming colored logs on the website but it can be helpful to have the logs availiable to the command line. (e.g. so that they can be processed by other commands).\n\nThe watch command can be used to do just this. You can also view builds that are running in your pipeline, or builds that have already finished.\n\nNote that unlike fly execute, killing fly watch via SIGINT or SIGTERM will not abort the build.\n\nTo watch the most recent one-off build, just run fly watch with no arguments. To watch a specific build (one-off or no), pass --build with the ID of the build to watch. This ID is available at the start of fly execute's output or by browsing to the builds list in the web UI.\n\nBy using the --job and --build flags you can pick out a specific build of a job to watch. For example, the following command will either show the archived logs for an old build if it has finished running or it will stream the current logs if the build is still in progress.\n\n$ fly -t example watch --job my-pipeline/tests --build 52\nIf the --job flag is specified and --build is omitted, the most recent build of the specified job will be selected.\n\n","depth":3,"section_tag":"fly-watch"},"fly-workers":{"location":"administration.html#fly-workers","title":"fly workers","text":"To list the currently registered workers, including additional metadata, run:\n\n$ fly -t example workers\nThis can be useful for monitoring the status of your workers, if you suspect that one keeps dropping out of the pool or getting tasked with too many containers, etc.\n\n","depth":4,"section_tag":"fly-workers"},"garbage-collection":{"location":"garbage-collection.html","title":"Garbage Collection","text":"One key difference between Concourse and other CI systems is that everything runs in isolated environments. Where some CI systems may just run builds one at a time on a single VM and reusing a working directory, Concourse creates fresh Containers and Volumes to ensure things can safely run in a repeatable environment, isolated from other workloads running on the same worker.\n\nThis introduces a new problem of knowing when Concourse should remove these containers and volumes. Safely identifying things for removal and then getting rid of them, releasing their resources, is the process of garbage collection.\n\n","depth":3,"section_tag":"garbage-collection"},"generating-keys":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nsession_signing_key: Used by the Running a web node for signing and verifying user session tokens.\n\n\ntsa_host_key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nworker_key (one per worker): Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized keys configuration in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n...and we'll also start on an authorized_keys file, currently listing this initial worker key:\n\ncp worker_key.pub authorized_worker_keys\n","depth":3,"section_tag":"concourse-generate-key"},"generic-oauth":{"location":"generic-oauth.html","title":"Generic oAuth","text":"A Concourse server can authenticate against any valid OAuth auth provider, though it's a bit \"closer to the metal\" as you'll need to explicitly configure the auth, token, and user-info URLs. You may want to see if you can use Generic OIDC auth if your auth provider is compatible with OIDC.\n\n","depth":4,"section_tag":"generic-oauth"},"generic-oauth-authentication":{"location":"generic-oauth.html#generic-oauth-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your oAuth provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nThe Generic oAuth provider has many values to set - for a full list consult concourse web --help.\n\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OAUTH_DISPLAY_NAME=Acme\nCONCOURSE_OAUTH_CLIENT_ID=myclientid\nCONCOURSE_OAUTH_CLIENT_SECRET=myclientsecret\nCONCOURSE_OAUTH_AUTH_URL=https://oauth.example.com/oauth2/auth\nCONCOURSE_OAUTH_TOKEN_URL=https://oauth.example.com/oauth2/token\nCONCOURSE_OAUTH_USERINFO_URL=https://oauth.example.com/oauth2/userinfo\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oauth-authentication"},"generic-oauth-authorization":{"location":"generic-oauth.html#generic-oauth-authorization","title":"Authorization","text":"OAuth users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oauth-user=USERNAME: Authorize an individual user.\n\n\n--oauth-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OAUTH_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oauth-user my-username \\\n    --oauth-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oauth:\n    users: [\"my-username\"]\n    groups: [\"my-group\"]\n","depth":5,"section_tag":"generic-oauth-authorization"},"generic-oidc-auth":{"location":"generic-oidc-auth.html","title":"Generic OIDC auth","text":"A Concourse server can authenticate against any valid OIDC auth provider. This provider is similar to Generic oAuth except it only requires an issuer URL rather than auth/token/userinfo URLs.\n\n","depth":4,"section_tag":"generic-oidc-auth"},"generic-oidc-authentication":{"location":"generic-oidc-auth.html#generic-oidc-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your OIDC provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OIDC_DISPLAY_NAME=Acme\nCONCOURSE_OIDC_CLIENT_ID=myclientid\nCONCOURSE_OIDC_CLIENT_SECRET=myclientsecret\nCONCOURSE_OIDC_ISSUER=https://oidc.example.com\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oidc-authentication"},"generic-oidc-authorization":{"location":"generic-oidc-auth.html#generic-oidc-authorization","title":"Authorization","text":"OIDC users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oidc-user=USERNAME: Authorize an individual user.\n\n\n--oidc-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OIDC_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oidc-user my-username \\\n    --oidc-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oidc:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"generic-oidc-authorization"},"get-step":{"location":"get-step.html","title":"get step","text":"Fetches a resource, making it available to subsequent steps via the given name.\n\n","depth":4,"section_tag":"get-step"},"get-step-get":{"location":"get-step.html#get-step-get","title":"get","text":"Required. The name of the resource once it is fetched. This name satisfies logical inputs to a Task, and may be referenced within the plan itself (e.g. in the file attribute of a task step).\n\nThis is also the name of the resource to fetch, if resource is not set.\n\n","depth":4,"section_tag":"get-step"},"get-step-params":{"location":"get-step.html#get-step-params","title":"params","text":"Optional. A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":4,"section_tag":"get-step"},"get-step-passed":{"location":"get-step.html#get-step-passed","title":"passed","text":"Optional. When specified, only the versions of the resource that made it through the given list of jobs will be considered when triggering and fetching.\n\n","depth":4,"section_tag":"get-step"},"get-step-resource":{"location":"get-step.html#get-step-resource","title":"resource","text":"Optional. Defaults to get, the name. The resource to fetch, as configured in resources.\n\nUse this attribute to rename a resource from the overall pipeline context into the job-specific context.\n\n","depth":4,"section_tag":"get-step"},"get-step-trigger":{"location":"get-step.html#get-step-trigger","title":"trigger","text":"Optional. Default false. Set to true to auto-trigger new builds of the plan's job whenever this step has new versions available, as specified by the resource and any passed constraints.\n\nOtherwise, if no get steps set this to true, the job can only be manually triggered.\n\n","depth":4,"section_tag":"get-step"},"get-step-version":{"location":"get-step.html#get-step-version","title":"version","text":"Optional. Defaults to latest. The version of the resource to fetch.\n\nIf set to latest, scheduling will just find the latest available version of a resource and use it, allowing versions to be skipped.  This is usually what you want, e.g. if someone pushes 100 git commits.\n\nIf set to every, builds will walk through all available versions of the resource. Note that if passed is also configured, it will only step through the versions satisfying the constraints.\n\nIf set to a specific version (e.g. {ref: abcdef123}), only that version will be used. Note that the version must be available and detected by the resource, otherwise the input will never be satisfied. You may want to use fly check-resource to force detection of resource versions, if you need to use an older one that was never detected (as all newly configured resources start from the latest version).\n\n","depth":4,"section_tag":"get-step"},"git-trigger-example":{"location":"git-trigger-example.html","title":"git-triggered job example","text":"The git resource can be used to trigger a job.\n\n","depth":2,"section_tag":"git-trigger-example"},"github-auth":{"location":"github-auth.html","title":"GitHub auth","text":"A Concourse server can authenticate against GitHub to leverage their permission model and other security improvements in their infrastructure.\n\n","depth":4,"section_tag":"github-auth"},"github-authentication":{"location":"github-auth.html#github-authentication","title":"Authentication","text":"First, you'll need to create an OAuth application on GitHub.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitHub - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITHUB_CLIENT_ID=myclientid\nCONCOURSE_GITHUB_CLIENT_SECRET=myclientsecret\nNote that the client must be created under an organization if you want to authorize users based on organization/team membership. If the client is created under a personal account, only individual users can be authorized.\n\nIf you're configuring GitHub Enterprise, you'll also need to set the following env:\n\nCONCOURSE_GITHUB_HOST=github.example.com\nCONCOURSE_GITHUB_CA_CERT=/path/to/ca_cert\nThe GitHub Enterprise host must not contain a scheme, or a trailing slash.\n\n","depth":5,"section_tag":"github-authentication"},"github-authorization":{"location":"github-auth.html#github-authorization","title":"Authorization","text":"Users, teams, and entire organizations can be authorized for a team by passing the following flags to fly set-team:\n\n--github-user=LOGIN: Authorize an individual user.\n\n\n--github-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--github-team=ORG_NAME:TEAM_NAME: Authorize a team's members within an organization.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --github-user my-github-login \\\n    --github-org my-org \\\n    --github-team my-other-org:my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  github:\n    users: [\"my-github-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"github-authorization"},"gitlab-auth":{"location":"gitlab-auth.html","title":"GitLab auth","text":"A Concourse server can authenticate against GitLab to leverage their permission model.\n\n","depth":4,"section_tag":"gitlab-auth"},"gitlab-authentication":{"location":"gitlab-auth.html#gitlab-authentication","title":"Authentication","text":"First you need to create an OAuth application on GitLab.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitLab - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITLAB_CLIENT_ID=myclientid\nCONCOURSE_GITLAB_CLIENT_SECRET=myclientsecret\nIf you're configuring a self hosted GitLab instance, you'll also need to set the following flag:\n\nCONCOURSE_GITLAB_HOST=https://gitlab.example.com\nThe GitLab host must contain a scheme and not a trailing slash.\n\n","depth":5,"section_tag":"gitlab-authentication"},"gitlab-authorization":{"location":"gitlab-auth.html#gitlab-authorization","title":"Authorization","text":"Users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--gitlab-user=USERNAME: Authorize an individual user.\n\n\n--gitlab-group=GROUP_NAME: Authorize an entire groups's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --gitlab-user my-gitlab-user \\\n    --gitlab-team my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  gitlab:\n    users: [\"my-gitlab-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"gitlab-authorization"},"global-resources":{"location":"global-resources.html","title":"Global Resources (experimental)","text":"Concourse v5.0 contains an experimental feature known as \"global resources\". It is enabled by passing the --enable-global-resources flag to the concourse web command.\n\nThe basic concept of global resources is to share detected resource versions between all resources that have the same type and source configuration.\n\nBefore v5.0.0, each pipeline resource had its own version history, associated to the resource by name. This meant that multiple pipelines with the same resource configs would redundantly collect the same version and metadata information.\n\nWith v5.0.0's experimental 'global resources' feature, resource versions are instead associated to an anonymous 'resource config' i.e. its type and source.\n\n","depth":3,"section_tag":"global-resources"},"goals":{"location":"garbage-collection.html#goals","title":"Goals","text":"Let's define our metrics for success:\n\n* Safe. There should never be a case where a build is running and a container or volume is removed out from under it, causing the build to fail. Resource checking should also never result in errors from check containers being removed. No one should even know garbage collection is happening.\n\n* Airtight. Everything Concourse creates, whether it's a container or volume on a worker or an entry in the database, should never leak. Each object should have a fully defined lifecycle such that there is a clear end to its use.  The ATC should be interruptible at any point in time and at the very least be able to remove any state it had created beforehand.\n\n* Resilient. Garbage collection should never be outpaced by the workload. A single misbehaving worker should not prevent garbage collection from being performed on other workers. A slow delete of a volume should not prevent garbage collecting of other things on the same worker.\n\n","depth":4,"section_tag":"goals"},"golang-library-example":{"location":"golang-library-example.html","title":"Golang library testing example","text":"You can run the tests for a Golang library across any specified versions.\n\n","depth":2,"section_tag":"golang-library-example"},"gracefully-removing-a-worker":{"location":"concourse-worker.html#gracefully-removing-a-worker","title":"Gracefully Removing a Worker","text":"When a worker machine is going away, it should be retired. This is similar to landing, except at the end the worker is completely unregistered, along with its volumes and containers. This should be done when a worker's VM or container is being destroyed.\n\nTo retire a worker, send SIGUSR2 to the worker process. This will switch the worker to retiring state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the worker will be removed and the worker process will exit.\n\nJust like with landing, you may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR2/300/TERM/15/KILL\nThis will send SIGUSR2, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\n","depth":5,"section_tag":"gracefully-removing-a-worker"},"group-jobs":{"location":"pipeline-groups.html#group-jobs","title":"jobs","text":"Optional. A list of jobs that should appear in this group. A job may appear in multiple groups. Neighbours of jobs in the current group will also appear on the same page in order to give context of the location of the group in the pipeline.\n\n","depth":3,"section_tag":"pipeline-groups"},"group-name":{"location":"pipeline-groups.html#group-name","title":"name","text":"Required. The name of the group. This should be short and simple as it will be used as the tab name for navigation.\n\n","depth":3,"section_tag":"pipeline-groups"},"group-resources":{"location":"pipeline-groups.html#group-resources","title":"resources","text":"Optional. A list of resources that should appear in this group. Resources that are inputs or outputs of jobs in the group are automatically added; they do not have to be explicitly listed here.\n\n","depth":3,"section_tag":"pipeline-groups"},"groups":{"location":"pipelines.html#groups","title":"groups","text":"A list of groups to use for cleaning up/organizing jobs in the web UI.\n\n","depth":2,"section_tag":"pipelines"},"hello-world-example":{"location":"hello-world-example.html","title":"Hello World pipeline","text":"A single job is the simplest form of pipeline.\n\n","depth":2,"section_tag":"hello-world-example"},"hooks-example":{"location":"hooks-example.html","title":"Job \u0026 task hooks example","text":"Job Hooks and Step hooks are available to perform actions based on the success, failure, or abortion of a job.\n\n","depth":2,"section_tag":"hooks-example"},"how-it-works":{"location":"garbage-collection.html#how-it-works","title":"How it Works","text":"The garbage collector is a batch operation that runs every 30 seconds. This number was chosen arbitrarily and may be reduced in the future. It's important to note that the collector must be able to run frequently enough to not be outpaced by the workload producing things, and so the batch operation should be able to complete pretty quickly.\n\nThe batch operation first performs garbage collection within the database alone, removing rows that are no longer needed. The removal of rows from one stage will often result in removals in a later stage.  They are run in the following order:\n\n* builds that no longer meet the Build Retention criteria are marked non-interceptible\n\n* workers are stepped through their state machine. Unresponsive workers become STALLED, workers that are RETIRING are deleted once drained, and workers that are LANDING become LANDED once drained.\n\n* build_image_resource_caches are removed for builds that finished over 24 hours ago.\n\n* resource_cache_uses are removed for builds that are no longer interceptible.\n\n* resource_configs that are no longer referenced by a resource_caches (resource_config_id) or a resource_config_check_sessions (resource_config_id) are removed.\n\n* resource_caches that are no longer referenced by a resource_configs (resource_cache_id) or a resource_cache_uses (resource_cache_id) are removed.\n\n* resource_config_check_sessions that have exceeded their resource_config_check_sessions (expires_at) are removed.\n\nIf any of the above operations fail, the garbage collector will just log an error and move on. This is so that failure to collect one class of objects does not prevent everything else from being garbage collected. Failure at any part of the garbage collection is OK; it can just retry on the next pass.\n\nAfter the initial pass of garbage collection in the database, there should now be a set of volumes and containers that meet criteria for garbage collection. These two are a bit more complicated to garbage-collect; they both require talking to a worker, and waiting on a potentially slow delete.\n\nContainers and volumes are the costliest resources consumed by Concourse. There are also many of them created over time as builds execute and pipelines perform their resource checking. Therefore it is important to parallelize this aspect of garbage collection so that one slow delete or one slow worker does not cause them to pile up.\n\nSo, the next two steps are Container Collection and Volume Collection.\n\n","depth":4,"section_tag":"how-it-works"},"http response time":{"location":"metrics.html#http response time","title":"http response time","text":"This metric is emitted for each HTTP request to an ATC (both API and web requests). It contains the duration (in milliseconds) for each request and is useful for finding slow requests.\n\nAttributes route\n\n: The route which the HTTP request matched. i.e. /builds/:id\n\n\npath\n\n: The literal path of the HTTP request. i.e. /builds/1234\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"iam-permissions":{"location":"aws-asm-credential-manager.html#iam-permissions","title":"IAM Permissions","text":"The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the Resource section can contain a wildcard to a secret or be restricted to an individual secret.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Sid\": \"AllowAccessToSecretManagerParameters\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"secretsmanager:ListSecrets\"\n        ],\n          \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"AllowAccessGetSecret\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\"\n            ],\n            \"Resource\": [\n                \"arn:aws:secretsmanager:::secret:/concourse/*\",\n                \"arn:aws:secretsmanager:::secret:/concourse/TEAM_NAME/*\",\n                \"arn:aws:secretsmanager:::secret:/concourse/TEAM_NAME/PIPELINE_NAME/*\"\n            ]\n        }\n    ]\n}\nNote that the TEAM_NAME and PIPELINE_NAME text above should be replaced to fit your Concourse setup.\n\nFor more information on how to use IAM roles to restrict access to Secrets Manager, review the official documentation.\n\n","depth":5,"section_tag":"iam-permissions"},"image-resource-params":{"location":"tasks.html#image-resource-params","title":"image_resource.params","text":"Optional. A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":2,"section_tag":"tasks"},"image-resource-source":{"location":"tasks.html#image-resource-source","title":"image_resource.source","text":"Required. The configuration for the resource; see source.\n\n","depth":2,"section_tag":"tasks"},"image-resource-type":{"location":"tasks.html#image-resource-type","title":"image_resource.type","text":"Required. The type of the resource. Usually docker-image.\n\n","depth":2,"section_tag":"tasks"},"image-resource-version":{"location":"tasks.html#image-resource-version","title":"image_resource.version","text":"Optional. A specific version of the resource to fetch. This should be a map with string keys and values. If not specified, the latest version will be fetched.\n\n","depth":2,"section_tag":"tasks"},"image_resource":{"location":"tasks.html#image_resource","title":"image_resource","text":"Where resource is:\n\nOptional. The base image of the container, as provided by a resource definition.\n\nYou can use any resource that returns a filesystem in the correct format (a /rootfs directory and a metadata.json file in the top level) but normally this will be the Docker Image resource. If you'd like to make a resource of your own that supports this please use that as a reference implementation for now.\n\nIf you want to use an artifact source within the plan containing an image, you must set the image in the plan step instead.\n\n","depth":2,"section_tag":"tasks"},"implementing-resource-types":{"location":"implementing-resource-types.html","title":"Implementing a Resource Type","text":"A resource type is implemented by a container image with three scripts:\n\n* /opt/resource/check for checking for new versions of the resource\n\n* /opt/resource/in for pulling a version of the resource down\n\n* /opt/resource/out for idempotently pushing a version up\n\nDistributing resource types as containers allows them to package their own dependencies. For example, the Git resource comes with git installed.\n\nAll resources must implement all three actions, though the actions can just be no-ops (which still must be correctly implemented as detailed below).\n\nResources can emit logs to the user by writing to stderr. ANSI escape codes (coloring, cursor movement, etc.) will be interpreted properly by the web UI, so you should make your output pretty.\n\n","depth":3,"section_tag":"implementing-resource-types"},"in":{"location":"implementing-resource-types.html#in","title":"in: Fetch a given resource.","text":"The in script is passed a destination directory as command line argument $1, and is given on stdin the configured source and a precise version of the resource to fetch.\n\nThe script must fetch the resource and place it in the given directory.\n\nIf the desired resource version is unavailable (for example, if it was deleted), the script must error.\n\nThe script must emit the fetched version, and may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* version is the same type of value passed to check: Check for new versions., and specifies the version to fetch.\n\n* params is an arbitrary JSON object passed along verbatim from params on a get step.\n\nExample request, in this case for the git resource:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ngit clone --branch develop git://some-uri $1\ncd $1\ngit checkout 61cebf\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Hulk Hogan\" }\n  ]\n}\n","depth":4,"section_tag":"in"},"index":{"location":"index.html","title":"Concourse","text":"Built on the simple mechanics of resources, tasks, and jobs, Concourse presents a general approach to automation that makes it great for CI/CD.\n\nYou can think of a pipeline as a distributed, higher-level, continuously-running Makefile.\n\nEach entry under resources is a dependency, and each entry under jobs describes a plan to run when the job is triggered (either manually or by a get step).\n\nJobs can depend on resources that have passed through prior jobs. The resulting sequence of jobs and resources is a dependency graph that continuously pushes your project forward, from source code to production.\n\nYour pipeline configuration is then visualized in the web UI, taking only one click to get from a red (failed) box to seeing why it failed.\n\nThe visualization also provides a \"gut check\" feedback loop - if it looks wrong, it probably is wrong.\n\nAll administration is done using the fly CLI. The fly set-pipeline command pushes the config up to Concourse. Once it looks good, you can then check the file in to source control. This makes it easy to recover if your Concourse server burns down.\n\nEverything runs in containers, ensuring a clean environment on every run. Each task specifies its own image, giving it full control over its dependencies, rather than managing them on your workers.\n\nThe fly intercept command will pop you right into one of your build's containers, which can be useful for debugging.\n\nThe fly execute command executes a task as a one-off build, with your local changes. This will run your code in exactly the same way it would run in your pipeline, without you having to repeatedly push broken commits until it works. Achieve the fabled green build #1!\n\nWhen a job fails, you can also use fly execute with -j flag to run with the same inputs as the failed job. You can then replace an input with your local changes with -i to test if your fix is valid.\n\nConcourse does not have a complex plugin system. Instead, it has a single strong abstraction.\n\nThe resources section of a pipeline lists Resources, which are abstract external locations where your pipeline will monitor for changes, fetch bits from, and push bits to.\n\nFor example, a resource with type git refers to a git repository, which will be cloned in a get step and pushed to in a put step. Behind the scenes, Concourse will continuously run git fetch to look for new commits that jobs may want to trigger on.\n\nAt its core, though, Concourse knows nothing about Git. It comes with a git resource type out of the box, but you could just as easily bring your own into your pipeline. Resource types are implemented as container images containing scripts - using the docker-image resource type, they can be fetched from a Docker registry.\n\n","depth":0,"section_tag":"index"},"input-name":{"location":"tasks.html#input-name","title":"inputs.name","text":"Required. The logical name of the input.\n\n","depth":2,"section_tag":"tasks"},"input-optional":{"location":"tasks.html#input-optional","title":"inputs.optional","text":"Optional. If true, then the input is not required by the task. The task may run even if this input is missing.\n\nAn optional input that is missing will not appear in the current directory of the running task.\n\n","depth":2,"section_tag":"tasks"},"input-path":{"location":"tasks.html#input-path","title":"inputs.path","text":"Optional. The path where the input will be placed. If not specified, the input's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"input_mapping":{"location":"task-step.html#input_mapping","title":"input_mapping","text":"Optional. A map from task input names to concrete names in the build plan. This allows a task with generic input names to be used multiple times in the same plan, mapping its inputs to specific resources within the plan.\n\nFor example:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: audit-diego-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: diego-release}\n- task: audit-cf-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: cf-release}\n","depth":4,"section_tag":"task-step"},"inputs_determined":{"location":"database-schema.html#inputs_determined","title":"inputs_determined","text":"The build scheduler does not schedule builds for jobs which have not yet had their inputs_determined.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"install":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI, which can be downloaded from the latest GitHub release - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"installing":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThe web and worker nodes both run via a single concourse CLI, which can be downloaded from the latest GitHub release - be sure to grab the appropriate archive for your platform of choice.\n\nEach archive contains the following:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere.\n\nOn Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguration basics\n\nAll Concourse node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent an interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":2,"section_tag":"install"},"intercept-admin-only":{"location":"global-resources.html#intercept-admin-only","title":"Intercepting check containers is no longer safe","text":"Now that check containers are shared across teams, it would be dangerous to allow anyone to fly intercept to check containers. For this reason, this capability is limited to admin users.\n\nWe recognize that this will make it a bit more difficult for end users to debug things like failing checks. We plan to improve this by introducing a way to provision a new check container to facilitate debugging. See 3344 for more information.\n\n","depth":5,"section_tag":"intercept-admin-only"},"internals":{"location":"internals.html","title":"Internals","text":"","depth":2,"section_tag":"internals"},"interruptible":{"location":"database-schema.html#interruptible","title":"interruptible","text":"Workers trying to land will wait until all builds are finished, unless the build is for an interruptible job.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"job-build-logs-to-retain":{"location":"jobs.html#job-build-logs-to-retain","title":"build_logs_to_retain","text":"Optional. If configured, only the last specified number of builds will have their build logs persisted. This is useful if you have a job that runs periodically but after some amount of time the logs aren't worth keeping around.\n\nExample:\n\njobs:\n- name: smoke-tests\n  build_logs_to_retain: 100\n  plan:\n  - get: 10m\n  - task: smoke-tests\n    # ...\n","depth":2,"section_tag":"jobs"},"job-disable-manual-trigger":{"location":"jobs.html#job-disable-manual-trigger","title":"disable_manual_trigger","text":"Optional. Default false. If set to true, manual triggering of the job (via the web UI or fly trigger-job) will be disabled.\n\n","depth":2,"section_tag":"jobs"},"job-ensure":{"location":"jobs.html#job-ensure","title":"ensure","text":"Optional. Step to execute regardless of whether the job succeeds, fails, errors, or aborts. Equivalent to the ensure step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-interruptible":{"location":"jobs.html#job-interruptible","title":"interruptible","text":"Optional. Default false. Normally, when a worker is shutting down it will wait for builds with containers running on that worker to finish before exiting. If this value is set to true, the worker will not wait on the builds of this job. You may want this if e.g. you have a self-deploying Concourse or long-running-but-low-importance jobs.\n\n","depth":2,"section_tag":"jobs"},"job-max-in-flight":{"location":"jobs.html#job-max-in-flight","title":"max_in_flight","text":"Optional. If set, specifies a maximum number of builds to run at a time. If serial or serial_groups are set, they take precedence and force this value to be 1.\n\n","depth":2,"section_tag":"jobs"},"job-name":{"location":"jobs.html#job-name","title":"name","text":"Required. The name of the job. This should be short; it will show up in URLs.\n\n","depth":2,"section_tag":"jobs"},"job-on-abort":{"location":"jobs.html#job-on-abort","title":"on_abort","text":"Optional. Step to execute when the job aborts. Equivalent to the on_abort step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-on-failure":{"location":"jobs.html#job-on-failure","title":"on_failure","text":"Optional. Step to execute when the job fails. Equivalent to the on_failure step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-on-success":{"location":"jobs.html#job-on-success","title":"on_success","text":"Optional. Step to execute when the job succeeds. Equivalent to the on_success step attribute.\n\n","depth":2,"section_tag":"jobs"},"job-plan":{"location":"jobs.html#job-plan","title":"plan","text":"Required. The sequence of steps to execute.\n\n","depth":2,"section_tag":"jobs"},"job-public":{"location":"jobs.html#job-public","title":"public","text":"Optional. Default false. If set to true, the build log of this job will be viewable by unauthenticated users. Unauthenticated users will always be able to see the inputs, outputs, and build status history of a job. This is useful if you would like to expose your pipeline publicly without showing sensitive information in the build log.\n\nNote: when this is set to true, any get step and put steps will show the metadata for their resource version, regardless of whether the resource itself has set public to true.\n\n","depth":2,"section_tag":"jobs"},"job-serial":{"location":"jobs.html#job-serial","title":"serial","text":"Optional. Default false. If set to true, builds will queue up and execute one-by-one, rather than executing in parallel.\n\n","depth":2,"section_tag":"jobs"},"job-serial-groups":{"location":"jobs.html#job-serial-groups","title":"serial_groups","text":"Optional. Default []. When set to an array of arbitrary tag-like strings, builds of this job and other jobs referencing the same tags will be serialized.\n\nThis can be used to ensure that certain jobs do not run at the same time, like so:\n\njobs:\n- name: job-a\n  serial_groups: [some-tag]\n- name: job-b\n  serial_groups: [some-tag, some-other-tag]\n- name: job-c\n  serial_groups: [some-other-tag]\nIn this example, job-a and job-c can run concurrently, but neither job can run builds at the same time as job-b.\n\nThe builds are executed in their order of creation, across all jobs with common tags.\n\n","depth":2,"section_tag":"jobs"},"jobs":{"location":"jobs.html","title":"Jobs","text":"Jobs determine the actions of your pipeline. They determine how resources progress through it, and how the pipeline is visualized.\n\nThe most important attribute of a job is its build plan, configured as plan. This determines the sequence of Steps to execute in any builds of the job.\n\nJobs are listed under the jobs: key in the pipeline configuration. Each configured job consists of the following fields:\n\n","depth":2,"section_tag":"jobs"},"jobs-table":{"location":"database-schema.html#jobs-table","title":"jobs","text":"The jobs table tracks the details of all jobs in a Concourse deployment.  Jobs reference the pipeline they belong to, their Job configuration, and their name in the pipeline.\n\nThere are also a number of attributes which are used by the runtime:\n\n","depth":4,"section_tag":"pipelineapi-objects"},"ldap-auth":{"location":"ldap-auth.html","title":"LDAP auth","text":"The LDAP provider can be used for operators who wish to authenticate their users against an LDAP server.\n\n","depth":4,"section_tag":"ldap-auth"},"ldap-authentication":{"location":"ldap-auth.html#ldap-authentication","title":"Authentication","text":"The LDAP provider is configured by pointing it to an LDAP host with a read-only bind DN and password. This bind DN and password is used for authenticating with the LDAP host and querying the users.\n\nAdditionally, the base DN under which users are searched as well as the attribute of the users to associate to 'usernames' must also be configured.\n\nThese can be specified via env to the Running a web node like so:\n\nCONCOURSE_LDAP_DISPLAY_NAME=Acme # optional; default \"LDAP\"\nCONCOURSE_LDAP_HOST=ldap.example.com # port defaults to 389 or 636\nCONCOURSE_LDAP_BIND_DN='cn=read-only-admin,dc=example,dc=com'\nCONCOURSE_LDAP_BIND_PW=read-only-admin-password\nCONCOURSE_LDAP_USER_SEARCH_BASE_DN='cn=users,dc=example,dc=com'\nCONCOURSE_LDAP_USER_SEARCH_USERNAME=uid\nTo configure TLS, you may need to set a CA cert:\n\nCONCOURSE_LDAP_CA_CERT=/path/to/ca_cert\nIf your LDAP host does not use TLS, you must set:\n\nCONCOURSE_LDAP_INSECURE_NO_SSL=true\nTo fine-tune which users are queried, you can specify a user search filter like so:\n\nCONCOURSE_LDAP_USER_SEARCH_FILTER='(objectClass=person)'\nTo set which user attributes map to the token claims, you can set the following:\n\nCONCOURSE_LDAP_USER_SEARCH_ID_ATTR=uid         # default\nCONCOURSE_LDAP_USER_SEARCH_EMAIL_ATTR=mail     # default\nCONCOURSE_LDAP_USER_SEARCH_NAME_ATTR=some-attr # no default\n","depth":5,"section_tag":"ldap-authentication"},"ldap-authorization":{"location":"ldap-auth.html#ldap-authorization","title":"Authorization","text":"LDAP users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--ldap-user=USERNAME: Authorize an individual user.\n\n\n--ldap-group=GROUP_NAME: Authorize anyone from the group.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --ldap-user my-username \\\n    --ldap-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  ldap:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"ldap-authorization"},"local-auth":{"location":"local-auth.html","title":"Local User auth","text":"Local User auth is a primitive username/password-based auth mechanism. All users and passwords are configured statically.\n\nIn general, we recommend configuring one of the other providers instead, but for small deployments with only a few users, local user auth may be all you need.\n\n","depth":4,"section_tag":"local-auth"},"local-authentication":{"location":"local-auth.html#local-authentication","title":"Authentication","text":"Local users are configured on the Running a web node by setting the following env:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass,anotheruser:anotherpass\nThis configures two users, myuser and anotheruser, with their corresponding passwords.\n\nWhen local users are configured, the log-in page in the web UI will show a username/password prompt.\n\nLocal users can also log in via fly login with the --username and --password flags.\n\n","depth":5,"section_tag":"local-authentication"},"local-authorization":{"location":"local-auth.html#local-authorization","title":"Authorization","text":"Local users are granted access to teams via fly set-team, using the --local-user flag:\n\n$ fly set-team -n my-team \\\n    --local-user some_username\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  local:\n    users: [\"some_username\"]\n","depth":5,"section_tag":"local-authorization"},"main-team":{"location":"main-team.html","title":"The main team","text":"Out of the box, Concourse comes with a single team called main.\n\nThe main team is an admin team, meaning members (specifically, users with the owner role) can create and update other teams. Currently there is no way to promote a team to become an admin team, so main is a special-case.\n\nThe main team is different in that all flags normally passed to fly set-team are instead passed to the concourse web command, prefixed with --main-team-. The values set in these flags take effect whenever the web node starts up. This is done so that you can't get locked out.\n\nTo learn how to configure your main team, continue on to the appropriate section for your auth provider of choice under Configuring Auth.\n\n","depth":3,"section_tag":"main-team"},"managing-jobs":{"location":"managing-jobs.html","title":"Managing Jobs","text":"","depth":3,"section_tag":"managing-jobs"},"managing-pipelines":{"location":"managing-pipelines.html","title":"Managing Pipelines","text":"","depth":3,"section_tag":"managing-pipelines"},"managing-resource-types":{"location":"managing-resource-types.html","title":"Managing Resource Types","text":"","depth":3,"section_tag":"managing-resource-types"},"managing-resources":{"location":"managing-resources.html","title":"Managing Resources","text":"","depth":3,"section_tag":"managing-resources"},"managing-teams":{"location":"managing-teams.html","title":"Managing Teams","text":"","depth":3,"section_tag":"managing-teams"},"manual-trigger-example":{"location":"manual-trigger-example.html","title":"Manually triggered job example","text":"A job can be triggered by a resource. After it's complete, the next job can run automatically or manually.\n\n","depth":2,"section_tag":"manual-trigger-example"},"max_in_flight_reached":{"location":"database-schema.html#max_in_flight_reached","title":"max_in_flight_reached","text":"The build scheduler does not schedule builds for jobs that have had their build max_in_flight_reached.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"metrics":{"location":"metrics.html","title":"Metrics","text":"Metrics are essential in understanding how any large system is behaving and performing. Concourse can emit metrics about both the system health itself and about the builds that it is running. Operators can tap into these metrics in order to observe the health of the system.\n\nIn the spirit of openness, the metrics from our deployment are public. We consider it a bug to emit anything sensitive or secret into our metrics pipeline.\n\n","depth":3,"section_tag":"metrics"},"nodejs-example":{"location":"nodejs-example.html","title":"Nodejs application testing example","text":"You can run the tests for a Nodejs application.\n\n","depth":2,"section_tag":"nodejs-example"},"on-abort-step-hook":{"location":"on-abort-step-hook.html","title":"on_abort step hook","text":"Any step can have on_abort tacked onto it, whose value is a second step to execute only if the parent step aborts.\n\n","depth":4,"section_tag":"on-abort-step-hook"},"on-error-step-hook":{"location":"on-error-step-hook.html","title":"on_error step hook","text":"Any step can have on_error tacked onto it, whose value is a second step to execute only if the parent step errors.\n\n","depth":4,"section_tag":"on-error-step-hook"},"on-failure-step-hook":{"location":"on-failure-step-hook.html","title":"on_failure step hook","text":"Any step can have on_failure tacked onto it, whose value is a second step to execute only if the parent step fails.\n\n","depth":4,"section_tag":"on-failure-step-hook"},"on-success-step-hook":{"location":"on-success-step-hook.html","title":"on_success step hook","text":"Any step can have on_success tacked onto it, whose value is a second step to execute only if the parent step succeeds.\n\n","depth":4,"section_tag":"on-success-step-hook"},"on_abort":{"location":"on-abort-step-hook.html#on_abort","title":"on_abort","text":"The step to execute when the parent step aborts. If the attached step succeeds, the entire step is still aborted.\n\n","depth":4,"section_tag":"on-abort-step-hook"},"on_error":{"location":"on-error-step-hook.html#on_error","title":"on_error","text":"The step to execute when the parent step errors. If the attached step fails, the outer step is considered to have failed.\n\n","depth":4,"section_tag":"on-error-step-hook"},"on_failure":{"location":"on-failure-step-hook.html#on_failure","title":"on_failure","text":"The step to execute when the parent step fails. If the attached step succeeds, the entire step is still failed.\n\n","depth":4,"section_tag":"on-failure-step-hook"},"on_success":{"location":"on-success-step-hook.html#on_success","title":"on_success","text":"The step to execute when the parent step succeeds. If the attached step fails, the outer step is considered to have failed.\n\n","depth":4,"section_tag":"on-success-step-hook"},"operation":{"location":"operation.html","title":"Operation","text":"The following sections are available to provide a deeper understanding of some of the concepts surrounding Concourse. They aren't necessary for using Concourse and understanding its general concept of pipelines and how to use them, but we do recommend at least learning how to set up Credential Management and Encryption.\n\n","depth":2,"section_tag":"operation"},"out":{"location":"implementing-resource-types.html#out","title":"out: Update a resource.","text":"The out script is called with a path to the directory containing the build's full set of sources as the first argument, and is given on stdin the configured params and the resource's source configuration.\n\nThe script must emit the resulting version of the resource. For example, the git resource emits the sha of the commit that it just pushed.\n\nAdditionally, the script may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* params is an arbitrary JSON object passed along verbatim from params on a put step.\n\nExample request, in this case for the git resource:\n\n{\n  \"params\": {\n    \"branch\": \"develop\",\n    \"repo\": \"some-repo\"\n  },\n  \"source\": {\n    \"uri\": \"git@...\",\n    \"private_key\": \"...\"\n  }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ncd $1/some-repo\ngit push origin develop\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Mick Foley\" }\n  ]\n}\n","depth":4,"section_tag":"out"},"output-name":{"location":"tasks.html#output-name","title":"outputs.name","text":"Required. The logical name of the output. The contents under path will be made available to the rest of the plan under this name.\n\n","depth":2,"section_tag":"tasks"},"output-path":{"location":"tasks.html#output-path","title":"outputs.path","text":"Optional. The path to a directory where the output will be taken from. If not specified, the output's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"output_mapping":{"location":"task-step.html#output_mapping","title":"output_mapping","text":"Optional. A map from task output names to concrete names to register in the build plan. This allows a task with generic output names to be used multiple times in the same plan.\n\nThis is often used together with input_mapping. For example:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: create-diego-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: diego-release}\n  output_mapping: {release-tarball: diego-release-tarball}\n- task: create-cf-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: cf-release}\n  output_mapping: {release-tarball: cf-release-tarball}\n","depth":4,"section_tag":"task-step"},"paused":{"location":"database-schema.html#paused","title":"paused","text":"The build scheduler does not schedule builds for jobs which are paused.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"permission-matrix":{"location":"user-roles.html#permission-matrix","title":"Permission Matrix","text":"| The fly CLI command | Anon | Admin | Owner | Member | Viewer |\n| fly abort-build |  |  |  |  |  |\n| fly builds |  |  |  |  |  |\n| fly check-resource |  |  |  |  |  |\n| fly containers |  |  |  |  |  |\n| fly destroy-pipeline |  |  |  |  |  |\n| fly destroy-team |  |  |  |  |  |\n| fly execute |  |  |  |  |  |\n| fly expose-pipeline |  |  |  |  |  |\n| fly format-pipeline |  |  |  |  |  |\n| fly get-pipeline |  |  |  |  |  |\n| fly hide-pipeline |  |  |  |  |  |\n| fly intercept |  |  |  |  |  |\n| fly jobs |  |  |  |  |  |\n| fly login |  |  |  |  |  |\n| fly logout |  |  |  |  |  |\n| fly order-pipelines |  |  |  |  |  |\n| fly pause-job |  |  |  |  |  |\n| fly pause-pipeline |  |  |  |  |  |\n| fly pipelines |  |  |  |  |  |\n| fly prune-worker |  |  |  |  |  |\n| fly rename-pipeline |  |  |  |  |  |\n| fly rename-team |  |  |  |  |  |\n| fly set-pipeline |  |  |  |  |  |\n| fly set-team |  |  |  |  |  |\n| fly status |  |  |  |  |  |\n| fly sync |  |  |  |  |  |\n| fly targets |  |  |  |  |  |\n| fly teams |  |  |  |  |  |\n| fly trigger-job |  |  |  |  |  |\n| fly unpause-job |  |  |  |  |  |\n| fly unpause-pipeline |  |  |  |  |  |\n| fly validate-pipeline |  |  |  |  |  |\n| fly volumes |  |  |  |  |  |\n| fly watch | * |  |  |  |  |\n| fly workers |  |  |  |  |  |\n\n| Web UI page | Action | Owner | Member | Viewer |\n| Home (HD/Dashboard) | View |  |  |  |\n|  | Login |  |  |  |\n|  | Logout |  |  |  |\n|  | Download The fly CLI |  |  |  |\n|  | Pause Pipeline |  |  |  |\n|  | Resume Pipeline |  |  |  |\n|  | Reorder Pipeline |  |  |  |\n| Pipeline Page | View |  |  |  |\n|  | Click to Resource |  |  |  |\n|  | Click to Build |  |  |  |\n|  | Click on Group |  |  |  |\n| Resource Page | View Resource |  |  |  |\n|  | View Version Details |  |  |  |\n|  | Pin Version |  |  |  |\n|  | Paginate (\u003c- -\u003e) |  |  |  |\n| Build Page | Trigger new Build |  |  |  |\n|  | View Build |  |  |  |\n|  | Build Details |  |  |  |\n| Job Page | View Job Page |  |  |  |\n|  | Pause Job |  |  |  |\n|  | Trigger new Build |  |  |  |\n|  | Build History |  |  |  |\n|  | Paginate (\u003c- -\u003e) |  |  |  |\n\n","depth":4,"section_tag":"permission-matrix"},"php-example":{"location":"php-example.html","title":"PHP application testing example","text":"You can run the tests for a PHP application.\n\n","depth":2,"section_tag":"php-example"},"pipeline-groups":{"location":"pipeline-groups.html","title":"Grouping Jobs","text":"As more resources and jobs are added to a pipeline it can become difficult to navigate in the web UI. Pipeline groups allow you to group jobs together under a header and have them show on different tabs in the user interface. Groups have no functional effect on your pipeline.\n\nNote: once you have added groups to your pipeline then all jobs must be in a group.\n\nEach group in the pipeline has the following attributes:\n\n","depth":3,"section_tag":"pipeline-groups"},"pipeline-jobs":{"location":"pipelines.html#pipeline-jobs","title":"jobs","text":"A set of jobs for the pipeline to continuously check.\n\n","depth":2,"section_tag":"pipelines"},"pipeline-resource-types":{"location":"pipelines.html#pipeline-resource-types","title":"resource_types","text":"A set of resource types for resources within the pipeline to use.\n\n","depth":2,"section_tag":"pipelines"},"pipeline-resources":{"location":"pipelines.html#pipeline-resources","title":"resources","text":"A set of resources for the pipeline to continuously check.\n\n","depth":2,"section_tag":"pipelines"},"pipeline-vars":{"location":"setting-pipelines.html#pipeline-vars","title":"Pipeline ((vars))","text":"The pipeline configuration can contain template variables in the form of ((foo-bar)). They will be replaced with values populated by repeated --var, --yaml-var, or --load-vars-from flags, or at runtime via a credential manager.\n\nThis allows for credentials to be extracted from a pipeline config, making it safe to check in to a public repository or pass around.\n\nFor example, if you have a pipeline.yml as follows:\n\nresources:\n- name: private-repo\n  type: git\n  source:\n    uri: git@...\n    branch: master\n    private_key: ((private-repo-key))\n...you could then configure this pipeline like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"private-repo-key=$(cat id_rsa)\"\nOr, if you had a credentials.yml as follows:\n\nprivate-repo-key: |\n  -----BEGIN RSA PRIVATE KEY-----\n  ...\n  -----END RSA PRIVATE KEY-----\n...you could configure it like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from credentials.yml\nConcatenation is supported for string values - foo-((var)) with var: bar will resolve to foo-bar.\n\nIf both --var/--yaml-var and --load-vars-from are specified, the --var and --yaml-var flags take precedence.\n\nValues other than strings (e.g. bools, arrays) may also be specified by using either --yaml-var or --load-vars-from.\n\n","depth":5,"section_tag":"pipeline-vars"},"pipeline-vars-example":{"location":"pipeline-vars-example.html","title":"Pipeline ((vars)) example","text":"You can use params in a pipelines configuration file.\n\n","depth":2,"section_tag":"pipeline-vars-example"},"pipelineapi-objects":{"location":"database-schema.html#pipelineapi-objects","title":"Pipeline/API Objects","text":"The following tables manage objects exposed directly to users, either via the API or by configuring pipelines.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"pipelines":{"location":"pipelines.html","title":"Pipelines","text":"A pipeline is the result of configuring Jobs and Resources together. When you configure a pipeline, it takes on a life of its own, to continuously detect resource versions and automatically queue new builds for jobs as they have new available inputs.\n\nPipelines are configured as declarative YAML files, fitting the following schema:\n\n","depth":2,"section_tag":"pipelines"},"pipelines-table":{"location":"database-schema.html#pipelines-table","title":"pipelines","text":"The pipelines table contains details about a pipeline; its name, which team it belongs to, whether it is paused, and whether it is public.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"pointing-to-external-dns-servers":{"location":"concourse-worker.html#pointing-to-external-dns-servers","title":"Pointing to external DNS servers","text":"If you have no need for special DNS resolution within your Concourse containers, you can just configure your containers to use specific DNS server addresses external to the VM.\n\nThis can be done by listing DNS servers in config.ini like so:\n\n[server]\n; configure Google DNS\ndns-server = 8.8.8.8\ndns-server = 8.8.4.4\nTo validate whether the changes have taken effect, you can fly intercept into any container and check /etc/resolv.conf once again:\n\n$ fly -t ci intercept -c concourse/concourse\nbash-5.0# cat /etc/resolv.conf\nnameserver 8.8.8.8\nnameserver 8.8.4.4\nbash-5.0# ping google.com\nPING google.com (108.177.111.139): 56 data bytes\n64 bytes from 108.177.111.139: seq=0 ttl=47 time=2.672 ms\n64 bytes from 108.177.111.139: seq=1 ttl=47 time=0.911 ms\n","depth":6,"section_tag":"pointing-to-external-dns-servers"},"postgresql-node":{"location":"postgresql-node.html","title":"Running a PostgreSQL node","text":"Concourse uses PostgreSQL for storing all data and coordinating work in a multi-Running a web node installation.\n\n","depth":3,"section_tag":"postgresql-node"},"project":{"location":"project.html","title":"Project","text":"Concourse started as a side-project by @vito (hi!) and @xoebus in 2014. Since then, Concourse has grown into a small but dedicated team of full-time engineers and part-time contributors.\n\nWhere does the magic happen?\n\n* The concourse/concourse GitHub repo is where the main code lives and where roadmap planning begins.\n\n* The concourse/concourse GitHub wiki is the main point of discovery for all community-created resource types/tools/tutorials/etc., and is also used as a contributor knowledge base.\n\n* Our Medium blog features (occasional) updates from the development side.\n\n* The Concourse forums for support, announcements, and general discussion.\n\n* Our Discord server is a great place to chat with other contributors.\n\nWhy make Concourse?\n\nWhen working on a sizable project, having a pipeline to reliably test, deploy, and publish the product is crucial for rapid iteration.\n\nBut with every CI system we used, we found ourselves constantly dealing with the same old problems: complicated configs hidden in many pages of the web UI, not knowing who changed what \u0026 when, managing dependencies and state on the workers, build pollution, annoying UX...\n\nOur project was growing larger, and with every box we checked and for every worker we hand-tweaked, the anxiety of having to do it all over again if something went wrong grew and grew. We started writing software to manage our CI instead of writing the software for the product we wanted to build.\n\nWe built Concourse to be a CI system that lets you sleep easier at night. A CI that's simple enough to fully grok and easy to manage as your project grows; in both the complexity of the product and the size of your team. We wanted to build a CI with strong abstractions and fewer things to learn, so that it can be easier to understand and so that Concourse can age gracefully.\n\nHow can I help?\n\nIt's pretty hard to write a CI system that makes everyone happy! Concourse is by no means perfect, and it sometimes takes us while to understand a problem space well enough to figure out how it should work in Concourse's puritanical world.\n\nWe tend to move slowly rather than tack on feature request after feature request. We are also extremely cautious about anti-patterns and introducing ways for users to shoot themselves in the foot.\n\nConcourse is getting bigger and bigger, and we really appreciate any help we can get. There are many ways to contribute to Concourse; only some of them involve writing code! You help us out by being active in GitHub issues, voting with reactions for issues that matter to you, engaging in discussions while we map out a feature request, hanging out in our forums, writing documentation, coming up with new designs, and of course contributing code!\n\nAs a contributor, you can jump on over to Contribute for a more complete list of resources for contributors\n\n","depth":1,"section_tag":"project"},"providing-multiple-inputs":{"location":"running-tasks.html#providing-multiple-inputs","title":"Providing multiple inputs","text":"Tasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current working directory) that has the same name as the directory.\n\nTasks must specify the inputs that they require as inputs. For fly to upload these inputs you can use the -i or --input arguments with name and path pairs. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\nThis would work together with a build-stemcell.yml if its inputs: section was as follows:\n\ninputs:\n- name: code\n- name: stemcells\nIf you specify an input then the default input will no longer be added automatically and you will need to explicitly list it (as with the code input above).\n\nThis feature can be used to mimic other resources and try out combinations of input that would normally not be possible in a pipeline.\n\n","depth":5,"section_tag":"providing-multiple-inputs"},"providing-values-for-params":{"location":"running-tasks.html#providing-values-for-params","title":"Providing values for params","text":"Any params listed in the task configuration can be specified by using environment variables.\n\nSo, if you have a task with the following params:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\n...and you run:\n\nBAR=hello fly execute\nThe task would then run with BAR as \"hello\", and FOO as \"fizzbuzz\" (its default value).\n\n","depth":5,"section_tag":"providing-values-for-params"},"put-step":{"location":"put-step.html","title":"put step","text":"Pushes to the given Resource. All artifacts collected during the plan's execution will be available in the working directory.\n\nWhen the put succeeds, the produced version of the resource will be immediately fetched via an implicit get step. This is so that later steps in your plan can use the artifact that was produced. The source will be available under whatever name put specifies, just like as with get.\n\n","depth":4,"section_tag":"put-step"},"put-step-get-params":{"location":"put-step.html#put-step-get-params","title":"get_params","text":"Optional. A map of arbitrary configuration to forward to the resource that will be utilized during the implicit get step.  Refer to the resource type's documentation to see what it supports.\n\n","depth":4,"section_tag":"put-step"},"put-step-inputs":{"location":"put-step.html#put-step-inputs","title":"inputs","text":"Optional. If specified, only the listed artifacts will be provided to the container. If not specified, all artifacts will be provided.\n\n","depth":4,"section_tag":"put-step"},"put-step-params":{"location":"put-step.html#put-step-params","title":"params","text":"Optional. A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":4,"section_tag":"put-step"},"put-step-put":{"location":"put-step.html#put-step-put","title":"put","text":"Required. The logical name of the resource being pushed. The pushed resource will be available under this name after the push succeeds.\n\n","depth":4,"section_tag":"put-step"},"put-step-resource":{"location":"put-step.html#put-step-resource","title":"resource","text":"Optional. Defaults to name. The resource to update, as configured in resources.\n\n","depth":4,"section_tag":"put-step"},"rails-example":{"location":"rails-example.html","title":"Rails application testing example","text":"You can run the tests for a Rails that requires a specific version of ruby and relies on a Postges database.\n\n","depth":2,"section_tag":"rails-example"},"random-strategy":{"location":"container-placement.html#random-strategy","title":"The random strategy","text":"With the random strategy, the Running a web node places get, put and task containers on any worker, ignoring any affinity.\n\nAs this is truly random, this will be fine until one day it's not fine.\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=random\n","depth":4,"section_tag":"random-strategy"},"reducing-redundant-data":{"location":"global-resources.html#reducing-redundant-data","title":"Reducing redundant data","text":"The majority of Concourse resources will benefit from having versions shared globally because most resource versions have an external source of truth.\n\nFor example, a check for the git resource that pulls in the concourse/concourse repository will always return the same set of versions as an equivalent resource pointing to the same repository. By consolidating the checks and the versions, there will essentially only be one set of versions collected from the repository and saved into the database.\n\n","depth":5,"section_tag":"reducing-redundant-data"},"references":{"location":"php-example.html#references","title":"References","text":"* Jobs\n\n* Steps\n\n* Tasks\n\n","depth":3,"section_tag":"references"},"reliable-resource-version-history":{"location":"global-resources.html#reliable-resource-version-history","title":"Reliable Resource Version History","text":"Prior to global resources, a resource's version history was directly associated to the resource name. This meant that any changes to a resource's configuration without changing its name would basically append the versions from the new configuration after the old versions, which are no longer accurate to the current configuration.\n\nGlobal resources instead associates the resource versions to the resource's type and source. Therefore, whenever a resource definition changes, the versions will \"reset\" and change along with it, resulting in truthful and reliable version histories.\n\n","depth":5,"section_tag":"reliable-resource-version-history"},"resource-certs":{"location":"implementing-resource-types.html#resource-certs","title":"Certificate Propagation","text":"Certificates can be automatically propagated into each resource container, if the worker is configured to do so. The BOSH release configures this automatically, while the concourse binary must be given a --certs-dir flag pointing to the path containing the CA certificate bundle.\n\nThe worker's certificate directory will then be always mounted at /etc/ssl/certs, read-only, in each resource container created on the worker. There's no single standard path for this so we picked one that would work out of the box in most cases.\n\nThis approach to certificate configuration is similar in mindset to the propagation of http_proxy/https_proxy - certs are kind of a baseline assumption when deploying software, so Concourse should do its best to respect it out-of-the-box, especially as they're often used in tandem with a man-in-the-middle corporate SSL proxy. (In this way it doesn't feel too much like the anti-pattern of hand-tuning workers.)\n\n","depth":4,"section_tag":"resource-certs"},"resource-check":{"location":"implementing-resource-types.html#resource-check","title":"check: Check for new versions.","text":"A resource type's check script is invoked to detect new versions of the resource. It is given the configured source and current version on stdin, and must print the array of new versions, in chronological order, to stdout, including the requested version if it's still valid.\n\nThe request body will have the following fields:\n\n* source is an arbitrary JSON object which specifies the location of the resource, including any credentials. This is passed verbatim from the resource configuration.\n\n  For git this would be the repo URI, which branch, and a private key if necessary.\n\n* version is a JSON object with string fields, used to uniquely identify an instance of the resource. For git this would be a commit SHA.\n\n  This will be omitted from the first request, in which case the resource should return the current version (not every version since the resource's inception).\n\nFor example, here's what the input for a git resource may look like:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\n[ -d /tmp/repo ] || git clone git://some-uri /tmp/repo\ncd /tmp/repo\ngit pull \u0026\u0026 git log 61cbef..HEAD\nNote that it conditionally clones; the container for checking versions is reused between checks, so that it can efficiently pull rather than cloning every time.\n\nAnd the output, assuming d74e01 is the commit immediately after 61cbef:\n\n[\n  { \"ref\": \"61cbef\" },\n  { \"ref\": \"d74e01\" },\n  { \"ref\": \"7154fe\" }\n]\nThe list may be empty, if there are no versions available at the source. If the given version is already the latest, an array with that version as the sole entry should be listed.\n\nIf your resource is unable to determine which versions are newer then the given version (e.g. if it's a git commit that was push -fed over), then the current version of your resource should be returned (i.e. the new HEAD).\n\n","depth":4,"section_tag":"resource-check"},"resource-check-container":{"location":"container-internals.html#resource-check-container","title":"Resource Check Container","text":"Resource Check Containers are created from the resource type's image and are used to check for new versions of a resource.  There will be one per resource config.\n\n","depth":5,"section_tag":"resource-check-container"},"resource-check-every":{"location":"resources.html#resource-check-every","title":"check_every","text":"Optional. Default 1m. The interval on which to check for new versions of the resource. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":2,"section_tag":"resources"},"resource-get-container":{"location":"container-internals.html#resource-get-container","title":"Resource Get Container","text":"Resource Get Container are created when a get step is executed in a build plan. They are based on the resource type's image and are used to download the bits for a given version of resource.\n\nThere will be one per resource config.\n\n","depth":5,"section_tag":"resource-get-container"},"resource-icon":{"location":"resources.html#resource-icon","title":"icon","text":"Optional. The name of a Material Design icon to show next to the resource name in the web UI. For example, github-circle.\n\n","depth":2,"section_tag":"resources"},"resource-metadata":{"location":"implementing-resource-types.html#resource-metadata","title":"Metadata","text":"When used in a get step or a put step, metadata about the running build is made available via the following environment variables:\n\n$BUILD_ID: The internal identifier for the build. Right now this is numeric but it may become a guid in the future. Treat it as an absolute reference to the build.\n\n\n$BUILD_NAME: The build number within the build's job.\n\n\n$BUILD_JOB_NAME: The name of the build's job.\n\n\n$BUILD_PIPELINE_NAME: The pipeline that the build's job lives in.\n\n\n$BUILD_TEAM_NAME: The team that the build belongs to.\n\n\n$ATC_EXTERNAL_URL: The public URL for your ATC; useful for debugging.\n\n\n\nIf the build is a one-off, $BUILD_NAME, $BUILD_JOB_NAME, and $BUILD_PIPELINE_NAME will not be set.\n\nNone of these variables are available to /check.\n\nThese variables should be used solely for annotating things with metadata for traceability, i.e. for linking to the build in an alert or annotating an automated commit so its origin can be discovered.\n\nThey should not be used to emulate versioning (e.g. by using the increasing build number). They are not provided to task steps to avoid this anti-pattern.\n\n","depth":4,"section_tag":"resource-metadata"},"resource-name":{"location":"resources.html#resource-name","title":"name","text":"Required. The name of the resource. This should be short and simple. This name will be referenced by build plans of jobs in the pipeline.\n\n","depth":2,"section_tag":"resources"},"resource-public":{"location":"resources.html#resource-public","title":"public","text":"Optional. Default false. If set to true, the metadata for each version of the resource will be viewable by unauthenticated users (assuming the pipeline has been exposed).\n\nResource metadata should never contain credentials or secret information, but this is off by default just to be safe in case users don't want to show things like commit messages and authors to the public.\n\nNote: even when set to false, the versions identifiers will be visible. In addition, if a resource is fetched in a build whose job is marked public, metadata will be visible in the build output.\n\n","depth":2,"section_tag":"resources"},"resource-retention":{"location":"caching-and-retention.html#resource-retention","title":"Resource Retention","text":"","depth":4,"section_tag":"resource-retention"},"resource-source":{"location":"resources.html#resource-source","title":"source","text":"Optional. The location of the resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use git as an example, the source may contain the repo URI, the branch of the repo to track, and a private key to use when pushing/pulling.\n\nBy convention, documentation for each resource type's configuration is in each implementation's README.\n\nYou can find the source for the resource types provided with Concourse at the Concourse GitHub organization.\n\n","depth":2,"section_tag":"resources"},"resource-tags":{"location":"resources.html#resource-tags","title":"tags","text":"Optional. Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also tags step modifier.\n\n","depth":2,"section_tag":"resources"},"resource-type":{"location":"resources.html#resource-type","title":"type","text":"Required. The resource type.\n\n","depth":2,"section_tag":"resources"},"resource-type-check-every":{"location":"resource-types.html#resource-type-check-every","title":"check_every","text":"Optional. Default 1m. The interval on which to check for new versions of the resource type. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-name":{"location":"resource-types.html#resource-type-name","title":"name","text":"Required. The name of the new resource type. This should be short and simple. This name will be referenced by resources defined within the same pipeline, and image_resources used by tasks running in the pipeline.\n\nPipeline-provided resource types can override the core resource types by specifying the same name.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-params":{"location":"resource-types.html#resource-type-params","title":"params","text":"Optional. Arbitrary params to pass when fetching the resource.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-privileged":{"location":"resource-types.html#resource-type-privileged","title":"privileged","text":"Optional. Default false. If set to true, the resource's containers will be run with full capabilities, as determined by the Garden backend the task runs on. For Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the root user is \"actual\" root (if set to true) or a user namespaced root (if set to false, the default).\n\nThis is a gaping security hole; only configure it if the resource type needs it (which should be called out in its documentation). This is not up to the resource type to decide dynamically, so as to prevent privilege escalation via third-party resource type exploits.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-source":{"location":"resource-types.html#resource-type-source","title":"source","text":"Optional. The location of the resource type's resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use docker-image as an example, the source would contain something like repository: username/reponame. See the Docker Image resource (or whatever resource type your resource type uses) for more information.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-tags":{"location":"resource-types.html#resource-type-tags","title":"tags","text":"Optional. Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also tags step modifier.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-type":{"location":"resource-types.html#resource-type-type","title":"type","text":"Required. The type of the resource used to provide the resource type's container image. Yes, this is a bit meta. Usually this will be docker-image, as the resource type must result in a container image, though there may be other image formats (possibly themselves defined as pipeline resource types!).\n\nA resource type's type can refer to other resource types, and can also use the core type that it's overriding. This is useful for bringing in a newer or forked registry-image resource.\n\n","depth":2,"section_tag":"resource-types"},"resource-type-unique-version-history":{"location":"resource-types.html#resource-type-unique-version-history","title":"unique_version_history","text":"Optional. Default false. Only relevant when Global Resources (experimental) is enabled. When set to true, resources using this resource type will have a version history that is unique to the resource, rather than sharing a global version history.\n\n","depth":2,"section_tag":"resource-types"},"resource-types":{"location":"resource-types.html","title":"Resource Types","text":"Each resource in a pipeline has a type. The resource's type determines what versions are detected, the bits that are fetched when used for a get step, and the side effect that occurs when used for a put step.\n\nOut of the box, Concourse comes with a few resource types to cover common CI use cases like dealing with Git repositories and S3 buckets. These are called the \"core\" resource types. These are packaged with the worker, and can be listed via fly workers -d.\n\nBeyond these core types, each pipeline can configure its own resource types by specifying resource_types at the top level. Each resource type is itself defined as a resource that provides the container image for the pipeline resource type (see Implementing a Resource Type). You will almost always be using the registry-image resource when doing this.\n\nA somewhat exhaustive list of all available resource types - both from the Concourse core project and from external contributors - is available in the Resource Types wiki page.\n\nSimilar to Resources, each configured resource type consists of the following attributes:\n\n","depth":2,"section_tag":"resource-types"},"resource-version":{"location":"resources.html#resource-version","title":"version","text":"Optional. A version to pin the resource to across the pipeline. This has the same effect as setting version on every get step referencing the resource.\n\nResources can also be temporarily pinned to a version via the API and web UI. However this functionality is disabled if the resource is pinned via configuration, and if a pipeline is configured to have a version pinned while also pinned in the web UI, the configuration takes precedence and will clear out the temporary pin.\n\n","depth":2,"section_tag":"resources"},"resource-webhook-token":{"location":"resources.html#resource-webhook-token","title":"webhook_token","text":"Optional. If specified, web hooks can be sent to trigger an immediate check of the resource, specifying this value as a primitive form of authentication via query params.\n\nAfter configuring this value, you would then configure your hook sender with the following painfully long path appended to your external URL:\n\n/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN\nNote that the request payload sent to this API endpoint is entirely ignored. You should configure the resource as if you're not using web hooks, as the resource config is still the \"source of truth.\"\n\n","depth":2,"section_tag":"resources"},"resource_cache_uses-table":{"location":"database-schema.html#resource_cache_uses-table","title":"resource_cache_uses","text":"A cache use is a join table between a resource cache and a 'user', which lets us know when the use is no longer needed. Think of it as a reference counter.\n\nA use can be tied to a builds, in which case the use can be removed when the build is finished. A cache is in use by a build when the build is fetching it as an input. This is to prevent the garbage collector from reaping caches that are no longer desired in the long run, but are still in use by a build.\n\nA use can be tied to a containers, in which case the use can be removed when the container goes away. A cache is in use by a container when it's being used as the container's image, either via image_resource or a resource type.\n\n","depth":4,"section_tag":"abstract-objects"},"resource_caches-table":{"location":"database-schema.html#resource_caches-table","title":"resource_caches","text":"A resource cache represents all information necessary to fetch a particular set of bits, i.e. the cache.\n\nA resource cache always points to its resource_configs, and also specifies the version and params.\n\nNote that resource_configs and resource_caches both point to each other. This loop is closed by a resource config that points to a base resource type at the end of the chain. This represents the case of a resource cache provided by a custom resource type, which in turn may be provided by another custom resource type, and so on, until they reach a base type.\n\n","depth":4,"section_tag":"abstract-objects"},"resource_config_check_sessions-table":{"location":"database-schema.html#resource_config_check_sessions-table","title":"resource_config_check_sessions","text":"A resource config check session is used for periodic resource checking of resources and resource_types by pointing to their resource config. There is only one check container per resource config, even if it's defined in many pipelines.\n\nA check session's expiry is tied to its worker's uptime, as a simple way to force balancing of check containers across workers more quickly after a deploy.\n\n","depth":4,"section_tag":"abstract-objects"},"resource_configs-table":{"location":"database-schema.html#resource_configs-table","title":"resource_configs","text":"A resource config is a distinct configuration of a resource type, not specific to any pipeline resource.\n\nA resource config is used for discovering versions. Unlike pipeline resources, they have no name, and are identified entirely by their type and source. Resource configs are shared across pipelines and teams, since identical resource configurations can be reused.\n\nA resource config's type is either a base resource type or, in the case of a custom resource type, the resource cache of its custom type's image resource.\n\n","depth":4,"section_tag":"abstract-objects"},"resource_types-table":{"location":"database-schema.html#resource_types-table","title":"resource_types","text":"An entry in the resource types table represents a custom resource type defined in a pipleine, and all of the information neccessary to pull the image bits for a custom resource type.  This includes the resource type used to fetch the resource type's image (either a base_resource_type or another custom resource) , the version of the image to fetch using that resource, and the configuration of the custom resource type.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"resources":{"location":"resources.html","title":"Resources","text":"Resources are the heart and soul of Concourse. They represent all external inputs to and outputs of jobs in the pipeline.\n\nResources are listed under the resources: key in the pipeline configuration. Each configured resource consists of the following fields:\n\n","depth":2,"section_tag":"resources"},"resources-table":{"location":"database-schema.html#resources-table","title":"resources","text":"An entry in the resources table represents a resource defined in a pipeline. The resources table has columns for configuration, name, type, and details about resource version checking.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"restarting-a-worker":{"location":"concourse-worker.html#restarting-a-worker","title":"Restarting a Worker","text":"Workers can be restarted in-place by sending SIGTERM to the worker process and starting it back up. Containers will remain running and Concourse will reattach to builds that were in flight.\n\nThis is a pretty aggressive way to restart a worker, and may result in errored builds - there are a few moving parts involved and we're still working on making this airtight.\n\nA safer way to restart a worker is to land it by sending SIGUSR1 to the worker process. This will switch the worker to landing state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the process will exit.\n\nYou may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR1/300/TERM/15/KILL\nThis will send SIGUSR1, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\nOnce the timeout is enforced, there's still a chance that builds that were running will continue when the worker comes back.\n\n","depth":5,"section_tag":"restarting-a-worker"},"restarting-and-upgrading":{"location":"concourse-web.html#restarting-and-upgrading","title":"Restarting \u0026 Upgrading","text":"The web nodes can be killed and restarted willy-nilly. No draining is necessary; if the web node was orchestrating a build it will just continue where it left off when it comes back or, or the build will be picked up by one of the other web nodes.\n\nTo upgrade a web node, stop its process and start a new one using the newly installed concourse. Any migrations will be run automatically on start. If web nodes are started in parallel, only one will run the migrations.\n\nNote that we don't currently guarantee a lack of funny-business if you're running mixed Concourse versions - database migrations can perform modifications that confuse other web nodes. So there may be some turbulence during a rolling upgrade, but everything should stabilize once all web nodes are running the latest version.\n\n","depth":5,"section_tag":"restarting-and-upgrading"},"risks-and-side-effects":{"location":"global-resources.html#risks-and-side-effects","title":"Risks and Side Effects","text":"","depth":4,"section_tag":"risks-and-side-effects"},"rotating-the-encryption-key":{"location":"encryption.html#rotating-the-encryption-key","title":"Rotating the Encryption Key","text":"To swap out the encryption key, you'll need to pass the previous key as --old-encryption-key (or old_encryption_key), and the new key as --encryption-key (or encryption_key).\n\nOn startup, the Running a web node will decrypt all existing data and re-encrypt it with the new key, in one go. If it encounters a row which is already encrypted with the new key, it will continue on (as may be the case when restarting with the flags again, or if the ATC died in the middle of rotating).\n\nIf the ATC encounters a row which cannot be decrypted with neither the old key nor the new one, it will log loudly and fail to start, telling you which row it choked on. This data must be dealt with in some way, either by re-configuring the key the row was encrypted with as the old key, or manually performing database surgery to remove the offending row. Hopefully this doesn't happen to you!\n\n","depth":4,"section_tag":"rotating-the-encryption-key"},"running-tasks":{"location":"running-tasks.html","title":"Running Tasks","text":"One of the most common use cases of fly is taking a local project on your computer and submitting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.\n\n","depth":3,"section_tag":"running-tasks"},"runtime":{"location":"database-schema.html#runtime","title":"Runtime","text":"","depth":4,"section_tag":"runtime"},"scaling":{"location":"concourse-web.html#scaling","title":"Scaling","text":"The Running a web node can be scaled up for high availability. They'll also roughly share their scheduling workloads, using the database to synchronize. This is done by just running more web commands on different machines, and optionally putting them behind a load balancer.\n\nTo run a cluster of Running a web nodes, you'll first need to ensure they're all pointing to the same PostgreSQL server.\n\nNext, you'll need to configure a peer URL. This is a URL that can be used to reach this web node's web server from other web nodes. Typically this uses a private IP, like so:\n\nCONCOURSE_PEER_URL=http://10.10.0.1:8080\nFinally, if all of these nodes are going to be accessed through a load balancer, you'll need to configure the external URL that will be used to reach your Concourse cluster:\n\nCONCOURSE_EXTERNAL_URL=https://ci.example.com\nAside from the peer URL, all configuration must be consistent across all web nodes in the cluster to ensure consistent results.\n\n","depth":5,"section_tag":"scaling"},"scaling-workers":{"location":"concourse-worker.html#scaling-workers","title":"Scaling Workers","text":"More workers should be added to accomodate more pipelines. To know when this is necessary you should probably set up Metrics and keep an eye on container counts. If it's starting to approach 200 or so, you should probably add another worker. Load average is another metric to keep an eye on.\n\nTo add a worker, just create another machine for the worker and follow the Running instructions again.\n\nNote: it doesn't really make sense to run multiple workers on one machine, since they'll both just be contending for the same physical resources. Workers should be given their own VMs or physical machines.\n\n","depth":5,"section_tag":"scaling-workers"},"scheduling: full duration (ms)":{"location":"metrics.html#scheduling: full duration (ms)","title":"scheduling: full duration (ms)","text":"This is the time taken (in milliseconds) to schedule an entire pipeline including the time taken to load the version information from the database and calculate the latest valid versions for each job.\n\nAttributes pipeline\n\n: The pipeline which was being scheduled.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"scheduling: job duration (ms)":{"location":"metrics.html#scheduling: job duration (ms)","title":"scheduling: job duration (ms)","text":"This is the time taken (in milliseconds) to calculate the set of valid input versions when scheduling a job. It is emitted once for each job per pipeline scheduling tick.\n\nAttributes pipeline\n\n: The pipeline which was being scheduled.\n\n\njob\n\n: The job which was being scheduled.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"scheduling: loading versions duration (ms)":{"location":"metrics.html#scheduling: loading versions duration (ms)","title":"scheduling: loading versions duration (ms)","text":"This is the time taken (in milliseconds) to load the version information from the database.\n\nAttributes pipeline\n\n: The pipeline which was being scheduled.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"security-contact":{"location":"security-contact.html","title":"Security Contact","text":"To be notified of any security issues or vulnerabilities, join the Concourse Security mailing list. To report a security issue, send an email to concourseteam+security@gmail.com.\n\n","depth":2,"section_tag":"security-contact"},"serial-job-example":{"location":"serial-job-example.html","title":"Serial job example","text":"Setting the serial flag restricts a job to run one build at a time.\n\n","depth":2,"section_tag":"serial-job-example"},"setting-pipelines":{"location":"setting-pipelines.html","title":"Setting Pipelines","text":"Pipelines are configured entirely via the The fly CLI. There is no GUI.\n\n","depth":3,"section_tag":"setting-pipelines"},"setting-roles":{"location":"managing-teams.html#setting-roles","title":"Setting User Roles","text":"By default, authorization config passed to set-team configures the Team Member role.\n\nMore advanced roles configuration can be specified can be specified through the --configuration or -c flag.\n\nThe -c flag expects a .yml file with a single field, roles:, pointing to a list of role authorization configs.\n\nAll of the attributes in each config will vary by provider. Consult the appropriate section for your provider under Configuring Auth for specifics.\n\nFor example, the following config sets three roles with different auth config for each role's provider:\n\nroles:\n- name: owner\n  github:\n    users: [\"admin\"]\n- name: member\n  github:\n    teams: [\"org:team\"]\n- name: viewer\n  github:\n    orgs: [\"org\"]\n  local:\n    users: [\"visitor\"]\n","depth":5,"section_tag":"setting-roles"},"some-resources-should-opt-out":{"location":"global-resources.html#some-resources-should-opt-out","title":"Some resources should opt-out","text":"Sharing versions isn't always a good idea. For example, the time resource is often used to generate versions on an interval so that jobs can fire periodically. If version history were to be shared for all users with e.g. a 10 minute interval, that would lead to a thundering herd of builds storming your workers, leading to load spikes and a lot of unhappy builds.\n\nFor this reason, resource types can opt out of sharing version history for all resources that use them. This way all existing usage of the time resource don't have to change, and continue to have their own version history, unique to the pipeline resource.\n\nThe time resource opts out of this by configuring unique_version_history: true in its metadata.json - but this is something that only \"core\" resource types can do. We plan on supporting this as part of the Resources v2 RFC.\n\nUsers can also set this value themselves by configuring unique_version_history on the resource type.\n\nAnother case where version history shouldn't be shared is when resources \"automagically\" learn their auth credentials using things like IAM roles. In these cases, the credentials aren't in the source. If version history were to be shared, anyone could configure the same source:, not specifying any credentials, and see the version history discovered by some other pipeline that ran its checks on workers that had access via IAM roles.\n\nFor this reason, any resource types that acquire credentials outside of source: should not share version history. Granted, the user won't be able to fetch these versions, but it's still an information leak.\n\nIAM roles are a bit of a thorn in our side when it comes to designing features like this. We're planning on introducing support for them in a way that doesn't have this problem in 3023.\n\n","depth":5,"section_tag":"some-resources-should-opt-out"},"steps":{"location":"steps.html","title":"Steps","text":"Each Job has a single build plan. When a build of a job is created, the plan determines what happens.\n\nA build plan is a sequence of steps to execute. These steps may fetch down or update Resources, or execute Tasks.\n\nA new build of the job is scheduled whenever get steps with trigger: true have new versions available.\n\nTo visualize the job in the pipeline, resources that appear as get steps are drawn as inputs, and resources that appear in put steps appear as outputs.\n\n","depth":3,"section_tag":"steps"},"tags":{"location":"tags-step-modifier.html#tags","title":"tags","text":"Optional. Default []. The tags by which to match workers.\n\nFor example, if [a, b] is specified, only workers advertising the a and b tags (or any others) will be used for running the step.\n\n","depth":4,"section_tag":"tags-step-modifier"},"tags-step-modifier":{"location":"tags-step-modifier.html","title":"tags step modifier","text":"Any step can be directed at a pool of workers for a given set of tags, by adding the tags attribute to it.\n\n","depth":4,"section_tag":"tags-step-modifier"},"taking-artifacts-from-the-build-with---output":{"location":"running-tasks.html#taking-artifacts-from-the-build-with---output","title":"Taking artifacts from the build with --output","text":"If a task specifies outputs then you're able to extract these back out of the build and back to your local system. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\nThis would work together with a build-stemcell.yml if its outputs: section was as follows:\n\noutputs:\n- name: stemcell\nThis feature is useful to farm work out to your Concourse server to build things in a repeatable manner.\n\n","depth":5,"section_tag":"taking-artifacts-from-the-build-with---output"},"targeting-a-specific-worker-with---tag":{"location":"running-tasks.html#targeting-a-specific-worker-with---tag","title":"Targeting a specific worker with --tag","text":"If you want to execute a task on a worker that has a specific tag, you can do so by passing --tag:\n\nfly -t example execute --config task.yml --tag bar\nThis will execute the task specified by task.yml on a worker that has been tagged bar.\n\n","depth":5,"section_tag":"targeting-a-specific-worker-with---tag"},"task":{"location":"task-step.html#task","title":"task","text":"Required. A freeform name for the task that's being executed. Common examples would be unit or integration.\n\n","depth":4,"section_tag":"task-step"},"task-caches":{"location":"tasks.html#task-caches","title":"caches","text":"Where cache is:\n\nOptional. The cached directories shared between task runs.\n\nOn the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those artifacts.\n\nCaches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.\n\nCaches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off builds.\n\n","depth":2,"section_tag":"tasks"},"task-containers":{"location":"container-internals.html#task-containers","title":"Task Containers","text":"Task containers are created when a task step is executed in a build plan. They are based on the image produced by the configured image_resource or image.\n\n","depth":5,"section_tag":"task-containers"},"task-environment":{"location":"task-environment.html","title":"Task Environment","text":"A task runs in a new container every time, using the image provided by image_resource as its base filesystem (i.e. /).\n\nThe command specified by run will be executed in a working directory containing each of the inputs. If any inputs are missing the task will not run (and the container will not even be created).\n\nThe working directory will also contain empty directories for each of the outputs. The task must place artifacts in the output directories for them to be exported. This meshes well with build tools with configurable destination paths.\n\nIf your build tools don't support output paths you'll have to copy bits around. If it's a git repo that you're modifying you can do a local git clone ./input ./output, which is much more efficient than cp, and then work out of ./output.\n\nAny params configured will be set in the environment for the task's command, along with any environment variables provided by the task's image (i.e. ENV rules from your Dockerfile).\n\nThe user the command runs as is determined by the image. If you're using the Docker Image resource, this will be the user set by a USER rule in your Dockerfile, or root if not specified.\n\nAnother relevant bit of configuration is privileged, which determines whether the user the task runs as will have full privileges (primarily when running as root). This is intentionally not configurable by the task itself, to prevent privilege escalation by way of pull requests to repositories containing task configs.\n\nPutting all this together, the following task config:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: golang\n    tag: '1.6'\n\nparams:\n  SOME_PARAM: some-default-value\n\ninputs:\n- name: some-input\n- name: some-input-with-custom-path\n  path: some/custom/path\n\noutputs:\n- name: some-output\n\nrun:\n  path: sh\n  args:\n  - -exc\n  - |\n    whoami\n    env\n    go version\n    find .\n    touch some-output/my-built-artifact\n...will produce the following output:\n\n+ whoami\nroot\n+ env\nUSER=root\nHOME=/root\nGOLANG_DOWNLOAD_SHA256=5470eac05d273c74ff8bac7bef5bad0b5abbd1c4052efbdbc8db45332e836b0b\nPATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nGOPATH=/go\nPWD=/tmp/build/e55deab7\nGOLANG_DOWNLOAD_URL=https://golang.org/dl/go1.6.linux-amd64.tar.gz\nGOLANG_VERSION=1.6\nSOME_PARAM=some-default-value\n+ go version\ngo version go1.6 linux/amd64\n+ find .\n.\n./some-input\n./some-input/foo\n./some\n./some/custom\n./some/custom/path\n./some/custom/path/bar\n./some-output\n+ touch some-output/my-built-artifact\n...and propagate my-built-artifact to any later task steps or put steps that reference the some-output artifact, in the same way that this task had some-input as an input.\n\n","depth":3,"section_tag":"task-environment"},"task-image-resource":{"location":"tasks.html#task-image-resource","title":"image_resource","text":"Where resource is:\n\nOptional. The base image of the container, as provided by a resource definition.\n\nYou can use any resource that returns a filesystem in the correct format (a /rootfs directory and a metadata.json file in the top level) but normally this will be the Docker Image resource. If you'd like to make a resource of your own that supports this please use that as a reference implementation for now.\n\nIf you want to use an artifact source within the plan containing an image, you must set the image in the plan step instead.\n\n","depth":2,"section_tag":"tasks"},"task-inputs":{"location":"tasks.html#task-inputs","title":"inputs","text":"Where input is:\n\nOptional. The set of artifacts used by task, determining which artifacts will be available in the current directory when the task runs.\n\nThese are satisfied by get steps or outputs of a previous task. These can also be provided by -i with fly execute.\n\nIf any required inputs are missing at run-time, then the task will error immediately.\n\n","depth":2,"section_tag":"tasks"},"task-inputs-outputs-example":{"location":"task-inputs-outputs-example.html","title":"Task inputs and outputs example","text":"A task can pass an artifacts to another task in the same job.\n\n","depth":2,"section_tag":"task-inputs-outputs-example"},"task-outputs":{"location":"tasks.html#task-outputs","title":"outputs","text":"Where output is:\n\nOptional. The artifacts produced by the task.\n\nEach output configures a directory to make available to later steps in the build plan. The directory will be automatically created before the task runs, and the task should place any artifacts it wants to export in the directory.\n\n","depth":2,"section_tag":"tasks"},"task-params":{"location":"tasks.html#task-params","title":"params","text":"Optional. A key-value mapping of values that are exposed to the task via environment variables.\n\nUse this to provide things like credentials to a task.\n\n","depth":2,"section_tag":"tasks"},"task-platform":{"location":"tasks.html#task-platform","title":"platform","text":"Required. The platform the task should run on. By convention, windows, linux, or darwin are specified. This determines the pool of workers that the task can run against. The base deployment provides Linux workers.\n\n","depth":2,"section_tag":"tasks"},"task-rootfs-uri":{"location":"tasks.html#task-rootfs-uri","title":"rootfs_uri","text":"Optional. A string specifying the rootfs uri of the container, as interpreted by your worker's Garden backend.\n\nimage_resource is a preferred way to specify base image and rootfs_uri is not recommended. With rootfs_uri image fetching is delegated to backend which does not guarantee image caching and might result in some permission errors. You should only use this if you cannot use image_resource for some reason, and you know what you're doing.\n\n","depth":2,"section_tag":"tasks"},"task-run":{"location":"tasks.html#task-run","title":"run","text":"Where run-config is:\n\nRequired. The command to execute in the container.\n\nNote that this is not provided as a script blob, but explicit path and args values; this allows fly to forward arguments to the script, and forces your config .yml to stay fairly small.\n\n","depth":2,"section_tag":"tasks"},"task-run-args":{"location":"tasks.html#task-run-args","title":"run.args","text":"Optional. Arguments to pass to the command. Note that when executed with Fly, any arguments passed to Fly are appended to this array.\n\n","depth":2,"section_tag":"tasks"},"task-run-dir":{"location":"tasks.html#task-run-dir","title":"run.dir","text":"Optional. A directory, relative to the initial working directory, to set as the working directory when running the script.\n\n","depth":2,"section_tag":"tasks"},"task-run-path":{"location":"tasks.html#task-run-path","title":"run.path","text":"Required. The command to execute.\n\nThis is commonly a path to a script provided by one of the task's inputs, e.g. my-resource/scripts/test. It could also be a command like bash (respecting standard $PATH lookup rules), or an absolute path to a file to execute, e.g. /bin/bash.\n\n","depth":2,"section_tag":"tasks"},"task-run-user":{"location":"tasks.html#task-run-user","title":"run.user","text":"Optional. Explicitly set the user to run as. If not specified, this defaults to the user configured by the task's image. If not specified there, it's up to the Garden backend, and may be e.g. root on Linux.\n\n","depth":2,"section_tag":"tasks"},"task-step":{"location":"task-step.html","title":"task step","text":"Executes a Task, either from a file fetched via the preceding steps, or with inlined configuration.\n\nIf any task in the build plan fails, the build will complete with failure. By default, any subsequent steps will not be performed. You can perform additional steps after failure by adding a on_failure or ensure step hook.\n\nWhen a task completes, the files in its declared outputs will be made avaliable to subsequent steps. This allows those subsequent steps to process the result of a task.\n\n","depth":4,"section_tag":"task-step"},"task-step-config":{"location":"task-step.html#task-step-config","title":"config","text":"Required. The task config to execute. An alternative to file.\n\n","depth":4,"section_tag":"task-step"},"task-step-file":{"location":"task-step.html#task-step-file","title":"file","text":"Required. A dynamic alternative to config.\n\nfile points at a .yml file containing the task config, which allows this to be tracked with your resources.\n\nThe first segment in the path should refer to another source from the plan, and the rest of the path is relative to that source.\n\nFor example, if in your plan you have the following get step:\n\n- get: something\nAnd the something resource provided a unit.yml file, you would set file: something/unit.yml.\n\nThe content of the config file may contain template ((vars)), which will be filled in using vars or a configured credential manager.\n\n","depth":4,"section_tag":"task-step"},"task-step-image":{"location":"task-step.html#task-step-image","title":"image","text":"Optional. Names an artifact source within the plan containing an image to use for the task. This overrides any image_resource configuration present in the task configuration.\n\nThis is very useful when part of your pipeline involves building an image, possibly with dependencies pre-baked. You can then propagate that image through the rest of your pipeline, guaranteeing that the correct version (and thus a consistent set of dependencies) is used throughout your pipeline.\n\nFor example, here's a pipeline building an image in one job and propagating it to the next:\n\nresources:\n- name: my-project\n  type: git\n  source: {uri: https://github.com/my-user/my-project}\n\n- name: my-task-image\n  type: docker-image\n  source: {repository: my-user/my-repo}\n\njobs:\n- name: build-task-image\n  plan:\n  - get: my-project\n  - put: my-task-image\n    params: {build: my-project/ci/images/my-task}\n\n- name: use-task-image\n  plan:\n  - get: my-task-image\n    passed: [build-task-image]\n  - get: my-project\n    passed: [build-task-image]\n  - task: use-task-image\n    image: my-task-image\n    file: my-project/ci/tasks/my-task.yml\nThis can also be used in the simpler case of explicitly keeping track of dependent images, in which case you just wouldn't have a job building it (build-task-image in the above example).\n\n","depth":4,"section_tag":"task-step"},"task-step-params":{"location":"task-step.html#task-step-params","title":"params","text":"Optional. A map of task parameters to set, overriding those configured in config or file.\n\nThe difference between params and vars is that vars allows you to interpolate any template variable in an external task, while params can be used to overwrite task parameters (i.e. env variables) specifically. Also, params can have default values declared in the task.\n\nFor example:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    REMOTE_SERVER: 10.20.30.40:8080\n    USERNAME: my-user\n    PASSWORD: my-pass\nThis is often used in combination with ((vars)) in the pipeline.\n\nFor example:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    REMOTE_SERVER: 10.20.30.40:8080\n    USERNAME: ((integration-username))\n    PASSWORD: ((integration-password))\nLooking into the task.yml, a common pattern is to list the param in the task definition with no value. This indicates that you expect the pipeline to provide the value.\n\nFor example, in the task.yml:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\nAnd in the pipeline.yml:\n\nparams:\n  BAR: qux\nIf the pipeline used this task.yml but did not set BAR, the value of $BAR would be set to the empty string in the task container.\n\n","depth":4,"section_tag":"task-step"},"task-step-privileged":{"location":"task-step.html#task-step-privileged","title":"privileged","text":"Optional. Default false. If set to true, the task will run with full capabilities, as determined by the Garden backend the task runs on. For Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the root user is \"actual\" root (if set to true) or a user namespaced root (if set to false, the default).\n\nThis is a gaping security hole; use wisely and only if necessary. This is not part of the task configuration to prevent privilege escalation via pull requests.\n\n","depth":4,"section_tag":"task-step"},"task-step-vars":{"location":"task-step.html#task-step-vars","title":"vars","text":"Optional. A map of template variables to pass to an external task. Only works with external tasks defined in file.\n\nFor example:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: \"Hello World!\"\nThis is often used in combination with ((vars)) in the pipeline.\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: ((text))\nAnd task.yml:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\nrun:\n  path: echo\n  args: [\"((text))\"]\nThis will resolve \"((text))\" to \"Hello World!\", while ((myuser)) and ((mypass))  will be resolved in runtime via a credential manager, if it has been configured.\n\n","depth":4,"section_tag":"task-step"},"tasks":{"location":"tasks.html","title":"Tasks","text":"The smallest configurable unit in a Concourse pipeline is a single task. A task can be thought of as a function from inputs to outputs that can either succeed or fail.\n\nGoing a bit further, ideally tasks are pure functions: given the same set of inputs, it should either always succeed with the same outputs or always fail. This is entirely up to your script's level of discipline, however. Flaky tests or dependencies on the internet are the most common source of impurity.\n\nOnce you have a running Concourse deployment, you can start configuring your tasks and executing them interactively from your terminal with the Fly commandline tool.\n\nOnce you've figured out your tasks's configuration, you can reuse it for a Job in your Pipeline.\n\nConventionally a task's configuration is placed in the same repository as the code it's testing, possibly under some ci directory.\n\nA task's configuration specifies the following:\n\n","depth":2,"section_tag":"tasks"},"team-member-role":{"location":"user-roles.html#team-member-role","title":"Team Member role","text":"Team Member lets users operate within their teams in a read \u0026 write fashion; but prevents them from changing the auth configurations of their team.\n\n","depth":4,"section_tag":"team-member-role"},"team-owner-role":{"location":"user-roles.html#team-owner-role","title":"Team Owner role","text":"Team Owners have read, write and auth management capabilities within the scope of their team. For those familiar with Concourse today, the scope of allowed actions for a Team Owner is very closely aligned to todays Concourse team member. The new change is that you can no longer rename your own team or destroy your own team as an owner.\n\n","depth":4,"section_tag":"team-owner-role"},"team-viewer-role":{"location":"user-roles.html#team-viewer-role","title":"Team Viewer role","text":"Team Viewer gives users \"read-only\" access to a team. This locks everything down, preventing users from doing a fly set-pipeline or fly intercept.\n\n","depth":4,"section_tag":"team-viewer-role"},"teams":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"teams-caveats":{"location":"teams-caveats.html","title":"Security Caveats","text":"At present, teams only provide trusted multi-tenancy. This means it should be used for cases where you know and trust who you're allowing access into your Concourse cluster.\n\nThere are a few reasons it'd be a bad idea to do otherwise:\n\n* Any team can run builds with privileged tasks. A bad actor in the mix could easily use this to harm your workers and your cluster.\n\n  In the future, we'll probably have this as a flag on a team, indicating whether they're permitted to run privileged builds.\n\n* There are no networking restrictions in place, and traffic to and from the workers is currently unencrypted and unauthorized. Anyone could run a task that does horrible things to your worker's containers, possibly stealing sensitive information.\n\n  This can be remedied with configuration specified on Garden to restrict access to the internal network, but this is not detailed in our docs, and we'll probably want to find a better answer than configuration in the future.\n\n","depth":3,"section_tag":"teams-caveats"},"thanks":{"location":"contribute.html#thanks","title":"Thanks!","text":"It's been a long journey and we've got a lot of people to thank for our continued success. We are deeply indebted to any and all who help keep this project going, but the heroic effort of the following organizations is worth giving special props.\n\n* Pivotal\n\n  Concourse wouldn't be what it is today without Pivotal. This goes beyond the sponsorship, which began in early 2015 - without the experiences we had and the practices we learned while working on Cloud Foundry and BOSH, we would have neither the technical experience nor the strong opinions that led to Concourse being born.\n\n  Pivotal's sponsorship continues strong into 2019, where we have a team of full-time engineers, PMs, and designers dedicated to pushing Concourse forward.\n\n* Stark \u0026 Wayne\n\n  The Concourse Tutorial by Stark \u0026 Wayne  is probably the only reason a lot of people were able to learn Concourse. It's a great asset and does its job so well that we've decided to basically just kill our tutorials and delegate to theirs. Thanks to Dr. Nic for writing and maintaining it, and everyone else that has helped out!\n\n","depth":3,"section_tag":"thanks"},"time-trigger-example":{"location":"time-trigger-example.html","title":"time-triggered job example","text":"The time resource can be used to trigger a job.\n\n","depth":2,"section_tag":"time-trigger-example"},"timeout":{"location":"timeout-step-modifier.html#timeout","title":"timeout","text":"The amount of time to limit the step's execution to, e.g. 30m for 30 minutes.\n\nWhen exceeded, the step will be interrupted, with the same semantics as aborting the build (except the build will be failed, not aborted, to distinguish between human intervention and timeouts being inforced).\n\n","depth":4,"section_tag":"timeout-step-modifier"},"timeout-step-modifier":{"location":"timeout-step-modifier.html","title":"timeout step modifier","text":"Any step can have a hard time limit enforced by attaching timeout and the number of seconds to limit it to.\n\n","depth":4,"section_tag":"timeout-step-modifier"},"trademarks":{"location":"trademarks.html","title":"Trademark Policy","text":"* PURPOSE. Pivotal Software, Inc. (Pivotal) owns a number of international trademarks and logos that identify the Concourse community and individual Concourse projects (Concourse Marks). These trademarks include, but are not limited to:\n\n  * Words: CONCOURSE\n\n  * Logos: {image: images/trademarks/concourse-black.png}\n\n  This policy outlines Pivotals policy and guidelines about the use of the Concourse trademarks by members of the Concourse development and user community.\n\n* WHY HAVE TRADEMARK GUIDELINES? The Concourse Marks are a symbol of the quality and community support associated with the Concourse open source software. Trademarks protect not only those using the marks but the entire community as well. Our community members need to know that they can rely on the quality and capabilities represented by the brand. We also want to provide a level playing field. No one should use the Concourse marks in ways that mislead or take advantage of the community or make unfair use of the trademarks. Also, use of the Concourse Marks should not be in a disparaging manner because we prefer that our marks not be used to be rude about the Concourse open source project or its members.\n\n* OPEN SOURCE LICENSE VS. TRADEMARKS. The Apache 2.0 license gives you the right to use, copy, distribute and modify the Concourse software. However, open source licenses like the Apache 2.0 license do not address trademarks.  Concourse Marks need to be used in a way consistent with trademark law, and that is why we have prepared this policy  to help you understand what branding is allowed or required when using our software under the Apache license.\n\n* PROPER USE OF THE CONCOURSE MARKS. We want to encourage a robust community for the Concourse open source project. Therefore, you may do any of the following, as long as you do so in a way that does not devalue, dilute, or disparage the Concourse brand. In other words, when you do these things, you should behave responsibly and reasonably in the interest of the community, but you do not need a trademark license from us to do them.\n\n  * Nominative Use. You may engage in nominative use of the Concourse name, but this does not allow you to use the logo.  Nominative use is sometimes called fair use of a trademark, and does not require a trademark license from us.  Here are examples:\n\n    * You may use the Concourse Marks in connection with the development of tools, add-ons, or utilities that are compatible with bit-for-bit identical copies of official Concourse software. For example, if you are developing a Foobar tool for Concourse, acceptable project titles would be Foobar for Concourse\".\n\n    * You may use the Concourse Marks in connection with your non-commercial redistribution of (1) bit-for-bit identical copies of official Concourse software, and (2) unmodified copies of official Concourse source packages.  For example, if your Foobar product included a full redistribution of official Concourse Software or source code packages: \"Concourse-powered Foobar Product\". We strongly discourage, and likely would consider it a trademark problem, to use a name such as Concourse Foobar.\n\n    * If you offer maintenance, support, or hosting services for Concourse software, you may accurately state that in your marketing materials or portfolio, without using the Concourse logo.\n\n    * You may modify the Concourse software and state that your modified software is based on the Concourse software or a similar accurate statement, without using the Concourse logo.\n\n    * You may engage in community advocacy. The Concourse software is developed by and for its community. We will allow the use of the word trademark in this context, provided:\n\n      * The trademark is used in a manner consistent with this policy.\n\n      * There is no commercial purpose behind the use.\n\n      * There is no suggestion that your project is approved, sponsored, or affiliated with Concourse.\n\n      * You may create Concourse user or development groups, and publicize meetings or discussions for those groups.\n\n  * Attribution. Identify the trademarks as trademarks of Concourse, as set forth in Section 7.\n\n  * Redistribution of Binaries. If you redistribute binaries that you have downloaded from the Concourse repository, you should retain the logos and name of the product.  However, if you make any changes to the binaries (other than configuration or installation changes that do not involve changes to the source code), or if you re-build binaries from our source code, you should not use our logos.  Our logos represent our quality control, so they should be retained where the product has been built by us, but not otherwise.\n\n  * Capitalization. Concourse should always be capitalized and one word.\n\n* IMPROPER USE OF THE TRADEMARKS AND LOGOS. Use of the logo is reserved solely for use by Concourse in its unaltered form. Examples of unauthorized use of the Concourse trademarks include:\n\n  * Commercial Use: You may not use the Concourse Marks in connection with commercial redistribution of Concourse software (commercial redistribution includes, but is not limited to, redistribution in connection with any commercial business activities or revenue-generating business activities) regardless of whether the Concourse software is unmodified.\n\n  * Entity Names. You may not form a company, use a company name, or create a software product name that includes the Concourse trademark, or implies any foundational or authorship role. If you have a software product that works with Concourse, it is suggested you use terms such as \u003cproduct name\u003e for Concourse or \u003cproduct name\u003e, Concourse Edition. If you wish to form an entity for a user or developer group, please contact us and we will be glad to discuss a license for a suitable name.\n\n  * Class or Quality. You may not imply that you are providing a class or quality of Concourse (e.g., \"enterprise-class\" or \"commercial quality\") in a way that implies Concourse is not of that class, grade or quality, nor that other parties are not of that class, grade, or quality.\n\n  * Combinations. Use of the Concourse Marks to identify software that combines any portion of the Concourse software with any other software, unless the combined distribution is an official Concourse distribution. For example, you may not distribute a combination of the Concourse  software with software released by the Foobar project under the name Concourse Foobar Distro.\n\n  * False or Misleading Statements. You may not make false or misleading statements regarding your use of Concourse (e.g., \"we wrote the majority of the code\" or \"we are major contributors\" or \"we are committers\").\n\n  * Domain Names. You must not use Concourse or any confusingly similar phrase in a domain name. For instance www.concoursehost.com is not allowed.  If you wish to use such a domain name for a non-commercial user or developer group to engage in community advocacy, please contact us and we will be glad to discuss a license for a suitable domain name.  Because of the many persons who, unfortunately, seek to spoof, swindle or deceive the community by using confusing domain names, we must be very strict about this rule.\n\n  * Merchandise. You must not manufacture, sell or give away merchandise items, such as T-shirts and mugs, bearing the Concourse logo, or create any mascot for the project. If you wish to use the logo to do this for a non-commercial user or developer group to engage in community advocacy, please contact us and we will be glad to discuss a license to do this.\n\n  * Variations, takeoffs or abbreviations. You may not use a variation of the Concourse name or logo for any purpose other than common usage of these in community communications. For example, the following are not acceptable:\n\n    * CONCRSE\n\n    * MyConcourse\n\n    * ConcourseDB\n\n    * ConcourseHost\n\n    * ConcourseGuru\n\n  * Endorsement or Sponsorship. You may not use the Concourse trademarks in a manner that would imply Concourses affiliation with or endorsement, sponsorship, or support of a product or service.\n\n  * Rebranding. You may not change the trademark on unmodified Concourse software to your own brand.  You may not hold yourself out as the source of the Concourse software, except to the extent you have modified it as allowed under the Apache 2.0 license, and you make it clear that you are the source only of the modification.\n\n  * Combination Marks. Do not use our trademarks in combination with any other marks or logos (for example Foobar Concourse, or the name of your company or product typeset to look like the Concourse logo).\n\n  * Web Tags. Do not use the Concourse trademark in a title or meta tag of a web page to influence search engine rankings or result listings, rather than for discussion or advocacy of the Concourse project.\n\n* PROPER ATTRIBUTION. When you use a Concourse trademark you should include a statement attributing the trademark to Concourse. For example, \"Concourse is a trademark of Concourse in the U.S. and other countries.\"\n\n* MORE QUESTIONS? If you have questions about this policy, please contact us at concourseteam+trademarks@gmail.com.\n\n","depth":2,"section_tag":"trademarks"},"troubleshooting-and-fixing-dns-resolution":{"location":"concourse-worker.html#troubleshooting-and-fixing-dns-resolution","title":"Troubleshooting and fixing DNS resolution","text":"By default, containers created by Guardian will carry over the /etc/resolv.conf from the host into the container. This is often fine, but some Linux distributions configure a special 127.x.x.x DNS resolver (e.g. systemd-resolved).\n\nWhen Guardian copies the resolv.conf over, it removes these entries, as they won't be reachable from the container's network namespace. As a result, your containers may not have any valid nameservers configured.\n\nTo diagnose this problem you can fly intercept into a failing container and check which nameservers are in /etc/resolv.conf:\n\n$ fly -t ci intercept -c concourse/concourse\nbash-5.0# grep nameserver /etc/resolv.conf\nbash-5.0#\nIn this case it is empty, as the host only listed a single 127.0.0.53 address which was then stripped out. To fix this, you'll just need to explicitly configure DNS instead of relying on the gdn default behavior.\n\n","depth":5,"section_tag":"troubleshooting-and-fixing-dns-resolution"},"try":{"location":"try-step.html#try","title":"try","text":"Performs the given step, swallowing any failure.\n\nThis can be used when you want to perform some side-effect, but you don't really want the whole build to fail if it doesn't work.\n\n","depth":4,"section_tag":"try-step"},"try-step":{"location":"try-step.html","title":"try step","text":"","depth":4,"section_tag":"try-step"},"types-of-containers":{"location":"container-internals.html#types-of-containers","title":"Types of Containers","text":"These are the types of containers:\n\n","depth":4,"section_tag":"types-of-containers"},"types-of-volumes":{"location":"volume-internals.html#types-of-volumes","title":"Types of Volumes","text":"","depth":4,"section_tag":"types-of-volumes"},"upgrading-concourse":{"location":"upgrading-concourse.html","title":"Upgrading Concourse","text":"Concourse is upgraded by stopping the Concourse process, swapping out the concourse binary with the new one, and re-starting it. This should be done on each Running a web node and Running a worker node. Be careful to check the release notes for anything marked 'breaking' - in particular, you'll want to look for any flags that have changed.\n\nEach Running a web node will automatically run migrations on start and locks via the database to ensure only one of them runs the migrations. We currently do not guarantee zero-downtime upgrades, as migrations may make changes that confuse the older web nodes. This should resolve as each node is upgraded, and shouldn't result in any inconsistent state.\n\nTypically, Concourse can be upgraded from any version to any other version, though around 3.x and 4.x we made some changes to how migrations are run, and as a result the following upgrade paths must be followed:\n\n| Current Version | Upgrade Path |\n| \u003c v3.6.0 | v3.6.0 -\u003e v4.0.0 -\u003e latest |\n| = v3.6.0 | v4.0.0 -\u003e latest |\n\nWe'll try to minimize this kind of thing in the future.\n\n","depth":3,"section_tag":"upgrading-concourse"},"user-roles":{"location":"user-roles.html","title":"User Roles \u0026 Permissions","text":"Concourse comes with four roles: Concourse Admin, Team Owner, Team Member, Team Viewer.\n\n","depth":3,"section_tag":"user-roles"},"using-a-local-dns-server":{"location":"concourse-worker.html#using-a-local-dns-server","title":"Using a local DNS server","text":"If you would like to use Consul, dnsmasq, or some other DNS server running on the worker VM, you'll have to configure the internal address of the VM as the DNS server and allow the containers to reach the address, like so:\n\n[server]\n; internal IP of the worker machine\ndns-server = 10.1.2.3\n\n; allow containers to reach the above IP\nallow-host-access = true\nSetting allow-host-access will, well, allow containers to access your host VM's network. If you don't trust your container workloads, you may not want to allow this.\n\nTo validate whether the changes have taken effect, you can fly intercept into any container and check /etc/resolv.conf once again:\n\n$ fly -t ci intercept -c concourse/concourse\nbash-5.0# cat /etc/resolv.conf\nnameserver 10.1.2.3\nbash-5.0# nslookup concourse-ci.org\nServer:         10.1.2.3\nAddress:        10.1.2.3#53\n\nNon-authoritative answer:\nName:   concourse-ci.org\nAddress: 185.199.108.153\nName:   concourse-ci.org\nAddress: 185.199.109.153\nName:   concourse-ci.org\nAddress: 185.199.110.153\nName:   concourse-ci.org\nAddress: 185.199.111.153\nIf nslookup times out or fails, you may need to open up firewalls or security group configuration so that the worker VM can send UDP/TCP packets to itself.\n\n","depth":6,"section_tag":"using-a-local-dns-server"},"variables":{"location":"pipeline-vars-example.html#variables","title":"Variables","text":"---\nfirst: initial\nnumber: 9000\nhello: HAL\n","depth":3,"section_tag":"variables"},"vault-approle-auth":{"location":"vault-credential-manager.html#vault-approle-auth","title":"Using the approle auth backend","text":"The approle backend allows for an app (in this case, Concourse) to authenticate with a role pre-configured in Vault.\n\nWith this backend, the Running a web node is configured with a role_id corresponding to a pre-configured role, and a secret_id which is used to authenticate and acquire a token.\n\nThe approle backend must first be configured in Vault. Vault's approle backend allows for a few parameters which you may want to set to determine the permissions and lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\ntoken_ttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\ntoken_max_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and token_max_ttl as the max TTL will be ignored.\n\n\ntoken_num_uses=count: This sets a limit on how often a token can be used. We do not recommend setting this value, as it will effectively hamstring Concourse after a few credential acquisitions. The web node does not currently know to re-acquire a token when this limit is reached.\n\n\nsecret_id_ttl=duration and secret_id_num_uses=count: These two configurations will result in the secret ID expiring after the configured time or configured number of log-ins, respectively.\n\nYou should only set these if you have something periodically re-generating secret IDs and re-configuring your web nodes accordingly.\n\n\n\nGiven all that, a typical configuration may look something like this:\n\n$ vault auth enable approle\nSuccess! Enabled approle auth method at: approle/\n$ vault write auth/approle/role/concourse policies=concourse period=1h\nSuccess! Data written to: auth/approle/role/concourse\nNow that the backend is configured, we'll need to obtain the role_id and generate a secret_id:\n\n$ vault read auth/approle/role/concourse/role-id\nKey        Value\n---        -----\nrole_id    5f3420cd-3c66-2eff-8bcc-0e8e258a7d18\n$ vault write -f auth/approle/role/concourse/secret-id\nKey                   Value\n---                   -----\nsecret_id             f7ec2ac8-ad07-026a-3e1c-4c9781423155\nsecret_id_accessor    1bd17fc6-dae1-0c82-d325-3b8f9b5654ee\nThese should then be set on the Running a web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"approle\"\nCONCOURSE_VAULT_AUTH_PARAM=\"role_id:5f3420cd-3c66-2eff-8bcc-0e8e258a7d18,secret_id:f7ec2ac8-ad07-026a-3e1c-4c9781423155\"\n","depth":6,"section_tag":"vault-approle-auth"},"vault-cert-auth":{"location":"vault-credential-manager.html#vault-cert-auth","title":"Using the cert auth backend","text":"The cert auth method allows authentication using SSL/TLS client certificates.\n\nWith this backend, the Running a web node is configured with a client cert and a client key. Vault must be configured with TLS, which you should be almost certainly be doing anyway.\n\nThe cert backend must first be configured in Vault. The backend is associated to a policy and a CA cert used to verify the client certificate. It may also be given the client certificate itself.\n\nThe cert backend must first be configured in Vault. Vault's cert backend allows for a few parameters which you may want to set to determine the lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\nttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\nmax_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and max_ttl as the max TTL will be ignored.\n\n\n\n$ vault auth enable cert\nSuccess! Enabled cert auth method at: cert/\n$ vault write auth/cert/certs/concourse policies=concourse certificate=@out/vault-ca.crt ttl=1h\nSuccess! Data written to: auth/cert/certs/concourse\nOnce that's all set up, you'll just need to configure the client cert and key on the web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"cert\"\nCONCOURSE_VAULT_CLIENT_CERT=vault-certs/concourse.crt\nCONCOURSE_VAULT_CLIENT_KEY=vault-certs/concourse.key\nIn this case no additional auth params are necessary, as the Vault's TLS auth backend will check the certificate against all roles if no name is specified.\n\n","depth":6,"section_tag":"vault-cert-auth"},"vault-credential-lookup-rules":{"location":"vault-credential-manager.html#vault-credential-lookup-rules","title":"Credential lookup rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* /concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* /concourse/TEAM_NAME/foo_param\n\nVault credentials are actually key-value, so for ((foo)) Concourse will default to the field name value. You can specify the field to grab via . syntax, e.g. ((foo.bar)).\n\nIf the action is being run in the context of a pipeline (e.g. a check or a step in a build of a job), Concourse will first look in the pipeline path. If it's not found there, it will look in the team path. This allows credentials to be scoped widely if they're common across many pipelines.\n\nIf an action is being run in a one-off build, Concourse will only look in the team path.\n\nThe leading /concourse can be changed by specifying the following:\n\nCONCOURSE_VAULT_PATH_PREFIX=/some-other-prefix\n","depth":5,"section_tag":"vault-credential-lookup-rules"},"vault-credential-manager":{"location":"vault-credential-manager.html","title":"The Vault credential manager","text":"Concourse can be configured to pull credentials from a Vault instance.\n\nTo configure this, first configure the URL of your Vault server by setting the following env on the Running a web node:\n\nCONCOURSE_VAULT_URL=https://vault.example.com:8200\nYou may also need to configure the CA cert for Vault:\n\nCONCOURSE_VAULT_CA_CERT=path/to/ca.crt\nYou'll also need to configure how the web node authenticates with Vault - see Authenticating with Vault for more details as that step is quite involved.\n\n","depth":4,"section_tag":"vault-credential-manager"},"vault-periodic-token":{"location":"vault-credential-manager.html#vault-periodic-token","title":"Using a periodic token","text":"The simplest way to authenticate is by generating a periodic token:\n\n$ vault token create --policy concourse --period 1h\nKey                Value\n---                -----\ntoken              s.mSNnbhGAqxK2ZbMasOQ91rIA\ntoken_accessor     0qsib5YcYvROm86cT08IFxIT\ntoken_duration     1h\ntoken_renewable    true\ntoken_policies     [concourse default]\nChoose your --period wisely, as the timer starts counting down as soon as the token is created. You should also use a duration long enough to account for any planned web node downtime.\n\nOnce you have the token, just set the following env on the web node:\n\nCONCOURSE_VAULT_CLIENT_TOKEN=s.mSNnbhGAqxK2ZbMasOQ91rIA\nPeriodic tokens are the quickest way to get started, but they have one fatal flaw: if the web node is down for longer than the token's configured period, the token will expire and a new one will have to be created and configured. This can be avoided by using the approle auth backend.\n\n","depth":6,"section_tag":"vault-periodic-token"},"versioned_resources-table":{"location":"database-schema.html#versioned_resources-table","title":"versioned_resources","text":"This table contains versions of resources discovered by checking.  Versioned resources have attributes describing their version, metadata, resource type, and whether the version is enabled in the pipeline.\n\n","depth":4,"section_tag":"pipelineapi-objects"},"volume-collection":{"location":"garbage-collection.html#volume-collection","title":"Volume Collection","text":"Volume collection is quite a bit simpler than Container Collection.\n\nFirst, volumes are found for deletion. This is just a query for volumes that have NULL references for all four volume owners:\n\n* volumes (worker_resource_cache_id)\n\n* volumes (worker_base_resource_type_id)\n\n* volumes (worker_resource_cache_id)\n\n* volumes (worker_resource_cache_id)\n\nNext, each CREATED volume is transitioned to DESTROYING. This transition can fail if the volume is being used as the parent of a copy-on-write volume that is still in use (e.g. by a build).\n\nThen, for each volume for DESTROYING state, including those that were just transitioned, we execute the following in parallel (as with containers, there is a max-in-flight limit per worker):\n\n* First, look up the volume on the worker and destroy it if it's found.\n\n* Next, delete the volume from the database.\n\nAs with containers, if any part of the deletion sequence returns an error, the volume is skipped. A volume is only ever removed from the database when it's guaranteed that everything has been cleaned up.\n\n","depth":5,"section_tag":"volume-collection"},"volume-internals":{"location":"volume-internals.html","title":"Volumes","text":"","depth":3,"section_tag":"volume-internals"},"volume-lifecycle":{"location":"volume-internals.html#volume-lifecycle","title":"Lifecycle","text":"Volumes can be in one of 3 states; CREATING, CREATED, DESTROYING.\n\nCREATING, volumes are still being initialized in BaggageClaim and are not yet ready to be used. CREATING, volumes can only transition to CREATED.\n\nCREATED volumes are initialized in BaggageClaim and are ready to be used. CREATED volumes can only be transitioned to DESTROYING.\n\nDESTROYING volumes are marked for removal from BaggageClaim, and should no longer be used; they will be removed from the database when they no longer exist in the BaggageClaim server.\n\n","depth":4,"section_tag":"volume-lifecycle"},"volume-locality-strategy":{"location":"container-placement.html#volume-locality-strategy","title":"The volume-locality strategy","text":"When using volume-locality, the Running a web node places task step and put step containers on workers where a majority of their inputs are already present. This is the default strategy.\n\nThe advantage of this approach is that it reduces the likelihood that large artifacts will have to be streamed from one Running a worker node, through the Running a web node, and to the target Running a worker node. For large artifacts, this can result in quite a bit of overhead.\n\nThe disadvantage of this approach is that it can sometimes result in builds \"gravitating\" to a particular worker and overloading it, at least until the resource caches warm across the worker pool.\n\nIf your builds tend to be light on artifacts and heavy on task execution, you may want to try the The fewest-build-containers strategy instead.\n\n","depth":4,"section_tag":"volume-locality-strategy"},"volumes-container_id":{"location":"database-schema.html#volumes-container_id","title":"container_id","text":"The container this volume is associated with.\n\n","depth":4,"section_tag":"runtime"},"volumes-handle":{"location":"database-schema.html#volumes-handle","title":"handle","text":"The unique identifier of the volume in BaggageClaim.\n\n","depth":4,"section_tag":"runtime"},"volumes-state":{"location":"database-schema.html#volumes-state","title":"state","text":"The stage in the lifecycle of the volume. See Volumes.\n\n","depth":4,"section_tag":"runtime"},"volumes-table":{"location":"database-schema.html#volumes-table","title":"volumes","text":"The volumes table represents the set of all volumes across all the workers, and keeps track of their state such that no volume is ever left behind on a worker.\n\nVolumes have a handful of really important attributes.\n\n","depth":4,"section_tag":"runtime"},"volumes-worker_base_resource_type_id":{"location":"database-schema.html#volumes-worker_base_resource_type_id","title":"worker_base_resource_type_id","text":"If this volume is used for a worker base resource type this column points to it.\n\n","depth":4,"section_tag":"runtime"},"volumes-worker_name":{"location":"database-schema.html#volumes-worker_name","title":"worker_name","text":"The name of the worker where the volume located.\n\n","depth":4,"section_tag":"runtime"},"volumes-worker_resource_cache_id":{"location":"database-schema.html#volumes-worker_resource_cache_id","title":"worker_resource_cache_id","text":"If this volume is for a worker resource cache this column points to it.\n\n","depth":4,"section_tag":"runtime"},"volumes-worker_task_cache_id":{"location":"database-schema.html#volumes-worker_task_cache_id","title":"worker_task_cache_id","text":"If this volume is for a worker task cache this column points to it.\n\n","depth":4,"section_tag":"runtime"},"web-ingress":{"location":"concourse-web.html#web-ingress","title":"Ingress","text":"If your web nodes are going to be accessed over the network, you will need to set CONCOURSE_EXTERNAL_URL to a URL accessible by your consumers. If you don't set this property, logins will redirect to its default value of 127.0.0.1.\n\nIf your instance is available on the public internet, you may wish to prevent the Concourse UI from being nefariously embedded as an iframe by setting CONCOURSE_X_FRAME_OPTIONS to deny (to prevent any iframe embeddings) or sameorigin (to only allow iframe embeddings in pages served from the same subdomain). This protects against clickjacking.\n\nNote: If setting the value to allow-from, please note that not all browsers support this value and when not supported, the header is ignored by the browser.\n\n","depth":5,"section_tag":"web-ingress"},"web-node":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"web-operation":{"location":"concourse-web.html#web-operation","title":"Operation","text":"The web nodes themselves are stateless - they don't store anything on disk, and coordinate entirely using the database.\n\n","depth":4,"section_tag":"web-operation"},"web-prerequisites":{"location":"concourse-web.html#web-prerequisites","title":"Prerequisites","text":"Nothing special - the web node is a pretty simple Go application that can be run like a 12-factor app.\n\n","depth":4,"section_tag":"web-prerequisites"},"web-properties":{"location":"concourse-web.html#web-properties","title":"Properties","text":"CPU usage: peaks during pipeline scheduling, primarily when scheduling jobs. Mitigated by adding more web nodes. In this regard, web nodes can be considered compute-heavy more than anything else at large scale.\n\nMemory usage: not very well classified at the moment as it's not generally a concern. Give it a few gigabytes and keep an eye on it.\n\nDisk usage: none\n\nBandwidth usage: aside from handling external traffic, the web node will at times have to stream bits out from one worker and into another while executing Steps.\n\nHighly available: yes; web nodes can all be configured the same (aside from --peer-address) and placed behind a load balancer. Periodic tasks like garbage-collection will not be duplicated for each node.\n\nHorizontally scalable: yes; they will coordinate workloads using the database, resulting in less work for each node and thus lower CPU usage.\n\nOutbound traffic:\n\n* db on its configured port for persistence\n\n* db on its configured port for locking and coordinating in a multi-web node deployment\n\n* directly-registered worker nodes on ports 7777, 7788, and 7799 for checking resources, executing builds, and performing garbage-collection\n\n* other web nodes (possibly itself) on an ephemeral port when a worker is forwarded through the web node's TSA\n\nInbound traffic:\n\n* worker connects to the TSA on port 2222 for registration\n\n* worker downloads inputs from the ATC during fly execute via its external URL\n\n* external traffic to the ATC API via the web UI and The fly CLI\n\n","depth":4,"section_tag":"web-properties"},"web-running":{"location":"concourse-web.html#web-running","title":"Running","text":"The concourse CLI can run as a web node via the web subcommand.\n\nBefore running it, let's configure a local user so we can log in:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass\nCONCOURSE_MAIN_TEAM_LOCAL_USER=myuser\nThis will configure a single user, myuser, with the password mypass. You'll probably want to change those to sensible values, and later you may want to configure a proper auth provider - check out Auth \u0026 Teams whenever you're ready.\n\nNext, you'll need to configure the session signing key, the SSH key for the worker gateway, and the authorized worker key. Check Generating Keys to learn what these are and how they are created.\n\nCONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key\nCONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key\nCONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys\nFinally, web needs to know how to reach your Postgres database. This can be set like so:\n\nCONCOURSE_POSTGRES_HOST=127.0.0.1 # default\nCONCOURSE_POSTGRES_PORT=5432      # default\nCONCOURSE_POSTGRES_DATABASE=atc   # default\nCONCOURSE_POSTGRES_USER=my-user\nCONCOURSE_POSTGRES_PASSWORD=my-password\nIf you're running PostgreSQL locally, you can probably just point it to the socket and rely on the peer auth:\n\nCONCOURSE_POSTGRES_SOCKET=/var/run/postgresql\nNow that everything's set, run:\n\nconcourse web\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"web-running"},"whats-emitted":{"location":"metrics.html#whats-emitted","title":"What's emitted?","text":"This reference section lists of all of the metrics that Concourse emits. We don't include the warning and critical levels as they will keep changing as we optimise the system. To find those, please refer to the source of truth: the code.\n\n","depth":4,"section_tag":"whats-emitted"},"whats-encrypted":{"location":"encryption.html#whats-encrypted","title":"What's encrypted?","text":"The following values are expected to contain credentials, and so will be encrypted:\n\n* Resource sources, as they often contain private keys and other credentials for writing to (or simply granting access to) the resource.\n\n* Resource type sources, for the same reason as above, though this is probably a less common use case.\n\n* Pipeline vars and params, in case they contain sensitive information such as usernames and/or passwords.\n\n* Put step params and get step params are also encrypted, even though they rarely should contain credentials (they're usually in source).\n\n* Team auth configurations, as they often contain things like GitHub or other oAuth client secrets.\n\nNote that the actual implementation encrypts things in a more heavy-handed way than the above list implies. For example, pipeline configs are actually encrypted as one large blob.\n\nNotably, the following things are NOT encrypted:\n\n* Build logs. If your jobs are outputting credentials, encryption won't help you. We have chosen not to tackle this initially as it would introduce a performance burden for what is not as much of an obvious win.\n\n* Resource versions. These should never contain credentials, and are often meaningless on their own.\n\n* Resource metadata. These are visible to anyone if your pipeline is exposed, and should never contain credentials.\n\n* Pipeline names, job names, etc. - anything else that is not a high-risk target for credential leakage, as opposed to regular information leaks.\n\n  Resources and jobs in particular exist in their own tables, with their names in plaintext, and only their config encrypted. In this way, names are not protected, even though the pipeline config itself is also stored as one big encrypted blob.\n\n","depth":4,"section_tag":"whats-encrypted"},"worker containers":{"location":"metrics.html#worker containers","title":"worker containers","text":"The number of containers that are currently running on your workers.\n\nAttributes worker\n\n: The name of the worker.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"worker volumes":{"location":"metrics.html#worker volumes","title":"worker volumes","text":"The number of volumes that are currently present on your workers.\n\nAttributes worker\n\n: The name of the worker.\n\n\n\n\n\n","depth":4,"section_tag":"whats-emitted"},"worker-configuration":{"location":"concourse-worker.html#worker-configuration","title":"Configuration","text":"If there's something special about your worker and you'd like to target builds at it specifically, you can configure tags like so:\n\nCONCOURSE_TAG=tag-1,tag-2\nA tagged worker is taken out of the default placement logic. To run build steps on it, specify the tags step modifier. Or, to perform resource checks on it, specify tags on the resource itself.\n\n","depth":4,"section_tag":"worker-configuration"},"worker-heartbeating-and-stalling":{"location":"concourse-worker.html#worker-heartbeating-and-stalling","title":"Worker Heartbeating \u0026 Stalling","text":"Workers will continuously heartbeat to the Concourse cluster in order to remain registered and healthy. If a worker hasn't checked in in a while, possibly due to a network partition, being overloaded, or having crashed, its state will transition to stalled and new workloads will not be scheduled on it until it recovers.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":5,"section_tag":"worker-heartbeating-and-stalling"},"worker-internals":{"location":"worker-internals.html","title":"Workers","text":"","depth":3,"section_tag":"worker-internals"},"worker-lifecycle":{"location":"worker-internals.html#worker-lifecycle","title":"Lifecycle","text":"","depth":4,"section_tag":"worker-lifecycle"},"worker-node":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"worker-operation":{"location":"concourse-worker.html#worker-operation","title":"Operation","text":"The worker nodes are designed to be stateless and as interchangeable as possible. Tasks and Resources bring their own Docker images, so you should never have to install dependencies on the worker.\n\nIn Concourse, all important data is represented by Resources, so the workers themselves are dispensible. Any data in the work-dir is ephemeral and should go away when the worker machine is removed - it should not be persisted between worker VM or container re-creates.\n\n","depth":4,"section_tag":"worker-operation"},"worker-prerequisites":{"location":"concourse-worker.html#worker-prerequisites","title":"Prerequisites","text":"* Linux: kernel v3.19 or later with support for user namespaces enabled. (This is off by default in some distributions!)\n\n  To enforce memory limits on tasks, memory + swap accounting must be enabled.\n\n* Windows/Darwin: no special requirements (that we know of).\n\n  Note that containerization is fairly primitive on these two platforms, so don't expect great support for multi-tenancy.\n\n","depth":4,"section_tag":"worker-prerequisites"},"worker-properties":{"location":"concourse-worker.html#worker-properties","title":"Properties","text":"CPU usage: almost entirely subject to pipeline workloads. More resources configured will result in more checking, and in-flight builds will use as much CPU as they want.\n\nMemory usage: also subject to pipeline workloads. Expect usage to increase with the number of containers on the worker and spike as builds run.\n\nBandwidth usage: again, almost entirely subject to pipeline workloads. Expect spikes from periodic checking, though the intervals should spread out over enough time. Resource fetching and pushing will also use arbitrary bandwidth.\n\nDisk usage: arbitrary data will be written as builds run, and resource caches will be kept and garbage collected on their own life cycle. We suggest going for a larger disk size if it's not too much trouble. All state on disk must not outlive the worker itself; it is all ephemeral. If the worker is re-created (i.e. fresh VM/container and all processes were killed), it should be brought back with an empty disk.\n\nHighly available: not applicable. Workers are inherently singletons, as they're being used as drivers running entirely different workloads.\n\nHorizontally scalable: yes; workers directly correlate to your capacity required by however many pipelines, resources, and in-flight builds you want to run. It makes sense to scale them up and down with demand.\n\nOutbound traffic:\n\n* external traffic to arbitrary locations as a result of periodic resource checking and running builds\n\n* external traffic to the web node's configured external URL when downloading the inputs for a fly execute\n\n* external traffic to the web node's TSA port (2222) for registering the worker\n\nInbound traffic:\n\n* various connections from the Running a web node on port 7777 (Garden), 7788 (BaggageClaim), and 7799 (garbage collection)\n\n* repeated connections to 7788 and 7788 from the Running a web node's TSA component as it heartbeats to ensure the worker is healthy\n\n","depth":4,"section_tag":"worker-properties"},"worker-running":{"location":"concourse-worker.html#worker-running","title":"Running","text":"The concourse CLI can run as a worker node via the worker subcommand.\n\nFirst, you'll need to configure a directory for the worker to store data:\n\nCONCOURSE_WORK_DIR=/opt/concourse/worker\nThis is where all the builds run, and where all resources are fetched in to, so make sure it's backed by enough storage. Then, configure it like so:\n\nNext, point the worker at your Running a web node like so:\n\nCONCOURSE_TSA_HOST=127.0.0.1:2222\nCONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub\nCONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key\nFinally, run:\n\n# run with -E to forward env config, or just set it all as root\nsudo -E concourse worker\nNote that the worker must be run as root, as it orchestrates containers.\n\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"worker-running"},"worker-table-addr":{"location":"database-schema.html#worker-table-addr","title":"addr","text":"The full address of the Garden server running for the worker.\n\n","depth":4,"section_tag":"runtime"},"worker-table-baggageclaim-url":{"location":"database-schema.html#worker-table-baggageclaim-url","title":"baggageclaim_url","text":"The full address of the worker's baggageclaim server; used to store volumes on the worker.\n\n","depth":4,"section_tag":"runtime"},"worker-table-name":{"location":"database-schema.html#worker-table-name","title":"name","text":"The unique name for the worker used to identify it in the cluster.\n\n","depth":4,"section_tag":"runtime"},"worker-table-state":{"location":"database-schema.html#worker-table-state","title":"state","text":"The current state of the worker. This keeps track of the worker's lifecycle in order to handle the lifecycle of workers and their ability to have new containers or volumes created on them.\n\n","depth":4,"section_tag":"runtime"},"worker_base_resource_types-table":{"location":"database-schema.html#worker_base_resource_types-table","title":"worker_base_resource_types","text":"An entry in the worker base resource type table represents a location on a particular worker where the rootFS for a specific version of a base_resource_type is found. When a worker registers, its base_resource_types are synced by first removing old versions and then inserting new ones.\n\n","depth":4,"section_tag":"abstract-objects"},"worker_resource_caches-table":{"location":"database-schema.html#worker_resource_caches-table","title":"worker_resource_caches","text":"A worker resource cache is a join between a resource cache and a worker base resource type.\n\nA worker resource cache is automatically removed when its worker base resource type or resource cache is removed. This is used to automatically invalidate any caches that were fetched with an old version of a base resource type.\n\n","depth":4,"section_tag":"abstract-objects"},"worker_resource_config_check_sessions-table":{"location":"database-schema.html#worker_resource_config_check_sessions-table","title":"worker_resource_config_check_sessions","text":"A worker resource config check session is a join table tying a resource config check session to a worker base resource type. When either go away, the worker resource config check session goes away. This is so that containers using an old base resource type are reaped and recreated.\n\n","depth":4,"section_tag":"abstract-objects"},"worker_task_caches-table":{"location":"database-schema.html#worker_task_caches-table","title":"worker_task_caches","text":"The worker task caches table tracks all of the details of a task cache for a specific worker. Worker task caches are created for any task step in a job for all of the paths specified task cache.\n\nThis table is used to identify worker task cache volumes by their id.\n\n","depth":4,"section_tag":"runtime"},"workers-table":{"location":"database-schema.html#workers-table","title":"workers","text":"The workers table represents all of the workers that have registered with the Concourse cluster.\n\n","depth":4,"section_tag":"runtime"}}
