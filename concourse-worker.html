<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=775" />
<title>Running a worker node - Concourse CI</title>
<link rel="stylesheet" type="text/css" href="css/normalize.css" />
<link rel="stylesheet" type="text/css" href="css/pipeline.css" />
<link rel="stylesheet" type="text/css" href="css/iosevka.css" />
<link rel="stylesheet" type="text/css" href="css/booklit.css" />
<link rel="stylesheet" type="text/css" href="css/prism.css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<link href="https://fonts.googleapis.com/css?family=Barlow:400,400i,700|Roboto+Slab:300,400,700" rel="stylesheet" />
<script src="js/search.js"></script>
<script async type="text/javascript" src="js/prism.js"></script>
<script data-goatcounter="https://concourse.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
  </head>
  <body>
    

<div class="page-top">
  <nav class="top-nav">
    <a class="top-link logo-link" href="/"><img src="images/logo-white.svg" />Concourse</a>
    
    <a class="top-link active" href="docs.html">Docs</a>
    
    <a class="top-link" href="examples.html">Examples</a>
    
    <a class="top-link" href="project.html">Project</a>
    
    <a class="top-link" href="ecosystem.html">Ecosystem</a>
    
    <a class="top-link" href="https://resource-types.concourse-ci.org">Resource Types</a>
    <a class="top-link" href="https://blog.concourse-ci.org">blog</a>
    <a class="top-link" href="https://github.com/concourse/concourse/discussions">discuss</a>

    <div id="search"></div>

    <script type="text/javascript">
      var app = Elm.Main.init({
        node: document.getElementById('search')
      });

      app.ports.emitSearchTerm.subscribe(function(term) {
        goatcounter.count({
          path: "search:" + term,
          title: "user searched: " + term,
          event: true
        });
      });
    </script>
  </nav>
</div>

    <div class="page-body">
      <div class="page-nav">
        






  
  
  
  
  

  
  <div class="context">
    <nav>
      <div class="top">
        <span class="section-number">1</span><a href="docs.html"  class="active">Docs</a>
      </div>

      <div class="children">
        <table>
        
          <tr>
            <td class="number-cell">1.1</td>
            <td class="title-cell"><a href="getting-started.html" >Getting Started</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2</td>
            <td class="title-cell"><a href="install.html"  class="active">Install</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.3</td>
            <td class="title-cell"><a href="auth.html" >Auth &amp; Teams</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.4</td>
            <td class="title-cell"><a href="fly.html" >The <code>fly</code> CLI</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.5</td>
            <td class="title-cell"><a href="config-basics.html" >Config Basics</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.6</td>
            <td class="title-cell"><a href="pipelines.html" >Pipelines</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.7</td>
            <td class="title-cell"><a href="vars.html" >Vars</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.8</td>
            <td class="title-cell"><a href="resources.html" >Resources</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.9</td>
            <td class="title-cell"><a href="resource-types.html" >Resource Types</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.10</td>
            <td class="title-cell"><a href="jobs.html" >Jobs</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.11</td>
            <td class="title-cell"><a href="steps.html" >Steps</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.12</td>
            <td class="title-cell"><a href="tasks.html" >Tasks</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.13</td>
            <td class="title-cell"><a href="builds.html" >Builds</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.14</td>
            <td class="title-cell"><a href="how-to-guides.html" >How-To Guides</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.15</td>
            <td class="title-cell"><a href="operation.html" >Operation</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.16</td>
            <td class="title-cell"><a href="observation.html" >Observation</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.17</td>
            <td class="title-cell"><a href="internals.html" >Internals</a></td>
          </tr>
        
        </table>
      </div>
    </nav>
  </div>
  

  

  
  <div class="context">
    <nav>
      <div class="top">
        <span class="section-number">1.2</span><a href="install.html"  class="active">Install</a>
      </div>

      <div class="children">
        <table>
        
          <tr>
            <td class="number-cell">1.2.1</td>
            <td class="title-cell"><a href="postgresql-node.html" >Running a PostgreSQL node</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.2</td>
            <td class="title-cell"><a href="concourse-cli.html" >The <code>concourse</code> CLI</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.3</td>
            <td class="title-cell"><a href="concourse-generate-key.html" >Generating Keys</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.4</td>
            <td class="title-cell"><a href="concourse-web.html" >Running a <code>web</code> node</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.5</td>
            <td class="title-cell"><a href="concourse-worker.html"  class="self">Running a <code>worker</code> node</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.6</td>
            <td class="title-cell"><a href="upgrading-concourse.html" >Upgrading Concourse</a></td>
          </tr>
        
        </table>
      </div>
    </nav>
  </div>
  

  

  
  <div class="context">
    <nav>
      <div class="top">
        <span class="section-number">1.2.5</span><a href="concourse-worker.html"  class="self">Running a <code>worker</code> node</a>
      </div>

      <div class="children">
        <table>
        
          <tr>
            <td class="number-cell">1.2.5.1</td>
            <td class="title-cell"><a href="concourse-worker.html#worker-prerequisites" >Prerequisites</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.5.2</td>
            <td class="title-cell"><a href="concourse-worker.html#worker-running" >Running <code>concourse worker</code></a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.5.3</td>
            <td class="title-cell"><a href="concourse-worker.html#worker-operation" >Operating a <code>worker</code> node</a></td>
          </tr>
        
          <tr>
            <td class="number-cell">1.2.5.4</td>
            <td class="title-cell"><a href="concourse-worker.html#worker-configuration" >Configuring the <code>worker</code> node</a></td>
          </tr>
        
        </table>
      </div>
    </nav>
  </div>
  



      </div>

      <div class="body-content">
        <div class="page-content">
          <div class="section" id="section_concourse-worker">
  
  <h1 class="section-header"><a class="anchor-target" name="concourse-worker" href="#concourse-worker"></a><span class="section-number">1.2.5 </span>Running a <code>worker</code> node</h1>
  

  <p>The <code>worker</code> node registers with the <a href="concourse-web.html"><code>web</code> node</a> and is then used for executing builds and performing resource <code>check</code>s. It doesn&#39;t really decide much on its own.</p><div class="table-of-contents">
  <span class="toc-header">Table of contents:</span>

  
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.1</span>
      <a href="concourse-worker.html#worker-prerequisites">Prerequisites</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.2</span>
      <a href="concourse-worker.html#worker-running">Running <code>concourse worker</code></a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.2.1</span>
      <a href="concourse-worker.html#worker-resource-utilization">Resource utilization</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.3</span>
      <a href="concourse-worker.html#worker-operation">Operating a <code>worker</code> node</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.3.1</span>
      <a href="concourse-worker.html#scaling-workers">Scaling Workers</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.3.1.1</span>
      <a href="concourse-worker.html#horizontal-vs-vertical-scaling">Horizontal vs Vertical Scaling</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.3.2</span>
      <a href="concourse-worker.html#worker-heartbeating-and-stalling">Worker Heartbeating &amp; Stalling</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.3.3</span>
      <a href="concourse-worker.html#restarting-a-worker">Restarting a Worker</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.3.4</span>
      <a href="concourse-worker.html#gracefully-removing-a-worker">Gracefully Removing a Worker</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4</span>
      <a href="concourse-worker.html#worker-configuration">Configuring the <code>worker</code> node</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.4.1</span>
      <a href="concourse-worker.html#tagging-workers">Tagging Workers</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.2</span>
      <a href="concourse-worker.html#team-workers">Team Workers</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.4.2.1</span>
      <a href="concourse-worker.html#tags-and-team-workers">Tags and Team Workers</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.3</span>
      <a href="concourse-worker.html#worker-healthcheck-endpoint">Healthcheck Endpoint</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.4</span>
      <a href="concourse-worker.html#worker-resource-types">Resource Types</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.4.4.1</span>
      <a href="concourse-worker.html#bundled-resource-types">Bundled Resource Types</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.4.2</span>
      <a href="concourse-worker.html#installing-or-upgrading-bundled-resource-types">Installing or Upgrading Bundled Resource Types</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.5</span>
      <a href="concourse-worker.html#configuring-runtimes">Configuring Runtimes</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.4.5.1</span>
      <a href="concourse-worker.html#containerd-runtime"><code>containerd</code> runtime</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.5.2</span>
      <a href="concourse-worker.html#transitioning-from-guardian-to-containerd">Transitioning from Guardian to containerd</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.5.3</span>
      <a href="concourse-worker.html#guardian-runtime"><code>Guardian</code> runtime</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.5.4</span>
      <a href="concourse-worker.html#troubleshooting-and-fixing-dns-resolution">Troubleshooting and fixing DNS resolution</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.4.5.4.1</span>
      <a href="concourse-worker.html#pointing-to-external-dns-servers">Pointing to external DNS servers</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.5.4.2</span>
      <a href="concourse-worker.html#using-a-local-dns-server">Using a local DNS server</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.5.4.3</span>
      <a href="concourse-worker.html#note-on-allow-host-access">A note on allowing host access and DNS proxy</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
  </ol>
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.6</span>
      <a href="concourse-worker.html#configuring-peer-to-peer-volume-streaming">Configuring Peer-to-Peer Volume Streaming</a>

      
  
  <ol class="toc">
  
    <li>
      <span class="section-number">1.2.5.4.6.1</span>
      <a href="concourse-worker.html#p2p-worker-configuration">P2P Worker Configuration</a>

      
  
  
    </li>
  
    <li>
      <span class="section-number">1.2.5.4.6.2</span>
      <a href="concourse-worker.html#p2p-web-configuration">P2P Web Configuration</a>

      
  
  
    </li>
  
  </ol>
  
  
    </li>
  
  </ol>
  
  
    </li>
  
  </ol>
  
  
</div>

  
    
      <div class="section" id="section_worker-prerequisites">
  
  <h2 class="section-header"><a class="anchor-target" name="worker-prerequisites" href="#worker-prerequisites"></a>Prerequisites</h2>
  

  <ul>

  <li><p>Linux: We test and support the following distributions. Minimum kernel version tested is 4.4.</p><ul>

  <li><p>Ubuntu 16.04 (kernel 4.4)</p></li>

  <li><p>Ubuntu 18.04 (kernel 5.3)</p></li>

  <li><p>Ubuntu 20.04 (kernel 5.4)</p></li>

  <li><p>Debian 10 (kernel 4.19)</p></li>

</ul><p>Other Requirements: <ul>

  <li><p>User namespaces must be enabled.</p></li>

  <li><p>To enforce memory limits on tasks, memory &#43; swap accounting must be enabled.</p></li>

  <li><p>The <a href="concourse-worker.html#guardian-runtime"><code>Guardian</code> runtime</a> only supports cgroupsV1. Use the <a href="concourse-worker.html#containerd-runtime"><code>containerd</code> runtime</a> if you want to use cgroupsV2 or <a href="https://fedoraproject.org/wiki/Changes/CGroupsV2#Upgrade.2Fcompatibility_impact">downgrade to cgroupsV1</a>.</p></li>

</ul></p></li>

  <li><p>Windows/Darwin: no special requirements (that we know of).</p><blockquote class="aside"><p><strong>NOTE:</strong> Windows containers are currently not supported and Darwin does not have native containers. Steps will run inside a temporary directory on the Windows/Darwin worker. Any dependencies needed for your tasks (e.g. git, .NET, golang, ssh) should be pre-installed on the worker. Windows/Darwin workers do not come with any resource types.</p></blockquote></li>

</ul>

  
    
  
</div>
    
      <div class="section" id="section_worker-running">
  
  <h2 class="section-header"><a class="anchor-target" name="worker-running" href="#worker-running"></a>Running <code>concourse worker</code></h2>
  

  <p>The <code>concourse</code> CLI can run as a <code>worker</code> node via the <code>worker</code> subcommand.</p><p>First, you&#39;ll need to configure a directory for the worker to store data:</p><pre><code class="language-bash">CONCOURSE_WORK_DIR=/opt/concourse/worker</code></pre><p>This is where all the builds run, and where all resources are fetched in to, so make sure it&#39;s backed by enough storage.</p><p>Next, point the worker at your <a href="concourse-web.html"><code>web</code> node</a> like so:</p><pre><code class="language-bash">CONCOURSE_TSA_HOST=10.0.2.15:2222
CONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub
CONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key</code></pre><p>Finally start the worker:</p><pre><code class="language-bash"># run with -E to forward env config, or just set it all as root
sudo -E concourse worker</code></pre><p>Note that the worker must be run as <code>root</code> because it orchestrates containers.</p><p>All logs will be emitted to <code>stdout</code>, with any panics or lower-level errors being emitted to <code>stderr</code>.</p>

  
    
      <div class="section" id="section_worker-resource-utilization">
  
  <h3 class="section-header"><a class="anchor-target" name="worker-resource-utilization" href="#worker-resource-utilization"></a>Resource utilization</h3>
  

  <p><strong>CPU usage</strong>: almost entirely subject to pipeline workloads. More resources configured will result in more checking, and in-flight builds will use as much CPU as they want.</p><p><strong>Memory usage</strong>: also subject to pipeline workloads. Expect usage to increase with the number of containers on the worker and spike as builds run.</p><p><strong>Bandwidth usage</strong>: again, almost entirely subject to pipeline workloads. Expect spikes from periodic checking, though the intervals should spread out over enough time. Resource fetching and pushing will also use arbitrary bandwidth.</p><p><strong>Disk usage</strong>: arbitrary data will be written as builds run, and resource caches will be kept and garbage collected on their own life cycle. We suggest going for a larger disk size if it&#39;s not too much trouble. All state on disk must not outlive the worker itself; it is all ephemeral. If the worker is re-created (i.e. fresh VM/container and all processes were killed), it should be brought back with an empty disk.</p><p><strong>Highly available</strong>: not applicable. Workers are inherently singletons, as they&#39;re being used as drivers running entirely different workloads.</p><p><strong>Horizontally scalable</strong>: yes; workers directly correlate to your capacity required by however many pipelines, resources, and in-flight builds you want to run. It makes sense to scale them up and down with demand.</p><p><strong>Outbound traffic</strong>: <ul>

  <li><p>External traffic to arbitrary locations as a result of periodic resource checking and running builds</p></li>

  <li><p>External traffic to the <code>web</code> node&#39;s configured external URL when downloading the inputs for a <a href="tasks.html#running-tasks"><code>fly execute</code></a></p></li>

  <li><p>External traffic to the <code>web</code> node&#39;s TSA port (<code>2222</code>) for registering the worker</p></li>

  <li><p>If P2P streaming is enabled there will be traffic to other workers.</p></li>

</ul></p><p><strong>Inbound traffic</strong>: <ul>

  <li><p>From the <a href="concourse-web.html"><code>web</code> node</a> on port <code>7777</code> (Garden) and <code>7788</code> (BaggageClaim). These ports do not need to be exposed, they are forwarded to the web node via the ssh connection on port 2222.</p></li>

  <li><p>If P2P streaming is enabled there will be traffic to other workers.</p></li>

</ul></p>

  
    
  
</div>
    
  
</div>
    
      <div class="section" id="section_worker-operation">
  
  <h2 class="section-header"><a class="anchor-target" name="worker-operation" href="#worker-operation"></a>Operating a <code>worker</code> node</h2>
  

  <p>The <code>worker</code> nodes are designed to be stateless and as interchangeable as possible. <a href="tasks.html">Tasks</a> and <a href="resources.html">Resources</a> bring their own Docker images, so you should never have to install dependencies on the worker. Windows and Darwin workers are the exception to this. Any dependencies should be pre-installed on Windows and Darwin workers.</p><p>In Concourse, all important data is represented by <a href="resources.html">Resources</a>, so the workers themselves are dispensible. Any data in the work-dir is ephemeral and should go away when the worker machine is removed - it should not be persisted between worker VM or container re-creates.</p>

  
    
      <div class="section" id="section_scaling-workers">
  
  <h3 class="section-header"><a class="anchor-target" name="scaling-workers" href="#scaling-workers"></a>Scaling Workers</h3>
  

  <p>More workers should be added to accommodate more pipelines. To know when this is necessary you should probably set up <a href="metrics.html">Metrics</a> and keep an eye on container counts. If average container count starts to approach 200 or so per worker, you should probably add another worker. Load average is another metric to keep an eye on.</p><p>To add a worker, just create another machine for the worker and follow the <a href="concourse-worker.html#worker-running">Running <code>concourse worker</code></a> instructions again.</p><blockquote class="aside"><p><strong>Note</strong>: it doesn&#39;t make sense to run multiple workers on one machine since they&#39;ll both be contending for the same physical resources. Workers should be given their own VMs or physical machines to maximize resource usage.</p></blockquote>

  
    
      <div class="section" id="section_horizontal-vs-vertical-scaling">
  
  <h4 class="section-header"><a class="anchor-target" name="horizontal-vs-vertical-scaling" href="#horizontal-vs-vertical-scaling"></a>Horizontal vs Vertical Scaling</h4>
  

  <p>The answer to whether you should scale your workers horizontally or vertically depends heavily on what workloads your pipelines are running. Anecdotally though, we have seen that a lot of smaller workers (horizontal scaling) is usually better than a few large workers (vertical scaling).</p><p>Again, this is not an absolute answer! You will have to test this out against the workloads your pipelines demand and adjust based on the <a href="metrics.html">Metrics</a> that you are tracking.</p>

  
    
  
</div>
    
  
</div>
    
      <div class="section" id="section_worker-heartbeating-and-stalling">
  
  <h3 class="section-header"><a class="anchor-target" name="worker-heartbeating-and-stalling" href="#worker-heartbeating-and-stalling"></a>Worker Heartbeating &amp; Stalling</h3>
  

  <p>Workers will continuously heartbeat to the Concourse cluster in order to remain registered and healthy. If a worker hasn&#39;t checked in after a while, possibly due to a network error, being overloaded, or having crashed, the web node will transition its state to <code>stalled</code> and new workloads will not be scheduled on that worker until it recovers.</p><p>If the worker remains in this state and cannot be recovered, it can be removed using the <a href="administration.html#fly-prune-worker"><code>fly prune-worker</code></a> command.</p>

  
    
  
</div>
    
      <div class="section" id="section_restarting-a-worker">
  
  <h3 class="section-header"><a class="anchor-target" name="restarting-a-worker" href="#restarting-a-worker"></a>Restarting a Worker</h3>
  

  <p>Workers can be restarted in-place by sending <code>SIGTERM</code> to the worker process and starting it back up. Containers will remain running and Concourse will reattach to builds that were in flight.</p><p>This is a pretty aggressive way to restart a worker, and may result in errored builds - there are a few moving parts involved and we&#39;re still working on making this airtight.</p><p>A safer way to restart a worker is to <em>land</em> it by sending <code>SIGUSR1</code> to the <code>worker</code> process. This will switch the worker to the <code>landing</code> state and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the process will exit.</p><p>You may want to enforce a timeout for draining - that way a stuck build won&#39;t prevent your workers from being upgraded. This can be enforced by common tools like <code>start-stop-daemon</code>:</p><pre><code class="language-bash">start-stop-daemon \
  --pidfile worker.pid \
  --stop \
  --retry USR1/300/TERM/15/KILL</code></pre><p>This will send <code>SIGUSR1</code>, wait up to 5 minutes, and then send <code>SIGTERM</code>. If it&#39;s <em>still</em> running, it will be killed after an additional 15 seconds.</p><p>Once the timeout is enforced, there&#39;s still a chance that builds that were running will continue when the worker comes back.</p>

  
    
  
</div>
    
      <div class="section" id="section_gracefully-removing-a-worker">
  
  <h3 class="section-header"><a class="anchor-target" name="gracefully-removing-a-worker" href="#gracefully-removing-a-worker"></a>Gracefully Removing a Worker</h3>
  

  <p>When a worker machine is going away, it should be <em>retired</em>. This is similar to <em>landing</em>, except at the end the worker is completely unregistered, along with its volumes and containers. This should be done when a worker&#39;s VM or container is being destroyed.</p><p>To retire a worker, send <code>SIGUSR2</code> to the <code>worker</code> process. This will switch the worker to <code>retiring</code> state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the worker will be removed and the <code>worker</code> process will exit.</p><p>Just like with landing, you may want to enforce a timeout for draining - that way a stuck build won&#39;t prevent your workers from being upgraded. This can be enforced by common tools like <code>start-stop-daemon</code>:</p><pre><code class="language-bash">start-stop-daemon \
  --pidfile worker.pid \
  --stop \
  --retry USR2/300/TERM/15/KILL</code></pre><p>This will send <code>SIGUSR2</code>, wait up to 5 minutes, and then send <code>SIGTERM</code>. If it&#39;s <em>still</em> running, it will be killed after an additional 15 seconds.</p>

  
    
  
</div>
    
  
</div>
    
      <div class="section" id="section_worker-configuration">
  
  <h2 class="section-header"><a class="anchor-target" name="worker-configuration" href="#worker-configuration"></a>Configuring the <code>worker</code> node</h2>
  

  

  
    
      <div class="section" id="section_tagging-workers">
  
  <h3 class="section-header"><a class="anchor-target" name="tagging-workers" href="#tagging-workers"></a>Tagging Workers</h3>
  

  <p>If there&#39;s something special about your worker and you&#39;d like to target builds at it specifically, you can configure tags like so:</p><pre><code class="language-bash">CONCOURSE_TAG=&#34;tag-1,tag-2&#34;</code></pre><p>A tagged worker is taken out of the default placement logic. Tagged workers will not be used for any untagged <a href="steps.html">Steps</a>.</p><p>To run build steps on a tagged worker, specify the <a href="tags-step.html#schema.tags"><code class="schema-attribute-name"><bold>tags</bold></code></a> on any particular step in your <a href="jobs.html">job</a>.</p><p>To perform resource <code>check</code>s on on a tagged worker, specify <a href="resources.html#schema.resource.tags"><code>tags</code></a> on the resource declaration.</p>

  
    
  
</div>
    
      <div class="section" id="section_team-workers">
  
  <h3 class="section-header"><a class="anchor-target" name="team-workers" href="#team-workers"></a>Team Workers</h3>
  

  <p>If you want to isolate <a href="global-resources.html#complications-with-reusing-containers"><strong>all workloads</strong></a> for a <a href="managing-teams.html">team</a> then you can configure a worker to belong to a single team like so:</p><pre><code class="language-bash">CONCOURSE_TEAM=&#34;lightweavers&#34;</code></pre><p>Once an untagged team worker is registered Concourse will schedule all untagged builds for that team on its team worker(s). Builds for this team will no longer be scheduled on any untagged, non-team workers.</p><p>It is possible to have a Concourse cluster made up of only team workers and have zero non-team workers, though this is not a common setup because resource utilization across all workers ends up underutilized. It is useful though if you have a particular team with heavy workloads that usually bothers other teams pipelines.</p>

  
    
      <div class="section" id="section_tags-and-team-workers">
  
  <h4 class="section-header"><a class="anchor-target" name="tags-and-team-workers" href="#tags-and-team-workers"></a>Tags and Team Workers</h4>
  

  <p>When you have a worker configured with tag(s) and a team like so:</p><pre><code class="language-bash">CONCOURSE_TAG=&#34;tag-1,tag-2&#34;
CONCOURSE_TEAM=&#34;lightweavers&#34;</code></pre><p>Only steps that are tagged and from the specified team will be scheduled on such a worker. Any untagged work the team has will land on either:</p><ol>

  <li><p>Untagged team workers belonging to the team, or</p></li>

  <li><p>Untagged workers not configured to a specific team</p></li>

</ol>

  
    
  
</div>
    
  
</div>
    
      <div class="section" id="section_worker-healthcheck-endpoint">
  
  <h3 class="section-header"><a class="anchor-target" name="worker-healthcheck-endpoint" href="#worker-healthcheck-endpoint"></a>Healthcheck Endpoint</h3>
  

  <p>The worker will automatically listen on port <code>8888</code> as its healthcheck endpoint. It will return a <code>HTTP 200</code> status code with an empty body on a successful check. A successful check means the worker can reach the <a href="internals.html#architecture-worker">Garden and BaggageClaim servers</a>.</p><p>The healthcheck endpoint is configurable through three variables: <pre><code class="language-">--healthcheck-bind-ip=
IP address on which to listen for health checking requests. (default: 0.0.0.0)

--healthcheck-bind-port
Port on which to listen for health checking requests. (default: 8888)

--healthcheck-timeout
HTTP timeout for the full duration of health checking. (default: 5s)</code></pre></p>

  
    
  
</div>
    
      <div class="section" id="section_worker-resource-types">
  
  <h3 class="section-header"><a class="anchor-target" name="worker-resource-types" href="#worker-resource-types"></a>Resource Types</h3>
  

  <blockquote class="aside"><p>The following section only applies to Linux workers. Resource types are simply Linux container images and therefore can&#39;t be run on Windows or Darwin workers.</p></blockquote>

  
    
      <div class="section" id="section_bundled-resource-types">
  
  <h4 class="section-header"><a class="anchor-target" name="bundled-resource-types" href="#bundled-resource-types"></a>Bundled Resource Types</h4>
  

  <p>Workers come prepackaged with a bundle of resource types. They are included in the tarball from the <a href="https://github.com/concourse/concourse/releases">GitHub release page</a> and are part of the <a href="https://hub.docker.com/r/concourse/concourse">concourse/concourse image</a>.</p><p>To view the resource types available on a worker run:</p><pre><code class="language-bash">fly workers --details</code></pre><p>If you want more details, like the version number of each resource, you can run:</p><pre><code class="language-bash">fly curl api/v1/workers</code></pre>

  
    
  
</div>
    
      <div class="section" id="section_installing-or-upgrading-bundled-resource-types">
  
  <h4 class="section-header"><a class="anchor-target" name="installing-or-upgrading-bundled-resource-types" href="#installing-or-upgrading-bundled-resource-types"></a>Installing or Upgrading Bundled Resource Types</h4>
  

  <p>You may want to upgrade the bundled resource types outside of Concourse upgrades or even install additional resource types on your workers to reduce the polling on some external image repository like <a href="https://hub.docker.com/">Docker Hub</a>.</p><p>We will use the <a href="https://github.com/concourse/git-resource">git resource</a> as our example. We will assume your Concourse installation is at <code>/usr/local/concourse</code>.</p><p>First, pull and create a container of the resource you&#39;re installing/upgrading. Grab the ID of the container that Docker creates.</p><pre><code class="language-bash">$ docker run -d concourse/git-resource
b253417142565cd5eb43902e94a2cf355d5354b583fbc686488c9a153584c6ba</code></pre><p>Export the containers file system into a gzip compressed tar archive named <code>rootfs.tgz</code></p><pre><code class="language-bash">docker export b253417142 | gzip &gt; rootfs.tgz</code></pre><p>Create a file called <code>resource_metadata.json</code> and populate it with the following contents. Make sure the <code>type</code> does not conflict with an existing resource type when you&#39;re installing a new resource type. In our example here we&#39;re calling the type <code>gitv2</code> to avoid conflicting with the pre-existing <code>git</code> resource.</p><pre><code class="language-json">{
  &#34;type&#34;: &#34;gitv2&#34;,
  &#34;version&#34;: &#34;1.13.0&#34;,
  &#34;privileged&#34;: false,
  &#34;unique_version_history&#34;: false
}</code></pre><p>At this point you should have two files: <code>rootfs.tgz</code> and <code>resource_metadata.json</code>.</p><p>Create a new directory under the <code>resource-types</code> folder in your Concourse installation directory. By convention it should be the same name as the <code>type</code>.</p><pre><code class="language-bash">mkdir /usr/local/concourse/resource-types/gitv2</code></pre><p>Place the <code>rootfs.tgz</code> and <code>resource_metadata.json</code> inside the folder. <a href="concourse-worker.html#restarting-a-worker">Restart your worker</a> and verify the new resource type is on there by running one of the following commands:</p><pre><code class="language-bash">fly workers --details
# or
fly curl api/v1/workers</code></pre><p>You can also verify that Concourse can create a container with the <code>rootfs.tgz</code> you made by running a simple pipeline:</p><pre><code class="language-yaml">resources:
- name: some-resource
  type: gitv2 #change to your resource type
  source:
    uri: https://github.com/concourse/git-resource.git

jobs:
- name: simple-job
  plan:
  - get: some-resource</code></pre>

  
    
  
</div>
    
  
</div>
    
      <div class="section" id="section_configuring-runtimes">
  
  <h3 class="section-header"><a class="anchor-target" name="configuring-runtimes" href="#configuring-runtimes"></a>Configuring Runtimes</h3>
  

  <p>The worker can be run with multiple container runtimes - <a href="https://github.com/containerd/containerd/">containerd</a>, <a href="https://github.com/cloudfoundry/guardian">Guardian</a>, and <a href="https://github.com/vito/houdini">Houdini</a> (an experimental and the only runtime for Darwin and Windows). Only <code>containerd</code> and <code>Guardian</code> are meant for production use. <code>Guardian</code> is the default runtime for Concourse.</p><blockquote class="aside"><p><strong>Note about architecture</strong>: The web node (ATC) talks to all 3 runtimes via a single interface called the <a href="https://github.com/cloudfoundry/garden">Garden</a> server. While Guardian comes packaged with a Garden server and its flags in Concourse are unfortunately prefixed with <code>--garden-*</code>, Guardian (a runtime) and Garden (an interface and server) are two separate tools. An analogy for Garden would be the <a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface (CRI)</a> used in Kubernetes. Kubernetes uses containerd via CRI. Concourse uses containerd via Garden.</p></blockquote>

  
    
      <div class="section" id="section_containerd-runtime">
  
  <h4 class="section-header"><a class="anchor-target" name="containerd-runtime" href="#containerd-runtime"></a><code>containerd</code> runtime</h4>
  

  <p>To use the <code>containerd</code> runtime manually set the <code>--runtime</code> (<code>CONCOURSE_RUNTIME</code>) to <code>containerd</code> on the <code>concourse worker</code> command.</p><p>The following is a list of the <code>containerd</code> runtime specific flags for Concourse that can be set. They are all optional and have default values.</p><pre><code class="language-ini">Containerd Configuration:
  --containerd-config=                               Path to a config file to use for the Containerd daemon. [$CONCOURSE_CONTAINERD_CONFIG]
  --containerd-bin=                                  Path to a containerd executable (non-absolute names get resolved from $PATH). [$CONCOURSE_CONTAINERD_BIN]
  --containerd-init-bin=                             Path to an init executable (non-absolute names get resolved from $PATH). (default: /usr/local/concourse/bin/init) [$CONCOURSE_CONTAINERD_INIT_BIN]
  --containerd-cni-plugins-dir=                      Path to CNI network plugins. (default: /usr/local/concourse/bin) [$CONCOURSE_CONTAINERD_CNI_PLUGINS_DIR]
  --containerd-request-timeout=                      How long to wait for requests to Containerd to complete. 0 means no timeout. (default: 5m) [$CONCOURSE_CONTAINERD_REQUEST_TIMEOUT]
  --containerd-max-containers=                       Max container capacity. 0 means no limit. (default: 250) [$CONCOURSE_CONTAINERD_MAX_CONTAINERS]

Containerd Container Networking:
  --containerd-external-ip=                          IP address to use to reach container&#39;s mapped ports. Autodetected if not specified. [$CONCOURSE_CONTAINERD_EXTERNAL_IP]
  --containerd-dns-server=                           DNS server IP address to use instead of automatically determined servers. Can be specified multiple times. [$CONCOURSE_CONTAINERD_DNS_SERVER]
  --containerd-restricted-network=                   Network ranges to which traffic from containers will be restricted. Can be specified multiple times. [$CONCOURSE_CONTAINERD_RESTRICTED_NETWORK]
  --containerd-network-pool=                         Network range to use for dynamically allocated container subnets. (default: 10.80.0.0/16) [$CONCOURSE_CONTAINERD_NETWORK_POOL]
  --containerd-mtu=                                  MTU size for container network interfaces. Defaults to the MTU of the interface used for outbound access by the host. [$CONCOURSE_CONTAINERD_MTU]
  --containerd-allow-host-access                     Allow containers to reach the host&#39;s network. This is turned off by default. [$CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS]

DNS Proxy Configuration:
  --containerd-dns-proxy-enable                      Enable proxy DNS server. Note: this will enable containers to access the host network. [$CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE]</code></pre><div class="warning">
  <p>Make sure to read <a href="concourse-worker.html#note-on-allow-host-access">A note on allowing host access and DNS proxy</a> to understand the implications of using <code>--containerd-allow-host-access</code> and <code>--containerd-dns-proxy-enable</code></p>
</div>

  
    
  
</div>
    
      <div class="section" id="section_transitioning-from-guardian-to-containerd">
  
  <h4 class="section-header"><a class="anchor-target" name="transitioning-from-guardian-to-containerd" href="#transitioning-from-guardian-to-containerd"></a>Transitioning from Guardian to containerd</h4>
  

  <p>If you are transitioning from <code>Guardian</code> to <code>containerd</code> you will need to convert any <code>--garden-*</code> (<code>CONCOURSE_GARDEN_*</code>) flags to their <code>containerd</code> (<code>CONCOURSE_CONTAINERD_*</code>) counterparts:</p><table>
  
  <tr>
    
    <td>Guardian Flags</td>
    
    <td>Containerd Flags</td>
    
  </tr>
  
  <tr>
    
    <td><p><code>--garden-request-timeout</code> <code>CONCOURSE_GARDEN_REQUEST_TIMEOUT</code></p></td>
    
    <td><p><code>--containerd-request-timeout</code> <code>CONCOURSE_CONTAINERD_REQUEST_TIMEOUT</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><code>--garden-dns-proxy-enable</code> <code>CONCOURSE_GARDEN_DNS_PROXY_ENABLE</code></p></td>
    
    <td><p><code>--containerd-dns-proxy-enable</code> <code>CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><em>No equivalent CLI flag</em> <code>CONCOURSE_GARDEN_ALLOW_HOST_ACCESS</code></p></td>
    
    <td><p><code>--containerd-allow-host-access</code> <code>CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><code>--garden-network-pool</code> <code>CONCOURSE_GARDEN_NETWORK_POOL</code></p></td>
    
    <td><p><code>--containerd-network-pool</code> <code>CONCOURSE_CONTAINERD_NETWORK_POOL</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><code>--garden-max-containers</code> <code>CONCOURSE_GARDEN_MAX_CONTAINERS</code></p></td>
    
    <td><p><code>--containerd-max-containers</code> <code>CONCOURSE_CONTAINERD_MAX_CONTAINERS</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><em>No equivalent CLI flag</em> <code>CONCOURSE_GARDEN_DENY_NETWORKS</code></p></td>
    
    <td><p><code>--containerd-restricted-network</code> <code>CONCOURSE_CONTAINERD_RESTRICTED_NETWORK</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><em>No equivalent CLI flag</em> <code>CONCOURSE_GARDEN_DNS_SERVER</code></p></td>
    
    <td><p><code>--containerd-dns-server</code> <code>CONCOURSE_CONTAINERD_DNS_SERVER</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><em>No equivalent CLI flag</em> <code>CONCOURSE_GARDEN_EXTERNAL_IP</code></p></td>
    
    <td><p><code>--containerd-external-ip</code> <code>CONCOURSE_CONTAINERD_EXTERNAL_IP</code></p></td>
    
  </tr>
  
  <tr>
    
    <td><p><em>No equivalent CLI flag</em> <code>CONCOURSE_GARDEN_MTU</code></p></td>
    
    <td><p><code>--containerd-mtu</code> <code>CONCOURSE_CONTAINERD_MTU</code></p></td>
    
  </tr>
  
</table>

  
    
  
</div>
    
      <div class="section" id="section_guardian-runtime">
  
  <h4 class="section-header"><a class="anchor-target" name="guardian-runtime" href="#guardian-runtime"></a><code>Guardian</code> runtime</h4>
  

  <p>Guardian is currently the default runtime for Concourse. It can also be set by setting the <code>--runtime</code> flag to <code>guardian</code> on the <code>concourse worker</code> command.</p><p>The <code>concourse worker</code> command automatically configures and runs <code>Guardian</code> using the <code>gdn</code> binary, but depending on the environment you&#39;re running Concourse in, you may need to pop open the hood and configure a few things.</p><p>The <code>gdn</code> server can be configured in two ways:</p><ol>

  <li><p>By creating a <code>config.ini</code> file and passing it as <code>--garden-config</code> (or <code>CONCOURSE_GARDEN_CONFIG</code>).</p><p>The <code>.ini</code> file should look something like this:</p><pre><code class="language-ini">[server]
flag-name=flag-value</code></pre><p>To learn which flags can be set, consult <code>gdn server --help</code>. Each flag listed can be set under the <code>[server]</code> heading.</p></li>

  <li><p>By setting <code>CONCOURSE_GARDEN_*</code> environment variables.</p><p>This is primarily supported for backwards compatibility, and these variables are not present in <code>concourse worker --help</code>. They are translated to flags passed to <code>gdn server</code> by lower-casing the <code>*</code> portion and replacing underscores with hyphens.</p></li>

</ol>

  
    
  
</div>
    
      <div class="section" id="section_troubleshooting-and-fixing-dns-resolution">
  
  <h4 class="section-header"><a class="anchor-target" name="troubleshooting-and-fixing-dns-resolution" href="#troubleshooting-and-fixing-dns-resolution"></a>Troubleshooting and fixing DNS resolution</h4>
  

  <blockquote class="aside"><p><strong>Note</strong>: The Guardian runtime took care of a lot of container creation operations for Concourse in the past. It was very user-friendly for the project to use as a container runtime. While implementing the containerd runtime most reported bugs were actually a difference in containerd&#39;s default behaviour compared to Guardian&#39;s. Currently Concourse&#39;s containerd runtime <strong>mostly</strong> behaves like the Guardian runtime did. Most of the following DNS section should apply to both runtimes.</p></blockquote><p>By default, containers created by the Guardian or containerd (will refer to both as <em>runtime</em>) runtime will carry over the <code>/etc/resolv.conf</code> from the host into the container. This is often fine, but some Linux distributions configure a special <code>127.x.x.x</code> DNS resolver (e.g. <code>systemd-resolved</code>).</p><p>When the runtime copies the <code>resolv.conf</code> over, it removes these entries as they won&#39;t be reachable from the container&#39;s network namespace. As a result, your containers may not have any valid nameservers configured.</p><p>To diagnose this problem you can <a href="builds.html#fly-intercept"><code>fly intercept</code></a> into a failing container and check which nameservers are in <code>/etc/resolv.conf</code>:</p><pre><code class="language-bash">$ fly -t ci intercept -j concourse/concourse
bash-5.0$ grep nameserver /etc/resolv.conf
bash-5.0$</code></pre><p>In this case it is empty, as the host only listed a single <code>127.0.0.53</code> address which was then stripped out. To fix this you&#39;ll need to explicitly configure DNS instead of relying on the default runtime behavior.</p>

  
    
      <div class="section" id="section_pointing-to-external-dns-servers">
  
  <h5 class="section-header"><a class="anchor-target" name="pointing-to-external-dns-servers" href="#pointing-to-external-dns-servers"></a>Pointing to external DNS servers</h5>
  

  <p>If you have no need for special DNS resolution within your Concourse containers, you can configure your containers to use specific DNS server addresses external to the VM.</p><p>The Guardian and containerd runtimes can have their DNS servers configured with flags or envs vars.</p><div class="titled-codeblock">
  <div class="codeblock-title">
    <code>DNS Servers via flags (containerd runtime only)</code>
  </div>

  <pre><code class="language-bash">concourse worker --containerd-dns-server=&#34;1.1.1.1&#34; --containerd-dns-server=&#34;8.8.8.8&#34;</code></pre>
</div><p><div style="margin: 0 2em 1em" class="inset"></div></p><div class="titled-codeblock">
  <div class="codeblock-title">
    <code>DNS Servers via env vars</code>
  </div>

  <pre><code class="language-bash"># containerd runtime
CONCOURSE_CONTAINERD_DNS_SERVER=&#34;1.1.1.1,8.8.8.8&#34;
# Guardian runtime
CONCOURSE_GARDEN_DNS_SERVER=&#34;1.1.1.1,8.8.8.8&#34;</code></pre>
</div><p><div style="margin: 0 2em 1em" class="inset"></div></p><div class="titled-codeblock">
  <div class="codeblock-title">
    <code>config.ini (Guardian runtime only)</code>
  </div>

  <pre><code class="language-ini">[server]
; configure Google DNS
dns-server=8.8.8.8
dns-server=8.8.4.4</code></pre>
</div><p>To verify this solves your problem you can <a href="builds.html#fly-intercept"><code>fly intercept</code></a> into a container and check which nameservers are in <code>/etc/resolv.conf</code>:</p><pre><code class="language-bash">$ fly -t ci intercept -j my-pipeline/the-job
bash-5.0$ cat /etc/resolv.conf
nameserver 1.1.1.1
nameserver 8.8.8.8
bash-5.0$ ping google.com
PING google.com (108.177.111.139): 56 data bytes
64 bytes from 108.177.111.139: seq=0 ttl=47 time=2.672 ms
64 bytes from 108.177.111.139: seq=1 ttl=47 time=0.911 ms</code></pre>

  
    
  
</div>
    
      <div class="section" id="section_using-a-local-dns-server">
  
  <h5 class="section-header"><a class="anchor-target" name="using-a-local-dns-server" href="#using-a-local-dns-server"></a>Using a local DNS server</h5>
  

  <p>If you would like to use Consul, <code>dnsmasq</code>, or some other DNS server running on the worker VM, you&#39;ll have to configure the LAN address of the VM as the DNS server <em>and</em> allow the containers to reach the address, like so:</p><div class="titled-codeblock">
  <div class="codeblock-title">
    <code>Local DNS Servers via flags (containerd runtime only)</code>
  </div>

  <pre><code class="language-bash">concourse worker --containerd-dns-server=&#34;10.0.1.3&#34; --containerd-allow-host-access=&#34;true&#34;</code></pre>
</div><p><div style="margin: 0 2em 1em" class="inset"></div></p><div class="titled-codeblock">
  <div class="codeblock-title">
    <code>Local DNS Servers via env vars</code>
  </div>

  <pre><code class="language-bash"># containerd runtime
CONCOURSE_CONTAINERD_DNS_SERVER=&#34;10.0.1.3&#34;
CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS=&#34;true&#34;
# Guardian runtime
CONCOURSE_GARDEN_DNS_SERVER=&#34;10.0.1.3&#34;
CONCOURSE_GARDEN_ALLOW_HOST_ACCESS=&#34;true&#34;</code></pre>
</div><p><div style="margin: 0 2em 1em" class="inset"></div></p><div class="titled-codeblock">
  <div class="codeblock-title">
    <code>config.ini (Guardian runtime only)</code>
  </div>

  <pre><code class="language-ini">[server]
; internal IP of the worker machine
dns-server=10.0.1.3

; allow containers to reach the above IP
allow-host-access=true</code></pre>
</div><p><div style="margin: 0 2em 1em" class="inset"></div></p><div class="warning">
  <p>Make sure to read <a href="concourse-worker.html#note-on-allow-host-access">A note on allowing host access and DNS proxy</a> to understand the implications of using <code>allow-host-access</code></p>
</div><p>To validate whether the changes have taken effect, you can <a href="builds.html#fly-intercept"><code>fly intercept</code></a> into any container and check <code>/etc/resolv.conf</code> once again:</p><pre><code class="language-bash">$ fly -t ci intercept -j my-pipeline/the-job
bash-5.0$ cat /etc/resolv.conf
nameserver 10.1.2.3
bash-5.0$ nslookup concourse-ci.org
Server:         10.1.2.3
Address:        10.1.2.3#53

Non-authoritative answer:
Name:   concourse-ci.org
Address: 185.199.108.153
Name:   concourse-ci.org
Address: 185.199.109.153
Name:   concourse-ci.org
Address: 185.199.110.153
Name:   concourse-ci.org
Address: 185.199.111.153</code></pre><p>If <code>nslookup</code> times out or fails, you may need to open up firewalls or security group configuration so that the worker VM can send UDP/TCP packets to itself.</p>

  
    
  
</div>
    
      <div class="section" id="section_note-on-allow-host-access">
  
  <h5 class="section-header"><a class="anchor-target" name="note-on-allow-host-access" href="#note-on-allow-host-access"></a>A note on allowing host access and DNS proxy</h5>
  

  <p>Setting <code>allow-host-access</code> will, well, allow containers to access your host VM&#39;s network. If you don&#39;t trust your container workloads, you may not want to allow this.  With host network access, containers will be able to reach out to any other locally running network processes running on the worker including the garden and baggageclaim servers <strong>which would allow them to issue commands and manipulate other containers and volumes on the same worker</strong>.</p><p>Setting <code>dns-proxy-enable</code> will also enable <code>allow-host-access</code> (since the dns proxy will be run on the host, therefore requiring host access be enabled).</p>

  
    
  
</div>
    
  
</div>
    
  
</div>
    
      <div class="section" id="section_configuring-peer-to-peer-volume-streaming">
  
  <h3 class="section-header"><a class="anchor-target" name="configuring-peer-to-peer-volume-streaming" href="#configuring-peer-to-peer-volume-streaming"></a>Configuring Peer-to-Peer Volume Streaming</h3>
  

  <p>Peer-to-Peer (P2P) volume streaming enables the workers to stream volumes directly to each other instead of always streaming volumes through the web node(s). This can reduce the time it takes for individual steps in a job to start and reduce the amount of network traffic used by the Concourse cluster.</p><div class="warning">
  <p>NOTE: This feature is experimental. It is not as robust as the default volume streaming setup which always goes through web nodes.</p>
</div><p><strong>Pre-Requisites</strong></p><ul>

  <li><p>All worker nodes need to be able to reach each other via IP address. This usually means they are on the same LAN. You can test this by trying to ping one worker from another worker. If even one worker does not meet this requirement then you cannot use P2P volume streaming.</p></li>

  <li><p>The baggageclaim port (<code>7788</code> is the default) is open to traffic on all worker nodes. You can verify the port is open and reaching the baggageclaim API server by hitting the <code>/volumes</code> endpoint.</p><p><code>curl http://&lt;worker-IP-address&gt;:7788/volumes</code></p></li>

</ul><p>To enable P2P volume streaming you need to configure some settings on the web and worker nodes. Configure the worker nodes first. Configure the web node(s) last.</p>

  
    
      <div class="section" id="section_p2p-worker-configuration">
  
  <h4 class="section-header"><a class="anchor-target" name="p2p-worker-configuration" href="#p2p-worker-configuration"></a>P2P Worker Configuration</h4>
  

  <ul>

  <li><p><code>CONCOURSE_BAGGAGECLAIM_BIND_IP=0.0.0.0</code> - <em>Required.</em> The worker needs to listen for traffic over <code>127.0.0.1</code> (to receive info from the web node) as well as its LAN IP in a P2P setup. Therefore we need to set the IP baggageclaim binds to to <code>0.0.0.0</code>.</p></li>

  <li><p><code>CONCOURSE_BAGGAGECLAIM_P2P_INTERFACE_NAME_PATTERN=eth0</code> - <em>Optional.</em> Regular expression to match a network interface for P2P streaming. This is how a worker determines its own LAN IP address, by looking it up via the LAN interface specified by this flag.</p><p>You can determine the name of the LAN interface for any worker by listing all network interfaces and noting which interface has the LAN IP that you want the worker to use.</p><p>To view all available network interfaces on your worker: <ul>

  <li><p>On Linux run <code>ip addr list</code></p></li>

  <li><p>On MacOS run <code>ifconfig</code></p></li>

  <li><p>On Windows run <code>ipconfig</code>. Windows network interface names are very different from Unix device names. Example network interface names for Windows include: <pre><code class="language-">Ethernet 4
Local Area Connection* 2
Local Area Connection* 12
Wi-Fi 5
Bluetooth Network Connection 2
Loopback Pseudo-Interface 1</code></pre></p></li>

</ul></p></li>

  <li><p><code>CONCOURSE_BAGGAGECLAIM_P2P_INTERFACE_FAMILY=4</code> - <em>Optional.</em> Tells the worker to use IPv4 or IPv6. Defaults to <code>4</code> for IPv4. Set to <code>6</code> for IPv6.</p></li>

</ul>

  
    
  
</div>
    
      <div class="section" id="section_p2p-web-configuration">
  
  <h4 class="section-header"><a class="anchor-target" name="p2p-web-configuration" href="#p2p-web-configuration"></a>P2P Web Configuration</h4>
  

  <p>You need to tell the web node(s) to use P2P volume streaming.</p><p><code>CONCOURSE_ENABLE_P2P_VOLUME_STREAMING=true</code></p><p>Once that flag is set and the web node is restarted, P2P volume streaming will start occurring in your Concourse cluster.</p>

  
    
  
</div>
    
  
</div>
    
  
</div>
    
  
</div>

          <nav class="prev-next">
          
            <div class="prev">
              prev:

              
              <span class="section-number">1.2.4</span>
              

              <a href="concourse-web.html">Running a <code>web</code> node</a>
            </div>
          

          
            <div class="next">
              next:

              
              <span class="section-number">1.2.6</span>
              

              <a href="upgrading-concourse.html">Upgrading Concourse</a>
            </div>
          
          </nav>
        </div>

        <div class="page-aside"></div>
      </div>
    </div>

    <ul class="improve-docs">
      <li><img src="images/icons/github-box.svg" /> <a href="https://github.com/concourse/docs">help improve this site</a></li>
      <li><img src="images/booklit.svg" /> powered by <a href="https://vito.github.io/booklit">Booklit</a></li>
    </ul>
  </body>
</html>