{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Concourse CI","text":""},{"location":"resource-types-list/","title":"Resource Types","text":""},{"location":"resource-types-list/#resource-types","title":"Resource Types","text":"<p>This is a list of Resource Types that users of the community have written and made public for others to use. If you'd like to add a resource type that you've made, make a Pull Request in the <code>concourse/docs</code> repo.</p> Values in this column are visually hidden and are used for the search bar Name Description Add to Pipeline                metadata Returns the build metadata for a given build            metadata                Returns the build metadata for a given build in a <code>build.json</code> file.            <pre><code>- name: metadata\n  type: registry-image\n  source:\n    repository: pixelairio/metadata-resource\n</code></pre>                github-pr Let Concourse pipelines to interact with GitHub Pull Requests            github-pr                Let Concourse pipelines to interact with GitHub Pull Requests            <pre><code>- name: github-pr\n  type: registry-image\n  source:\n    repository: cfcommunity/github-pr-resource\n</code></pre>                slack-notifier A structured and opinionated Slack notification resource            slack-notifier                A structured and opinionated Slack notification resource            <pre><code>- name: slack-notifier\n  type: registry-image\n  source:\n    repository: mockersf/concourse-slack-notifier\n</code></pre>                keyval A resource for passing arbitrary data between steps/jobs and curating dynamic filesystem content            keyval                A resource for passing arbitrary data between steps/jobs and curating dynamic filesystem content            <pre><code>- name: keyval\n  type: registry-image\n  source:\n    repository: ghcr.io/cludden/concourse-keyval-resource\n</code></pre>                sentry-releases Manage releases in Sentry, can be used to upload sourcemaps.            sentry-releases                Manage releases in Sentry, can be used to upload sourcemaps.            <pre><code>- name: sentry-releases\n  type: registry-image\n  source:\n    repository: rubenv/concourse-sentry-releases-resource\n</code></pre>                semver Automated semantic version bumping            semver                Automated semantic version bumping            <pre><code>- name: semver\n  type: registry-image\n  source:\n    repository: concourse/semver-resource\n</code></pre>                appcenter-resource Concourse resource for distributing a build artifact to Microsoft App Center.            appcenter-resource                Concourse resource for distributing a build artifact to Microsoft App Center.            <pre><code>- name: appcenter-resource\n  type: registry-image\n  source:\n    repository: tomoyukim/concourse-appcenter-resource\n</code></pre>                s3 Concourse resource for interacting with AWS S3            s3                Concourse resource for interacting with AWS S3            <pre><code>- name: s3\n  type: registry-image\n  source:\n    repository: concourse/s3-resource\n</code></pre>                registry-image A resource for images in a Docker or OCI registry            registry-image                A resource for images in a Docker or OCI registry            <pre><code>- name: registry-image\n  type: registry-image\n  source:\n    repository: concourse/registry-image-resource\n</code></pre>                git Tracks commits in a branch of a Git repository            git                Tracks commits in a branch of a Git repository            <pre><code>- name: git\n  type: registry-image\n  source:\n    repository: concourse/git-resource\n</code></pre>                datadog-event Fetch or emit events to Datadog.            datadog-event                Fetch or emit events to Datadog.            <pre><code>- name: datadog-event\n  type: registry-image\n  source:\n    repository: concourse/datadog-event-resource\n</code></pre>                coralogix-event Sends build events to Coralogix            coralogix-event                Sends build events to Coralogix            <pre><code>- name: coralogix-event\n  type: registry-image\n  source:\n    repository: quay.io/coralogix/eng-coralogix-event-resource\n</code></pre>                artifactory-docker Concourse resource for triggering, getting and putting new versions of docker / container image artifacts within Artifactory repositories.            artifactory-docker                Concourse resource for triggering, getting and putting new versions of docker / container image artifacts within Artifactory repositories.            <pre><code>- name: artifactory-docker\n  type: registry-image\n  source:\n    repository: digitalocean/artifactory-docker-resource\n</code></pre>                prometheus-pushgateway Send metrics to Prometheus Push Gateway            prometheus-pushgateway                Send metrics to Prometheus Push Gateway            <pre><code>- name: prometheus-pushgateway\n  type: registry-image\n  source:\n    repository: michaellihs/prometheus-pushgateway-resource\n</code></pre>                sonarqube-notifier Gets Sonarqube results            sonarqube-notifier                Gets Sonarqube results            <pre><code>- name: sonarqube-notifier\n  type: registry-image\n  source:\n    repository: lgohr/sonarqube\n</code></pre>                irccat A resource to send notifications to irc via an irccat service            irccat                A resource to send notifications to irc via an irccat service            <pre><code>- name: irccat\n  type: registry-image\n  source:\n    repository: haiku/irccat-resource\n</code></pre>                sonarqube Performs SonarQube analyses and tracks the state of SonarQube quality gates.            sonarqube                Performs SonarQube analyses and tracks the state of SonarQube quality gates.            <pre><code>- name: sonarqube\n  type: registry-image\n  source:\n    repository: cathive/concourse-sonarqube-resource\n</code></pre>                apache-directory-index Tracks changes to an Apache Directory Index (e.g. http://mirror.easyname.ch/apache/tomcat/)            apache-directory-index                Tracks changes to an Apache Directory Index (e.g. <code>http://mirror.easyname.ch/apache/tomcat/</code>)            <pre><code>- name: apache-directory-index\n  type: registry-image\n  source:\n    repository: mastertinner/apache-directory-index-resource\n</code></pre>                pulumi Manages infrastructure via Pulumi            pulumi                Manages infrastructure via Pulumi            <pre><code>- name: pulumi\n  type: registry-image\n  source:\n    repository: ghcr.io/ringods/pulumi-resource\n</code></pre>                github-app-token Get an installation token for your GitHub App to access the GitHub API            github-app-token                Get an installation token for your GitHub App to access the GitHub API            <pre><code>- name: github-app-token\n  type: registry-image\n  source:\n    repository: tenjaa/concourse-github-app-token\n</code></pre>                gcs Concourse resource for interacting with Google Cloud Storage            gcs                Concourse resource for interacting with Google Cloud Storage            <pre><code>- name: gcs\n  type: registry-image\n  source:\n    repository: frodenas/gcs-resource\n</code></pre>                slack-alert A structured Slack notification resource for Concourse            slack-alert                A structured Slack notification resource for Concourse            <pre><code>- name: slack-alert\n  type: registry-image\n  source:\n    repository: arbourd/concourse-slack-alert-resource\n</code></pre>                datetime-version A resource to generate a date time version            datetime-version                A resource to generate a date time version            <pre><code>- name: datetime-version\n  type: registry-image\n  source:\n    repository: dcsg/datetime-version-resource\n</code></pre>                steampipe A resource for implementing a wide variety of triggers and data integrations via Steampipe and its expansive plugin ecosystem            steampipe                A resource for implementing a wide variety of triggers and data integrations via Steampipe and its expansive plugin ecosystem            <pre><code>- name: steampipe\n  type: registry-image\n  source:\n    repository: ghcr.io/cludden/concourse-steampipe-resource\n</code></pre>                bitbucket-pr Tracks pull requests on BitBucket and updates it's status            bitbucket-pr                Tracks pull requests on BitBucket and updates it's status            <pre><code>- name: bitbucket-pr\n  type: registry-image\n  source:\n    repository: n7docker/concourse-bitbucket-pr\n</code></pre>                chartmuseum Fetches, verifies and publishes Helm Charts from a running ChartMuseum instance. Harbor works as well, since it uses ChartMuseum under the hood.            chartmuseum                Fetches, verifies and publishes Helm Charts from a running ChartMuseum instance.               Harbor works as well, since it uses ChartMuseum under the hood.            <pre><code>- name: chartmuseum\n  type: registry-image\n  source:\n    repository: cathive/concourse-chartmuseum-resource\n</code></pre>                github-release A resource for github releases            github-release                A resource for github releases            <pre><code>- name: github-release\n  type: registry-image\n  source:\n    repository: concourse/github-release-resource\n</code></pre>                vrealize-automation Executes vRealize Automation pipelines            vrealize-automation                Executes vRealize Automation pipelines            <pre><code>- name: vrealize-automation\n  type: registry-image\n  source:\n    repository: projects.registry.vmware.com/concourse-vra-resource/concourse-vra-resource\n</code></pre>                artifactory-deb Fetch from, and publish to Debian/Ubuntu apt repositories in Artifactory.            artifactory-deb                Fetch from, and publish to Debian/Ubuntu apt repositories in Artifactory.            <pre><code>- name: artifactory-deb\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-artifactory-deb-resource\n</code></pre>                aptly-cli Enables you to transfer packages between your job and an aptly repository            aptly-cli                Enables you to transfer packages between your job and an aptly repository            <pre><code>- name: aptly-cli\n  type: registry-image\n  source:\n    repository: shyxormz/aptly-cli-resource\n</code></pre>                k8s Custom K8s resource type to deploy manifests to K8s kubernetes            k8s                Custom K8s resource type to deploy manifests to K8s            <pre><code>- name: k8s\n  type: registry-image\n  source:\n    repository: srinivasavasu/concourse-k8s\n</code></pre>                http-jq Exposing version of a resource over an HTTP endpoint and parsing it via jq into Concourse jobs            http-jq                Exposing version of a resource over an HTTP endpoint and parsing it via jq into Concourse jobs            <pre><code>- name: http-jq\n  type: registry-image\n  source:\n    repository: qudini/concourse-http-jq-resource\n</code></pre>                githubapps-content-resource A resource for clone github private repository with GithubApps credential.            githubapps-content-resource                A resource for clone github private repository with GithubApps credential.            <pre><code>- name: githubapps-content-resource\n  type: registry-image\n  source:\n    repository: ghcr.io/totegamma/githubapps-content-resource:master\n</code></pre>                digitalocean-kubernetes-resource Create, update and delete Digitalocean Kubernetes Clusters.            digitalocean-kubernetes-resource                Create, update and delete Digitalocean Kubernetes Clusters.            <pre><code>- name: digitalocean-kubernetes-resource\n  type: registry-image\n  source:\n    repository: kdihalas/digitalocean-kubernetes-resource\n</code></pre>                terraform Manages infrastructure via Terraform            terraform                Manages infrastructure via Terraform            <pre><code>- name: terraform\n  type: registry-image\n  source:\n    repository: ljfranklin/terraform-resource\n</code></pre>                slack-notification A resource for sending notifications to Slack            slack-notification                A resource for sending notifications to Slack            <pre><code>- name: slack-notification\n  type: registry-image\n  source:\n    repository: cfcommunity/slack-notification-resource\n</code></pre>                Artifactory Check, deploy and retrieve artifacts using the \"builds\" and \"artifact properties\" Artifactory features.            artifactory                Check, deploy and retrieve artifacts using the \"builds\" and \"artifact properties\" Artifactory features.            <pre><code>- name: artifactory\n  type: registry-image\n  source:\n    repository: springio/artifactory-resource\n</code></pre>                consul-kv Get or set a key/value pair in Consul's KV store.            consul-kv                Get or set a key/value pair in Consul's KV store.            <pre><code>- name: consul-kv\n  type: registry-image\n  source:\n    repository: clapclapexcitement/concourse-consul-kv-resource\n</code></pre>                cf Concourse resource for interacting with Cloud Foundry            cf                Concourse resource for interacting with Cloud Foundry            <pre><code>- name: cf\n  type: registry-image\n  source:\n    repository: concourse/cf-resource\n</code></pre>                bosh-config A resource for interacting with configs (Cloud, Runtime, CPI, etc\u2026) living on a Bosh server            bosh-config                A resource for interacting with configs (Cloud, Runtime, CPI, etc\u2026) living on a Bosh server            <pre><code>- name: bosh-config\n  type: registry-image\n  source:\n    repository: cfcommunity/bosh-config-resource\n</code></pre>                pool Atomically manages the state of the world (e.g. external environments)            pool                Atomically manages the state of the world (e.g. external environments)            <pre><code>- name: pool\n  type: registry-image\n  source:\n    repository: concourse/pool-resource\n</code></pre>                rubygems Fetch and publish Ruby gem packages to a RubyGems repository.            rubygems                Fetch and publish Ruby gem packages to a RubyGems repository.            <pre><code>- name: rubygems\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-rubygems-resource\n</code></pre>                repo Track changes for projects using Google's repo (Gerrit).            repo                Track changes for projects using Google's repo (Gerrit).            <pre><code>- name: repo\n  type: registry-image\n  source:\n    repository: mkorpershoek/repo-resource\n</code></pre>                helm-chart Download and publish helm charts from helm repositories.            helm-chart                Download and publish helm charts from helm repositories.            <pre><code>- name: helm-chart\n  type: registry-image\n  source:\n    repository: jghiloni/helm-chart-resource\n</code></pre>                kubectl Deploys resources to a Kubernetes cluster using \"kubectl apply -f\" command            kubectl                Deploys resources to a Kubernetes cluster using \"kubectl apply -f\" command            <pre><code>- name: kubectl\n  type: registry-image\n  source:\n    repository: jmkarthik/concourse-kubectl-resource\n</code></pre>                dhall Tracks the changes in a remote Dhall expression, and makes them available locally.            dhall                Tracks the changes in a remote Dhall expression, and makes them available locally.            <pre><code>- name: dhall\n  type: registry-image\n  source:\n    repository: quay.io/coralogix/eng-concourse-resource-dhall\n</code></pre>                fly Manipulate the Concourse fly command-line client.            fly                Manipulate the Concourse fly command-line client.            <pre><code>- name: fly\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-fly-resource\n</code></pre>                teams-notification Resource for pipeline notifications to Microsoft Teams            teams-notification                Resource for pipeline notifications to Microsoft Teams            <pre><code>- name: teams-notification\n  type: registry-image\n  source:\n    repository: navicore/teams-notification-resource\n</code></pre>                tfe Create/read/update workspace variables, create/apply runs, and               read root-level state outputs for Hashicorp Terraform Cloud and               Terraform Enterprise workspaces.            tfe                Create/read/update workspace variables, create/apply runs, and               read root-level state outputs for Hashicorp Terraform Cloud and               Terraform Enterprise workspaces.            <pre><code>- name: tfe\n  type: registry-image\n  source:\n    repository: orstensemantics/concourse-tfe-resource\n</code></pre>                maven Integrate your pipeline with any Maven Repository Manager (Artifactory, Nexus, GitHub Packages, etc.)            maven                Integrate your pipeline with any Maven Repository Manager (Artifactory, Nexus, GitHub Packages, etc.)            <pre><code>- name: maven\n  type: registry-image\n  source:\n    repository: nulldriver/maven-resource\n</code></pre>                rclone Publish arbitrary files and directories using Rclone.            rclone                Publish arbitrary files and directories using Rclone.            <pre><code>- name: rclone\n  type: registry-image\n  source:\n    repository: sothr/concourse-rclone-resource\n</code></pre>                static A resource to expose static information as a directory            static                A resource to expose static information as a directory            <pre><code>- name: static\n  type: registry-image\n  source:\n    repository: ktchen14/static-resource\n</code></pre>                awx Launches AWX job templates and workflows            awx                Launches AWX job templates and workflows            <pre><code>- name: awx\n  type: registry-image\n  source:\n    repository: quay.io/mamercad/concourse-awx-resource\n</code></pre>                blackduck Use Blackduck from Concourse            blackduck                Use Blackduck from Concourse            <pre><code>- name: blackduck\n  type: registry-image\n  source:\n    repository: lgohr/blackduck-resource\n</code></pre>                irc-notification Sends notification messages to an IRC channel            irc-notification                Sends notification messages to an IRC channel.            <pre><code>- name: irc-notification\n  type: registry-image\n  source:\n    repository: flavorjones/irc-notification-resource\n</code></pre>                gate A generic gate resource for Concourse CI. Allows you to model               quality gates and pipeline control flow.            gate                A generic gate resource for Concourse CI. Allows you to model               quality gates and pipeline control flow.            <pre><code>- name: gate\n  type: registry-image\n  source:\n    repository: meshcloud/gate-resource\n</code></pre>                github-pr-comment Monitors incoming comments on a Github Pull Request and is able               to monitor for comments matching regular expressions, match               comment author's association with the project, the pull request's               state and any labels it has been assigned.            github-pr-comment                Monitors incoming comments on a Github Pull Request and is able               to monitor for comments matching regular expressions, match               comment author's association with the project, the pull request's               state and any labels it has been assigned.             <pre><code>- name: github-pr-comment\n  type: registry-image\n  source:\n    repository: ndrjng/concourse-github-pr-comment-resource\n</code></pre>                webhook-notification Sends notification messages to services like Discord and Gitter               via webhook. Easily extensible to other services.            webhook-notification                Sends notification messages to services like Discord and Gitter               via webhook. Easily extensible to other services.            <pre><code>- name: webhook-notification\n  type: registry-image\n  source:\n    repository: flavorjones/webhook-notification-resource\n</code></pre>                time A resource for triggering on an interval            time                A resource for triggering on an interval            <pre><code>- name: time\n  type: registry-image\n  source:\n    repository: concourse/time-resource\n</code></pre>                capistrano Enables you to run Capistrano deployments from your pipeline            capistrano                Enables you to run Capistrano deployments from your pipeline            <pre><code>- name: capistrano\n  type: registry-image\n  source:\n    repository: shyxormz/capistrano-resource\n</code></pre>                github-list-repos Lists the repositories that belong to a GitHub organization or team, but does not clone them.            github-list-repos                Lists the repositories that belong to a GitHub organization or team, but does not clone them.            <pre><code>- name: github-list-repos\n  type: registry-image\n  source:\n    repository: quay.io/coralogix/concourse-resource-github-list-repos\n</code></pre>                bugsnag-build Notifies Bugsnag Build API of a new release            bugsnag-build                Notifies Bugsnag Build API of a new release            <pre><code>- name: bugsnag-build\n  type: registry-image\n  source:\n    repository: dcsg/bugsnag-build-resource\n</code></pre>                cf-cli The missing link between Concourse and Cloud Foundry. Push apps, create services, manage container networking and more!            cf-cli                The missing link between Concourse and Cloud Foundry. Push apps, create services, manage container networking and more!            <pre><code>- name: cf-cli\n  type: registry-image\n  source:\n    repository: nulldriver/cf-cli-resource\n</code></pre>                k8s-resource Tracks resources in a Kubernetes cluster            k8s-resource                Tracks resources in a Kubernetes cluster            <pre><code>- name: k8s-resource\n  type: registry-image\n  source:\n    repository: jgriff/k8s-resource\n</code></pre>                grafana-annotation Creates or updates a Grafana annotation            grafana-annotation                Creates or updates a Grafana annotation            <pre><code>- name: grafana-annotation\n  type: registry-image\n  source:\n    repository: gdsre/grafana-annotation-resource\n</code></pre>                pagerduty-incident Triggers PagerDuty incidents            pagerduty-incident                Triggers PagerDuty incidents (for example, on pipeline failure)            <pre><code>- name: pagerduty-incident\n  type: registry-image\n  source:\n    repository: ghcr.io/coralogix/eng-concourse-resource-pagerduty-incident\n</code></pre>                bosh-io-release Tracks BOSH releases published on bosh.io            bosh-io-release                Tracks BOSH releases published on https://bosh.io            <pre><code>- name: bosh-io-release\n  type: registry-image\n  source:\n    repository: concourse/bosh-io-release-resource\n</code></pre>                artifacthub Tracks and gets new versions of Helm Charts which are registered at artifacthub.io            artifacthub                Tracks and gets new versions of Helm Charts which are registered at https://artifacthub.io/            <pre><code>- name: artifacthub\n  type: registry-image\n  source:\n    repository: ghcr.io/hdisysteme/artifacthub-resource:latest\n</code></pre>                cron Implements a resource that reports new versions when the current time matches the crontab expression            cron                Implements a resource that reports new versions when the current time matches the crontab expression            <pre><code>- name: cron\n  type: registry-image\n  source:\n    repository: cftoolsmiths/cron-resource\n</code></pre>                http-resource Tracks resources from HTTP endpoints using custom version strategies (headers, body jq, hash, etc) from response            http-resource                Tracks resources from HTTP endpoints using custom version strategies (headers, body jq, hash, etc) from response            <pre><code>- name: http-resource\n  type: registry-image\n  source:\n    repository: jgriff/http-resource\n</code></pre>                hg Mercurial resource for Concourse            hg                Mercurial resource for Concourse            <pre><code>- name: hg\n  type: registry-image\n  source:\n    repository: concourse/hg-resource\n</code></pre>                openssl-source-code Concourse resource to track and fetch OpenSSL source code tarballs            openssl-source-code                Concourse resource to track and fetch OpenSSL source code tarballs            <pre><code>- name: openssl-source-code\n  type: registry-image\n  source:\n    repository: gstack/openssl-source-code-resource\n</code></pre>                ansible-playbook Execute ansible-playbook to provision remote systems.            ansible-playbook                Execute ansible-playbook to provision remote systems.            <pre><code>- name: ansible-playbook\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-ansible-playbook-resource\n</code></pre>                Cogito updates the GitHub status of a commit during a build. Simple configuration and lightweight image.            cogito                Cogito updates the GitHub status of a commit during a build. Simple configuration and lightweight image.            <pre><code>- name: cogito\n  type: registry-image\n  source:\n    repository: pix4d/cogito\n</code></pre>                docker-compose Use docker-compose to control Docker containers on remote hosts.            docker-compose                Use docker-compose to control Docker containers on remote hosts.            <pre><code>- name: docker-compose\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-docker-compose-resource\n</code></pre>                registry-tag A resource for image tags in a Docker registry            registry-tag                A resource for image tags in a Docker registry            <pre><code>- name: registry-tag\n  type: registry-image\n  source:\n    repository: ghcr.io/tlwr/registry-tag-resource\n</code></pre>                rocketchat-notification Send notification messages to RocketChat            rocketchat-notification                Send notification messages to RocketChat            <pre><code>- name: rocketchat-notification\n  type: registry-image\n  source:\n    repository: michaellihs/rocket-notify-resource\n</code></pre>                marathon Deploy applications to Marathon            marathon                Deploy applications to Marathon            <pre><code>- name: marathon\n  type: registry-image\n  source:\n    repository: ckaznocha/marathon-resource\n</code></pre>                wechat-notification-resource Send notification messages to WeChat            wechat-notification-resource                Send notification messages to WeChat            <pre><code>- name: wechat-notification-resource\n  type: registry-image\n  source:\n    repository: dockerhuangyisan/wechat-notification-resource\n</code></pre>                Artifactory Publish artifacts, such as tarballs, to Artifactory generic repositories.            artifactory                Publish artifacts, such as tarballs, to Artifactory generic repositories.            <pre><code>- name: artifactory\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-artifactory-resource\n</code></pre>                bitbucket-build-status Lets you update the build status for commits in Bitbucket            bitbucket-build-status                Lets you update the build status for commits in Bitbucket            <pre><code>- name: bitbucket-build-status\n  type: registry-image\n  source:\n    repository: shyxormz/bitbucket-build-status-resource\n</code></pre>                phraseapp A resource to trigger a job on changes on a PhraseApp project            phraseapp                A resource to trigger a job on changes on a PhraseApp project            <pre><code>- name: phraseapp\n  type: registry-image\n  source:\n    repository: tenjaa/concourse-phraseapp-resource\n</code></pre>                rss Tracks an RSS feed and provides the pubDate of items as new versions.            rss                Tracks an RSS feed and provides the pubDate of items as new versions.            <pre><code>- name: rss\n  type: registry-image\n  source:\n    repository: suhlig/concourse-rss-resource\n</code></pre>                docker-image A resource for docker images            docker-image                A resource for docker images            <pre><code>- name: docker-image\n  type: registry-image\n  source:\n    repository: concourse/docker-image-resource\n</code></pre>                bosh-io-stemcell Tracks BOSH stemcells published on bosh.io            bosh-io-stemcell                Tracks BOSH stemcells published on https://bosh.io            <pre><code>- name: bosh-io-stemcell\n  type: registry-image\n  source:\n    repository: concourse/bosh-io-stemcell-resource\n</code></pre>                helm3 Download and deploy Helm 3 charts to Kubernetes.            helm3                Download and deploy Helm 3 charts to Kubernetes.            <pre><code>- name: helm3\n  type: registry-image\n  source:\n    repository: typositoire/concourse-helm3-resource\n</code></pre>                concourse-webhook-resource Use files in git repo to track and throttle API calls as pipeline trigger.            concourse-webhook-resource                Use files in git repo to track and throttle API calls as pipeline trigger.            <pre><code>- name: concourse-webhook-resource\n  type: registry-image\n  source:\n    repository: ardavanhashemzadeh/concourse-webhook-resource\n</code></pre>                lastpass Tracks LastPass items            lastpass                Tracks LastPass items.            <pre><code>- name: lastpass\n  type: registry-image\n  source:\n    repository: ansd/lastpass\n</code></pre>                observability-event Publish events to VMware Tanzu Observability by Wavefront            observability-event                Publish events to VMware Tanzu Observability by Wavefront            <pre><code>- name: observability-event\n  type: registry-image\n  source:\n    repository: projects.registry.vmware.com/tanzu/observability-event-resource\n</code></pre>                hipchat-notification Send notification messages to HipChat            hipchat-notification                Send notification messages to HipChat            <pre><code>- name: hipchat-notification\n  type: registry-image\n  source:\n    repository: jgriff/hipchat-notification-resource\n</code></pre>                fossil Tracks commits in a branch of a Fossil repository            fossil                Tracks commits in a branch of a Fossil repository            <pre><code>- name: fossil\n  type: registry-image\n  source:\n    repository: avalos/fossil-concourse-resource\n</code></pre>                newrelic-deployment-resource A concourse resource for adding deployment markers in New Relic            newrelic-deployment-resource                A concourse resource for adding deployment markers in New Relic            <pre><code>- name: newrelic-deployment-resource\n  type: registry-image\n  source:\n    repository: shyamz22/newrelic-resource\n</code></pre>                mock A resource for testing. reflects the version it's told, and is able to mirror itself            mock                A resource for testing; reflects the version it's told, and is able to mirror itself            <pre><code>- name: mock\n  type: registry-image\n  source:\n    repository: concourse/mock-resource\n</code></pre>                key-value A resource that passes key-value pairs between jobs, using plain files in some directory            key-value                A resource that passes key-value pairs between jobs, using plain files in some directory            <pre><code>- name: key-value\n  type: registry-image\n  source:\n    repository: gstack/keyval-resource\n</code></pre>                semver-config Detect desired semantic version changes and retrieve a set of semantic version-based configs from one single YAML file.            semver-config                Detect desired semantic version changes and retrieve a set of semantic version-based configs from one single YAML file.            <pre><code>- name: semver-config\n  type: registry-image\n  source:\n    repository: itstarting/semver-config-concourse-resource\n</code></pre>                metadata Simple concourse resource which saves build metadata to a file               which may be used by tasks            metadata                Simple concourse resource which saves build metadata to a file               which may be used by tasks.            <pre><code>- name: semver-config\n  type: registry-image\n  source:\n    repository: ghcr.io/ardavanhashemzadeh/metadata-resource\n    tag: main\n</code></pre>                duct-tape Generic custom Concourse resource with which one can define the               check, in, and out as inline scripts right in the pipeline               resource definition. It is for when you quickly need a Concourse               resource for a specific task, but writing one from scratch would               take too long. With this resource, one can glue things together.            duct-tape                Generic custom Concourse resource with which one can define the               check, in, and out as inline scripts right in the pipeline               resource definition. It is for when you quickly need a Concourse               resource for a specific task, but writing one from scratch would               take too long. With this resource, one can glue things together.            <pre><code>- name: semver-config\n  type: registry-image\n  source:\n    repository: ghcr.io/homeport/duct-tape-resource\n</code></pre>                google-chat-alert A structured Google Chat notification resource for Concourse            google-chat-alert                A structured Google Chat notification resource for Concourse            <pre><code>- name: semver-config\n  type: registry-image\n  source:\n    repository: epic2/concourse-google-chat-alert-resource\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/2017-09-29-the-concourse-crew-2017/","title":"The Concourse Crew (2017)","text":"<p>In 2014 the Concourse CI project started with just two engineers; Alex Suraci and Chris Brown. At the time, both Alex and Chris were working on the Pivotal Cloud Foundry team. Over time, they became increasingly frustrated by existing CI/CD solutions. In response, Alex and Chris worked on designing a new CI/CD system in their spare time; imagining a new type of CI/CD system that would treat pipelines as first class citizens. After building some early prototypes and seeing early success internally within Pivotal, Concourse as released as an open source project with sponsorship from Pivotal.</p> <p>Fast forward to 2017 and the Concourse team has grown considerably. We now have 6 full time engineers (soon to be 8), a product manager (that\u2019s me!) and a product designer. The Concourse team is distributed across two countries (US and Canada); with a majority of the team working out of the Pivotal office in Toronto. Alex is still around and remains a key contributor to the project.</p> <p>The Concourse open source community has grown considerably as well. We do our best to to keep engaged with everyone through GitHub issues, Slack and StackOverflow. And now, using Medium we are going to try to do a better job at covering the bigger topics like: the Concourse roadmap, the Concourse philosophy, and more generally \u201chow things work\u201d.</p> <p>If you have any specific comments that you\u2019d like us to cover, comment below, hit me up on Twitter (@pioverpi) or reach out on the Concourse Slack (@jma)</p>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/","title":"How the Concourse Team Organize Issues","text":"<p>As the Concourse team continues to grow in size and in the # of incoming issues, the team has been experimenting with new ways of managing our backlog. So far we have tried three different setups:</p> <ol> <li>GitHub    issues + Customs/Tracksuit + Pivotal Tracker</li> <li>GitHub issues + aggressive labelling + CodeTree</li> <li>GitHub issues + GitHub Projects</li> </ol> <p>We\u2019ve been using the third setup, GitHub issues + GitHub Projects, for the past few months and we\u2019ve been mildly happy with the experience.</p>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#github-issues","title":"GitHub Issues","text":"<p>All issues are reported in through the concourse/concourse repo. Issues can include community-reported bugs, feature requests, technical chores, features, etc. If you want something done against the Concourse codebase, it gets reported there.</p> <p>Relying on a single location for all issues has the benefit of consolidating our backlogs; making it easier for the community to submit issues and track its progress. However with over 400 open issues against Concourse, the question becomes: how does the team decide what to work on first?</p> <p>Our first approach was to simply prioritize issues that were slated for the next release and burn through the list top-down. This naive approach became problematic for our growing team because of the incredible breadth of problems that Concourse covers. One day an engineer could be working on Elm and the next they would be working on our garbage collector. Even with pairing, new engineers found it very frustrating to master the codebase as they were constantly context-switching through thematically different issues.</p> <p>To address this, Alex Suraci bucketed our issues into five \u201cprojects\u201d:</p> <ul> <li>Operations</li> <li>Runtime</li> <li>Integrations</li> <li>Core</li> <li>UX</li> </ul> <p>By bucketing our issues into projects, engineers can now spend more time in thematically similar problem spaces in the Concourse codebase.</p>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#github-projects","title":"GitHub Projects","text":"<p>A snapshot of the Concourse UX project</p> <p>Our Concourse Projects manifest themselves as GitHub Projects in the Concourse GitHub organization. Annoyingly, GitHub doesn\u2019t (yet?) allow us to share these projects publicly.</p> <p>Each project has an engineering \u201canchor\u201d assigned to it. The anchor responsible for deeply understanding the issues under that project and usually sticks on the project for a long period of time. Each project also has its own roadmap and short-term goals.</p> <p>Every week we have an Iteration Planning Meeting (IPM) where we discuss the backlog for each project team. This is where our team discusses what\u2019s been done, what\u2019s in-flight, and what the upcoming issues are for the week ahead.</p> <p>I hope this post gives everyone in the community a bit of insight into how the Concourse team manages incoming issues and incoming work. We\u2019re hoping that Github announces improvements to the GitHub Projects system in their upcoming conference, GitHub Universe. If we aren\u2019t able to make our projects public in the near future, the Concourse team is committed to looking into alternative tools to publicly share our roadmap.</p> <p>For those who are interested, I\u2019ve also listed the specifics of our Five Concourse Project below:</p>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#the-five-concourse-projects","title":"The Five Concourse Projects","text":""},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#operations","title":"Operations","text":"<p>Ensuring Concourse is deployable and manageable in various environments, and able to meet organizations' authorization requirements.</p> <p>Subject matter:</p> <ul> <li>Various deployment scenarios (BOSH, binaries, Docker, Kubernetes, Windows, Darwin)</li> <li>Understanding resource demands of Concourse, both minimum requirements and \"at scale\"</li> <li>Systems knowledge to support and improve all of the above</li> <li>Multi-tenant operator demands (auth, inspectability)</li> </ul>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#runtime","title":"Runtime","text":"<p>Bring the theory to life. How do we go from a declarative configuration to efficiently running things across a pool of VMs?</p> <p>Subject matter:</p> <ul> <li>Containers: what &amp; why, what is their \u201ccost\u201d</li> <li>Copy-on-write volume management</li> <li>Scheduling to most efficiently utilize a pool of VMs</li> <li>Safely managing containers/volumes/etc. across VMs without leaking resources</li> </ul>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#integration","title":"Integration","text":"<p>Defining interfaces and patterns for how Concourse interacts with the real world.</p> <p>Subject matter:</p> <ul> <li>Supporting the core set of resources</li> <li>Establishing and documenting patterns for resources, watching out for anti-patterns</li> <li>Defining the best interfaces for extending Concourse; recognizing resources alone may not be enough</li> </ul>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#concourse-core","title":"Concourse Core","text":"<p>Pushing Concourse concepts forward by distilling customer needs into abstract primitives.</p> <p>Subject matter:</p> <ul> <li>Recognize that most feature requests are valid, but should not be implemented \u201cas-is\u201d</li> <li>Networking and pattern-matching; peel back the layers of the GitHub onion to identify common ground between various   issues</li> <li>Leave space for innovation and re-framing our existing concepts</li> <li>Define the REST API, and how pipelines/tasks are configured</li> </ul>"},{"location":"blog/2017-10-03-how-the-concourse-team-organize-issues/#ux","title":"UX","text":"<p>The face of Concourse \u2014 web UI, fly, and user research to find the best representations of what Concourse does and how users want to interact with it.</p> <p>Subject matter:</p> <ul> <li>Tie Concourse\u2019s pretentious high-level super-abstract concepts to users\u2019 needs around automation.</li> <li>Consume Concourse\u2019s API and ensure it doesn\u2019t get too bogged down in specific user flows.</li> <li>Drive feedback into the rest of the project through user research.</li> </ul>"},{"location":"blog/2017-10-26-build-page-improvements/","title":"Build Page Improvements","text":"<p>Concourse v3.6.0 comes with two new features on the build output page: timestamps and keyboard shortcuts.</p>"},{"location":"blog/2017-10-26-build-page-improvements/#timestamps-and-output-sharing","title":"Timestamps and Output\u00a0Sharing","text":"<p>When looking at the build page, you will now see timestamps reported against each line of output using your browser\u2019s reported timezone. As you hover over the timestamp, you can select single line of output or you can SHIFT select multiple lines of output. You\u2019ll also notice that the build page URL is updated to reflect the lines you have selected. You can use this URL to share specific build outputs with your team members.</p> <p>This feature addresses issue #361, #838 and #1423. Thank you for your patience!</p>"},{"location":"blog/2017-10-26-build-page-improvements/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<p>{{&lt; image src=\"/images/downloaded_images/Build-Page-Improvements/1-8-_eZ3qsDLB8Sqq5I-9vTw.png\" alt=\"\" width=\"50%\" &gt;}}</p> <p>The build page also supports basic vim-style keyboard shortcuts as well. You can bring up a handy reference menu using <code>?</code> or <code>SHIFT + /</code> if you\u2019re really having trouble finding it.</p> <p>The supported keyboard shortcuts are:</p> <ul> <li><code>h</code> and <code>l</code> for previous / next build</li> <li><code>j</code> and <code>k</code> for scrolling up and down</li> <li><code>T</code> to trigger a new build</li> <li><code>A</code> to abort the build</li> <li><code>gg</code> to scroll back to the top of the page</li> <li><code>G</code> to scroll to the bottom of the page</li> <li><code>?</code> to toggle the keyboard hint on and off</li> </ul> <p>This feature closes out issue #439</p>"},{"location":"blog/2017-11-01-sneak-peek-spatial-resources/","title":"Sneak Peek: Spatial Resources","text":"<p>An early visualization of Spatial Resources</p> <p>If you\u2019ve been paying close attention to our issues on GitHub you may have noticed a small flurry of activity around one specific issue: #1707 Spike: spatial resource flows.</p>"},{"location":"blog/2017-11-01-sneak-peek-spatial-resources/#what-are-spatial-resources-flows-aka-space","title":"What are Spatial Resources Flows (aka Space)?","text":"<p>The first reference to \u201cspatial\u201d resources came up in a proposal between Alex Suraci and Christopher Hendrix for Multi-branch workflows ( #1172). In that issue we focused specifically on one recurring problem: it\u2019s a real pain to deal with the Git resource when you have multiple branches representing different streams of work.</p> <p>Over time we researched similar build paradigms and thought deeply about generalized solutions that would fit nicely with the Concourse philosophy\u2122:</p> <p>Concourse makes it very easy to model changes over time. Resources do this for you; you point them at a given source of truth, and it\u2019ll let you know everything that happened.Some workflows, however, can\u2019t just be modeled as change over time. Multiple branches of a repo, or pull requests to a repository, are over space, not time. They are parallel pipelines, which today are hard to manage, and impossible to correlate (you cannot fan-in). Build matrixes are another example of wanting to run over many spaces (i.e. versions of a product). This can be done today, within a pipeline, but results in a massive pipeline with every combination explicitly configured... \u2026(a spatial resource) introduces the ability to have arbitrary build matrixes within one pipeline, which should dramatically improve the experience for people testing many variations/combinations.</p>"},{"location":"blog/2017-11-01-sneak-peek-spatial-resources/#would-you-like-to-know-more","title":"Would you like to know more?","text":"<p>There\u2019s a few ways you can get involved as the Concourse team continues to build out spaces:</p> <ol> <li>Read the proposal for spatial resources on GitHub here. Join in    on the discussion and \u2764\ufe0f the proposal if you\u2019re excited to see this happen!</li> <li>We\u2019re building out some common pipeline use cases to test out spaces. You can see our current list of proposals in    issue #1766. If you\u2019d like to add your own personal experiences    and pipeline use case, I\u2019d encourage you to add your notes on the issue.</li> <li>Spaces will introduce a lot of new abstractions into the pipeline visualization. We\u2019re still experimenting with the    visualization of spaces in the pipeline view; and are looking for fresh new ideas on how to visualize this. If you    have a sketch, a doodle, or even a simple recommendation; drop us a line on    the proposalor on our Slack.</li> </ol>"},{"location":"blog/2017-11-20-earning-our-wings/","title":"Earning our Wings","text":"<p>In April 2017 the Concourse Crew set up a large BOSH deployment of Concourse for internal use across Pivotal; we called this shared instance \u201cWings\u201d. We started on this endeavor to iterate and improve on Concourse as a large scale multi-tenant CI/CD solution, to drive out new features and to validate the Pivotal supported BOSH release of Concourse.</p> <p>After running an inception and talking about the goals, risks, and anti-goals, the team came up with the following goals:</p> <ul> <li>Better understand the \u2018operator\u2019 persona and the needs of an operator when running a multi-tenant Concourse</li> <li>Observe Concourse performance \u201cat scale\u201d</li> <li>Build out a set of recommendations for operationalizing, monitoring and logging a Concourse installation</li> <li>Identify common support issues and their solutions</li> </ul> <p>These goals led us to identify a number of risks involved:</p> <ul> <li>Scaling too aggressively before monitoring is sufficient or an operator \u201cgod view\u201d is in place.</li> <li>Not enough debugging information to gather in case of an incident</li> <li>Backing up user data in case of an emergency</li> </ul>"},{"location":"blog/2017-11-20-earning-our-wings/#day-1","title":"Day 1","text":"<p>With these goals and risks in mind, we set out on deploying Wings and providing teams to groups within Pivotal who requested access. Since we ran an inception to identify goals and risks, we were already aware some key features we would need as operators of a large deployment of Concourse.</p> <p>Our initial BOSH deployment on GCP had 2 ATCs, 5 Workers and a single PostgreSQL database instance.</p> <p>We mitigated the debugging and monitoring risks early on in the process of setting up Wings by adding metrics. Taking advantage of our existing deployed metrics infrastructure, the team configured the ATC, Workers and Database instances to report their stats to InfluxDB.</p> <p></p> <p>Grafana dashboards on metrics.concourse.ci</p> <p>We also configured a <code>blackbox</code> job for all of our ATCs and workers in the BOSH deployment manifest, which will send all of the logs to Papertrail.</p> <pre><code>blackbox_job: &amp;blackbox_job\n  name: blackbox\n  release: concourse\n  properties:\n    blackbox:\n      syslog:\n        destination:\n          transport: tls\n          address: ((papertrail_log_destination))\n</code></pre> <p>This job can be easily added to any of the instances we want to manage by referencing the YML anchor in the jobs list:</p> <pre><code>jobs:\n  - *blackbox_job\n</code></pre> <p>With our dashboard in place, we made it a habit to keep an eye on a few key indicators:</p> <ul> <li>The number of containers on any worker should not approach the 256 container limit</li> <li>Worker volumes and disk usage should remain relatively flat</li> <li>HTTP Response duration should remain flat ( and ideally below 100ms)</li> <li>Goroutines should not be leaked and pile up on the ATCs</li> </ul>"},{"location":"blog/2017-11-20-earning-our-wings/#day-2-and-beyond","title":"Day 2... and Beyond!","text":"<p>The \u201cDay 2\u201d experience of running a large Concourse brought us to some realizations about the scale of our deployment, and what we will need to mitigate the risks we outlined as we scale.</p> <p>Since one of our risks was \u201cBacking up user data in case of an emergency\u201d, and we saw some initial pressure and usage growth on the database, we quickly switched to a more highly available Google CloudSQL postgres instance. This immediately provided us with improved availability, scalability, automated backups, and automatic storage increases.</p> <p>Wings grew very quickly within the month of operations. Starting off with only 7 teams, Wings had close to 20 teams by the end of the first month. As we began to bear the burden of maintaining a large-scale multi-tenant Concourse for the whole of Pivotal we adopted a single mantra: \u201cuser pipelines aren\u2019t the problem\u201d. We recognized that even though they may cause operations and performance headaches, our users should be able to configure whatever pipelines they wish, and Concourse should support them.</p> <p>Since its more a \u201ccontinuous thing do-er\u201d than it is an opinionated CI/CD platform, we wanted users to let loose on out a large Concourse deployment, and experiment, which led to some interesting pipelines:</p> <p></p> <p>162 resources used by 2 tasks</p> <p>An initial reaction I personally had to this pipeline was \u201cum can you not?\u201d, but because we focused inward on operating and improving the product around users\u2019 pipelines, we found and resolved a number of issues:</p> <ol> <li>https://github.com/concourse/concourse/issues/1032</li> <li>https://github.com/concourse/concourse/issues/1297</li> <li>https://github.com/concourse/concourse/issues/1579</li> <li>https://github.com/concourse/concourse/issues/1589</li> </ol> <p>This is just a small set of highlights along the way, and these issues would have been much more difficult to diagnose without operating a large-scale deployment where users\u2019 pipelines are a black-box to us.</p> <p>We also identified the great need for more features in Concourse to support operators and open up some visibility into users\u2019 pipelines. The \u201coperator\u201d role as it exists currently can log in to the \u201cmain\u201d team, add more teams, and manage the deployment infrastructure... and that's about it. There\u2019s no easy way as an operator to gain insight into the types of containers on workers, running builds in a pipeline, the volumes cached in baggageclaim, and many more scenarios. This realization led to a much more serious consideration and discussion around Fine-grained, Role-based Access Control, specifically to support \u201coperator\u201d users.</p>"},{"location":"blog/2017-11-20-earning-our-wings/#service-level-objectives","title":"Service-Level Objectives","text":"<p>Even with our monitoring metrics in place, the Concourse Crew was still struggling with user-reported performance issues. If our metrics were to be believed, Wings seemed to be operating within acceptable parameters. However, we were still getting reports of slow pipelines, slow UI and build pages, and failures to hijack into containers.</p> <p>With our early learnings on operating Wings, we reached out to the Pivotal CloudOps team for help. Their advice to us was to define and track Service-Level Objectives for Wings.</p> <p>Following the practices learned by working closely with Google\u2019s Customer Reliability Engineering (CRE) initiative for Pivotal Cloud Foundry, the CloudOps team helped us define the targets we want to meet, based on key user-facing actions.</p> <p></p> <p>Wings\u2019 SLO Targets</p> <p>To monitor these objectives as Service Level Indicators (SLIs), we created a set of Concourse pipelines (of course) to perform the user facing action and report back it\u2019s success. These monitoring pipelines were hosted on a separate Concourse deployment and reported up to a Datadog a dashboard for office-wide visibility.</p> <p></p> <p>Wing\u2019s first month of SLO availability</p> <p>You can check out our pipeline setup for yourself to see how this \u201cConcourse pipeline to monitor a Concourse deployment\u201d works!</p> <p>The SLI dashboard led us to a series of investigations to improve Concourse performance. Starting off with our worst SLI (view Build History), we discovered a few issues:</p> <ul> <li>Chrome is particularly bad at rendering the data being provided by Elm. If you have really large build histories the   current workaround is to use Firefox #1543</li> <li>Our poor performance was in large-part due to inefficient queries against our PostgresDB. We were constantly hitting   our database connection limit on CloudSQL. After a lot of debugging, we were able to fix this issue   in #1734</li> </ul> <p>In addition to a few more optimizations, our Wings instance runs a lot smoother, which is reflected by our SLIs Dashboard.</p> <p></p> <p>Hitting 3 out of our 5 SLO targets in the month of November 2017</p>"},{"location":"blog/2017-11-20-earning-our-wings/#key-takeaways","title":"Key Takeaways","text":"<p>Today, Wings is operating with 3 ATCs, 30 workers, 5 external workers and 1 Cloud SQL instance on GCP. We support over 50 teams and hundreds of pipelines from all across Pivotal. We couldn\u2019t have done this without establishing a measure, learn and fix feedback loop on Wings. Getting all of this infrastructure took a lot of work and detracted from our \u201cusual\u201d operations of working on issues and stories. But, it was all worth it because we were able to fix a lot of nasty performance and stability issues by observing them happen in Wings.</p> <p>The level of operational stability we achieved on Wings has also opened up new possibilities for the Concourse team. We are now able to push experimental changes to Wings as a way to measure and observe changes at large-scales. This has allowed us to pre-emptively catch bugs in our new features, and to get feedback on new UX features at-scale.</p> <p>Great Success!</p>"},{"location":"blog/2017-11-27-designing-a-dashboard-for-concourse/","title":"Designing a Dashboard for Concourse","text":"<p>With the growing popularity of Concourse, we noticed that our development teams wanted to observe and monitor multiple pipelines simultaneously. This behavior wasn\u2019t limited to just Pivotal engineering teams; in fact, it was even more prevalent amongst our Open Source Community. Our users currently solve this by cramming multiple browser windows into TV their monitor view, or they use the Concourse Summary (aka Crystal) by David Goddard of the Pivotal Buildpacks team.</p> <p></p> <p>Concourse pipelines (left) Datadog and Concourse Summary (right) https://github.com/dgodd/concourse-summary</p> <p>So, we embarked on a deeper Discovery effort with the goal of understanding and evaluating our assumptions around how Concourse users were solving this problem today.</p> <p>At Pivotal, we believe that products\u2019 solutions need to be designed with the user in mind and we practice a style of user-centered design that progresses in four phases:</p> <ol> <li>Learning: How are Users are solving this problem today?</li> <li>Framing: Formulate a hypothesis based on your learnings. Create a prototype or experiment that is based on the    exploratory research.</li> <li>Assessing: Put your experiment in front of users to see if your hypothesis is right.</li> <li>Iterating: Repeat steps 1\u20133 to iterate on the solution as you get feedback.</li> </ol>"},{"location":"blog/2017-11-27-designing-a-dashboard-for-concourse/#learning-understanding-the-problem-space","title":"Learning: Understanding the Problem space","text":"<p>We began this process by thinking about the assumptions that were made about this feature and what we needed to validate in our interviews.</p> <ul> <li>Users are not satisfied with the current single pipeline view \u2014 especially on a monitor vs. a dedicated CI display   like a TV</li> <li>Only seeing red or green pipeline status is all that is important for pipeline summary</li> <li>Users want to understand the state of all their teams</li> <li>Users recognize their pipelines by their shape in the UI</li> </ul> <p></p> <p>Realtimeboard</p> <p>After we went out in the field to talk to users, we came back to synthesize our findings using this virtual whiteboard tool called Realtimeboard. One of our team members was remote, so this tool allowed us to easily collaborate on our research.</p> <p>\u201cDid it just turn red 10 seconds ago, or one week ago? I have no idea.\u201d \u2014 a Pipeline Engineer</p> <p>From our research, we found that users only care about failed jobs and the amount of time that their pipeline has been failing. This information is crucial for engineering teams as it is used to triage their pipeline errors and influences the prioritization of work. Many development teams we talked to are using a micro-service based architecture and therefore most of their pipelines are composed of four jobs (build, test, deploy and health check). While we assumed that the shape of a pipeline would be identifying, it was more important for the user to see the status.</p> <p>Based on the feedback we collected we began to prioritize our insights and frame our solution. We proceeded to brainstorm and sketch ideas for a prototype experiment.</p> <p></p> <p>Early Dashboard prototype</p> <p>Our first InVision prototype represented each team\u2019s pipeline as a series of thumbnails. We believed this approach would help users identify their pipelines, and at the same time have an at-a-glance view of the pipeline status. Our first round of feedback from users revealed that the thumbnail was not as useful as we had thought, and our approach made it more difficult to understand what the pipeline status was.</p> <p>So, we pivoted and started to explore the idea of a pipeline thumbnail that abstracts the current pipeline representation into a more substantial information radiator.</p> <p>Alex Suraci, co-creator of Concourse, had been working on a UI experiment, based on a treemap chart (below), that looked like something we could expand upon. I hypothesized that by removing the resources from this view and stripping down the thumbnail to just jobs we could provide the user with just enough information for \u2018at a glance\u2019 triaging.</p> <p></p> <p>Alex\u2019s pipeline treemap algorithm experiment</p> <p></p> <p>Thumbnail compression of the pipeline for the Concourse Dashboard</p> <p>This was a radical idea with significant departures from the current visual style of Concourse. We didn\u2019t want to just \u201cdo it\u201d and release it to our community of users without some kind of feedback first. As a product designer, my first inclination was to start drawing up thumbnail variations that we could test with our users. However, there was no clear taxonomy of pipelines because every team within Pivotal has a drastically different pipeline configuration. We needed a quick way to test this design with \u201crealistic\u201d pipeline configurations at scale. Luckily for us, the Concourse team runs an internally managed multi-tenant instance of Concourse called Wings. We use Wings as a sandbox for new features, so I paired with an engineer to do a lightweight implementation for Wings.</p> <p></p> <p>Since our initial rollout of the dashboard on Wings in September 2017, we have undergone at least 3 major revisions of the dashboard based on the feedback we had received from teams within Pivotal. Our next step was to incorporate this dashboard into the core product as a beta feature without disrupting users who are looking for a more stable Concourse experience.</p> <p>As of Concourse 3.5.0 you can find the dashboard under <code>/dashboard/</code> and as of Concourse 3.6.0 you can find the dashboard under <code>/beta/dashboard</code>. We hope you like this feature and are actively looking for feedback from the community.</p> <p>If you have a comment and want to participate in the conversation for the dashboard UI, please visit the issue in GitHub: https://github.com/concourse/concourse/issues/1829.</p>"},{"location":"blog/2017-12-01-concourse-at-springone-2017/","title":"Concourse at SpringOne 2017","text":"<p>by Pivotal</p> <p>Topher Bullock, Lindsay Auchinachie, Alex Suraci and I will be traveling to San Francisco next week to attend the SpringOne Platform 2017 Conference. Not only are there a lot of exciting Spring talks this year, but there will be a lot of CI/CD talks from some amazing speakers.</p> <p>Unfortunately none of the Concourse core contribution team will be speaking at SpringOne this year; but that doesn\u2019t mean there\u2019s a shortage of Concourse-related talks. Here\u2019s a short list of talks from people who will be sharing their real-world Concourse knowledge and experiences at SpringOne:</p> <ul> <li>Automated PCF Upgrades with Concourse   with Rich Ruedin from Express Scripts</li> <li>Building Developer Pipelines with PKS, Harbor, Clair and Concourse   with Thomas Kraus and Merlin Glynn from VMWare</li> <li>Concourse in the Real World: A Case Study in CI/CD and DevOp   s with Bryan Kelly and Greg Meyer from Cerner Corp</li> <li>Ensuring Platform Security with Windows BOSH Add-ons and Runtime Config at Boeing   with Sheryl Maris, Brad Schaefbauer and James Coppock from Boeing</li> <li>Enterprise CI/CD\u200a\u2014\u200aScaling the Build Pipeline at Home Depot   with Matt MacKenny from The Home Depot</li> <li>How to Continuously Delivery Your Platform with Concourse   with Brian Kirland from Verizon and Ryan Pei from Pivotal</li> </ul> <p>You can find a full list of sessions on the SpringOne sessions page here. Be sure to check out the DevOps, CI, CD</p> <p>Looking forward to seeing ya\u2019ll there!</p>"},{"location":"blog/2018-02-02-concourse-updates-jan-29--feb-2-2018/","title":"Concourse Updates (Jan 29 \u2014 Feb 2, 2018)","text":"<p>As a Product Manager at Pivotal, one of my responsibilities is to write weekly updates to let Pivots know what the Concourse team has been up to for the past week. When the Concourse team got together earlier this month for our 2018 planning, we decided that we should be sharing these updates with our community as a whole. So, without further ado, here\u2019s our first update of 2018!</p>"},{"location":"blog/2018-02-02-concourse-updates-jan-29--feb-2-2018/#features","title":"Features","text":"<p>UX</p> <ul> <li>Fixed https://github.com/concourse/concourse/issues/1978</li> <li>We gave a shot at doing lazy-loading and pagination of builds, but it didn\u2019t work very well. Reverting in lieu of some   more UX research on that   page https://github.com/concourse/concourse/issues/1855</li> </ul> <p>Core</p> <ul> <li>Currently looking for additional feedback on use-cases for Spatial Resources. If you have an opinion on this, **PLEASE   ** jump on this issue and   comment: https://github.com/concourse/concourse/issues/1766</li> <li>Continued work on refactoring auth providers model in preparation for Users in Concourse.   See https://github.com/concourse/concourse/issues/1991   and https://github.com/concourse/concourse/issues/1888</li> </ul> <p>Runtime</p> <ul> <li>Wrapped up work on \u201cBind-mount certs to resource containers at   <code>/etc/ssl/certs</code>\"https://github.com/concourse/concourse/issues/1938.   This was a tough one. Look forward to a post from Topher Bullock explaining some   of the nuances behind this implementation</li> </ul>"},{"location":"blog/2018-02-02-concourse-updates-jan-29--feb-2-2018/#design-research","title":"Design Research","text":"<ul> <li>Lindsay Auchinachie and Sam Peinado mocked   up a new \u201cHigh Density\u201d view of the Concourse   dashboard (https://github.com/concourse/concourse/issues/1899).   This new design would be an add-on to the current beta dashboard, and would be activated using a toggle in the status   bar.</li> <li>The design team is also beginning to research new designs to support adding comments to the current build page</li> <li>We\u2019re also beginning to work on new designs for the http://concourse.ci/ homepage! New year,   new look!</li> </ul>"},{"location":"blog/2018-02-02-concourse-updates-jan-29--feb-2-2018/#feedback","title":"Feedback","text":"<p>This is our first time posting updates publicly like this, so please let us know if they\u2019re helpful by giving us a \u201cclap\u201d or by responding to the story below! We also plan to announce a new portal where community members can follow along with our progress in the coming weeks, so look forward to more information coming your way!</p>"},{"location":"blog/2018-02-09-concourse-updates-feb-5--feb9/","title":"Concourse Updates (Feb 5 \u2014 Feb9)","text":"<p>We spent some time this week wrapping up additional testing on our certs management across workers. We also put down some of work on Spaces this week to play around with something fun: a high density dashboard view. A lot of you have been asking us when Concourse v3.9.0 will be available, and the answer is: very soon!</p> <p>On to the update:</p>"},{"location":"blog/2018-02-09-concourse-updates-feb-5--feb9/#features","title":"Features","text":"<p>UX</p> <ul> <li>High Density View! Original Git issue #1899 and a demo version   of it up and running can be found in our production environment</li> <li>Merged PR #227, thanks for the   contribution SwamWithTurtles!</li> </ul> <p>Runtime</p> <ul> <li>Picked up #2016 Move TSA beacon operations to \u2018worker\u2019. We need   to do this to fix some nasty behaviour we\u2019ve observed in our large scale Concourse   installation Pivotal</li> </ul> <p>Core</p> <ul> <li>Continued breaking out our backend auth systems to use   Dex #1888, (   see #1886for additional background)</li> </ul>"},{"location":"blog/2018-02-09-concourse-updates-feb-5--feb9/#design-research","title":"Design Research","text":"<ul> <li>Did some research and prototypes to see what build page commenting would look like and how it would behave in   Concourse</li> <li>Continuing some design prototyping for Concourse brand assets</li> </ul>"},{"location":"blog/2018-02-16-concourse-update-feb-1216/","title":"Concourse Update (Feb 12\u201316)","text":"<p>If you haven\u2019t heard the news by now, we released Concourse v3.9.0 this week \ud83c\udf89\ud83c\udf89\ud83c\udf89! Two of the top-line features in this release are:</p> <ul> <li>Concourse will now automatically propagate certificates from the worker machine into resource containers (GH   issue #1027)</li> <li>Improved btrfs volume driver stability. So if you\u2019re getting hit hard by overlay weirdness, I\u2019d suggest you give the   btrfs driver another shot!</li> </ul> <p>To find out what else we\u2019ve packed into this release, I\u2019d encourage you to read the full release notes on the concourse.ci/downloads page!</p> <p>On to the update...</p>"},{"location":"blog/2018-02-16-concourse-update-feb-1216/#features","title":"Features","text":"<p>UX</p> <ul> <li>Started to look at slow page-load times on the web-ui. The team identified that a large source of the pain came when   we introduced timestamps last year. We\u2019ve since been able to drastically improve the load times on that   page GH issue 1912</li> </ul> <p>Runtime</p> <ul> <li>As I mentioned last week, the Concourse team runs a relatively large installation of Concourse that is used by Pivotal   employees for internal projects. As a result of running this giant Concourse, we\u2019ve discovered that our Garbage   Collector needs significant improvement in order to keep up with the workloads that we\u2019ve been observing. GH   issue #2016 has been consuming a lot of our thoughts and   feelings this week.</li> </ul> <p>Core</p> <ul> <li>Same as last week: continued breaking out our backend auth systems to use   Dex #1888, (   see #1886for additional background)</li> <li>^^ Refactoring our complex backend to support individual auth is going to take some time, and we recognize that :)</li> </ul>"},{"location":"blog/2018-02-16-concourse-update-feb-1216/#design-research","title":"Design Research","text":"<p>We\u2019ve picked up design research work on Spatial Resources again. Lindsay Auchinachie and Sam Peinado are currently exploring different ways to visualize the (potentially) dense permutations and combinations of work.</p>"},{"location":"blog/2018-02-23-concourse-update-feb-2023/","title":"Concourse Update (Feb 20\u201323)","text":"<p>Monday, Feb 19 was Family Day for us here in Canada, so its been a relatively short work week for the Concourse team. With the release of v3.9.0 last week, we\u2019ve gotten some reports of new bugs and issues, so thanks to everyone who reported them in via our GitHub issues and Slack. Please make sure to check the updated release notes (here) for the full details! We\u2019re planning to cut a new patch release early next week with some of the fixes to the reported issues.</p> <p>On to the update:</p>"},{"location":"blog/2018-02-23-concourse-update-feb-2023/#features","title":"Features","text":"<p>UX</p> <ul> <li>We fixed issue #1912(slow build page due to timestamps)! The fix   for this should be rolled into the next patch release as well</li> <li>Started working on search hint and autocomplete on the Concourse   Dashboard #1713</li> <li>Tried adding buffering to fly outputs, it didn\u2019t help #1912</li> </ul> <p>Core</p> <ul> <li>Fixed an issue with noisy logging from skymarshall by lowering the the log   level #2044</li> <li>SURPRISE: we\u2019re still refactoring our backend to support users</li> </ul> <p>Operations</p> <ul> <li>Pulled in PR #2030so we could fix the BOSH deployment issue where   the ATC will fail due to function esc not being defined #2029</li> <li>Fixed a CredHub integration bug #2034</li> </ul>"},{"location":"blog/2018-03-02-concourse-update-feb-26--mar2/","title":"Concourse Update (Feb 26 \u2014 Mar2)","text":"<p>After some wrestling with our production pipelines last week we managed to release a patch update in the form of Concourse v3.9.1. We\u2019ve fixed some of the reported bugs from the previous release (3.9.0) so definitely go and check it out!</p> <p>I don\u2019t have much else regarding updates this week so here\u2019s a fun fact for you to chew on: did you know that Concourse uses Concourse to deploy Concourse? Its true! You can check out our publishing pipelines here: https://ci.concourse-ci.org/teams/main/pipelines/main?groups=publish</p>"},{"location":"blog/2018-03-08-were-switchin-domains/","title":"We\u2019re switchin\u2019 domains.","text":"<p>(UPDATE March 9 @ ~10 AM: The old domain appears to now be hosting a very old snapshot of our website. This is either targeted or part of a phishing scam. Do not go to it.)</p> <p>Well, that sucked.</p> <p>Wednesday morning I woke up to a ton messages because Concourse\u2019s site was gone, and in its place was a blank domain registrar placeholder.</p> <p>Before you say anything, I totally remembered to renew the domain. It was due to expire in August. Not even close to expiring.</p> <p>As far as I can tell, our registrar just didn\u2019t do the one thing it\u2019s supposed to do: renew the damned domain. They took the money, bumped the expiry date on the website, and\u2026apparently stopped there. They had literally one job and they didn\u2019t do it.</p> <p>So some Joe Schmoe out of Macedonia went ahead and registered it somewhere else, presumably to act as part of some spam network (the only thing set up were MX records). We contacted the new registrar\u2019s abuse email and they basically told us that the domain was registered normally, not transferred, and must have been available. And that there is nothing they can do.</p> <p>I contacted our registrar, and the latest word is this:</p> <p>We are contacting Domain Authorities in Ivory Coast to know more about this. I will contact you back as soon as possible.</p> <p>Soooo at this point I\u2019m calling the domain a loss. I\u2019m giving up pretty easily here for a reason: the .ci TLD is under the authority of the Ivory Coast and has next to zero legitimate registrars willing to reserve domains for it. I could tell from day one that my registrar was hot garbage, but didn\u2019t find any other choices.</p> <p>It seems like the registrar messed up so badly that there\u2019s not much leverage for getting it back. Even if I could get it back, I don\u2019t really want to deal with something like this again in the future. Luckily, before we got too big I went ahead and registered concourse-ci.org, .net, and .com in case something like this happened.</p> <p>So here's our new home: https://concourse-ci.org</p> <p>I\u2019d already been considering this switch for a while (anticipating trouble with .ci), but a more graceful transition would have been nice. Unfortunately there is a ton of material pointing to the old website, and it\u2019ll probably take time for the new location to bubble up in Google search results.</p> <p>I want to highlight that this doesn\u2019t seem to have been a targeted attack, but that you should be careful to not accidentally go to the old domain or send any traffic or emails there. It may not be a targeted attack, but the new owner still has full control over it, and they\u2019re receiving a bunch of free traffic. I wouldn\u2019t be surprised if they wisened up and pulled something nefarious.</p> <p>We really appreciate the support y\u2019all have shown us, and all the folks who offered to help. Sorry for the trouble.</p>"},{"location":"blog/2018-03-09-concourse-update-mar-59/","title":"Concourse Update (Mar 5\u20139)","text":"<p>Whelp, that felt like a long week. If you haven\u2019t heard the news by now you should definitely read Alex Suraci\u2019s post regarding our domain.</p> <p>I want to take this time to thank the Concourse fans out there who offered their help, support, and positive vibes throughout the whole ordeal. The team here really appreciates it \ud83d\ude4f</p> <p>Luckily for us this event didn\u2019t consume our entire engineering team. We WERE able to get some issues resolved this week and are planning for an imminent release of Concourse 3.9.2</p> <p>On to the update:</p> <p>UX:</p> <ul> <li>Resolved #1841 \u201cANSI cursor escapees wreak havoc with Concourse   build output\u201d</li> <li>Resolved #1999 where buttons weren\u2019t working on the build page   when using Firefox</li> <li>Experimented with adding scroll effects to pipeline names in the   Dashboard #2026. It was hilarious and many \\&lt;marquee&gt; jokes   were made.</li> <li>Brought back the/dashboard route to app #2051, which should make   you be able to login again #1801</li> </ul> <p>Docker Image Resource</p> <ul> <li>Fixed #170. According to GitHub, I\u2019m the original   author of that, but I honestly can\u2019t remember writing it. I do remember that it was supposed to help a Concourse   user, so hurrah!</li> </ul> <p>Runtime</p> <ul> <li>Resolved #2031 \u201ccannot_invalidate_during_initialization   constraint bubbles up to the user\u201d</li> <li>Resolved #2059   and#2058, two similar issues that influenced our decision to   make a 3.9.2 patch release</li> <li>Resolved #1499, tasks occasionally failing when interacting with   Vault</li> </ul>"},{"location":"blog/2018-03-16-concourse-update-mar-1216/","title":"Concourse Update (Mar 12\u201316)","text":"<p>Its been a first week in a long time where we were back to full strength and fully co-located. It was nice!</p> <p>Oh, and Concourse v3.9.2 was released this week as well,check it out!</p> <p>On to our update:</p> <p>UX</p> <ul> <li>Started to make our dashboard a bit more mobile friendly #1712</li> <li>Started to tackle the problem where our dashboard holds too many open   connections #1806</li> </ul> <p>Core</p> <ul> <li>Removed a dependancy to provide an external URL for fly   execute #2069. To   quote Alex Suraci:</li> </ul> <p>\u201cIt also makes the \u2018getting started with Docker\u2019 flow a bit complicated on platforms like Darwin where Docker is actually run via a Linux VM, in a separate namespace. fly execute can't be made to automatically work; the container IP would probably work for fly execute but isn't really what they should be setting as the external URL (as they can't reach it from their own machine).\u201d</p> <ul> <li>Continued our refactoring of Concourse APIs to support multiple teams</li> </ul> <p>Runtime</p> <ul> <li>Completed work on #2070, making it so that workers can retry   against multiple TSAs</li> <li>Picked up a reported issue around custom resources on tagged   workers #1371. We\u2019re not quite sure how it got so bad</li> </ul> <p>Docs</p> <p>We\u2019ve been working on a new website! The focus of it is to make it less flashy, less marketing, and more content driven. The design of it is still in progress but we want to start sharing it out very soon \ud83d\udc4d</p>"},{"location":"blog/2018-03-23-concourse-update-april-1923/","title":"Concourse Update (April 19\u201323)","text":"<p>Hi folks,</p> <p>Had an interesting week talking to customers about how we might improve their Concourse operations and deployments. More info on that soon!</p> <p>On to the update:</p> <p>UX</p> <ul> <li>Fixed an issue with timestamps #2088</li> </ul> <p>Core</p> <ul> <li>Continued our refactoring of the API to support dex and   users #1888</li> </ul> <p>Runtime</p> <ul> <li>Finished the issue around custom resources on tagged workers, it should work   now #1371</li> <li>Restricted the list of allowed TLS ciphers for more security   checkboxing #1997</li> </ul> <p>Design</p> <p>And now, some words from our Product Design team:</p> <p>Lindsay Auchinachie and Sam Peinado are continuing work on the Space and Causality features in Concourse.The Space features give users the ability to have arbitrary build matrixes within one pipeline and of a resource to solve for the pain around people testing many variations/combinations. Causality allows users a view into what is going through the pipeline, and how far it has made it through the pipeline. Read the proposals for Spatial Resource Flows and Resource Causality Flow on GitHub. This week we shared out our Space framing and talked through feasibility and technical constraints of this with the engineering team. Started work on a small Invision with the current Ruby and Git use case. Pairing with engineering next week to define additional more complex uses cases for coding a prototype with real pipeline.</p>"},{"location":"blog/2018-03-29-a-renewed-focus--community-changes/","title":"A renewed focus & community changes","text":"<p>Phew, we\u2019ve been busy for the past couple of months! There\u2019s a lot to give y\u2019all an update on.</p>"},{"location":"blog/2018-03-29-a-renewed-focus--community-changes/#new-website","title":"New website","text":"<p>First off, check out our new website! We\u2019ve completely redesigned it and redone how we organize the documentation, in hopes that it\u2019ll be much easier to find what you\u2019re looking for.</p> <p>We also hope that the new style, language, and tone will feel a bit more inclusive and humble. For example, we got rid of the \u201cConcourse vs.\u201d section \u2014 the effort it took to keep that up-to-date is better spent elsewhere. Use whatever dang tool you want! Our old site, as pretty as it was, felt a bit too much like we were trying to sell a finished product.</p> <p>We\u2019ve added an \u201cAbout\u201d page which provides all the background and motivation you should need to get a good idea of who we are and what we\u2019re about. There\u2019s also a \u201cContribute\u201d section which contains reference material for developers as well as general guidance. We\u2019re also fleshing out an \u201cOperation\u201d section which should help out those who are deploying Concourse for the first time or managing it at scale.</p> <p>In addition to these new sections, we\u2019ve also consolidated many pages and simplified the organization. There are now top-level sections for all the \u201cthings\u201d you\u2019ll be working with (Pipelines, Tasks, etc.), and each section contains the schema right up-front with examples to the side. This should make the docs much more effective when used as a reference.</p> <p>Search is back, and we\u2019ve made a lot better than it was before its unceremonious removal. Try searching \u201cimgrespar\u201d and you\u2019ll find <code>image_resource.params</code>. It\u2019s not full-text, but there\u2019s always Google for that. I tried but it\u2019s pretty  slow and janky.</p>"},{"location":"blog/2018-03-29-a-renewed-focus--community-changes/#community-platform-changes","title":"Community platform changes","text":"<p>Along with the new site, we\u2019re changing a few things in an effort to foster a healthier, more collaborative community:</p> <ul> <li>There\u2019s a new community forum! This will   be a much better format for support, long-form discussion, announcing cool new resource types, and whatever else y\u2019all   want to talk about.</li> <li>We\u2019re switching from Slack to Discord! We hope to have   this new chat platform be an organized place for contributors to have meaningful discussions, rather than a firehose   of help requests. There\u2019s still a #need-help channel, but we\u2019d prefer if most support went through the forums instead,   as persistent threads are much easier to keep tabs on and are much easier to find in Google search results.</li> <li>We\u2019ve got a publicly visible roadmap! This is thanks to a tool   called Cadet, which provides visibility into each of our GitHub projects (which are   normally hidden on GitHub). It also provides a networked view of issues and PRs that helps us identify the \u201cboulders\u201d   vs. the \u201cpebbles\u201d when it comes to understanding problem spaces to tackle.</li> </ul>"},{"location":"blog/2018-03-29-a-renewed-focus--community-changes/#simpler-deployment","title":"Simpler deployment","text":"<p>We\u2019ve coordinated all this with the launch of 3.10.0, which simplifies how Concourse is deployed. We\u2019ve made it easier to spin up a single-instance Concourse via the quickstart command, which we\u2019re in turn using for the quick intro on the front page, via Docker Compose. We also no longer require you to configure an external URL (which was the main obstacle in the way of a single-command intro).</p> <p>Instead of documenting four different deployment methods (and scaring away people in the process), we\u2019re focusing on the concourse binary distribution as the lingua franca on the main site. It\u2019s the most general and assumes the least about how you want to deploy it. For platform-specific documentation, each GitHub repo will be the source of truth:</p> <ul> <li>Concourse BOSH Deployment</li> <li>Concourse Docker</li> <li>Concourse Helm Chart (official soon)</li> </ul> <p>These repos are linked to by the \u201cDownload\u201d page as their own platform alongside the binaries, so they should feel just as official, while not feeling like a necessary mental hurdle for beginners.</p>"},{"location":"blog/2018-03-29-a-renewed-focus--community-changes/#halp","title":"HALP","text":"<p>Lastly, I want to apologize for the recent slowdown in processing pull requests. I\u2019ve been pretty focused on getting all this out there, and it\u2019s definitely taken away from my other duties.</p> <p>I hope that with our continued focus on community building in 2018, more of these responsibilities can be shared among a broader, stronger network of contributors. If you\u2019re interested in stepping up and helping out in a meaningful way, let us know early and we can help! That\u2019s part of the reason for introducing Discord and the forums.</p> <p>We\u2019re still figuring things out, and hope to provide more structure to the contribution process for those who need it, but a conversation is a great start.</p> <p>As always, thanks everyone for your patience and support.</p> <p>Alex</p>"},{"location":"blog/2018-04-06-concourse-updates-april-26/","title":"Concourse Updates (April 2\u20136)","text":"<p>If you haven\u2019t done so already please check out Alex Suraci\u2019s recent update post on \u201cA renewed focus &amp; community changes\u201d. It covers all the recent changes that we\u2019ve been making; starting with the new styling of the website, our new discussion forum, and our migration to Discord chat.</p> <p>Specifically, we\u2019ve been getting some mixed feedback on the new format of the site. Some folks love it, other folks miss the highly visual styling of the old site. As always, the Concourse team is always open to hearing your feedback in the usual channels. If you\u2019d like, you can even open issues against the docs repo itself here.</p> <p>And now, on to the update:</p> <p>UX:</p> <ul> <li>Finished up #1806 where our dashboard keeps spamming the ATC and   the db with connection requests.</li> </ul> <p>Core</p> <ul> <li>Started to spike on the spatial resource visualization, you can follow along   at #2131</li> </ul> <p>Runtime</p> <ul> <li>Tackling the large story on adding batch volume &amp; container deletion to a worker #2109</li> </ul> <p>PRs</p> <p>Apologies to everyone who\u2019s been waiting for feedback on their PRs. Alex Suraci has been working down the list this week; so we\u2019re slowly making our way down the list and merging them in.</p>"},{"location":"blog/2018-04-13-concourse-update-april-913/","title":"Concourse Update (April 9\u201313)","text":"<p>Concourse v3.11.0 came out today! Go get it: https://concourse-ci.org/download.html#v3110</p> <p>On another note: I\u2019ve noticed some interesting articles and guides come out on writing custom resources for Concourse. There\u2019s one on the Pivotal blog https://content.pivotal.io/blog/developing-a-custom-concourse-resource and another from fellow Medium writer Shin Myung Yoon (https://itnext.io/writing-a-custom-resource-for-concourse-detecting-pull-request-close-merge-events-e40468eb2a81)!</p> <p>Topher Bullock and I will also be travelling to Boston next week for CF Summit 2018. We\u2019ll be around to meet some Pivotal PCF customers and answer questions about Concourse. Make sure you visit some of the awesome talks on Concourse as well. You can find some articles about it on the Cloud Foundry blog:</p> <ul> <li>https://www.cloudfoundry.org/blog/5-cloud-foundry-summit-sessions-developers-build-cicd-practice/</li> <li>https://content.pivotal.io/blog/dialing-platform-ops-to-eleven-5-cloud-foundry-summit-sessions-that-operators-shouldnt-miss?_lrsc=96881456-a782-4c9e-999a-3be986b65b16&amp;utm_source=employee-social&amp;utm_medium=twitter&amp;utm_campaign=employee_advocacy</li> </ul> <p>Fun fact: we got some new stickers printed up, and we\u2019ll be handing some out at Summit:</p> <p></p> <p></p>"},{"location":"blog/2018-04-27-concourse-update-april-2327/","title":"Concourse Update (April 23\u201327)","text":"<p>Well, that was fun! Topher Bullock absolutely killed it last week on the CF Summit 2018 main stage with his demo of the experimental Concourse \u2764 \ufe0fK8s runtime project. We also had a great time talking to companies who were using Concourse to continuously do things in the cloud. One of my favorite talks was from Jason Immerman and Derek Van Assche from Zipcar (Concourse All the Things, All the Time); really inspirational stuff!</p> <p>Now, on to the update.</p> <p>We released Concourse v3.12.0 earlier this week. As usual it contains a lot of new improvements; but notably this release fixes the earlier memory leak reported in v3.11.0. A few things to highlight:</p> <ul> <li>We\u2019ve been doing some work behind the scenes to improve our GC behaviour on workers. To do this we\u2019ve started work on   distributing container/volume garbage-collection across   workers (#1959). This release has the early signs of this work,   and we expect it to be done in just a few more weeks (tm). However, this DOES mean that you\u2019ll need to open up port   7799 on the worker in order to have workers behave properly with v3.12.0 (please see release notes for more details!)</li> <li>We also pulled in a change that made the git tag fetch behavior toggle-able\u2026a lot of folks were hit by that so thanks   to GH user mdomke for the quick change!</li> <li>We think we finally hunted down some weird UI issues where certain versions of Chrome / Safari didn\u2019t let you click   into jobs. Let us know how that goes for ya\u2019ll when you upgrade!</li> </ul> <p>This week we\u2019ve been cranking away at three key areas:</p> <ul> <li>Working on the visualization to spatial   resources https://github.com/concourse/concourse/issues/2131</li> <li>Building out Users in Concourse with   dex https://github.com/concourse/concourse/issues/1888</li> <li>Distributing GC across   workers: https://github.com/concourse/concourse/issues/1959</li> </ul> <p>On the design front, our team has been working on something that we call \u201cdesign snacks\u201d; minor changes to the UI that could make big improvements to the overall experience with the app. Given our current tracks of work, we may not be able to pick them up right away, but at least the designs are attached to issues for contributors to pick up; if they felt so inclined :D</p> <p>A few examples:</p> <ul> <li>Concourse pipeline groups should be   responsive https://github.com/concourse/concourse/issues/2130</li> <li>Breadcrumb on Nav   Bar https://github.com/concourse/concourse/issues/2139</li> <li>Paused jobs should indicate they\u2019re paused on the build page(scroll to the bottom for that   one https://github.com/concourse/concourse/issues/1915</li> <li>Build-level   commenting https://github.com/concourse/concourse/issues/2025</li> </ul> <p>As always, feel free to jump into the discussion on our forum (https://discuss.concourse-ci.org/) or on Discord (https://discord.gg/MeRxXKW)</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/","title":"Getting Started with Concourse on macOS","text":"<p>If you haven\u2019t checked out Concourse yet, you definitely should! Simple primitives (Resources, Jobs, Tasks), a heavy emphasis on continuous workflows defined by YAML, and an active &amp; growing community are just some reasons why it\u2019s worth taking a look. This is a quick guide to getting an instance running on your machine in minutes.</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#my-setup","title":"My Setup","text":"<ul> <li>Docker (docker version 18.04.0-ce, build 3d479c0)</li> <li>Docker Machine (docker-machine version 0.14.0, build 89b8332)</li> <li>Docker Compose (docker-compose version 1.21.0, build unknown)</li> </ul> <p>As you can tell, I\u2019m using Docker and its tools to handle all the heavy lifting.</p> <p>Concourse has a GitHub repository dedicated to compatibility with Docker, so all that\u2019s needed is to grab the right bits and let it do its thing! \ud83d\ude4c</p> <p>Note</p> <p>Due to my lifestyle, I keep a few docker hosts active so I use Docker Machine. I\u2019ve noted the difference that will  occur with the newer Docker implementation so you should be fine either way.</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#preflight-setup","title":"Preflight Setup","text":"<p>Opening up the terminal and running <code>docker info</code> will make sure that Docker is ready and waiting.</p> <pre><code>$ docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 2\nServer Version: 18.03.1-ce\n</code></pre> <p>Note</p> <p>If you get an error or timeout, take a gander at the  Docker Machine docs.</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#setup-concourse","title":"Setup Concourse","text":"<p>Grab the quickstart Docker Compose file provided by Concourse and save it as <code>docker-compose.yml</code>.</p> <pre><code>$ wget -nv -O docker-compose.yml https://concourse-ci.org/docker-compose.yml\nURL:https://concourse-ci.org/docker-compose.yml [737/737] -&gt; \"docker-compose.yml\" [1]\n</code></pre> <p>Run <code>docker-compose up -d</code> (The <code>-d</code> flag will run the containers in the background. If you\u2019re interested in seeing the logs, feel free to omit.)</p> <pre><code>$ docker-compose up\nCreating network \"concourse-quickstart_default\" with the default driver\nPulling concourse-db (postgres:)...\nlatest: Pulling from library/postgres\n2a72cbf407d6: Pull complete\n...\nDigest: sha256:836e78858b76bac4ab3c36a43488b0038cc54d7d459618781e3544fbc15c0344\nStatus: Downloaded newer image for postgres:latest\nPulling concourse (concourse/concourse:)...\nlatest: Pulling from concourse/concourse\nd3938036b19c: Pull complete\n...\nDigest: sha256:9d91afe06ec5910ea7b187a6f4dbad9af0711334b19566e79667810f5adaa6ee\nStatus: Downloaded newer image for concourse/concourse:latest\nCreating concourse-quickstart_concourse-db_1 ... done\nCreating concourse-quickstart_concourse_1    ... done\n</code></pre> <p>By running <code>docker ps</code>, you can see that Docker Compose has conveniently setup two Docker containers; one for Concourse and another for Postgres. All the ports, permissions, and credentials have been taken care of for us.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                    NAMES\nb1a3e135ceed        concourse/concourse   \"/usr/local/bin/dumb\u2026\"   11 minutes ago      Up 11 minutes       0.0.0.0:8080-&gt;8080/tcp   concourse_concourse_1\n6a1ed198d47f        postgres              \"docker-entrypoint.s\u2026\"   11 minutes ago      Up 11 minutes       5432/tcp                 concourse_concourse-db_1\n</code></pre> <p>Note</p> <p>If there is something already occupying the default port, adjust the <code>docker-compose.yml</code> file and try again.</p> <p>That\u2019s a fully functional local Concourse installation in two commands! Not bad so far. Now comes the fun part.</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#accessing-concourse","title":"Accessing Concourse","text":"<p>Docker Machine will let you know the address of the host by running <code>docker-machine env | grep HOST</code>. Copy it and add the correct port for glory (ex. http://192.168.99.100:8080/).</p> <p>Note</p> <p>If you\u2019re using the newer Docker installation method for macOS, you should be able to go to http://127.0.0.1:8080/.</p> <p></p> <p>Now you should be able to see your first glimpse of Concourse!</p> <p>To install the Concourse CLI (<code>fly</code>) on your system, click on the Apple logo to download, and run the following commands...</p> <pre><code>$ cd ~/Downloads/\n$ install fly /usr/local/bin\n\n$ which fly\n/usr/local/bin/fly\n\n$ fly -v\n3.12.0\n</code></pre> <p>Last step is to login. Using the <code>fly login</code> command will help accomplish this.</p> <pre><code>$ fly login -t hello -u test -p test -c http://192.168.99.100:8080\nlogging in to team 'main'\n\ntarget saved\n</code></pre> <p>Note</p> <p>The <code>-t</code> flag is a target alias and is required for almost every command. This alias comes in very handy when you\u2019re targeting multiple concourse installations.</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#setting-up-a-pipeline","title":"Setting up a Pipeline","text":"<p>Grabbing a bare bones Concourse pipeline is a good way to start kicking the tires. Luckily, there are lots of examples to choose from in the Concourse repos.</p> <pre><code>$ wget -nv https://raw.githubusercontent.com/concourse/testflight/8db3bc5680073ec4eb3e36c8a2318297829b9ce0/pipelines/fixtures/simple.yml\nURL:https://raw.githubusercontent.com/concourse/testflight/8db3bc5680073ec4eb3e36c8a2318297829b9ce0/pipelines/fixtures/simple.yml [238/238] -&gt; \"simple.yml\" [1]\n</code></pre> <p>You can now use <code>fly set-pipeline</code> to upload this pipeline to Concourse.</p> <pre><code>$ fly -t hello set-pipeline -p hello-world -c simple.yml \njobs:\n  job simple has been added:\n    + name: simple\n    + plan:\n    + - task: simple-task\n    +   config:\n    +     platform: linux\n    +     image_resource:\n    +       type: docker-image\n    +       source:\n    +         repository: busybox\n    +     run:\n    +       path: echo\n    +       args:\n    +       - Hello, world!\n\napply configuration? [yN]: y\npipeline created!\nyou can view your pipeline here: http://192.168.99.100:8080/teams/main/pipelines/hello-world\n\nthe pipeline is currently paused. to unpause, either:\n  - run the unpause-pipeline command\n  - click play next to the pipeline in the web ui\n</code></pre> <p>Once completed, the output will let you know where to view the new pipeline. You\u2019ll be prompted to login in the Web Interface (GUI), but since this is a local instance the no authentication option is on by default. So you can just click through.</p> <p></p> <p></p> <p>You\u2019ll see a single job pipeline called \u201csimple\u201d and the top navigation will be blue. This color confirms that you\u2019re pipeline is paused.</p> <p>At this point, we\u2019re able to do conduct actions like <code>unpause-pipeline</code> and <code>trigger-pipeline</code> via the web interface or <code>fly</code>. For the sake of continuity, we\u2019ll stick to <code>fly</code>. It\u2019s also cool to see how the GUI reacts to your terminal commands. \ud83d\ude2c</p> <pre><code>$ fly -t hello unpause-pipeline -p hello-world\nunpaused 'hello-world'\n</code></pre> <p>The navigation bar should lose its blue colouring at this point. Indicating that it\u2019s ready to run jobs. This pipeline doesn\u2019t have any Resources that can trigger the \u201csimple\u201d job, so we\u2019ll use <code>fly trigger-job</code> to do it manually.</p> <pre><code>$ fly -t hello trigger-job -j hello-world/simple\nstarted hello-world/simple #1\n</code></pre> <p></p> <p>Concourse will schedule and run your job. Once complete, it ends with a mighty green box which is a developer\u2019s universal sign for victory.</p> <p>Clicking on this job will give you a more detailed view.</p> <p></p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#done","title":"Done","text":"<p>That\u2019s all it takes to start from scratch with Concourse. This is just the start, there is lots more to explore to take full advantage of the opportunities it presents. Hope this helps, and thanks for reading! \ud83d\ude0a\ud83d\ude47\u200d\u2642\ufe0f</p>"},{"location":"blog/2018-04-28-getting-started-with-concourse-on-macos/#ps","title":"P.S.","text":"<p>If you <code>fly -t hello destroy-pipeline -p hello-world</code>and refresh your browser, you can check out its cool 404 page. \ud83d\ude0e</p>"},{"location":"blog/2018-05-01-freedom-fridays/","title":"Freedom Fridays","text":"<p>When I started as anchor of the Concourse team, one of the things I wanted to improve was the human problem of on-boarding new engineers. Concourse is a large project spanning many areas of expertise (distributed systems, container runtimes, functional programming, user experience, etc.) and several Git repos (atc, fly, tsa, baggageclaim, etc.), so ramping up on ALL THE CONCOURSE can be a difficult task for even the most skilled engineers.</p> <p>When all of Concourse is considered as a monolithic backlog, a single Pivot could possibly be working on the Elm front-end on Monday, fixing issues with the Garden container scheduling on Wednesday, and rotate onto a story implementing a new Fly CLI feature on Friday.</p>"},{"location":"blog/2018-05-01-freedom-fridays/#break-it-down","title":"Break It Down!","text":"<p>To ease the cognitive load, we broke down the Concourse GitHub issues into Projects based on their subject matter and (thanks toAlex Suraci\u2019s work on Cadet) these tracks of work are visible at https://project.concourse-ci.org/.</p> <p>Breaking down the Concourse project into domains which can be understood as a smaller unit allowed us to be more strategic about rotating pairs, and we could nominate \u201ctrack anchors\u201d to lead longer running features within a track.</p>"},{"location":"blog/2018-05-01-freedom-fridays/#the-innovative-mindset","title":"The Innovative Mindset","text":"<p>Concourse is a project born out of innovation and giving engineers freedom to create. It all started in 2014, when Alex Suraci (<code>@vito</code>) and Christopher Brown ( <code>@xoebus</code>) worked out of a garage into the wee hours of the night<sup>1</sup> to build their vision of a new CI/CD tool. As the project has grown and the need for more process around the tracks of work arose, a big concern was losing this innovative edge and becoming stagnant. The project still has many ways to improve and grow beyond what we\u2019ve planned out in the tracks of work, and the issues raised by the community on GitHub.</p> <p>One morning, Alex Suraci, James Ma, and I talked about wanting to officially facilitate engineers exploring more of the code beyond the track they were already feeling comfortable on, or take on refactoring part of the code they saw during their focused work on a story. We wanted to encourage our peers to experiment, refactor, and explore new features or changes which might benefit Concourse.</p> <p>So, every week, the team has \u201cFREE-dom Friday\u201d where F.R.E.E is an acronym<sup>2</sup>:</p>"},{"location":"blog/2018-05-01-freedom-fridays/#flexibility","title":"Flexibility","text":"<p>One of the goals of this new experiment was to allow flexibility to move beyond the lanes of engineering work. We wanted our engineers to be able to explore other tracks, dive into parts of the code they hadn\u2019t explored, or even perform \u201catypical\u201d tasks like engaging with the community on PRs, forum discussions, writing blog posts, or triaging GitHub issues.</p>"},{"location":"blog/2018-05-01-freedom-fridays/#refactoring","title":"Refactoring","text":"<p>Technical debt is a beast. While I\u2019m a big proponent of constant refactoring being part of every backlog item (Ron Jeffries\u2019 article on the subject is a great read), sometimes a necessary refactoring feels just beyond the borders of where the work for a bug fix or feature implementation lies. An allotted safe space for digging around in areas of the code just beyond the scope of a story lets the team focus on implementing a feature, or fixing a bug, but also earmarks some time for refactoring later.</p> <p>One Friday I set out to fix issues raised by  <code>go vet</code> in a package which I had little exposure to in my day-to-day work on the project backlog. The results? I found a handful of unhandled errors in the code, untested error cases, and was able to share some best-practices with the team.</p>"},{"location":"blog/2018-05-01-freedom-fridays/#exploration","title":"Exploration","text":"<p>Often when working on an issue or new feature in one of the tracks of works, an engineer will see several parts of the puzzle that is Concourse and think: \u201cI wonder if we could do something different here\u201d. Often these thoughts feel like too much of a rabbit hole to pull a pair down with you, but our Fridays have become a place for these ideas to thrive.</p> <p>A good example of a win from allowing exploration is breaking up the ATC repo into smaller components. The separate auth component \u2018skymarshal\u2019 exists largely due to some exploratory work by Josh Winters, who went on to be the \u201ctrack anchor\u201d for adding users to Concourse.</p>"},{"location":"blog/2018-05-01-freedom-fridays/#experimentation","title":"Experimentation","text":"<p>One of the rules we had for Fridays was that there were no specific deliverables beyond the immeasurable effect of growing the team\u2019s innovative spirit. Fridays are a safe place for experimentation and failure.</p> <p>Having a day where engineers can experiment beyond the tracks of work we have scheduled in the project allows them to look at things from a fresh perspective, gain insight into other aspects of the codebase, and come up with new ideas for Concourse.</p>"},{"location":"blog/2018-05-01-freedom-fridays/#free-as-in","title":"Free as in... ?","text":"<p>As we frame this new practice within the day-to-day of the team, and talk about it with other teams. There\u2019s been a lot of questions about the name and intent behind \u201cFreedom Fridays\u201d, and the gist is: we wanted to nurture the F.R.E.E acronym outlined above and we happened to land on carving out time for this on Fridays.</p> <p>As we see the results of \u201cFreedom Fridays\u201d, we\u2019re actively looking for a better name to capture the value that is delivered by more Flexibility, Refactoring, Exploration, and Experimentation on an agile team. Also (just like the Ron Jeffries article about refactoring) we hope that by supporting a day of F.R.E.E-dom, these tenants become baked into the DNA of the team, and happen naturally.</p> <ol> <li> <p>Okay... you got me... I made the garage part up. They were both working at Pivotal, so I imagine they worked on Concourse after hours at the office in SF.\u00a0\u21a9</p> </li> <li> <p>This part is 100% made up for this post after the fact... I\u2018m applying a little something called artistic license here, people!\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2018-05-04-concourse-update-april-30--may-4/","title":"Concourse Update (April 30 \u2014 May 4)","text":"<p>I\u2019ve gotten some questions about Freedom Friday from some readers after last week\u2019s update. Well it turns out that Topher Bullock wrote a great article about it this week; you read up on it here: FREEdom Fridays</p> <p>We also release Concourse v3.13.0 earlier this week. Make sure you check it out if you were hit by the accumulating logs issue introduced in v3.12.0.</p> <p>On to the update:</p>"},{"location":"blog/2018-05-04-concourse-update-april-30--may-4/#space","title":"Space","text":"<p>We\u2019ve been building out some of the frontend code for representing Spaces as part of #2131. You can see some of the early visualizations below:</p> <p></p> <p></p> <p></p> <p>We now have the capability of testing Space end-to-end i.e. write the yml -&gt; fly sp -&gt; check out the web visualization.</p> <p>EXCITING</p>"},{"location":"blog/2018-05-04-concourse-update-april-30--may-4/#distributed-gc-on-workers","title":"Distributed GC on Workers","text":"<p>We\u2019ve been hacking away on master issue #1959 for distributed GC. If you\u2019ve been following along closely you\u2019ll notice that the number of boxes that we\u2019ve checked has increased\u2026and that\u2019s a good thing! We\u2019re in the final stretches of this work and will be prepping to test them in our internal Concourse \u201cWings\u201d very soon</p>"},{"location":"blog/2018-05-04-concourse-update-april-30--may-4/#user-auth","title":"User Auth","text":"<p>As always, we continue to work on our User Auth master issue #1888. We\u2019ve now transitioned into building out specific auth connectors using the dex library. We\u2019ve completed the GitHub and CF connectors, and are currently working on the generic OAuth provider</p>"},{"location":"blog/2018-05-04-open-sourcing-the-design-system-for-concourse/","title":"Open Sourcing the Design System for Concourse","text":"<p>This past week a community member reached out to begin work on a PR for a UI change in Concourse. This sparked some conversations within the team about how might we support this, since a lot of our design assets are private.</p> <p>We started by tagging smaller, less risky issues in the backlog with a <code>design-snack</code> label. We hope that these <code>design-snacks</code> make it more welcoming for contributors begin making UI contributions into the Concourse codebase.</p> <p></p> <p></p>"},{"location":"blog/2018-05-04-open-sourcing-the-design-system-for-concourse/#so-how-do-we-get-rid-of-the-friction-and-make-it-an-easy-process","title":"So, how do we get rid of the friction and make it an easy process?","text":"<p>We\u2019re going to start by open sourcing the Concourse design system in this repo: https://github.com/concourse/design-system. It\u2019s a bit sparse for now but we will be making more additions to it over the course of the coming weeks.</p>"},{"location":"blog/2018-05-04-open-sourcing-the-design-system-for-concourse/#our-expectations","title":"Our Expectations","text":"<p>We\u2019re hoping that this design system will give contributors some insight into our visual design philosophies. Ultimately, we want to give contributors all the design assets they need up front so any discussions around a PR will be feedback focused and not UI nitpicks. It\u2019s going to be an iterative process, so don\u2019t be shy in giving us feedback!</p>"},{"location":"blog/2018-05-04-open-sourcing-the-design-system-for-concourse/#feedback","title":"Feedback","text":"<p>Stay tuned for more as we continue to update the design system. As always, feedback is welcome via issues in the design-system repo!</p>"},{"location":"blog/2018-05-11-concourse-update-may-711/","title":"Concourse Update (May 7\u201311)","text":"<p>Hi folks,</p> <p>Joshua Winters has spent a lot of time refactoring Concourse so that it can finally support Users. We\u2019re finally at a point where we can share some our work with you, so I\u2019d really encourage you to check out his recent blog post Oh, Auth</p> <p>Outside of that, we\u2019ve got a bunch of vacations going on this week, so it\u2019s been more of the same three tracks of work:</p> <ul> <li>Users</li> <li>Distributed GC on workers</li> <li>Spatial Resource flows</li> </ul> <p>See you next week!</p>"},{"location":"blog/2018-05-11-oh-auth/","title":"Oh, Auth","text":"<p>As most of you know we\u2019ve been working hard on introducing Users into Concourse. Today, I\u2019m excited to share with you some of the changes we\u2019ve made for an upcoming release of Concourse.</p> <p>In the old world, you used to log in as a team:</p> <p></p> <p>Now you log in as a user:</p> <p></p> <p>You can do this using external auth providers:</p> <p></p> <p>Or logging in as a local Concourse user:</p> <p></p> <p>How did we manage to accomplish this magic? With the power of dex!</p> <p>In this new model:</p> <ul> <li>We rely on dex to fetch the user\u2019s identity and group affiliations from the external auth provider.</li> <li>We then take this information and cross reference against all Concourse teams to establish team memberships for the   user.</li> <li>Team owners can whitelist external users and groups with the <code>fly set-team</code> command.</li> </ul> <p>What this doesn\u2019t fix:</p> <ul> <li>We still don\u2019t have an easy way for people to identify what users/groups are currently whitelisted for a team. This   should be something that we can add to the <code>fly teams</code> command in the future (PRs welcome), but in the mean time you   can always check the database ;).</li> <li>We also aren\u2019t implementing auditing or RBAC just yet...</li> </ul> <p>...but it's a good start!</p> <p>Now, let\u2019s dive into some of the technical changes that might affect your continuous thing doing.</p>"},{"location":"blog/2018-05-11-oh-auth/#atc-startup","title":"ATC Startup","text":"<p>When you start up the ATC you will need to provide configuration information (i.e. <code>client_id</code>, <code>client_secret</code>, etc.) for ALL auth providers that you want to use. Up to this point, each team would have to configure their own providers, now the startup information will be shared across teams.</p> <p>If you want to use the GitHub provider, you will need to provide:</p> <pre><code>atc ...\n --github-client-id client-123\n --github-client-secret 1234567890\n --main-team-github-group my-org:my-team\n</code></pre> <p>Note that the configuration of the <code>main</code> team is slightly different now too.</p>"},{"location":"blog/2018-05-11-oh-auth/#fly-set-team","title":"Fly Set Team","text":"<p>Since providers are now configured at startup, the <code>fly set-team</code> command has gotten a whole lot simpler.</p> <p>You used to configure teams with all the provider information:</p> <pre><code>fly ...\n --github-auth-client-id client-123\n --github-auth-client-secret 1234567890\n --github-auth-user my-github-login\n --github-auth-team my-org/my-team      # \u2190 Note the slash \u201c/\u201d\n</code></pre> <p>Now you simply whitelist a bunch of users and groups:</p> <pre><code>fly ...\n --github-user my-github-login\n --github-team my-org:my-team           # \u2190 Note the colon \u201c:\u201d\n</code></pre> <p>Yeah, sorry we changed the delimiter from a slash to a colon, but that\u2019s what dex uses, so we went with it.</p>"},{"location":"blog/2018-05-11-oh-auth/#bearer-tokens","title":"Bearer Tokens","text":"<p>We\u2019ve updated our bearer tokens to include the user\u2019s identity and ALL their team memberships. Including their identity will enable us to do auditing down the road. And having multiple teams in the token lets us update our APIs to return a list of ALL resources visible to a user, not just those for a specific team.</p> <p>So the decoded token used to look like this:</p> <pre><code>{\n  ...\n  \"isAdmin\": true,\n  \"teamName\": \"main\"\n}\n</code></pre> <p>And now it looks like this:</p> <pre><code>{\n  ...\n  \"sub\": \"CgcxMDcyMjMzEgZnaXRodWI\",\n  \"is_admin\": true,\n  \"email\": \"user@email.com\",\n  \"name\": \"Some User\",\n  \"user_id\": \"1072233\",\n  \"user_name\": \"my-github-login\",\n  \"teams\": [\n    \"main\"\n  ]\n}\n</code></pre>"},{"location":"blog/2018-05-11-oh-auth/#no-auth","title":"No Auth","text":"<p>In the past, you could use the <code>--no-really-i-dont-want-any-auth</code> flag to disable auth for your team. This would allow access to anyone without having to provide credentials.</p> <p>In the new world, you can\u2019t do this. We don\u2019t allow access without credentials, and teams can no longer be open to the public. You can, however, whitelist all authenticated users in the system using the <code>--allow-all-users</code> flag when setting up your team.</p>"},{"location":"blog/2018-05-11-oh-auth/#fly-actions","title":"Fly Actions","text":"<p>The Fly experience does not change in our new auth system. You will still need to provide a target, which includes the team that you are operating under for team-specific actions (such as <code>fly set-pipeline</code>, etc.).</p>"},{"location":"blog/2018-05-11-oh-auth/#exposed-pipelines","title":"Exposed pipelines","text":"<p>The flow of setting and accessing exposed pipelines will remain unchanged. Exposed pipelines will still be visible to the public, and do not require a user to be logged in.</p>"},{"location":"blog/2018-05-11-oh-auth/#no-provider-left-behind-sort-of","title":"No provider left behind\u2026 sort of","text":"<p>For this track of work, we still need to implement some missing providers (dex calls them connectors) for external auth systems. We\u2019re hoping to have feature parity with the old providers, but we want to understand your use cases for needing them first.</p> <p>For example, Bitbucket Server may be problematic since it still relies on OAuth1. Now that we have more auth options ( hey we have an LDAP connector now), would current BitBucket users be willing to switch over to something else? Let us know in GH issue #1888.</p>"},{"location":"blog/2018-05-11-oh-auth/#feedback","title":"Feedback","text":"<p>For those who want to get caught up on the discussion, you can check out the GitHub issues:</p> <ul> <li>Discussion; Fine-grained / Role based access control</li> <li>Investigate a new approach to Auth</li> <li>Rely on coreos/dex for third party integrations</li> </ul> <p>If you have any questions or concerns about the work that\u2019s being done, feel free to drop a comment on those issues, or reach out to us on our usual channels.</p>"},{"location":"blog/2018-05-14-my-first-month-on-concourse/","title":"My first month on Concourse","text":"<p>I have been working as Software Engineer with Pivotal for about 3 years now, during which much of my contributions were primarily within Cloud Foundry teams, such as MySQL service, Routing, and UAA. Early this year I rotated onto Concourse team in Toronto and I would like to share my thoughts on why Concourse team does things differently and why they work.</p> <p>On CF teams, part of our of daily routine was entrenched in using various productivity tools that help teams stay coordinated and connected. A couple of key examples are Pivotal Tracker that allowed for ideating and managing stories/features across team members and Slack for communication and pair programming on workdays. On Concourse, I was surprised to see Cadet and GitHub instead of Tracker, Discord instead of Slack, and a Concourse forum for detailed discussions. It became apparent that the team landed on using these tools to achieve one of its core goals: engaging more closely with the Open Source community through discussions, issues, and PRs.</p> <p>Let me expand more on how these tools serve their purpose. We use Cadet, a tool built by Alex Suraci to manage GitHub issues/PRs. As a developer, it helps me stay aware of high impact issues and backlog items based on importance, active tracks or epics, and stories in-flight or done. It also serves to provide additional transparency within the company as well as to the community and allows everyone to contribute towards the solution.</p> <p>We use GitHub in combination with Cadet to track the progress of a story. Stories could be created by PM, team members, and even community members. We also use this combination to prioritize, track features, bugs, and PRs issued by the community across multiple tracks of work. We use comments to transfer context or knowledge dump or even breaking down huge issue into smaller tasks. We achieve the same goal by using GitHub. Furthermore, GitHub makes it very easy to take or transfer ownership of issues that interest us through its built-in assignment functionality.</p> <p>These tools provide the foundation to my daily work and play an important role in rapid ramp up with the Concourse team. I tried to take advantage of unique team structure with multiple track anchors, spending a couple of weeks experimenting with different tracks, I eventually chose the authentication track, after pairing with Josh Winters for three weeks until I could drive the track comfortably on my own. This was a huge boost to my confidence and cemented my abilities within my new team. Another thing that helped my ramp up was Freedom Fridays. It gave me enough time to read through Concourse documents, explore the vast codebase, and start working on self-directed small features. Suffice to say, this is easily one my fastest ramp ups so far, and it continues to be an enjoyable experience.</p> <p>Structurally, Concourse shares the same operational philosophy as other Pivotal R&amp;D teams. We still pair from Monday to Thursday in additional to having team discussions, retros, and IPMs. While we do not use all the traditional Pivotal tools, the focus on our values remains the same: doing the right thing, doing what works and being kind to ourselves, users, community, and Pivotal.</p>"},{"location":"blog/2018-05-18-concourse-update-may-1418/","title":"Concourse Update (May 14\u201318)","text":"<p>In case you missed it, I\u2019d encourage you to check out some of the recent posts from Shashwathi Reddy on \u201cMy first month on Concourse\u201d and Joshua Winters regarding upcoming changes to our authentication; \u201cOh, Auth\u201d. We\u2019d love to hear your feedback!</p> <p>Heads up: the Concourse team will be taking Monday, May 21st off for Victoria Day holiday.</p> <p>And now, on to the update:</p> <p>Core</p> <ul> <li>Continued banging our heads against new auth connectors with Dex. Note: We\u2019ve started to centralize backwards-(in)   compatibilities with user auth in issue #2218</li> <li>We\u2019ve stood up a new Concourse with our experimental Spaces work. We\u2019re looking for volunteers who are interested in   trying out their pipelines before and after \u201cspace\u201d. Tweet at me if you\u2019re   interested https://twitter.com/pioverpi!</li> </ul> <p>Runtime</p> <ul> <li>Completed all the volume collection work for distributed GC   in #1959. We\u2019re currently deploying this change to our internal   environments to see how it works at scale \ud83e\udd1e</li> <li>Fixed issue #2168, wherein \u201cDuplicate resource type volumes   created over time\u201d</li> </ul>"},{"location":"blog/2018-05-25-concourse-update-may-2225/","title":"Concourse Update (May 22\u201325)","text":"<p>It was a short week for us here in Canada, but we had a few interesting updates:</p> <ul> <li>We attempted to deploy our distributed GC changes to our   internal environment \u201cWings\u201d last Friday. Turns out that was an incredibly bad idea. The deployment failed   horrifically and we had to roll back all our changes. We\u2019re still investigating why our code worked in   our \u201cprod\u201d environment but failed when deployed onto Wings. We\u2019re tracking this work   in issue #2202.</li> <li>Our team conducted our first round of interviews on Spatial resources with Pivots in the Toronto office. We\u2019re getting   a lot of interesting feedback and are making tweaks for next week\u2019s batch of interviews</li> <li>In the mean time, we managed to work through some design snacks, addressing the lack   of breadcrumbs   and responsive design on groups.</li> <li>Investigated migration paths forward with the new dex auth. Keep an eye on   issue #2218 for more information on future incompatibilities   with this upgrade!</li> </ul>"},{"location":"blog/2018-06-01-concourse-update-may-28--june-1/","title":"Concourse Update (May 28 \u2014 June 1)","text":"<p>If you\u2019ve been experiencing \u201cAw Snap\u201d errors on Chrome with Concourse 3.13.0 or 3.12.0 we traced the root case to two lines of CSS. This seems to happen only on Chrome 67; so a temporary workaround is to switch over to Chrome canary or use Firefox/Safari/Edge. You can follow along in our discussion at GitHub issue #2236</p> <p>Now, on to the update</p>"},{"location":"blog/2018-06-01-concourse-update-may-28--june-1/#runtime","title":"Runtime","text":"<p>We were able to successfully test our distributed volume GC collection code on our Wings environment this week. Overall we\u2019ve seen a significant drop in Database Queries and a ~10% decrease in Web CPU usage.</p> <p></p> <p></p> <p>Notice how the Database Queries now look like a sawtooth; this is a result of our new \u201cmark and sweep\u201d GC strategy on workers.</p>"},{"location":"blog/2018-06-01-concourse-update-may-28--june-1/#core","title":"Core","text":"<p>In an effort to make our new Users work backwards compatible and downgrade-able, we spent a good chunk of this week figuring out down-migrations. The conversation around this, and the compromises we\u2019ve had to make can be found in Josh\u2019s follow up comment here</p>"},{"location":"blog/2018-06-01-concourse-update-may-28--june-1/#ux","title":"UX","text":"<p>Check out the new breadcrumbs and responsive groups on our prod environment!</p> <p>We\u2019re still looking for users who would be interested in testing out our new spatial resources view! Please reach out to me over Twitter or @jma on Discord if you\u2019re interested!</p>"},{"location":"blog/2018-06-04-distributed-garbage-collection/","title":"Distributed Garbage Collection","text":"<p>Before diving into the weeds of Concourse architecture, let\u2019s briefly take a look at container and volume lifecycles in Concourse. Going forward in this post I will refer to container and volume together as a \u201cworker resource\u201d. In the current architecture, there are multiple ways for worker resource creation to be triggered; like when jobs are started in pipelines or checking for a newer version of a resource. The ATC is responsible for managing worker resource lifecycle like creating, transitioning and destroying them. Workers are designed to be dumb and follow the orders issued by ATC. Removal of worker resource on worker and its reference (in ATC) is done as part of Garbage collection (GC) process in ATC.</p> <p>GC in ATC includes cleaning up of finished builds, build logs, old resource cache, worker resource cleanup, transitioning workers state etc. One of the classic techniques used for GC is \u201cMark and Sweep\u201d, it essentially marks the worker resources that are to be cleaned up and then runs another process to actually remove them. In this post we are going to dive deep into worker resource cleanup only. What are the conditions under which a worker resource should be marked for removal? Few reasons could be that containers are around past the expiration time, corresponding volumes of past builds that need not be cached, etc. Once the mark phase is done, ATC would connect to Garden server (container management server) and baggage claim server (volume management server) on each worker and then destroy worker resource iteratively.</p> <p>During GC cycle, the number of outbound connections from ATC would proportionally increase with number of workers and churn of builds. One potential solution is to increase the number of instances/vms of ATC to distribute load. The problem with this approach is that GC synchronization requires mutex so ATC cannot reap the benefits of multiple instances.</p> <p></p> <p>Current Garbage collection process</p> <p>What is the simplest thing that we could do to decrease the connections from ATC to worker instances?</p> <p>One way would be to introduce a bulk destroy API on the worker for worker resources. We implemented a bulk destroy call on the baggage claim server and Garden server so ATC needs to make just one network call for Sweep phase. It was definitely a step forward in terms of reducing the number of network connections from ATC to worker.</p> <p>We felt the present Concourse design was constricting us from making any further enhancements to GC. In current world, the ATC holds the desired state of the system (i.e., number of worker resources that needs to be created for builds or resource checks). It also holds information on what is the actual state of the system such as worker resources across all workers. The ATC is further responsible for transitioning from actual to desired state too. We spent a significant effort to move this central design system into more scalable and well balanced architecture. The first step towards this change was to make workers smarter. We enabled worker with process to periodically poll the ATC for what is the desired state and also report their actual status to ATC. This design will</p> <ol> <li>Makes worker responsible for maintaining the desired state.</li> <li>Reduce the amount of work to be done by ATC to maintain worker state.</li> <li>Make workers more autonomous.</li> </ol> <p></p> <p>Distributed Garbage collection process</p> <p>We want to continue to make further enhancements on this path of work, and continue to give more power to the worker process; like defining a CRUD API for worker resources. ATC is currently still responsible for creating worker resources so hopefully (not so) far out in the future we can have worker resource APIs manage the lifecycle. This will also reduce the intertwined dependency of ATC on Garden server API and baggageclaim server API. It will also also pave the path for any future container/volume management systems to implement these worker resource APIs.</p>"},{"location":"blog/2018-06-04-distributed-garbage-collection/#success-on-wings","title":"Success on Wings!","text":"<p>Metrics from Concourse production deployment before and after deploying GC changes.</p> <p></p> <p>Database Queries by ATC (4 ATCs in this deployment) reduced by 40% after the deployment of GC changes</p> <p></p> <p>CPU Utilization of ATC vm reduced by 10% after deployment of GC changes.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/","title":"How We Build Concourse","text":"<p>Building on some of our previous posts on the Concourse team mechanics<sup>1</sup>, I wanted to spend some time going over how we actually build Concourse.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#tracking-features-and-bugs","title":"Tracking Features and Bugs","text":"<p>Concourse tracks all of its bugs, features and epics through GitHub Issues.</p> <p>For items regarding the core functionality of Concourse itself, you can find the master issues list here: https://github.com/concourse/concourse/issues</p> <p>For issues regarding the Concourse website and documentation, you can find the backlog here: https://github.com/concourse/docs</p> <p>Concourse resources, both the ones included with Concourse and the ones that are community made, live in their own repositories. Issues, bugs and features should be reported against the resource\u2019s GitHub issues repo.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#triage-review-and-prioritization","title":"Triage, Review and Prioritization","text":"<p>We do our best to review and triage new issues that come into the Concourse repository on a daily basis. Triaging an issue requires us to:</p> <ul> <li>Identify whether the issue is a bug or new feature (aka enhancement)</li> <li>Identify whether the issue requires more investigation</li> <li>Apply relevant labels for easier search down-the road (e.g. web-ui, fly, security, etc.)</li> <li>Follow up with any questions or comments if issue was unclear</li> <li>Connect issues to related issues already entered previously</li> <li>If applicable, assign to one of the Concourse GitHub Projects</li> </ul> <p>Issues assigned to a GitHub Project are automatically assigned into the project\u2019s Icebox. The Concourse team follows a very similar development approach to XP and Pivotal workflow where only active and prioritized items are assigned to the Backlog, and all finished stories are required to be \u201cAccepted\u201d or \u201cRejected\u201d by a Product Manager or some other knowledgeable subject matter expert.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#design-research","title":"Design &amp; Research","text":"<p>Issues that require design and UX feedback are labeled with needs-design. These are usually picked up by our product design team.</p> <p>We also use the label design-snack on bite-sized UX/UI issues that are ready to be picked up by an engineer. design-snacks aren\u2019t highly prioritized issues but are nonetheless very useful for Concourse users!</p> <p>Sometimes we work on big issues that require more research and testing before we can actually write issues. This work is often tracked separately through various tools (both online and offline). We do our best to post updates in blog posts and GitHub issues along the way.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#iteration-planning-meeting-ipm","title":"Iteration Planning Meeting (IPM)","text":"<p>The Concourse team conducts IPM every week on Monday afternoon. During this time we review each GitHub Project\u2019s backlog. This includes discussions on stories that were recently complete, are currently in flight, new stories added to the backlog and any change in Backlog priorities. The Concourse team uses this custom-build project view (https://project.concourse-ci.org/) as a way to quickly access the backlogs of all our projects.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#acceptance","title":"Acceptance","text":"<p>Issues that are resolved are moved into the \u201cDone\u201d column of each project. This means that the issue is ready to be reviewed for Acceptance. Typically, work that is ready for acceptance is reviewed on our \u201cprod\u201d instance, that is, https://ci.concourse-ci.org/ The issue is typically reviewed by a product manager or a subject matter expert who can determine whether the completed issue is acceptable for general distribution. Some changes require additional load and/or \u201creal-world\u201d testing; in that case we deploy to Pivotal\u2019s internal large-scale Concourse \u201cWings\u201d; which currently runs 3 ATCs, 38 workers and has &gt; 70 teams.</p> <p>Rejected issues are returned to the top of the GitHub Project Backlog and commented on for revision.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#prcommunity","title":"PR/Community","text":"<p>PRs and \u201cCommunity\u201d work (e.g. answering questions on Discord, our Forum, in GitHub issues) is usually handled by a dedicated person. Currently this person is Alex Suraci (Product Manager). In addition to helping the community, he is building out new proposals for long-term changes to Concourse in the RFCs repo.</p>"},{"location":"blog/2018-06-06-how-we-build-concourse/#what-do-we-build","title":"What do we build?","text":"<p>In my next blog post, I plan on covering what the core team is working on, how we make those decisions, and how we are working to make those plans more obvious</p> <ol> <li> <p>See The Concourse Crew and How the Concourse Team Organizes Issues \u21a9</p> </li> </ol>"},{"location":"blog/2018-06-08-concourse-update-june-48/","title":"Concourse Update (June 4\u20138)","text":"<p>Big release this week! After lots internal load testing on Wings we finally felt comfortable releasing Concourse 3.14.0. In addition to the new Distributed Garbage Collection, breadcrumbs, responsive groups, and Windows worker, we have 14 new features a whole bunch of bug fixes. But wait! Don\u2019t download that one; get Concourse v3.14.1 instead.</p> <p>A few other updates. First, be sure to check out my write up on How We Build Concourse. I plan on writing more posts like this in hopes of giving you more insight into the internals of the Concourse team. Hope you like it!</p> <p>And now, on to the update; starting with a note on RFCs:</p>"},{"location":"blog/2018-06-08-concourse-update-june-48/#rfcs","title":"RFCs","text":"<ul> <li>We\u2019re looking for feedback on how to improve our existing implementation of credential management. You can read more   about it in issue #5.</li> <li>The RFC around Resources v2is moving along with some new changes. Thanks   to all the reviewers (itsdalmo, cwlbraa   and dprotaso). I\u2019d REALLY encourage ya\u2019ll to read   the full proposal and provide your   inputs; since we\u2019ll be relying on these changes for new features like Spatial Resources.</li> </ul>"},{"location":"blog/2018-06-08-concourse-update-june-48/#ux","title":"UX","text":"<ul> <li>We\u2019re seriously, absolutely, most definitely tacking the slow performance on the build   page #1543</li> <li>Spatial Resource testing continues! Here\u2019s a peek at our most recent iteration:   </li> </ul>"},{"location":"blog/2018-06-08-concourse-update-june-48/#core","title":"Core","text":"<ul> <li>Now that 3.14.1 is out, we\u2019re now ready to rebase and merge in our Users change and prime that for release in 3.15.0.   Testing begins next week after we finish getting everything merged in</li> </ul>"},{"location":"blog/2018-06-15-concourse-update-jun-1115/","title":"Concourse Update (Jun 11\u201315)","text":"<p>This was a post-release week, so we spent a lot of time merging in new code from the Users track, fixing our pipelines, and working on some neglected issues. All in all a solid week\u2019s worth of work! On to the update</p>"},{"location":"blog/2018-06-15-concourse-update-jun-1115/#ux","title":"UX","text":"<ul> <li>Changed the behaviour of the breadcrumb so that clicking on the pipeline name resets the pipeline view and group   settings (#2258)</li> <li>Fixed a bug with the breadcrumb where it wouldn\u2019t render whitespace   correctly (#2267)</li> <li>Fixed a bug with team name overflowing on breadcrumbs (#2241)</li> <li>Fixed a UI bug on the navigation arrows (#2276)</li> <li>Added JSON stdout to Fly CLI (#952)</li> </ul> <p>We haven\u2019t done work on this yet, but based on our observations and feedback from the community, we\u2019re planning to push the dashboard up to / level. This will require a few items of polish first; but you can refer to issue #2282 for details</p>"},{"location":"blog/2018-06-15-concourse-update-jun-1115/#core","title":"Core","text":"<ul> <li>Spent most of the week trying to rebase and merge in changes from the Users track. Our pipelines are finally green so   we\u2019re ready to push some of that work into our local environments for broad testing. Be sure to check up   on #2218 for any gotchas that might affect you!</li> </ul>"},{"location":"blog/2018-06-15-concourse-update-jun-1115/#space","title":"Space","text":"<ul> <li>Conducted two user interviews this week. We have only have one or two more interviews left next week. After that we\u2019ll   be figuring out what our MVP might look light so we can start exposing that feature to adventurous Concourse users.</li> </ul>"},{"location":"blog/2018-06-15-concourse-update-jun-1115/#rfcs","title":"RFCs","text":"<p>As with last week, we\u2019re looking for feedback on how to improve our existing implementation of credential management. You can read more about it in issue #5.</p> <p>The RFC around Resources v2 is moving along with some new changes. Thanks to all the reviewers (itsdalmo, cwlbraa and dprotaso). I\u2019d REALLY encourage ya\u2019ll to read the full proposal and provide your inputs; since we\u2019ll be relying on these changes for new features like Spatial Resources.</p>"},{"location":"blog/2018-06-22-concourse-update-jun-1822/","title":"Concourse Update (Jun 18\u201322)","text":"<p>It\u2019s been a busy week for myself and Topher Bullock. We spent some time in Boston meeting with some users operating large-scale Concourses. We learned a lot about the issues they were running into operating Concourse a scale\u2026and we ate a lot of Lobster!</p> <p>On to the update:</p>"},{"location":"blog/2018-06-22-concourse-update-jun-1822/#ux","title":"UX","text":"<ul> <li>Made some improvements to the build page in issue #1543 that   we\u2019re hoping to test soon on our internal Concourse. You can read into some more of the details in   our comments.</li> <li>Expanded our PR pipelines in #2305 to run web tests as a part of   pulling in the PR for Shareable Search on dashboard #2265.</li> <li>Started the move of the dashboard/ view to be the root level page (   issue #2282) by adding Logout to the dashboard page   in #1663</li> </ul>"},{"location":"blog/2018-06-22-concourse-update-jun-1822/#core","title":"Core","text":"<ul> <li>Continued our struggle to finish off Users work with some fixes to migrations and breakages to our own testing   pipeline</li> <li>Continued with our user testing on Spatial Resources. We\u2019re getting more confident with the designs, so we added a   story to bring those designs into beta/ in issue #2292</li> </ul>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/","title":"Designing for Space in Concourse","text":"<p>This feature is still in the early stages of development. We are picking up from our previous sneak peak post by James Ma. If you have a comment and want to participate in the conversation please visit the issue on GitHub.</p> <p>Concourse Pipeline Engineers have been asking for a better way to test variations and combinations of a resources in their pipelines. When pipelines are composed of resources that duplicate themselves to account for product versions (semver), branches of a repo, and different environments, the resulting pipeline can be quite difficult to navigate and edit.</p> <p></p> <p>Pivotal OSL Pipeline</p> <p>If we can simplify this new space configuration to just the one job and specify spaces on the corresponding resources we could greatly reduce the duplication that is happening within their pipelines.</p> <p>Adding this feature into the current anatomy of Concourse is a breaking change to the existing pipeline visualization. Resources themselves are not very legible in the current visualization; and we layer on additional information such as the spaces that are being used and the resulting job combinations that are automatically generated. So, how do we go about increasing the complexity of the visualization while making the smallest incremental change to test the new design?</p>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/#ui-inventory","title":"UI Inventory","text":"<p>We took the approach of starting by taking an inventory of the current UI actions and content, pain points and user goals.</p> <p>My design pair, Sam Peinado, and I did a dump of post-its in Realtimeboard to list out all of the actions and content that exists today on the pipeline page and what content those actions mapped to.</p> <p></p> <p></p> <p>Current pipeline: ci.concourse-ci.org</p> <p>Then we looked at the Goals of our users when they are on the pipeline view.</p> <p>Goals of the App Developer in the Pipeline view:</p> <p></p> <p>Goals of the Concourse Operator:</p> <p></p> <p>Followed by prioritizing the top pain points.</p> <p>Top Pains:</p> <p></p> <p>The research and prioritization exercise of the user pain points gave us a shared understanding, with my design pair, that prepared us for the next step of a brainstorming workshop on a possible UI direction.</p>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/#sketching-design-studio","title":"Sketching: Design Studio","text":"<p>Design Studio is a collaborative solution brainstorming activity to gather ideas and discover unknowns in the form of sketches. My design pair, Sam, and I decided this would be the best way to get the most amount of ideas out there in the shortest amount of time.</p>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/#how-does-a-design-studio-work","title":"How does a Design Studio work?","text":"<ol> <li>We\u2019ll take 5 - 10 minutes to sketch potential solution concepts</li> <li>Discuss and Critique \u2014 Review each person\u2019s concepts together</li> <li>Vote on sketches \u2014 Dot vote with a limited number of votes</li> <li>Repeat \u2014 As you repeat you can increase the fidelity and expand on the best solutions from the previous round of    sketching</li> </ol> <p>From the studio we brainstormed some wild possible pipelines from: Card Stacking, Circular pipelines, a Resource Centric View, a Molecule Map, Text only output.</p> <p></p> <p>From here we critiqued the solutions and decided on the best options to move forward into a higher fidelity prototype.</p>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/#experiments-starting-with-a-simple-scenario-for-a-testing-matrix","title":"Experiments: Starting with a simple Scenario for a testing matrix","text":"<p>We started with a simple use case to build out our concept in Sketch.</p> <p>As a Developer, I have a small Ruby project with build, test, and ship stages. I initially only test against the master branch with Ruby v2.1.x and ship only if master passes.</p> <p></p> <p>prototype v1.0</p> <ul> <li>Jobs are the primary focus for the user in this view and are abstracted to job boxes.</li> <li>Jobs are divided into a further grid are representing the visual matrix of space permutations.</li> <li>Resources, by clicking on them in the side drawer, highlight the jobs.</li> </ul>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/#our-findings","title":"Our Findings:","text":"<p>From our initial round of User testing one of the more interesting findings twas that we identified two workflows for the pipeline view:</p> <ol> <li>As an Operator, I am engineering a pipeline in yml and seeing the resulting connections and flow of    resources rendered in the pipeline UI. I want to see the jobs and resources where I expect them to be in the pipeline    map.</li> <li>As an App Developer, I am watching my commit go through the pipeline. I am waiting for the pipeline to be    green so that I know my commit was successful. If it goes red at any point, I want to know if it was my code that    broke the pipeline.</li> </ol> <p>Because of the complexity of Concourse pipelines it is hard for users who have not configured a pipeline to understand and read another user\u2019s or team\u2019s pipeline. The limits for testing these complex UI models with real scenarios in InVision is a big barrier to a user understanding the UI and giving meaningful feedback. At this point we realized InVision wasn\u2019t going to give us the fidelity of feedback we needed for the next iteration. So, we put the experiment into code by pairing with engineering and we created our own Concourse instance where we could render our users pipelines for the next phase of testing.</p>"},{"location":"blog/2018-06-28-designing-for-space-in-concourse/#coded-prototype","title":"Coded Prototype","text":"<p>For this second phase of User testing we asked our users to bring a <code>yml</code> from a familiar pipeline that they use day to day. This would let our users easily experience the potential benefits from a feature like Space. We were interested in observing the following interactions:</p> <ul> <li>users as they were updating their pipeline <code>yml</code> with the new syntax,</li> <li>the user reactions to the new configuration rendered in the UI</li> <li>the new pipeline UI optimized for Space</li> </ul> <p>This approach has been very successful in getting our users to wrap their heads around this new paradigm and to give us feedback on the how Space will impact their workflow.</p> <p></p> <p></p> <p></p> <p>Screen capture of our User Test with Resource contributor Benjamin Jung https://github.com/headcr4sh</p> <p>As we continue to test with users we ask for your participation in the conversation about Space!</p> <ul> <li>Do you think the payoff is worth the change?</li> <li>What problems will this cause for you?</li> <li>What impact would this have given your current workflow?</li> </ul> <p>Head over to the GitHub issue for feedback: https://github.com/concourse/concourse/issues/1707</p> <p>If you wish to participate in User Testing for this new feature please reach out to me directly.</p>"},{"location":"blog/2018-06-29-concourse-update-jun-2529/","title":"Concourse Update (Jun 25\u201329)","text":"<p>If you\u2019ve been following along with our Auth changes, you\u2019ll know that we\u2019ve been doing a lot of work behind the scenes to make the upgrade into this new world as seamless as possible. This week, we were able to do our first large-scale upgrade test against our Wings instance. The upgrade went well and we were able to find a few more areas of polish before we push this feature. You can find our updated list of future incompatibilities in GitHub issue #2218. Having considered the nature of the breaking changes, the next update of Concourse with Users will push us into 4.0.0!!!</p> <p>I also wanted to take this time to give a big thank you to all of the participants in our spatial resource interview. If you\u2019re curious to see the results of our research please read up on Lindsay Auchinachie\u2019s post here: Designing for Space in Concourse</p> <p>If you\u2019d like to get your hands on Space as soon as possible, then I\u2019d encourage you to also read and comment on our Resources v2 RFC. Alex Suraci made some recent updates to the proposal so definitely check it out, or read the fully rendered proposal.</p> <p>We\u2019d like to get the Resources v2 RFC closed out soon so we can implement the resource changes necessary to tap into the full potential of spatial resources!</p> <p>Some other updates:</p> <ul> <li>We fixed a known issue #2300 with the 3.14.x series whereby users noticed a significant   increase in CPU usage when connecting with CredHub. This has now been fixed</li> <li>Increased pipeline stability and fixed some flakes with our UI tests in topgun</li> <li>Fixed an issue where the auth token is shown in the address bar</li> <li>Picked up additional pipeline work by adding integration   for web PRs</li> </ul> <p>Edit</p> <p>I ALMOST FORGOT! We also improved build page performance #1543! In some instances we reduced the page load time from 25s to only 5s:</p> <p></p>"},{"location":"blog/2018-07-06-concourse-updates-july-36/","title":"Concourse Updates (July 3\u20136)","text":"<p>Concourse, beavers, poutine and maple syrup</p> <p>Since July 1st was the official day of Canada\u2019s birth, the Concourse team enjoyed a long weekend with no work on Monday. We were, however, able to get quite a bit done during this short week.</p> <p>A big win is that we added a k8s-testflight job to our official ci pipeline (check it out here); this will let us know in advance when we have broken the Concourse Kubernetes Helm Chart. Shout out to Divya Dadlani, Jamie Klassen and Rui Yang for working on that in the Concourse team!</p> <p>Here\u2019s also a few interesting reminders:</p> <ul> <li>For Pivotal-supported releases of Concourse (aka Concourse for PCF) you can find a compatibility matrix of common   dependencies   here: http://docs.pivotal.io/p-concourse/#Compatibility</li> <li>I realized this week that not a lot of people know about this; but Alex Suraci   wrote up a series of Concourse Anti-Patterns a while   back. Its definitely worth the read</li> <li>PLEASE take a second to review the upcoming Resource v2 RFC or its   rendered version here</li> <li>Concourse team is going to OSCON! Come by the Pivotal booth 406 and say \u201chi\u201d!</li> </ul> <p>Anyways, on to the update:</p>"},{"location":"blog/2018-07-06-concourse-updates-july-36/#ux","title":"UX","text":"<ul> <li>Fixed some minor UI issues across the   board: #2333, #2313, #2291,   and #2310</li> <li>Continued our work in routing Dashboard page to the Home page   in #2282. This, however, has turned into a bit of a scope creep   and we are now upgrading the entire UI to use the new dark theme:</li> </ul>"},{"location":"blog/2018-07-06-concourse-updates-july-36/#core","title":"Core","text":"<ul> <li>Discovered some additional backward incompatibilities with the new user-based auth that would be super annoying to   deal with; so we have addressed some of them   in#2326, #2299   and #1810. As always, you can read about future   incompatibilities with our new auth in issue #2218</li> <li>Alex Suraci had some time to pick up some low-hanging performance-improving fruit   in #285</li> </ul>"},{"location":"blog/2018-07-13-concourse-update-jul-913/","title":"Concourse Update (Jul 9\u201313)","text":"<p>DARK</p> <p>We\u2019re going dark themed for Concourse 4.0.0! In addition to the users work, we\u2019re promoting the Dashboard to the / level to take over the home page. You\u2019ll also notice that we added pipeline play/pause capabilities to the dashboard, NEAT!</p> <p>To keep things consistent, we\u2019re also propagating our new design to the existing pipeline views. You can play around with this new nav structure on our own CI: https://ci.concourse-ci.org/</p> <p>The team here is also planning to attend OSCON in Portland next week (July 18 &amp; 19). Drop by the Pivotal booth to say hi and grab a Concourse sticker!</p> <p>On to the update:</p>"},{"location":"blog/2018-07-13-concourse-update-jul-913/#ux","title":"UX","text":"<ul> <li>Fixed some resource alerting errors on the pipeline #2333</li> <li>Moved dashboard to home #2282</li> <li>Added Pause/Play pipeline buttons on homepage #2365</li> <li>Worked on dragging to re-order pipelines on the dashboard #2364</li> <li>Updated and propagated the new colours across the app #2370</li> </ul>"},{"location":"blog/2018-07-13-concourse-update-jul-913/#core","title":"Core","text":"<ul> <li>Added health check APIs to verify credential managers are properly   configured #2216</li> <li>Even more db optimizations! yay!</li> </ul>"},{"location":"blog/2018-07-13-concourse-update-jul-913/#runtime","title":"Runtime","text":"<ul> <li>Picked up an oldie but a goodie: Concourse should support imposing limits on container   resources #787</li> </ul>"},{"location":"blog/2018-07-13-concourse-update-jul-913/#operations","title":"Operations","text":"<ul> <li>Started to move our stemcells onto the   new Xenial stemcells</li> <li>Switch upgrade/downgrade testing jobs to the binaries #2371</li> </ul>"},{"location":"blog/2018-07-20-concourse-update-jul-1620/","title":"Concourse Update (Jul 16\u201320)","text":"<p>This week, the Concourse team went out to Portland to attend OSCON 2018. Topher Bullock gave a great intro to Concourse in the Open Source track. We even met some of the Concourse fans in person!</p> <p>In other news, we\u2019ve begun to sketch out what RBAC might look like in Concourse. Please check out #2389 when you have some time!</p> <p>On to the update:</p>"},{"location":"blog/2018-07-20-concourse-update-jul-1620/#ux","title":"UX","text":"<ul> <li>Team has been working on adding drag and drop re-arranging for the dashboard   in #2364</li> <li>We also found a weird quirk with the new team creation flow, where you won\u2019t see your team if it was just created and   has no pipelines. We will be addressing this in #2382</li> <li>Play/pause pipeline on the dashboard is mostly completed but was missing functionality when a search filter was   applied; so I had to reject that story for review in #2365</li> <li>Lindsay Auchinachie has also been entering some new UI polish issues to coincide   with our new dark   theme: #2370, #2385, #2387, #2361</li> </ul>"},{"location":"blog/2018-07-20-concourse-update-jul-1620/#core","title":"Core","text":"<ul> <li>Picked up some stories related our migrations, see #2380   and #2074</li> <li>Keen watchers of our repo will notice that we\u2019ve added a note in   our core backlog   to start sketching out what additional work we need to get space moving along.</li> <li>Reminder to check out and comment on   the Resources v2 proposal!</li> </ul>"},{"location":"blog/2018-07-20-concourse-update-jul-1620/#integrations","title":"Integrations","text":"<ul> <li>We closed #215 in the docker-image-resource recently   after we discovered a regression with a newer version of Docker. This seems to only affect large-scale Concourse   installations that have reliability issues accessing and connecting to their local registries. A short-term fix is to   target older versions of the docker-image-resource</li> </ul>"},{"location":"blog/2018-07-20-concourse-update-jul-1620/#runtime","title":"Runtime","text":"<ul> <li>Addressed #1516, wherein Concourse doesn\u2019t run any jobs if Vault   misconfigured</li> <li>Did some work to begin imposing limits on containers in #787.   Please review this issue carefully if this affects you; since our initial resolution is very specific and requires you   to understand the nature of your worker vms</li> <li>Worked on #2375 \u201cListing destroying volumes should not perform   any database write operations\u201d :D</li> </ul>"},{"location":"blog/2018-07-27-concourse-update-jul-2327/","title":"Concourse Update (Jul 23\u201327)","text":"<p>Concourse v4 Dashboard</p> <p>I\u2019m happy to announce that we released Concourse 4.0.0 this week! This was a HUGE release with over 28 new features and fixes. I\u2019d encourage you to read through the full list of changes on our Downloads page.</p> <p>Why did this release warrant a bump in the major version? Well, if you\u2019ve been following along closely you\u2019ll know that we had just finished our new auth work in 4.0.0. Users are now central to the authentication flows, not teams. Practically speaking, the user-centric auth flow means that you won\u2019t need to re-login to see pipelines in other teams that you already have access to! Underneath the hood though, \u201cWe\u2019re leveraging CoreOS\u2019s Dex project for all the moving parts, which already supports a ton of providers (Dex calls them \u201cconnectors\u201d). The only delta required for Concourse to support a Dex connector is a tiny bit of glue code in our new Skymarshal component to provide higher-level flags for our CLI.\u201d</p> <p>We spent a lot of time near the end of this cycle trying to make these changes backwards compatible, but ultimately decided that the changes were significant enough to warrant a bump in the major version. PLEASE PLEASE PLEASE refer to our release notes for all the breaking changes before executing your upgrade!</p> <p></p> <p>The second big change you\u2019ll notice in 4.0.0 is that the home (/) route now takes you to the dashboard. We\u2019ve also propagated the new colour scheme to the rest of the app and tightened up the fonts throughout the app.</p> <p>We hope you like it!</p> <p>So, what\u2019s next? We\u2019re focusing on three key areas:</p> <ul> <li>Resources v2 and Spatial resources.   Please review and comment on the RFC!</li> <li>Runtime efficiency &amp; Operational observability   into Concourse</li> <li>Role based access control. That\u2019s right, we\u2019re finally doing it. Please   read the RFC for this change. You can also find a copy of our initial permission   matrix here</li> </ul>"},{"location":"blog/2018-08-03-concourse-update-july-30--aug-3/","title":"Concourse Update (July 30 \u2014 Aug 3)","text":"<p>With the launch of Concourse 4.0.0, we\u2019ve been monitoring our typical communication channels carefully to watch out for any glaring new bugs. So far we seem to be safe from any crazy issues, but we have noticed that there has been some confusion in how to set the basic auth users in the new deployment method (see #2421 for details). Thanks everyone for your patience and working through these issues with us!</p> <p>The Concourse team will also be taking Monday, Aug 6 off for Canada\u2019s Civic Holiday. We\u2019ll be back at it on Tuesday,  Aug 7.</p> <p>On to the update:</p>"},{"location":"blog/2018-08-03-concourse-update-july-30--aug-3/#ux","title":"UX","text":"<ul> <li>You\u2019ll notice that our UX backlog is filled to the brim with clean-up and polish stories. Now that we\u2019ve release   4.0.0, we\u2019re taking some additional time to slow down to perform some additional polish and refactors</li> <li>Of note we have one regression which is prioritized highly \u201cNew resources are no longer highlighted in   UI\u201d #2423</li> <li>A lot of folks have noticed that the sidebar has been removed, and a bid to bring it back has started with   issue #2440</li> </ul>"},{"location":"blog/2018-08-03-concourse-update-july-30--aug-3/#core","title":"Core","text":"<ul> <li>We\u2019ve been working on a track of stories around \u201cpinning\u201d a version of a resource across the   pipeline #2439   and #2386</li> <li>Database migrations have always been a headache for us and we\u2019ve been looking at   issues #2074   and #2452</li> </ul>"},{"location":"blog/2018-08-03-concourse-update-july-30--aug-3/#runtime","title":"Runtime","text":"<ul> <li>We finally got rid of Yeller support #1819. I have no idea what   that did, or why it was there; but good riddance</li> <li>The much requested feature to stream build logs out is being worked   on #2104</li> </ul>"},{"location":"blog/2018-08-10-concourse-update-aug-710/","title":"Concourse Update (Aug 7\u201310)","text":"<p>As I mentioned last week, this was a short week for us in Canada due to the Civic Holiday. We did, however, manage to work on some pretty cool stuff!</p> <p>With the release of 4.0.0, we\u2019ve been shifting our new feature focus towards Operations and Runtime. We\u2019re intentionally slowing down on UX to focus on regressions and UI polish for existing screens.</p> <p>On to the update:</p>"},{"location":"blog/2018-08-10-concourse-update-aug-710/#core","title":"Core","text":"<ul> <li>Continued our work on \u201cpinning\u201d a version of a resource across the pipeline.   Completed #2439 but still   have #2386 in flight</li> </ul>"},{"location":"blog/2018-08-10-concourse-update-aug-710/#runtime","title":"Runtime","text":"<ul> <li>Wrapping up work around streaming build logs to an external   target #2104</li> <li>Investigated the issue around External Worker affinity on ATCs when using external   workers #2312</li> <li>Picked up the issue for adding a configurable timeout for resource and resource type   checks #2352</li> </ul>"},{"location":"blog/2018-08-10-concourse-update-aug-710/#operations","title":"Operations","text":"<ul> <li>In our continued exploration of k8s Helm Chart for Concourse, we\u2019re looking into how we might auto-magically generate   helm chart parameters #2472</li> </ul>"},{"location":"blog/2018-08-10-suspicious-volume-usage-on-workers/","title":"Suspicious Volume Usage on Workers","text":"<p>As a Product Manger at Pivotal I\u2019m often called on to help with our customer\u2019s Concourse-related issues. I recently spent some time hunting down an issue around suspiciously high volume usage on Concourse workers. It was an interesting problem that I wanted to share with the broader Concourse community.</p>"},{"location":"blog/2018-08-10-suspicious-volume-usage-on-workers/#platform-management-pipelines","title":"Platform Management Pipelines","text":"<p>One of the primary use case for Concourse within the Pivotal Cloud Foundry (PCF) ecosystem is to automate the toil of manual maintenance against the platform; specifically PAS and PKS. For the purposes of this issue, the specific details of the pipeline doesn\u2019t matter but an understanding the general flow of the pipeline will help frame problem:</p> <p></p> <p>A simplified version of a pipeline used to pull updates from the Pivotal Network and apply the changes onto Ops Manager</p> <p>In these pipelines the <code>pivnet-resource</code> is responsible for monitoring new product versions on network.pivotal.io (aka PivNet). When a new product version is released on PivNet, the <code>pivnet-resource</code> picks it up and initiates a download. These files are relatively large, from 500mb to over 1 GB</p>"},{"location":"blog/2018-08-10-suspicious-volume-usage-on-workers/#recreate-all-the-workers","title":"Recreate all the workers?","text":"<p>Over the course of the past year or so we would get sporadic reports of customers who used Concourse for PCF management running out of space on their workers. The typical manifestation of it comes from a <code>failed to stream in volume</code> error. It would appear that workers were running out of space; but it wasn\u2019t clear why. To mitigate the issue Concourse operators would be forced to periodically re-create their workers to get a \u201cclean slate\u201d.</p>"},{"location":"blog/2018-08-10-suspicious-volume-usage-on-workers/#but-why","title":"But why?","text":"<p>Having to periodically recreate workers is a huge pain and it doesn\u2019t give operators a lot of confidence in Concourse itself. The team decided to take a look into the root cause of this issue. We wanted to understand whether this was a bug in the system and whether we could do something to address it.</p> <p>After some poking and prodding, I think we figured out what was happening in this specific scenario. Using the same simplified pipeline above, consider the following scenario:</p> <p></p> <ul> <li>At <code>t=0</code> the pipeline is configured and idling; monitoring the external system for a new version.</li> <li>At <code>t=1</code> a new version of the \u201cMetrics\u201d product is released on PivNet, picked up by Concourse, downloaded and begins   to flow through your pipeline</li> <li>At <code>t=2</code> the <code>Upload to OM</code> (OM == Ops Manager) job kicks off and does its thing</li> <li>At <code>t=3</code> the artifact is used for some long running process like <code>Apply Changes</code> on OM. Concourse will hold on to that   downloaded data since its still running</li> </ul> <p>But wait, what\u2019s that new <code>Metrics 1.0</code> box in deep blue at <code>t=3</code>? Well, it's not uncommon for the metadata of a release to be modified just-after release. This could be a tweak to metadata (e.g. support dates, dependencies, supported stemcells, etc.), which causes PivNet to report a new version. Semantically, its still reported as Metrics v1.0 but Concourse will pick it up nonetheless. Because of this change we have effectively doubled the amount of storage used!</p>"},{"location":"blog/2018-08-10-suspicious-volume-usage-on-workers/#i-think-we-learned-a-valuable-lesson-today","title":"I think we learned a valuable lesson today...","text":"<p>The problem I described was a specific to the usage of the <code>pivnet-resource</code>, but there are a lot of common takeaways:</p> <ul> <li>Spend some time to understand your resources! The specific implementation of <code>check</code> can drastically affect your   pipeline</li> <li>Be wary of using large files with long running jobs. I could see how someone could easily re-create a similar scenario   with other resources</li> <li>Consider separating out the act of downloading an artifact from the act of operating on the artifact. For example, I   found that other teams in Pivotal worked around this by landing their PivNet artifacts in an s3 bucket and picking it   up in the next job via the <code>s3-resource</code></li> <li>Set up some monitoring! You can catch errors and creeping disk use this   in metrics dashboards</li> </ul>"},{"location":"blog/2018-08-10-suspicious-volume-usage-on-workers/#no-seriously-why-does-this-happen","title":"No seriously, why does this happen?!","text":"<p>In this blog post I covered a lot of the symptoms of the problem and touched on some abstract reasoning on why this happens. In the next blog post we (and by \u201cwe\u201d I mostly mean Topher Bullock) will cover the specific technical details of Resource Caching on Concourse so you can have a better understanding of exactly why this happens.</p>"},{"location":"blog/2018-08-15-psa-the-great-code-restructuring-of-2018/","title":"PSA: The Great Code Restructuring of 2018","text":"<p>Hi y\u2019all!</p> <p>As anyone trying to contribute will have probably noticed by now, we have a bunch of repos, and most of them are inter-dependent. This makes it difficult to contribute PRs, because you\u2019ll often have to PR to <code>fly</code>, <code>go-concourse</code>, and <code>atc</code> and cross-reference them. But then each of their builds will fail because they won\u2019t have their dependent changes (e.g. <code>fly</code> will fail until <code>go-concourse</code> is merged and bumped, but <code>go-concourse</code>'s build will fail until <code>atc</code> is merged and bumped, which we don\u2019t want to do because the other builds are failing!).</p> <p>Another, more human problem: our repo names are just plain ol\u2019 confusing. The toplevel <code>concourse/concourse</code> repo is actually our BOSH release, because it happened to be a good fit to also act as our <code>$GOPATH</code>. But then we have all the actually important code living in a repo called <code>atc</code> (wat?). The <code>fly</code> repo name makes sense 'cause that\u2019s the name of the CLI, but then there are components like <code>tsa</code> and <code>skymarshal</code> and nothing makes sense anymore. (Also our metaphors are all over the place.)</p> <p>Finally, Go 1.11 is coming out soon (RC1 is out now) and its new support for \"Go modules\" is looking like the best thing since sliced bread. We\u2019ve been looking for a way out from our submodule hell for a long time, and it looks like our savior is here. But in relying so heavily on Git submodules we\u2019ve papered over lots of coupling that has existed between these repos, because it\u2019s been so easy to pin to the specific refs that work together. Go\u2019s module system will only make this more painful, because our API contracts between the repos just aren\u2019t strong enough. And it won\u2019t help at all for people contributing PRs.</p> <p>So, here\u2019s the plan:</p> <ul> <li>Rename <code>concourse</code> to <code>concourse-bosh-release</code>. This will make it clearer that the BOSH release is just one   distribution method, not the centerpiece of all of Concourse.</li> <li>Introduce a new <code>concourse</code> repo which will contain the sum of <code>atc</code>, <code>fly</code>, <code>go-concourse</code>, <code>tsa</code>, and any other   components that are strongly inter-dependent.<ul> <li>In doing this, we\u2019ll try to carefully name things and demarcate portions that should be separate. We don\u2019t want   this to encourage tight coupling, only to express the current state of things.</li> <li>This would be a good time to come up with clearer names for things. These names are cute but not exactly clear for   someone wanting to come in and find where they need to change code for the feature they want to implement.</li> </ul> </li> <li>Somehow keep all the GitHub issues on <code>concourse/concourse</code>. This might mean we actually gut the repos instead of   renaming them. But I don\u2019t want to lose the commit history, so this may be difficult.   Once all this is done it should be much easier to submit larger changes as PRs, and contributing to Concourse should   feel a lot more intuitive.</li> </ul> <p>We\u2019re still early in the planning stages for this, but I thought I\u2019d give the community a heads-up so y\u2019all know what we\u2019re doing. I don\u2019t expect this to take that long, but it is inherently a \u201cstop the world\u201d process. This may mean some PRs have to be re-submitted if they\u2019re made just prior to us moving everything around. I\u2019ll try to whittle down the number of PRs before we make any big moves.</p>"},{"location":"blog/2018-08-17-concourse-update-august-1317/","title":"Concourse Update (August 13\u201317)","text":"<p>Combining repos for great justice</p> <p>Going to switch things up this week and start with some interesting community news:</p> <ul> <li>We\u2019ve decided to restructure our repositories to make things more understandable and less scary for   contributors. Alex Suraci has laid out a good explainer on why and how we\u2019re   going to start in   our PSA: the Great Code Restructing of 2018</li> <li>Lindsay Auchinachie wrote up a blog post describing some of the visual elements   of the Concourse pipeline view in a blog post   titled Concourse Pipeline UI Explained</li> <li>marco-m has been updating a \u201cconcourse-in-a-box\u201d formula that comes with a s3-compatible-store and a Vault. Check it   out here: https://github.com/marco-m/concourse-ci-formula</li> <li>concourse-up is a Concourse quick-start tool created by our   friends at EngineerBetter. The team there is looking for feedback on how to support the 4.0.0 authentication scheme   moving forwards. If you use their tool, please take some time to give them some love on their GitHub   issue https://github.com/EngineerBetter/concourse-up/issues/62</li> <li>Is our efforts to have Concourse un-flaky a myth? Find out on this forum post   by eedwards-sk, you won\u2019t believe   post #4!!</li> </ul> <p>On to some development news:</p> <ul> <li>We\u2019re still hacking away at   issue #2425 \u201cLogin session expired\u201d error with multiple ATCs.   Please check in on the story to follow along with our plans for a fix (it involves some migrations)</li> <li>Praise be, we fixed the UX regression on the resources page where new resources weren\u2019t being   highlighted #2423</li> <li>Still refactoring away to make way for #2386 in preparation for   spaces</li> <li>Finished #2352on Configurable timeout for resource checks. Turns   out that by fixing that issue we also fixed #2431. We did end up   making #2494 though in order to break out the specific ability   to configure a timeout for resource type checks.</li> <li>Paused work on #2104 on emitting build logs to an external   system this week, hoping to pick it back up next week!</li> </ul>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/","title":"Concourse Pipeline UI Explained","text":"<p>The Concourse UI is continuously growing and improving, and the goal of this article is to surface the latest design elements of the Concourse UI. Using our production pipeline, it will become easier to connect the mapping of an element to its meaning and purpose.</p> <p>The primary function of the pipeline web view is to show the flow of versioned resources through the connected jobs that make up a Concourse pipeline.</p> <p></p> <p></p> <p>Concourse Pipeline</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#what-is-the-concourse-pipeline-view-composed-of","title":"What is the Concourse Pipeline View composed of?","text":"<ol> <li>Resources and their corresponding state</li> <li>Jobs and their corresponding state</li> <li>Running state: Displayed as pulsating rings that appear yellow (running) and red (failed)</li> <li>Groups: users can show/hide groups of jobs and resources configured in the manifest to create focused views of    their pipeline</li> <li>Legend: Mapping of colour to status of jobs and resources<ul> <li>Fly CLI download for Linux, OS and Windows</li> <li>Concourse Version number</li> </ul> </li> </ol>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#actions-users-can-take-on-a-pipeline","title":"Actions Users can take on a pipeline","text":"<p>When users interact with this view they can take action to:</p> <ul> <li>Navigate to a Resource page</li> <li>Navigate to a Build page</li> <li>Hover to trace thread of a resource in the pipeline path</li> <li>Toggle view of groups</li> <li>Zoom, pan, and fit view (in Chrome you can use keyboard shortcut <code>f</code> to fit)</li> <li>Download the fly CLI for MacOS, Windows, Linux</li> </ul>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#1-resources-flow-of-artifacts-through-the-pipeline","title":"1. Resources: Flow of artifacts through the pipeline","text":"<p>Analyzing the Concourse production pipeline, everything starts off with the <code>concourse</code> GitHub resource. In our <code>pipeline.yml</code>, the <code>concourse</code> resource points to the main <code>concourse/concourse</code> GitHub repository.</p> <p></p> <p></p> <p>Concourse pipeline yaml (L) Concourse Web view (R)</p> <p>As Concourse is able to reach GitHub and is in a healthy state, the resource is rendered as a black box with white text.</p> <p>Zooming in on the beginning of the pipeline, we see that the <code>concourse</code> resource is connected to the <code>atc</code>, <code>fly</code>, <code>tsa</code>, <code>web</code>, <code>worker</code>, <code>skymarshal</code>, <code>baggageclaim</code> and <code>go-concourse</code> jobs by a solid line. This means that when a new version of the <code>concourse</code> resource is detected (a new commit into the repo), the subsequent jobs are automatically triggered to run the tasks of the job in the worker container Concourse spins up.</p> <p></p> <p></p> <p>\u2018bump atc\u2019 is the change that triggers the pipeline to kick-off</p> <p>Resources reference external systems such as Git repos, S3 buckets, docker-images, Slack notifications, and timed resources to name a few. These are rendered in the UI as a black box with white or grey text. The difference between grey text and white text is that white text is the first instance of the resource in the pipeline and is typically where a new version will appear. Subsequent references to that resource will be grey text; it is now a subset of something that appeared before it in the pipeline.</p> <p>In our example, the <code>concourse</code> resource that is inputted to those jobs, is repeated to indicate that it is being transferred to the next job (<code>rc</code>), and thus has grey text.</p> <p></p> <p>Errored Resource</p> <p>Resource errors are indicated in the interface by the resource box turning amber with white text. This means there is something wrong at the resource level, and it\u2019s time to do some investigation as to why.</p> <p>Paused resources are rendered in the UI as a blue background to the resource box.</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#2-jobs","title":"2. Jobs","text":"<p>Jobs are the execution phase of the pipeline and are composed of tasks and resource actions (get, put) defined in the build plan. The pipeline view today is Job Centric, allowing the app developer to monitor their code as it progresses through various jobs.</p> <p></p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#connecting-jobs-resources","title":"Connecting Jobs &amp; Resources","text":"<p>In Concourse, versioned resources connect to jobs through the input and output parameters. In our pipeline view, we can visually represent this as lines connecting jobs with resources in-between. A user can trace the path of a resource by hovering over one of these lines; exposing the full path of a resource across jobs.</p> <p></p> <p>Lines connecting the Resources to Jobs</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#triggering-vs-non-triggering","title":"Triggering vs non-triggering","text":"<p>Not all jobs are triggered automatically by incoming resources. This behaviour is typically specified in the pipeline yml as <code>trigger: true</code>. This is represented as a solid line between jobs. When a job is <code>trigger: false</code>, this relationship is represented by a dotted line between jobs.</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#3-running-state","title":"3. Running State:","text":"<p>A yellow halo pulsating out from the job box indicates that the job is in a running state. This gives the user an at a glance view at what jobs are running in real time. A job could be kicked off as a automatic trigger, or a manually triggered job.</p> <p></p> <p>Running state halo</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#4-navigation-and-groups","title":"4. Navigation and Groups:","text":"<p>The Concourse propellor in the top navigation lets a user navigate to the dashboard via the concourse logo.</p> <p>Groups: Jobs can be grouped in the pipeline yml for better visibility. In the pipeline view, multiple groups can be selected by <code>shift</code> selecting the group names.</p> <p></p> <p>Groups on the pipeline view</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#5-legend","title":"5. Legend","text":"<p>We have found that users want a visual reminder of what the various job states and visual indicators mean in the pipeline UI. The colours of the job boxes indicate to the user their state.</p> <p>Green indicates that the job has succeeded</p> <p>Red indicates that the job has failed</p> <p>Orange indicates that the job or resource has failed. We use this convention so that users will learn to trust their job failures (red).</p> <p>Blue indicates that the job or resource is paused. This indicates to the user that a job or resource has intentionally been paused, causing a disruption to the normal build cycle in their pipelines</p> <p>Brown indicates that a user has cancelled a job.</p> <p>Grey indicates that a job is pending.</p> <p>The yellow halo surrounds a job to indicate that the job is running.</p> <p>Red halo indicates that the job was running and has failed. In order to help with colour blindness we also added a handy fail icon; represented by a warning triangle at the top of the job column.</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#different-pipelines-for-different-teams","title":"Different pipelines for different teams","text":"<p>No two pipelines are rendered the same in Concourse. The visualization of the pipeline is displaying how a team does CI and their unique path to production.</p>"},{"location":"blog/2018-08-17-concourse-pipeline-ui-explained/#feedback","title":"Feedback","text":"<p>Stay tuned for more UI explained in this series as we tackle the Resource page the Build page next! As always, feedback is welcome via issues in the concourse and design-system repo!</p>"},{"location":"blog/2018-08-24-concourse-update-august-2024/","title":"Concourse Update (August 20\u201324)","text":"<p>Logs and resources</p>"},{"location":"blog/2018-08-24-concourse-update-august-2024/#kubernetes","title":"Kubernetes","text":"<p>As we continue our sporadic work on Kubernetes and its Helm chart, we\u2019re also starting to expand our thinking to cover the runtime aspects of Concourse + Kubernetes. We\u2019ve already prioritized the need to have Kubernetes as a supporting backend in addition to Garden, but what about the spiffy new developments in the Kubernetes world? We\u2019re hearing a lot about <code>knative</code> and <code>knative</code> services like <code>build</code> and <code>eventing</code>. Are there any kubernetes users who\u2019d like to weigh in on the topic? Let us know on our forums!</p>"},{"location":"blog/2018-08-24-concourse-update-august-2024/#request-for-comment","title":"Request for Comment","text":"<p>We\u2019ve written a new RFC titled: Merge <code>resource</code> and <code>resource_type</code>s. s. This RFC came about as a result of the work in pinning resources and #2386. You can comment on the RFC PR here</p> <p>We are also stuck on #2104 Streaming of Build Logs to Additional Target. We\u2019re specifically looking on feedback on the following questions before moving forward:</p> <ul> <li>Our test for syslog is flakey: failing/hanging sometimes</li> <li>Need to backfill a test for updating the <code>drained</code> column</li> <li>What metadata do we want to send with the build log? team/pipeline/job/build names?</li> <li>Is there a possibility for the build logs to be reaped before they are drained?</li> <li>What kind of database locks do we need for the operation?</li> </ul> <p>If you have an insight to shed on these questions, please hop on over to the issue #2104</p> <p>Here are the rest of the updates for this week:</p>"},{"location":"blog/2018-08-24-concourse-update-august-2024/#ux","title":"UX","text":"<ul> <li>Long pipeline names in the dashboards will now have a tooltip to let you read the full pipeline   name #2411</li> <li>Login button alignment on mobile views is pushed up #2433</li> </ul>"},{"location":"blog/2018-08-24-concourse-update-august-2024/#api","title":"API","text":"<ul> <li>Finished up the work on the multiple ATC login issue #2425. The   fix for this will require a db migration in the next version of Concourse!</li> <li>Added the ability to do a <code>fly check-resource-type</code> #2507 in order   to support #2494 resource type <code>check_timeout</code></li> </ul>"},{"location":"blog/2018-08-24-concourse-update-august-2024/#core","title":"Core","text":"<ul> <li>Continued work on issue #2386 <code>Equivalent resources defined   across pipelines and teams should only correspond to a single version history</code>. The work here has led to the creation   of the new RFC mentioned above: Merge <code>resource</code> and   <code>resource_type</code>s.</li> </ul>"},{"location":"blog/2018-08-24-concourse-update-august-2024/#operations","title":"Operations","text":"<ul> <li>We\u2019ve picked up an issue that aims to be better at inferring defaults for peer/external   URLS #2519. This should help with some of the 4.0.0 upgrade and   installation issues.</li> </ul>"},{"location":"blog/2018-08-24-concourse-resource-volume-caching/","title":"Concourse Resource Volume Caching","text":"<p>This is a follow-up to James Ma\u2019s post on \u201cSuspicious Volume Usage on Workers\u201d. James also helped out with the illustrative diagrams for this post!</p> <p>The Concourse team commonly gets questions about how Concourse treats resource caching on workers. For example, consider this scenario:</p> <p>If 3 pipelines on 3 teams use the same resource (let\u2019s say the  <code>pivnet-resource</code>) with the same source configuration (e.g. <code>product_slug: p-metrics</code>); does Concourse download and store the resource (the <code>p-metrics</code> tile) 3 times and store 3 cached volumes individually?</p> <p>Let's dig in to how this works under the hood:</p> <p>When Concourse needs to fetch a version of a resource a new container is spun up and allocated to a worker. This new container will be used to download the new version of the resource into the newly created Resource Cache Volume on the Baggage Claim portion of the worker. The reference to the Resource Cache Volume is stored in DB as an entry in the volumes table, referenced by the <code>worker_resource_cache_id</code> column.</p> <p></p> <p>For simplicity only the Worker and Database are shown; other components like ATC/TSA are not present</p> <p>As a rule of thumb the number of times a resource version is downloaded depends on:</p> <ul> <li>How the jobs from the 3 teams are scheduled across the pool of workers</li> <li>Whether the resource configuration across teams and pipelines is identical</li> </ul> <p>Consider the same scenario described above, where we have three different pipelines from three different teams running on the same Concourse instance.</p> <p>Each pipeline is defined to use the <code>pivnet-resource</code>, but are configured to reference different product slugs. For simplicity, suppose the ATC has allocated each \u201cresource get\u201d on to three separate workers:</p> <p></p> <p>As you can expect, because the steps happen on different workers, the 3 different products are downloaded to 3 distinct worker resource cache volumes.</p> <p>What happens when Pipeline 1 and Pipeline 2 get steps are allocated onto the same worker?</p> <p></p> <p>In this scenario, the files for the two resources are downloaded onto the same worker, while the third worker remains empty.</p> <p>Let\u2019s consider one more scenario. Team 2 updates Pipeline 2 to download <code>p-metrics</code> as well; and they end up having identical resource configs, secrets and all (assuming they use the same API token):</p> <p></p> <p>In this case, Concourse is aware that the two teams have set pipelines with identical resource configs. This results in a single volume where <code>p-metrics</code> is only downloaded once. This is safe to do because resource cache volumes are only ever mounted into tasks Copy on Write (COW) volumes, so the original source can\u2019t be mutated.</p> <p>A <code>get</code> step will correspond to a <code>resource_cache</code> entry in the database, which associates the version and any params to the <code>resource_config</code> in the database.</p>"},{"location":"blog/2018-08-24-concourse-resource-volume-caching/#i-thought-pipelines-were-isolated-what-about-my-teams-secrets","title":"I thought pipelines were isolated! What about my team\u2019s secrets?","text":"<p><code>resource_config</code>\u2018s hash the source with the secret values interpolated. If the values of the secrets are the same they\u2019re considered the same <code>resource_config</code>. So if 2 teams use the same git resource private key, they\u2019ll share the worker resource caches of a version of that git resource. All a user would be able to see is the credentials they configured and know are there, which they can already retrieve in other ways. As a user, I have no way of knowing whether someone is using the same secrets I\u2019m using.</p> <p>We\u2019re actively considering moving this behaviour up the chain, and having identical resources share their version history across teams.</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/","title":"Concourse Build Page Explained","text":"<p>Continuing on our Concourse UI Explained series, this week we are taking a look at a Concourse <code>job</code>\u2019s <code>build</code> page. As always, we will be using our production pipeline as a guide to connect the mapping of an element to their meaning and purpose.</p> <p></p> <p>Fly-rc build page in the ci.concourse-ci.org production pipeline</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#what-is-in-a-build","title":"What is in a Build?","text":"<p>As we discussed previously in the Concourse Pipeline UI Explained, Concourse helps developers visualize the flow of artifacts across jobs and builds. Jobs determine the actions of your pipeline, how resources progress through it. Every job has a build plan that determines the sequence of steps to execute the job.</p> <p>In Concourse, the execution of a job and its build plan can be viewed on the Build page for that job. As the build plan executes, information pertaining to the build progress e.g. <code>get</code> steps, <code>put</code> steps, and  <code>task</code> steps is updated in real time on the build page itself.</p> <p></p> <p></p> <p>The yml build plan for the ci.concourse-ci.org production pipeline.</p> <p>Using our production pipeline for Concourse as an example, the syntax for the job <code>atc</code> consists of the following steps: getting the resource concourse pulled from the <code>concourse repo</code> and running <code>check-bindata</code> and <code>go-unit</code> tests.</p> <p>As a user, I want to monitor this build so, in the web UI, I can select the <code>atc</code> job box from the pipeline UI to navigate to the build page for the <code>atc</code> job.</p> <p></p> <p>This is where an engineer can find the resource version, input versions that have passed, metadata for that version, output of results and logs of that build which is composed of integration and unit tests.</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#what-is-this-view-composed-of","title":"What is this View composed of?","text":"<ol> <li>Build status displayed as the background colour as the top banner</li> <li>Job name, build number and build metadata (started, finished, duration)</li> <li>Rendering of the <code>yml build plan</code> as Input resources and Outputs.</li> <li>Historical builds \u2014 the results of recent builds of that job. Did you know you can scroll that list with the mouse    wheel? Give it a shot!</li> <li>Output Logs for current and past builds that are coloured and time stamped for debugging</li> </ol>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#actions-users-can-take","title":"Actions Users can take:","text":"<ul> <li>Users can abort a build by selecting the <code>x</code> button when the build is preparing to run or currently running.</li> <li>Users can trigger a new build by clicking the <code>+</code> button on the web UI of a job</li> <li>Share specific build outputs with your team members. Selecting multiple lines of output in the logs will update the   URL to reflect the line's users have selected and they can then use this URL to share out specific outputs.</li> <li>Past builds are numbered and coloured with the past resulting status of each build. Users can select a previous build   in the tabbed horizontal list.</li> </ul>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#build-status","title":"Build Status","text":""},{"location":"blog/2018-08-30-concourse-build-page-explained/#status-of-a-build","title":"Status of a Build","text":"<p>The coloured banner behind the job name and build number gives the user an at a glance understanding of the status or stage of a build in real-time.</p> <p>A build has multiple stages: Pending, Running, Paused, Failing, Errored, and Aborted.</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#pending-builds","title":"Pending Builds","text":"<p>Pending due to waiting on \u2018passed\u2019 constraints or new versions of a resource is indicated in the UI by a grey status in the banner of the build and in the metadata. When a build is kicked off the first feedback the user will see that a build is pending.</p> <p></p> <p></p> <p>In this example, the <code>bosh-watsjs</code> build is pending on new versions as described in the pipeline.yml</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#running","title":"Running","text":"<p>Yellow indicates that the job is running and logs will begin to stream in real-time to the user.</p> <p></p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#succeeded","title":"Succeeded","text":"<p>Green indicates that the job has succeeded and the job is complete. Red status in the banner tells the user that the job has failed at some point of the build.</p> <p></p> <p></p> <p>Success Green (L) Red failing (R)</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#error","title":"Error","text":"<p>Orange indicates that something went wrong internally when trying to run the build. This is different from Red so that the user can trust that all Red failures are a result of misconfiguration/bad code and are something that they likely need to fix.</p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#aborted","title":"Aborted","text":"<p>Aborted builds have a brown banner. Builds are aborted when a user stops the job in the UI or through the <code>fly</code> CLI.</p> <p></p> <p></p> <p>Error (L) Aborted (R)</p> <p>The Build states are also reflected as browser Favicons for easy scanning when a user is in another tab of the browser.</p> <p></p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#logs","title":"Logs","text":"<p>Users are looking to the logs to see that tests are working the way they expect them to and when they fail what point did that failure occur.</p> <p>When a user navigates to a failed build the browser automatically scrolls to the line of the log which is failing because the output logs can be very, very long.</p> <p>Concourse uses colour coding for consistency with the output of the terminal and visual parsing when debugging.</p> <p>Timestamps on the left of the logs are reported against each line of output using the users browser reported timezone. These are important when debugging as a user; because I want to know how long individual actions are taking while they are doing the build and running tests.</p> <p></p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#triggering-aborting-builds","title":"Triggering / Aborting Builds","text":"<p>A job can be triggered by clicking the <code>+</code> button on the web UI of a job on the build page. Users also have the power to abort the build in the web UI.</p> <p></p> <p></p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#build-history","title":"Build History","text":"<p>Build History is listed in descending order as horizontal tabs. Users can scroll through them using the mouse wheel to access older build histories.</p> <p>For jobs with lots of build histories, older logs may have been reaped. This can be specified the  <code>build_logs_to_retain</code> parameter on a job.</p> <p></p> <p>you served us well <code>bosh-wats-periodic #10</code></p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#iconography-in-the-build","title":"Iconography in the Build","text":"<ol> <li>New versions are indicated by a yellow down arrow preceding the name</li> <li>Older versions to this job are white arrows</li> <li> <p>_ indicates a task</p> </li> <li>Successful check of a version</li> </ol> <p></p>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#keyboard-shortcuts","title":"Keyboard shortcuts","text":"<p>The build page also supports basic vim-style keyboard shortcuts as well. You can bring up a reference menu using <code>?</code> or <code>SHIFT + /</code></p> <p></p> <p>The supported keyboard shortcuts are:</p> <ul> <li><code>h</code> and <code>l</code> for previous/next build</li> <li><code>j</code> and <code>k</code> for scrolling up and down</li> <li><code>T</code> to trigger a new build</li> <li><code>A</code> to abort the build</li> <li><code>gg</code> to scroll back to the top of the page</li> <li><code>G</code> to scroll to the bottom of the page</li> <li><code>?</code> to toggle the keyboard hint on and off</li> </ul>"},{"location":"blog/2018-08-30-concourse-build-page-explained/#feedback","title":"Feedback","text":"<p>Stay tuned for more UI explained in this series as we tackle the Resource page next! As always, feedback is welcome via issues in the concourse and design-system repo.</p>"},{"location":"blog/2018-08-30-concourse-update-aug-2731/","title":"Concourse Update (Aug 27\u201331)","text":"<p>Photo courtesy of the CNE</p> <p>Apologies for the break from the usual update schedule; I wanted to get one last update out before I take some personal time, starting Fri. Aug 31 and coming back Sept 10. In my absence Scott Foerster and Alex Suraci will be writing the product update next week. The Concourse team will also be taking Monday, Sept 3rd off in observance of Labour day as well.</p> <p>On to the updates:</p> <ul> <li>Concourse 4.1.0 will be out\u2026soon! We\u2019ve begun the process of accepting all stories and deploying our pre-release   version onto the internal test environments. If you\u2019re curious as to what new features/bug fixes are coming out in   this release, you can get an at-a-glace view in our Milestones page.    You can expect the official release to come out very soon :D</li> <li>Lindsay Auchinachie wrote another entry in her Concourse UI Explained series;   this time covering the Concourse Build page.</li> <li>The Concourse mono-repo is coming! You can read more about the change in   issue #2534. Work on this will continue the moment we release   4.1.0</li> </ul>"},{"location":"blog/2018-08-30-concourse-update-aug-2731/#ux","title":"UX","text":"<ul> <li>Worked on some UI improvements to help users distinguish between teams they belong to vs exposed   pipelines #2427</li> </ul>"},{"location":"blog/2018-08-30-concourse-update-aug-2731/#core","title":"Core","text":"<ul> <li>Continued refactoring work on #2386</li> <li>Worked on discussion regarding a PR from GitHub user edtan to resolve   issue #2511</li> </ul>"},{"location":"blog/2018-08-30-concourse-update-aug-2731/#runtime","title":"Runtime","text":"<ul> <li>Since there didn\u2019t seem to be any strong opinions on how we managed log outputting   in #2104, we\u2019ve decided to move forward with some reasonable   assumptions.</li> </ul>"},{"location":"blog/2018-09-07-concourse-update-sept-03--sept-07/","title":"Concourse Update (Sept 03 \u2014 Sept 07)","text":"<p>\u201cDisco\u201d Dirk Nowitzki approves of release 4.1</p> <p>Howdy, Concourse community. James Ma is still enjoying his much deserved time off, so I will be taking over the weekly update today with an assist from Alex Suraci. Short week for the Concourse team with the office closed for Labour Day on Monday, but there was still a lot happening.</p> <p>Here\u2019s what we\u2019ve been up to:</p>"},{"location":"blog/2018-09-07-concourse-update-sept-03--sept-07/#concourse-410","title":"Concourse 4.1.0","text":"<p>As promised in last week\u2019s update, Concourse v4.1.0 was officially released last Thursday afternoon. The release had a bunch of valuable pull requests from the community, which was great to see. While the core team were mostly focused on fixing bugs, the community contributed a lot of the key features within the release. Expect 4.1.1 out early next week with more fixes.</p>"},{"location":"blog/2018-09-07-concourse-update-sept-03--sept-07/#ux","title":"UX","text":"<ul> <li>Started on an update that allows users to insert a token manually during login when running \u2018fly\u2019 from a remote   shell #2464</li> <li>Continued on a design enhancement to prioritize pipelines from teams a user belongs to over publicly exposed pipelines   within the Concourse dashboard #2427</li> </ul>"},{"location":"blog/2018-09-07-concourse-update-sept-03--sept-07/#core","title":"Core","text":"<ul> <li>Continued ahead on #2386</li> <li>We\u2019re working on extracting the migration library which was introduced in 4.1.0 into a separate package that we can   support for open source.</li> </ul>"},{"location":"blog/2018-09-07-concourse-update-sept-03--sept-07/#runtime","title":"Runtime","text":"<ul> <li>Mostly completed effort on a feature to emit build logs to an operator configured syslog   destination. #2104</li> </ul>"},{"location":"blog/2018-09-14-concourse-update-sept-10--sept-14/","title":"Concourse Update (Sept 10 \u2014 Sept 14)","text":"<p>Let us know if you\u2019d be interested in Concourse swag</p> <p>Following up from a discussion on our forums Scott Foerster has been looking at different options for selling Concourse swag online. Do you want Concourse leggings? or maybe a limited edition @vito pls pillow! Let us know in the thread Concourse merchandising.</p> <p>Please also take some time to fill out our **2018 Concourse Community survey **. Your feedback is really valuable to us and the information you provide will help us plan the future of Concourse. We only have a handful of responses so far, and we\u2019d like to get more before we publish the results!</p> <p>On to the update:</p> <p>API</p> <ul> <li>As a welcome back to Joshua Winters, we took a look   at #2463 and the possibility of doing an internal redirect for   all auth components. Unfortunately, that didn\u2019t work quite well. Check out the full issue thread for details</li> <li>Remember thatRBAC RFC? Well, we\u2019re going to buckle down and start working   on that now</li> </ul> <p>UX</p> <ul> <li>Following up on issue #2427, we\u2019re applying the same labelling   principals to the HD dashboard view in #2572</li> </ul> <p>Core</p> <ul> <li>Kept hacking away on good ol\u2019#2386</li> </ul> <p>Runtime</p> <ul> <li>Spiking on #2581, where we ask ourselves \u201cCan we determine when   a build step fails because the worker is unusable?\u201d</li> </ul> <p>Operations</p> <ul> <li>Continuing on #2312. This issue has exploded a bit to lots of   edge cases and race conditions; but our determination to finish this issue is strong</li> <li>Looked into why   our k8s-testflight job keeps   breaking.</li> </ul>"},{"location":"blog/2018-09-21-concourse-update-sept-1721/","title":"Concourse Update (Sept 17\u201321)","text":"<p>Cessna 421, Photo credit to wiltshirespotter</p> <p>Concourse 4.2.0 and Concourse 4.2.1 were released earlier this week. There\u2019s a lot of great fixes and features in this new release, so please upgrade now!</p> <p>Reminder that The Great Project Restructuring of 2018 is now underway. You\u2019ll notice that all our submodules (e.g. ATC, TSA fly) are now all under the root level of the concourse/concourse repo. Its cleaner.</p> <p>You\u2019ll also notice that the BOSH spec has moved from its usual place. We\u2019ve separated out the BOSH release code into its own repo under concourse-bosh-release. As always, you can find examples of how to use the BOSH release under concourse-bosh-deployment.</p> <p>Edit: I forgot to mention that Concourse user danhigham wrote an awesome Atom plugin for Concourse. Give the concourse-vis plugin a spin and show him some love!</p> <p>Finally, please take some time to fill out the 2018 Concourse community survey. We\u2019re at 80 responses right now and hoping to hit 100 before we publish the results!</p> <p>On to the update:</p>"},{"location":"blog/2018-09-21-concourse-update-sept-1721/#api","title":"API","text":"<ul> <li>RBAC IS COMING! Team is working away at implementing our first iteration of fine grained role based access control.   You can read the details about this work in the   RFC here.</li> </ul>"},{"location":"blog/2018-09-21-concourse-update-sept-1721/#ux","title":"UX","text":"<ul> <li>More UX polish and refactoring, specifically we\u2019re trying to merge the HD dashboard logic with the normal dashboard   logic. A lot of that work is hidden in #2572</li> </ul>"},{"location":"blog/2018-09-21-concourse-update-sept-1721/#core","title":"Core","text":"<ul> <li>#2386is close to completion! Hurray. Applying some final   polishes before shipping it. You\u2019re gonna love it.</li> </ul>"},{"location":"blog/2018-09-21-concourse-update-sept-1721/#runtime","title":"Runtime","text":"<ul> <li>Finished #2586, which should make things more efficient</li> <li>Made progress on #2588. Completed the GC container portion and   will re-apply the same logic on the volumes portion</li> </ul>"},{"location":"blog/2018-09-21-concourse-update-sept-1721/#operations","title":"Operations","text":"<ul> <li>Issued PR #7804 against the Concourse Helm Chart, which refactors the   values.yml to better map Concourse binary commands in the Helm Chart</li> </ul>"},{"location":"blog/2018-09-28-concourse-update-sept-2428/","title":"Concourse Update (Sept 24\u201328)","text":"<p>Reppin\u2019 Concourse at Spring One Platform</p> <p>The Concourse team went out to Washington D.C. this week to attend Spring One Platform 2018. Thanks to all the Concourse fans who stopped by to say hi, we really enjoyed meeting ya\u2019ll. All of the talks were recorded and should be uploaded to the SpringDeveloper YouTube channel in the coming weeks. Some of the interesting talks to check out are:</p> <ul> <li>Extreme Pipelines</li> <li>Zero to Multicloud   and Spinnaker and the Distributed Monorepo</li> <li>...and of   course Draupnir: A story about Managing Concourse in the Enterprise</li> </ul> <p></p> <p>Concourse \u2764 Spring &amp; PCF</p> <p>And now, on to the update:</p> <p></p> <p>You\u2019ll notice that our main pipelines are paused. This is because Alex Suraci is working away on #2534, refactoring our main pipeline to support our new mono-repo structure. This new pipeline is simply called the concoursepipeline.</p> <p>In addition to refactoring the pipeline, Alex Suraci has been fleshing out the new developer/contributor workflows under our new mono-repo. You can find the new updated information in CONTRIBUTING.md.</p> <p></p> <p>Bugs... or features?!</p> <p>You\u2019ll also notice that we ask whether you are reporting a bug or a new feature when creating issues. This will ( hopefully) help get our backlog more organized and reduce the up-front triaging!</p> <p>Fly</p> <ul> <li>Completed #2221 \u201cAdd fly command to land worker\u201d</li> <li>Added new fly flag to \u201cSupport manual token entry during login when running <code>fly</code> from a remote shell\u201d   in #2464</li> <li>Fixed #2539, where a login through fly may be \u201csuccessful\u201d if   you do not belong to a specific team</li> <li>Fixed #2598</li> </ul> <p>Core</p> <ul> <li>#2386 is done!</li> </ul> <p>Runtime</p> <ul> <li>Continuing on #2588</li> </ul> <p>Operations</p> <ul> <li>Tackling #2312, which is still giving us a run for our money</li> </ul>"},{"location":"blog/2018-10-05-concourse-update-oct-15/","title":"Concourse Update (Oct 1\u20135)","text":"<p>Turkey day == best day</p> <p>Alex Suraci is still tackling the chores on our One Big Repo issue #2534. Specifically, Alex is re-writing a new pipeline ( aka <code>concourse</code>) for our mono-repo structure so we can unblock ourselves from releasing updates.</p> <p>In other news, Concourse engineer Saman Alvi wrote up a short article on her experience pairing with a product designer during a discovery into the PivNet resource; check it out: Design &amp; Dev Pairing: What we learned during a one week technical discovery.</p> <p>Finally, the Concourse team will be taking Monday, Oct 5 off to celebrate Thanksgiving. We\u2019ll see you all next week!</p> <p>On to the update:</p>"},{"location":"blog/2018-10-05-concourse-update-oct-15/#rbac","title":"RBAC","text":"<p>We continue to work on the proposal for Role Based Access Control (RBAC). In the past few weeks we\u2019ve been focusing more on the experience of assigning roles to new users. Our early attempts at this was to require operators to supply those changes through the fly CLI:</p> <pre><code>fly -t mytarget set-team -n myteam --role viewer --allow-all-users\n\nfly -t mytarget set-team -n myteam --role member --github-user pivotal-jwinters --github-team myorg:myteam\n</code></pre> <p>This raises some questions though: how do you go about removing a role from a user on a team? should the role parameters be additive, or overriding like the other flags? Also, that\u2019s a lot of flags to supply through the set-team command, maybe this belongs in a configuration file.</p> <p>So with that we decided to move all of the user role configurations into a config file. We think that\u2019ll be much cleaner. Hop on over to the updated RFC for the update details.</p>"},{"location":"blog/2018-10-05-concourse-update-oct-15/#ux","title":"UX","text":"<ul> <li>We\u2019ve been doing some much needed refactoring on the Elm frontend code. That\u2019s also let us pick up some design polish   stories like #2434   and #2430</li> <li>The team has also had the opportunity to pick up a lot of issues around the fly   CLI: #2532, #963, #1062</li> </ul>"},{"location":"blog/2018-10-05-concourse-update-oct-15/#core","title":"Core","text":"<ul> <li>Space is back\u2026but really it never left! With the hard work of resource pinning and global caching, we\u2019re now ready to   resume the work around Spatial resources #2651</li> </ul>"},{"location":"blog/2018-10-05-concourse-update-oct-15/#runtime","title":"Runtime","text":"<ul> <li>Finished #1799 \"Permit overlapping inputs, outputs and task   caches\"</li> </ul>"},{"location":"blog/2018-10-05-concourse-update-oct-15/#operations","title":"Operations","text":"<ul> <li>We finished #2312!!!\u00a0\u2026.except we DO need to do some   acceptance testing to make sure we\u2019ve covered all our bases.</li> </ul>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/","title":"Design & Dev Pairing: What we learned during a one week technical discovery","text":"<p>During a one week technical discovery, we (Saman and Josephene - engineer and product designer respectively) spent some time analyzing the difficulties with PCF (Pivotal Cloud Foundry) installs and how Pivnet (Pivotal Network) could help with the problem. As a result of this pairing, we were able to understand the benefits of doing Design &amp; Dev pairing. The Concourse team also used this methodology to develop some of their features. In this article, we describe the results of this pairing on the Pivnet Resource, a Concourse resource to interact with Pivotal Network.</p>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#what-is-design-dev-pairing-how-does-it-differ-from-normal-pairing","title":"What is design-dev pairing? How does it differ from normal pairing?","text":"<p>Pivotal focuses on pairing as an important part of the process of transforming the way the world builds software. A traditional pairing session at Pivotal involves two engineers. The engineers both solve the same problem, discuss it with each other, and take turns driving and navigating. A Design and Dev pairing session is similar. The two are still working on the same problem and discuss with one another, but do not take on the traditional driver and navigator roles. They bring different skill sets, allowing developers to get a better understanding of customer needs, and for designers to better understand the technical complexities involved in implementation.</p> <p>The Concourse team utilized this methodology when doing their initial user research around Spaces and the new Dashboard, with Lindsay and Mark. The feedback loops were tighter and iteration cycles to test these new features were also shorter.</p> <p>Similarly, this methodology was applied to the technical discovery.</p>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#what-did-we-set-out-to-accomplish","title":"What did we set out to accomplish?","text":"<p>The problem we were trying to solve was broadly defined \u2014 how can we automate PCF installs to ease the lives of our customers using Pivnet related tools (Pivnet CLI, Pivnet Resource, and the Pivnet website).</p> <p>The three experiments ran to answer the question of automation from the perspective of a different combination of tools took about two days to run. Each experiment took a look at existing solutions to automate PCF installs, as well as a combination of Pivnet tools to augment the existing solution or generate a new one.</p> <p>It was valuable to have the perspective from the designer to understand customer problems with the tool as well as other domain specific problems. It was also valuable to get the perspective from the developer to be able to investigate and provide quicker answers about potential experiments. Both individuals brought important perspective and knowledge that allowed us to reach a solution faster, and answer questions in a more efficient manner than either would have been able to do alone.</p>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#pros-cons-of-the-pairing-session","title":"Pros / Cons of the pairing session","text":"<p>There are pros and cons to every situation, and it is necessary to understand them to ensure success in your own versions of the experiments.</p>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#pros","title":"Pros:","text":"<ul> <li>The developer was able to understand the problem space a lot more deeply than in a traditional setting. The   opportunity to talk to other component development teams, the field, and to go through customer interviews allowed for   a much more holistic view of the world.</li> <li>It became clear early on that another team was in the same problem space, and instead of duplicating effort, we   pivoted to work on something that would supplement their solution. Traditionally, the Project Manager has a deeper   understanding of the problem space, so this issue would have been hidden from the development team. The pairing   session allowed the developer to have a lot more autonomy.</li> <li>Design/Dev pairing allowed for delivery in a week as each experiment was quickly invalidated through user research and   there was a tight feedback loop.</li> <li>Both the developer and designer were able to see the full breadth of the problem \u2014 the issue could be tackled from all   sides.</li> <li>The developer gained much more in-depth view into customer problems.</li> <li>The designer was able to lean on the technical skills of the developer, and therefore free of the pressure to fully   understand the technical problem space.</li> <li>The designer was able to ramp up a lot more quickly through pairing \u2014 the chance to see a real world example of   working through a problem provided a lot of context which is difficult to gain otherwise.</li> <li>The solution felt concrete.</li> <li>We did not reinvent the wheel, we analyzed existing solutions to the problem and determined where our solution would   fit best.</li> <li>It became possible to invalidate three experiments in two days.</li> </ul>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#cons","title":"Cons:","text":"<ul> <li>The designer was not allocated to the project full time, and therefore was not able to fully dedicate their time. This   is more of a function of the fact that designers at Pivotal are generally split between many projects in general, and   thus are not able to fully dedicate themselves.</li> <li>Success metrics were clearly defined in the beginning, but as we pivoted and explored different options, the success   metrics became more ambiguous. Although we have an outcome and a deliverable, it is not clear if we were able to solve   the problem in a very valuable or meaningful way.</li> <li>The solution we arrived at involved documentation, and   documentation quickly goes stale.</li> </ul>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#takeaways","title":"Takeaways","text":"<p>The purpose of the Design/Dev pairing was to be able to have more of a balanced team approach \u2014 one where members from all parts of the organization feel empowered to have an impact and solve customer problems. As we had a Design/Dev pair, the turnaround time in the exploration stage was minimal \u2014 both members were able to learn from each other and arrive at conclusions more quickly. The experiment we ran was on a smaller scale \u2014 one pair doing the experiment, which is perhaps why it was more effective. It is not clear how useful it would be on a larger scale.</p> <p>Additionally, this pairing allowed for a lot of autonomy. In most cases, Design identifies systematic problems in the organization, but interfaces with the Project Manager who breaks the problem down into smaller tasks for the development team. Project Managers explain the customer needs and problems, but the development team is not able to interface or speak to customers / field directly. Design is not able to empathize as deeply with the pains or difficulties the development team goes through, and thus a wall generally exists between the two in terms of difficulty in what can be accomplished and what cannot. A pairing between Design/Dev allowed for a better understanding of the problem space, resulting in faster validation of experiments, quicker turnaround, and better results.</p>"},{"location":"blog/2018-10-05-design--dev-pairing-what-we-learned-during-a-one-week-technical-discovery/#the-solution","title":"The Solution","text":"<p>The current proposed solution to PCF installs involves giving customers more autonomy and flexibility with their installation pipelines. They want to be able to install Product Releases from Pivnet quickly, without having to reach out to Support. The team tasked with automating PCF installs decided to use the Pivnet Resource in the new installation pipelines, and we decided to focus on ease of use of understanding the Pivnet Resource.</p> <p>Operators working on the installation pipelines have an understanding of Concourse and Resources, but run into difficulties with using the Pivnet Resource in particular. We addressed the common problems encountered here.</p>"},{"location":"blog/2018-10-12-2018-community-survey/","title":"2018 Community Survey","text":"<p>About a month ago, we sent out a Concourse Community survey in order to gain insight into our users and measure our growth over the previous year. We received tons of great feedback from the OSS Community, Pivotal customers, and fellow Pivots. We gained a lot of valuable information from the survey and we\u2019re excited to share our findings.</p> <p>Before diving into the results, the Concourse team wanted to send a big shoutout to everyone who responded. At the time of this writing, we\u2019ve received over 100 responses and that number is still climbing. Your contributions are more than appreciated and will help us make Concourse even better in 2019 and beyond.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#verticals-lines-of-business","title":"Verticals / Lines of Business","text":"<p>Users / Vertical</p> <p>As expected, the vast majority of respondents were from traditional technology industries. However, we still saw responses from a diverse spectrum of businesses (Retail, Aerospace, Media, among others). It was encouraging to see the wide array of use-cases Concourse is being utilized for.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#how-long-have-you-used-concourse","title":"How Long Have You Used Concourse?","text":"<p>Most of the people who responded indicated they had been using Concourse for one year or less. That being said, about 40% of the responders have been using Concourse for 2+ years. It was important to grab feedback from both newbies and original contributors as these varying levels of experience added valuable depth to our data. These users have different pain-points in how they operate Concourse, so it was important to gain these differing perspectives.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#what-version-of-concourse-are-you-using","title":"What Version Of Concourse Are You Using?","text":"<p>Version of Concourse</p> <p>Of those that disclosed what version they were using, around 60% had already upgraded to at least 4.0 (our last major release). Given that a number of the respondents were running Enterprise-scale Concourse deployments (more on that below), this was an encouraging number.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#where-did-you-find-out-about-concourse","title":"Where Did You Find Out About Concourse?","text":"<p>About half of those who responded discovered Concourse directly through an engagement with Pivotal/Pivotal Labs while the other half discovered Concourse organically via social media channels and Google searches.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#where-do-you-go-for-concourse-support","title":"Where Do You Go For Concourse Support?","text":"<p>Support Channels</p> <p>The vast majority of respondents stated that they go to the documentation on concourse-ci.org when they have issues and questions. We plan on making upgrades to this documentation in the coming months, which will be of great benefit to our users. A decent amount of users who responded also tend to seek us out on social channels such as Discord (majority OSS) and Slack (exclusive to Pivots). We love using these channels to build relationships with the community and hope to continue to see these numbers rise.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#how-do-you-deploy-concourse","title":"How Do You Deploy Concourse?","text":"<p>Deployment Types</p> <p>There was almost an even split between respondents who use BOSH and respondents who use Docker. Slightly behind is Kubernetes. We believe that Kubernetes will only continue to grow and plan on increasing our investment in Helm over the next year to satisfy this user need.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#how-many-usersteams-are-on-your-concourse-deployment","title":"How Many Users/Teams Are On Your Concourse Deployment?","text":"<p>Users</p> <p></p> <p>Teams</p> <p>The data gathered shows that the majority of responders are operating small teams of 0\u201310 with approximately 1\u201320 users. There are also a few examples of large, enterprise scale deployments of 100+ users over 50+ teams.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#how-many-pipelinesworkers-are-on-your-concourse-deployment","title":"How Many Pipelines/Workers Are On Your Concourse Deployment?","text":"<p>Pipelines</p> <p></p> <p>Workers</p> <p>Similar to users/teams, we see that the majority of respondents are running small-medium size deployments consisting of 11\u201350 pipelines and 0\u201310 workers. Encouraging are the outliers on the higher end who are running enterprise-scale deployments of 100\u2013200+ pipelines on 26\u201350+ workers, showing the potential scale for Concourse.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#areas-for-improvement","title":"Areas For Improvement","text":"<p>We asked users to describe their biggest issue operating Concourse in order to determine key areas for improvement. After reading through the responses, a few key areas emerged:</p>"},{"location":"blog/2018-10-12-2018-community-survey/#slow-web-ui","title":"Slow Web UI","text":"<p>The build page load time performance was improved in 4.0.0 with #1543. This seemed to really improve the load times on the build page.</p> <p>We also got reports that highly complex pipelines being rendered on the dashboard destroys the ATC CPU utilization. That made us sad and logged it under #2644.</p> <p>And finally, as folks build out more complex pipelines, we seem to be hitting the limit of being able to efficiently render the pipeline view using current js tech. We\u2019ve been looking into a basic HTML version of pipelines to help with the visualization of even more complex pipelines.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#stability-especially-when-upgrading","title":"Stability \u2014 Especially when upgrading","text":"<p>\u201cUpgrading; Especially when changes are marked to be changes but no exemplary configuration of the manifest is given.\u201d \u2014 Survey respondent</p> <p>In addition to our work on runtime efficiency, I think there\u2019s a lot we can do in terms of educating the community on what to monitor so you can reason about the health of Concourse itself. As an operator, what are the metrics to keep an eye on, how and when should Concourse scale? Are we giving operators enough tools to identify noisy pipelines?</p>"},{"location":"blog/2018-10-12-2018-community-survey/#learning-curve-for-new-users","title":"Learning Curve For New Users","text":"<p>\u201cAt this point, we are still in learning mode\u201d \u2014 Survey respondent</p> <p>Multiple responders mentioned that there was a steep learning curve for new users as they scaled up their Concourse use. We have already begun to work towards updating our documentation with the concerns of these users in mind. Our hope is to make the process easier for users who want to move past a basic deployment and onto larger scale Concourse instances.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>The lack of RBAC for Concourse was pointed out frequently within the survey. RBAC is currently being worked on with the goal of releasing the first iteration by end of year. For more information, check out RFC#6</p>"},{"location":"blog/2018-10-12-2018-community-survey/#workers-running-out-of-disk-space","title":"Workers Running Out Of Disk Space","text":"<p>There\u2019s been a few suggestions in this area, specifically issue #1751, where workers should be a bit more intelligent in managing its own disk.</p>"},{"location":"blog/2018-10-12-2018-community-survey/#support-for-git-feature-branches","title":"Support For Git Feature Branches","text":"<p>\u201cPromoting resource versions that have passed one pipeline to another is challenging.\u201d \u2014 Survey respondent</p> <p>Multiple users mentioned a lack of support for Git Feature branches. Our hope is that this support issue will be solved via spaces. There is currently an epic in our backlog for this feature #1707</p>"},{"location":"blog/2018-10-12-2018-community-survey/#conclusions","title":"Conclusions","text":"<p>It was awesome to see the depth and variety of the Concourse community reflected in the responses to the survey. Two years ago when Concourse was mostly used in the context of Pivotal labs engagements, we couldn\u2019t possibly have predicted the amount of growth and the variety of uses outlined within the survey responses. The insights gathered from this survey will help improve the product, and we look forward to seeing where we are next year.</p>"},{"location":"blog/2018-10-12-concourse-update-oct-912/","title":"Concourse Update (Oct 9\u201312)","text":"<p>From the Smithsonian National Air and Space Museum in Washington D.C.</p> <p>The results of the Concourse 2018 Community survey is out! Thanks to everyone who took the time to fill it out; and to Scott Foerster and Lindsay Auchinachie for sifting through the data.</p> <p>It was a relatively short week for us due to Thanksgiving celebrations, but here\u2019s our update:</p>"},{"location":"blog/2018-10-12-concourse-update-oct-912/#ux","title":"UX","text":"<ul> <li>Continued our rampage in fixing fly   issues: #259, #267, #1038, #1062, #248</li> </ul> <p>I also wanted to add that we\u2019re trying to keep all issues under <code>concourse/concourse</code>. We\u2019re planning on migrating the issues under <code>concourse/fly</code> and closing off that repo in order to centralize everything under <code>concourse/concourse</code>.</p>"},{"location":"blog/2018-10-12-concourse-update-oct-912/#core","title":"Core","text":"<ul> <li>SPATIAL RESOURCES ARE BACK #2651</li> </ul>"},{"location":"blog/2018-10-12-concourse-update-oct-912/#runtime","title":"Runtime","text":"<ul> <li>Picked up #1954 (The ATC holds a lock on a resource type scan)   and #1796 (Task fails with \u201cconfig file not found\u201d after   restarting Docker service)</li> <li>Finished #1799 Permit overlapping inputs, outputs, and task   caches</li> </ul>"},{"location":"blog/2018-10-12-concourse-update-oct-912/#operations","title":"Operations","text":"<ul> <li>Picked up #2674 Emit metrics for locks held in the DB</li> </ul>"},{"location":"blog/2018-10-19-concourse-update-oct-1519/","title":"Concourse Update (Oct 15\u201319)","text":"<p>Torontonians typically overreact when they get their first snowfall of the year. Its just a bit of frost ya\u2019ll</p> <p>We finished our first implementation of Role Based Access Control (RBAC) this week! You can look forward to this change in our next release of Concourse.</p> <p>Speaking of which, the next release of Concourse is currently blocked while we try to re-build our new release pipelines. Along with our move to the mono-repo, we\u2019re focusing even more on making the binary distribution of Concourse the first-class distribution of Concourse. This means that you\u2019ll get everything you need for Concourse packaged into one nifty tgz! We\u2019re still working on finalizing the pipelines, so look forward to hearing more details about these changes in the coming weeks.</p> <p>This week, I\u2019ve also been doing some analysis on our internal Concourse instance Wings. Wings currently runs on GCP and has</p> <ul> <li>4 web instances</li> <li>31 workers @ 4 vCPUs, 16 GB memory, 1000 GB SSD</li> <li>Google CloudSQL as the db</li> <li>99 internal teams</li> </ul> <p>Since inception last year, we\u2019ve processed 238957900.6 build seconds, or 7 years of build activities for Pivotal. Our peak month was in July, 2018, where we processed 48978695.88 build seconds, or 1.5 build years.</p> <p>Neat.</p> <p>On to the update:</p>"},{"location":"blog/2018-10-19-concourse-update-oct-1519/#api","title":"API","text":"<ul> <li>We finished RBAC!</li> <li>Fixed an issue where Users who are not assigned to teams aren\u2019t able to   login #2670</li> </ul>"},{"location":"blog/2018-10-19-concourse-update-oct-1519/#ux","title":"UX","text":"<ul> <li>Working on finalizing the fix to #2414, which we thought was   implemented but found that it didn\u2019t work on Linux and Windows machines</li> <li>Continuing our UI cleanup work   with #2434, #2430, #2435</li> <li>Picked up the corresponding UI story for pinning resources in the Web   UI #2508</li> </ul>"},{"location":"blog/2018-10-19-concourse-update-oct-1519/#core","title":"Core","text":"<ul> <li>SPACE (#1202   and #2651)</li> </ul>"},{"location":"blog/2018-10-19-concourse-update-oct-1519/#runtime","title":"Runtime","text":"<ul> <li>Picked up some work on improving volume streaming #2676</li> </ul>"},{"location":"blog/2018-10-19-concourse-update-oct-1519/#operations","title":"Operations","text":"<ul> <li>Working on emitting more metrics for locks held in DB #2674</li> </ul>"},{"location":"blog/2018-10-26-concourse-update-oct-2226/","title":"Concourse Update (Oct 22\u201326)","text":"<p>Construction continues on our main pipeline. Photo credit MGI Construction Corp</p> <p>This week the team got together to discuss the initial groundwork and investigations required to publish and maintain a supported API. If you\u2019ve built any tools against our API and have feedback for us, please let us know by commenting on the original issue #1122.</p> <p>In another interesting update, the PivNet team has published an update to the <code>pivnet-resource</code> so \u201cyou no longer need to specify the access key, secret access key, bucket and region for creating releases.\u201d If you use that resource, you should definitely check it out!</p> <p>On to the update:</p>"},{"location":"blog/2018-10-26-concourse-update-oct-2226/#ux","title":"UX","text":"<ul> <li>Picked up the story for pinning versions of resources in the web   UI #2508</li> </ul>"},{"location":"blog/2018-10-26-concourse-update-oct-2226/#core","title":"Core","text":"<ul> <li>Continued our work on resources v2 and spatial resources   with #2651</li> </ul>"},{"location":"blog/2018-10-26-concourse-update-oct-2226/#runtime","title":"Runtime","text":"<ul> <li>Picked up failing tests in Testflight/Watsjs #2719</li> <li>Started work on retry / read deadline for Volume Streaming #2676</li> </ul>"},{"location":"blog/2018-10-26-concourse-update-oct-2226/#operations","title":"Operations","text":"<p>We\u2019ve added descriptions to our metrics graphs! You can check out the descriptions on our prod metrics here: https://metrics.concourse-ci.org/dashboard/db/concourse?refresh=1m&amp;orgId=1</p> <p>In other news we\u2019re also working on #2674, emit metrics for locks held in the database</p>"},{"location":"blog/2018-11-02-concourse-update-oct-29--nov-2/","title":"Concourse Update (Oct 29 \u2014 Nov 2)","text":"<p>The Concourse team\u2019s big yoga ball has returned to its rightful home</p> <p>As a part of our refactor of the prod pipeline, Alex Suraci cleaned up and refactored parts of the TSA to better support draining and rebalancing #2748. The numbers are looking really good!</p> <p>On to the update:</p>"},{"location":"blog/2018-11-02-concourse-update-oct-29--nov-2/#api","title":"API","text":"<ul> <li>We\u2019re deep into investigations around our API documentation and management strategy. Our current investigation work is   captured in #2739 but the original request comes   from #1122</li> </ul>"},{"location":"blog/2018-11-02-concourse-update-oct-29--nov-2/#core","title":"Core","text":"<ul> <li>SPACCEEEE #2651</li> </ul>"},{"location":"blog/2018-11-02-concourse-update-oct-29--nov-2/#ux","title":"UX","text":"<ul> <li>Continuing our work on supporting pinning of versions on resources from the UI. You can see some of our progress   on #2508</li> </ul>"},{"location":"blog/2018-11-02-concourse-update-oct-29--nov-2/#runtime","title":"Runtime","text":"<ul> <li>Continuing our work on #2676   and #1266</li> </ul>"},{"location":"blog/2018-11-02-concourse-update-oct-29--nov-2/#operations","title":"Operations","text":"<ul> <li>Adding jobs to our pipeline to better support the Concourse Helm   Chart #2743</li> <li>And in general Topher Bullock has been helping out with PRs and issues on the   Concourse Helm Chart.</li> </ul>"},{"location":"blog/2018-11-09-concourse-update-nov-59/","title":"Concourse Update (Nov 5\u20139)","text":"<p>You\u2019ll soon be able to pin versions in Concourse (Photo Credit George Barnett)</p> <p>Right off the bat I\u2019d like to give a shoutout to Jamie Klassen and his new post about the upcoming feature for pinning resources. You can check it out the new post here: Resource Page Explained</p> <p>I also wanted to mention that the Github Pull Request that was maintained by JT Archie (https://github.com/jtarchie/github-pullrequest-resource) has been officially deprecated.</p> <ol> <li>The official docs for the resource types no longer point to <code>jtarchie/pr</code> for the PR resource. They are pointing    to https://github.com/telia-oss/github-pr-resource now.</li> <li>There will no longer be any maintenance, issues or PRs accepted on the resource.</li> </ol> <p>We also spent some time this week finalizing our plans for the Concourse 2019 roadmap. We\u2019ll be writing it up in a wiki to share with everyone next week, so keep an eye out for another followup announcement!</p> <p>On to the update:</p>"},{"location":"blog/2018-11-09-concourse-update-nov-59/#pipeline","title":"Pipeline","text":"<ul> <li>We finally got a deploy going onto our prod environment. Everything broke but hey, its   the attempt that matters</li> </ul>"},{"location":"blog/2018-11-09-concourse-update-nov-59/#api","title":"API","text":"<ul> <li>We\u2019re still investigating various options for refactoring and documenting our   API. Joshua Winters is on it!</li> </ul>"},{"location":"blog/2018-11-09-concourse-update-nov-59/#ux","title":"UX","text":"<ul> <li>Pinning versions on resources. Make sure you read our write-up on   it here!</li> </ul>"},{"location":"blog/2018-11-09-concourse-update-nov-59/#core","title":"Core","text":"<ul> <li>Resource v2 and Spatial resource design! Most of that work is currently being done in   a feature branch.</li> </ul>"},{"location":"blog/2018-11-09-concourse-update-nov-59/#runtime","title":"Runtime","text":"<ul> <li>Picked up #2529</li> </ul>"},{"location":"blog/2018-11-09-concourse-update-nov-59/#operations-k8s","title":"Operations / K8s","text":"<ul> <li>In addition to picking up some issues reported by users around helm-deployed concourse in   4.2.1, Topher Bullock is going to try spending his Fridays making / looking at   Concourse Helm chart PRs.</li> </ul>"},{"location":"blog/2018-11-09-concourse-resource-page-explained/","title":"Concourse Resource Page Explained","text":"<p>The purpose of this document is to introduce and justify the existence of the resource page in its current incarnation, and to preview some upcoming changes to the page.</p>"},{"location":"blog/2018-11-09-concourse-resource-page-explained/#why-does-this-page-exist-what-does-it-do-right-now","title":"Why does this page exist? What does it do right now?","text":"<p>Concourse Resource Page for credhub-release resource in prod pipeline \u2014 currently employing the \u201cdisable everything and pause\u201d kludge described below</p> <ol> <li>Surface the resource check status and age \u2014 show check errors, if any.</li> <li>Radiate information about a resource\u2019s versions \u2014 at a glance, you can see the list of versions (that have been found    by checking) in reverse chronological order. For example, you can see the equivalent of running <code>git log</code> on a git    resource to understand which commits your Concourse instance has discovered.</li> <li>Get version metadata \u2014 sometimes the individual versions are not especially expressive; the git resource is a great    example of this. Only seeing the commit hash doesn\u2019t make it very easy for a human to identify the commit, but when    you expand a version that has been retrieved via a <code>get</code> step in a job, you can see the commit time, author, and    message, so you don\u2019t need to cross reference against GitHub or your local <code>git log</code> output to figure out what is    going on.</li> <li>Get inputs to/outputs of \u2014 this provides another means of visualizing how this resource interacts with your jobs, and    a jumping off point to go to specific builds for those jobs \u2014 if all the builds for a given version are red, maybe    it\u2019s because there is some breaking change in that version?</li> <li>Pause checking\u2014this button was introduced with use cases like the scenario below in mind. When an upstream dependency    introduces a breaking change, you may want to stop using new versions until you bring your code back into    compatibility.</li> <li>Enable/disable versions \u2014 this feature was created with the \u201cBad Version\u201d scenario (to be described in another story)    in mind.</li> </ol>"},{"location":"blog/2018-11-09-concourse-resource-page-explained/#the-pinned-version-scenario","title":"The \u201cPinned Version\u201d Scenario","text":"<p>In the following scenario, let\u2019s assume we are a development team delivering software via a a so-called \u201cthree-box\u201d continuous delivery pipeline like this (some fine details intentionally left out):</p> <pre><code>resources:\n  - name: third-party-dependency\n    type: gcs\n  - name: my-source-code\n    type: git\n  - name: my-build-artifacts\n    type: gcs\n  - name: my-app\n    type: cf\n\njobs:\n  - name: build\n    plan:\n    \u2014 get: my-source-code\n      trigger: true\n    \u2014 get: third-party-dependency\n    \u2014 task: build\n    \u2014 put: my-build-artifacts\n  - name: test\n    plan:\n    \u2014 get: my-build-artifacts\n      trigger: true\n    \u2014 task: test\n  - name: deploy\n    plan:\n    \u2014 get: my-build-artifacts\n      trigger: true\n      passed:\n        - test\n    \u2014 put: my-app\n</code></pre> <p></p> <p>Rendered view of the delivery pipeline in question</p> <p>Suppose that the authors of <code>third-party-dependency</code> release a new major version that breaks compatibility with <code>my-source-code</code> in a way that we\u2019re not ready to adapt to yet because we have more pressing issues in our backlog.</p> <p>That version might be perfect and bug-free in and of itself, but it causes the <code>test</code> job to fail. However, we\u2019ve still got features to deliver \u2014 we\u2019d like to just pin <code>third-party-dependency</code> at the last known working version so that we\u2019re not prevented from deploying (by the <code>passed:</code> constraint on the <code>get</code> step in our <code>deploy</code> job) for the time being, and we\u2019ll pay down the technical debt of bringing our dependency back up to date later.</p> <p>Now, strictly speaking, Concourse does provide an official solution to this problem: you can define a resource in the pipeline config so that only one version will ever be used. In our case, we could change the definition of <code>third-party-dependency</code> to be something like</p> <pre><code>- name: third-party-dependency\n  type: gcs\n  version:\n    path: third-party-dependency-v1.1.0.tgz\n</code></pre>"},{"location":"blog/2018-11-09-concourse-resource-page-explained/#the-disable-everything-and-pause-kludge","title":"The \u201cDisable Everything and Pause\u201d Kludge","text":"<p>The official solution for the \u201cpinned version\u201d scenario isn\u2019t perfect. Clever users, like krishicks on GitHub, noticed that often your desire to pin a dependency is really just temporary. Furthermore it is possible that not all of your development team has an account on your Concourse instance such that they can reset the pipeline themselves, and you may unwittingly block them from doing their work this way. This struck the Concourse team as a valid concern and something that the product should support.</p> <p>Furthermore, such clever users discovered an undocumented workflow for pinning the version of a resource without modifying their pipeline config:</p> <ol> <li>Visit the resource page and click the pause button to suspend checking.</li> <li>Disable all versions that were discovered after the one you wish to pin. Since jobs will use the latest available    version that passes their constraints in <code>get</code> steps, then you have effectively pinned your resource to the desired    version.</li> </ol> <p>Now, if there have been many versions discovered since the breaking change occurred, this solution can require a lot of clicks. If your project has a lot of dependencies (e.g., you are developing Cloud Foundry!) then you may find yourself performing this onerous, many-click operation frequently. This is painful, and so the Concourse team wants to make it better.</p>"},{"location":"blog/2018-11-09-concourse-resource-page-explained/#first-class-dynamic-version-pinning","title":"First-Class Dynamic Version Pinning","text":"<p>Current state of the new pin functionality \u2014 to be included in the next major release</p> <p>The Concourse team decided to add a pin icon to each version of the resource so that instead of performing all those clicks you can do just one \u2014 click the pin and then all other versions of the resource will be disabled (including new ones that are freshly discovered via checking). The road to this decision is documented in this GitHub issue.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/","title":"Concourse RBAC Preview","text":"<p>By Danpaluska \u2014 Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=17686969</p> <p>One of the big themes for Concourse in 2018 has been Users,  multiple auth connectors, and role-based access control ( aka RBAC). With RBAC in the final phases of development, I wanted to give you a preview of some of the functionality that you can expect in our upcoming release; Concourse 5.0</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#admins-owners-members-and-viewers","title":"Admins, Owners, Members and Viewers","text":"<p>Concourse 5.0 will come with 4 roles: Concourse Admin, Team Owner, Team Member, and Team Viewer.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#concourse-admin","title":"Concourse Admin","text":"<p>A Concourse Admin is the same as today\u2019s admin user. Members of <code>main</code>  team will automatically be Concourse Admins* and have the ability to administrate teams with <code>fly</code>: <code>set-team</code>, <code>destroy-team</code>, <code>rename-team</code>, and <code>teams</code>. Given that all Concourse Admins must be a member of the main team, all Concourse Admins must have at least one other role; and that should typically be the Team Owner role.</p> <p>* There\u2019s an open issue to restrict this grant to Team Owners on <code>main</code> in #2846</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#team-owner","title":"Team Owner","text":"<p>Team Owners have read, write and auth management capabilities within the scope of their team. For those familiar with Concourse today, the scope of allowed actions for a Team Owner is very closely aligned to today\u2019s Concourse team member. The new change is that you can no longer rename your own team or destroy your own team as an owner.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#team-member","title":"Team Member","text":"<p>Team Member is a new role that lets users operate within their teams in a read &amp; write fashion; but prevents them from changing the auth configurations of their team.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#team-viewer","title":"Team Viewer","text":"<p>Team Viewer is also a new role that gives users \u201cread-only\u201d access to a team. This locks everything down, preventing users from doing a <code>set-pipeline</code> or <code>hijack</code>.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#other-roles","title":"Other Roles","text":"<p>We considered other role types while developing this feature; including roles that would specifically prevent <code>intercept</code> and <code>abort</code>. We ultimately decided that our current configuration made more sense for the first release of RBAC. Ultimately every organization will have different needs for their access control, so we are also planning for a future where users can supply their own customized roles &amp; permissions matrix.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#full-roles-breakdown","title":"Full Roles Breakdown","text":"<p>For a full list of each role\u2019s allowed actions you can reference our handy permission matrix on Google Sheets here.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#configuring-roles-with-fly","title":"Configuring Roles with <code>fly</code>","text":"<p>Now that we\u2019ve gone over the new roles, we can do a quick overview of how we can go about setting users &amp; roles on teams.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#default-behaviour","title":"Default Behaviour","text":"<p>By default, if no configuration is provided the user is given theTeam Owner role:</p> <pre><code>fly -t dev set-team -n PowerRangers --local-user=Zordon\n</code></pre> <p>This behaviour also applies to groups as well, so be careful!</p> <pre><code>fly -t dev set-team -n A-Team \\\n  --github-team=MightyMorphin:PowerRangers\n</code></pre>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#specifying-roles-with-c","title":"Specifying Roles with <code>-c</code>","text":"<p>Roles must be specified in a separate configuration file using the <code>-c</code></p> <pre><code>fly -t dev set-team -n PowerTeam -c ./team.yml\n</code></pre> <p><code>team.yml</code>:</p> <pre><code>roles:\n  - name: owner\n    local:\n      users: [ \"Zordon\" ]\n  - name: member\n    local:\n      users: [ \"RedRanger\", \"BlueRanger\", \"GreenRanger\" ]\n  - name: viewer\n    local:\n      users: [ \"Alpha\" ]\n</code></pre>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#inspecting-roles-configuration","title":"Inspecting Roles Configuration","text":"<p>Once you\u2019ve set the team configuration you can verify it using the details flag on <code>fly teams</code>:</p> <pre><code>fly -t dev teams -d\n\nname users groups\nA-Team/member local:RedRanger, BlueRanger, GreenRanger none  \nA-Team/owner local:Zordon none  \nA-Team/viewer local:Alpha none\n</code></pre> <p>...where you\u2019ll find the output is now updated to list each <code>team/role</code> combination and its associated users/groups.</p>"},{"location":"blog/2018-11-23-concourse-rbac-preview/#whats-left","title":"What\u2019s left?","text":"<p>And that\u2019s RBAC in a nutshell! We\u2019re really excited to get this in your hands in our upcoming release of Concourse. There\u2019s only a few more issues that we want to finish off before releasing this feature, specifically:</p> <ul> <li>#2846 Admin users should be restricted to members of the <code>main</code>   team with the <code>owner</code> role. This is so you don\u2019t get weird cases of a Team Viewer on <code>main</code> getting Admin access</li> <li>#2843 Dashboard team labels updated to display User Role. We   need this otherwise users on the Web UI have no idea what they can / can\u2019t do</li> </ul>"},{"location":"blog/2018-11-23-concourse-update-nov-1923/","title":"Concourse Update (Nov 19\u201323)","text":"<p>Happy Thanksgiving to our friends south of the border (Photo Credit Alby Headrick)</p> <p>It was a relatively light week this week due to some vacations. I did, however, get a chance to do some acceptance work on our upcoming feature for role-based access control in Concourse. You can read more about how that\u2019ll work in our feature preview post.</p> <p>On to the update:</p>"},{"location":"blog/2018-11-23-concourse-update-nov-1923/#api","title":"API","text":"<ul> <li>Our investigation into the API continues and branches out into more areas of the codebase. If you haven\u2019t already,   make sure to check out the two related   RFCS: https://github.com/concourse/rfcs/pull/14   and https://github.com/concourse/rfcs/pull/15</li> </ul>"},{"location":"blog/2018-11-23-concourse-update-nov-1923/#ux","title":"UX","text":"<ul> <li>We\u2019ve decided to commit to completing our refactor the Web NavBar before picking up new stories. This\u2019ll hopefully   prevent regressions when we pick up new stories down the road. We now have over 300 unit tests for our web-ui!</li> </ul>"},{"location":"blog/2018-11-23-concourse-update-nov-1923/#runtime","title":"Runtime","text":"<ul> <li>Picked up #2577. We\u2019re having conversations internally around   specific strategies that would help with this. On the one hand, we could try computing resource utilization on the   first run to inform our future allocations; or we could go with naive container/volume balancing.</li> </ul>"},{"location":"blog/2018-11-23-concourse-update-nov-1923/#core","title":"Core","text":"<ul> <li>Continuing our planning for Spatial resources</li> </ul>"},{"location":"blog/2018-11-30-concourse-updates-nov-2630/","title":"Concourse Updates (Nov 26\u201330)","text":"<p>Coventry Airport control tower (Photo Credit Casa-Steve)</p> <p>As I mentioned last week I\u2019ve been doing story acceptance in our dev environments for the upcoming RBAC feature. The team\u2019s been working through some of the new issues that come out of that to give some final polish on to the release.</p> <p>Something that I haven\u2019t talked too much about in the past weeks is our work on the Concourse k8s Helm chart. If you pull up some of the PRs under <code>[stable/concourse]</code>, you\u2019ll see that we\u2019ve been proposing some changes to the chart. This all falls under our goals for helping the community stabilize the Concourse Helm Chart and to increase the scope of automated tests using the Helm chart. You can follow along some of our work in GH issues #2753 and #2876.</p> <p>On to the update</p>"},{"location":"blog/2018-11-30-concourse-updates-nov-2630/#api","title":"API","text":"<ul> <li>Removed \u201callow all users\u201d in #2721</li> <li>Added the restriction that only <code>owners</code> of <code>main</code> can be   <code>admins</code> #2846</li> </ul>"},{"location":"blog/2018-11-30-concourse-updates-nov-2630/#fly","title":"Fly","text":"<ul> <li>Fixed #2780</li> <li>Fixed #2414</li> <li>Fixed #2819</li> </ul>"},{"location":"blog/2018-11-30-concourse-updates-nov-2630/#ux","title":"UX","text":"<ul> <li>Implemented #2843 to help users understand what roles they have   on each team</li> <li>Finished #2795, which added the \u201cpin\u201d colors to the legend</li> <li>Completed an issue that lets users unpin from the top bar #2870</li> <li>Moved the Exposed state on a pipeline off the team and onto the   pipeline #2844</li> <li>Fixed an old issue where users of new teams can\u2019t un-pause their first   pipelines #2882</li> </ul>"},{"location":"blog/2018-11-30-concourse-updates-nov-2630/#core","title":"Core","text":"<ul> <li>Began building up large scale test environments for Global Resource   caching #2874</li> </ul>"},{"location":"blog/2018-11-30-concourse-updates-nov-2630/#runtime","title":"Runtime","text":"<p>Continued with #2577:</p> <p>\u201c..as a first effort solution, we have decided to go with using the existing number of active containers on the workers to determine container placement. This means that we are adding a placement strategy that adds the new task on to a worker with the least existing active containers.\u201d</p>"},{"location":"blog/2018-12-07-concourse-update-dec-37/","title":"Concourse Update (Dec 3\u20137)","text":"<p>photo credit Stephen A. Wolfe</p> <p>We\u2019re nearing the end on some UX refactoring work and finished off the issue regarding container scheduling. Between those improvements and the global resource caching, we\u2019re hoping to see a lot of efficiency improvements in 5.0</p> <p>That said, we\u2019ve decided that we need to perform some additional performance and load testing on Concourse 5.0 before we cut the release. And with the holidays coming up, its increasingly unlikely that we\u2019ll be able to push Concourse 5.0 before the end of this year. In the meantime, we\u2019re planning to make a big update post describing the new deployment pipeline, contribution structure, major features in Concourse 5.0, and much more; so keep an eye out for that in the coming days!</p> <p>If you\u2019re attending KubeCon next week I\u2019d encourage you to check out the talk on Using Concourse as a CI/CD Tool for Your Knative App. Concourse engineer Divya Dadlani will be co-speaker on this talk and if you ask nicely; she might give you one of our fancy Concourse stickers. You should also check out Fairfax Media\u2019s talk on Cloud Native Transformation too, I hear they use a lot of Concourse!</p> <p>And finally, I\u2019ll be taking some time off for the holidays starting Dec 13, and won\u2019t be returning to work until the new year. I\u2019ve got a few posts scheduled to come out until then, but for now happy holidays, happy new year, and thanks for another awesome year of Concourse.</p> <p>On to the update:</p>"},{"location":"blog/2018-12-07-concourse-update-dec-37/#api","title":"API","text":"<ul> <li>Resolved #2887</li> </ul>"},{"location":"blog/2018-12-07-concourse-update-dec-37/#ux","title":"UX","text":"<ul> <li>Fixed a bug that happens when you try to log out from   Concourse #2884</li> <li>Fixed an issue with Fly where using <code>-c</code> on <code>set-team</code> with RBAC will fail silently if you use a badly-formed   file #2904</li> <li>Fixed an issue regarding the output of <code>fly teams -d</code> #2880</li> </ul>"},{"location":"blog/2018-12-07-concourse-update-dec-37/#runtime","title":"Runtime","text":"<ul> <li>Slightly better scheduling #2577</li> </ul>"},{"location":"blog/2018-12-12-concourse-2018-year-in-review/","title":"Concourse 2018 Year in Review","text":"<p>Photo credit Bernal Saborio</p> <p>2018 has been an action-packed year for us. We saw a major release (Concourse 4.0.0) with a lot of new features: new auth connectors and users, dashboard, distributed GC and other runtime improvements. At the same time our team grew from 3 engineering pairs at the start of 2018 to 8 engineering pairs and an additional PM ( \ud83d\udc4b Scott Foerster) working on Concourse OSS and supporting Concourse for PCF.</p>"},{"location":"blog/2018-12-12-concourse-2018-year-in-review/#by-the-numbers","title":"By the Numbers","text":"<ul> <li>13 releases of Concourse</li> <li>1 TLD snafu leading to our domain change</li> <li>1 new website design</li> <li>2 new ways to contact us (Discord, Discuss)</li> <li>1,070 members on Concourse Discord, 45 members in #contributors</li> <li>937 new GitHub issues created in 2018. Given th at only 1,694 issues were opened prior to 2018, I\u2019d say that\u2019s a   pretty big jump in activity!</li> <li>417 PRs, up by 15% from last year</li> <li>3,300 stars, up by 38% from last year</li> </ul>"},{"location":"blog/2018-12-12-concourse-2018-year-in-review/#most-popular-posts","title":"Most Popular Posts","text":"<p>We started this blog in Sept of last year. Since then, we\u2019ve had thousands of readers over 70 posts. The top 5 most popular posts are:</p> <ul> <li>Designing a Dashboard for Concourse</li> <li>Getting Started with Concourse on macOS</li> <li>Earning our Wings</li> <li>Concourse Pipeline UI Explained</li> <li>Sneak Peek: Spatial Resources</li> </ul>"},{"location":"blog/2018-12-12-concourse-2018-year-in-review/#thanks-to-our-contributors","title":"Thanks to our Contributors","text":"<p>Finally, Thanks to all our contributors across all our repos:</p> <p>**concourse/concourse ** **: ** edtan, ralekseenkov, SHyz0rmZ, databus23, pivotal-kahin-ng, jmcarp, tkellen, aeijdenberg, rosenhouse, andrewedstrom, baptiste-bonnaudet, PavelTishkin, timchavez, JamesClonk, rfliam, ArthurHlt, christophermancini</p> <p>**concourse/concourse-docker: ** danielrs, scottbri, dbbaskette, ElfoLiNk, jmcduffie32, SergueiFedorov</p> <p>**concourse/concourse-bosh-release: ** JamesClonk, ramonskie, avanier, rkoster, SHyz0rmZ, aeijdenberg, jmcarp, ArthurHlt</p> <p>**concourse/docs: ** vlad-ro, charlieoleary, AnianZ, berlin-ab, a114m, ukabu, arbourd, baptiste-bonnaudet, marco-m, rosenhouse, patrickcrocker, crstamps2, JohannesRuolph, dbp587, headc4sh, aequitas</p> <p>**concourse/docker-image-resource: ** ghostsquad, dhinus, norbertbuchmueller, hephex, chrishiestand, kmacoskey, irfanhabib, simonjohansson, et7peho, itsdalmo, krishicks, mook-as</p> <p>**concourse/git-resource: ** norbertbuchmueller, talset, ppaulweber, elgohr, alucillo, goddenrich, njbennett, timchavez, suda, jamesjoshuahill, gcapizzi, mdomke, benmoss, ljfranklin, oliveralberini, krishicks</p> <p>**concourse/s3-resource: ** ghostsquad, talset, 46bit, bandesz, ruurdk</p> <p>...and many more. Special thanks to all the contributors who\u2019ve built new resources for Concourse in 2018, contributed to the health of resources and also took over old resources.</p>"},{"location":"blog/2018-12-12-concourse-2018-year-in-review/#see-you-in-2019","title":"See you in 2019!","text":"<p>I hope y\u2019all get to enjoy some time off this holiday season. We\u2019ve got a lot of updates planned for 2019, like our new contributor workflow, Concourse 5.0 and spatial resources! Look forward to an in-depth post from Alex Suraci in the next few days.</p> <p>Thanks again for all the support, and we\u2019ll see you in 2019!</p>"},{"location":"blog/2018-12-20-the-great-process-update-of-2018/","title":"The Great Process Update of 2018","text":"<p>Source: pixabay.com</p> <p>You may have noticed that our release cadence has slowed down significantly in the past few months. The bad news is we probably won\u2019t get a release out this year (mainly due to end-of-year vacations and slowing down in general), but the good news is the next release is huge \u2014 big enough to bump us to v5.0 \u2014 and it\u2019s just about ready. I\u2019ll have more information on the next release in an upcoming post.</p> <p>This post will go over all the changes we\u2019ve made to our project structure and processes surrounding contribution. These changes aren\u2019t very visible to end-users, but they set the stage for the community growth and collaboration that will make our future releases even better and bring more depth to our culture and ecosystem.</p>"},{"location":"blog/2018-12-20-the-great-process-update-of-2018/#a-newly-minted-process-for-rfcs","title":"A newly minted process for RFCs","text":"<p>We\u2019ve finally established a process for submitting and accepting RFCs! Head over to the <code>concourse/rfcs</code> repo if you want to check it out.</p> <p>This new process enables anyone in the community to have a big impact on Concourse\u2019s direction. I\u2019m really looking forward to seeing where this goes. We\u2019ll be posting status updates for RFCs on this blog to notify the community of RFCs that are newly opened or near acceptance.</p> <p>We've already started submitting RFCs for substantial features like Resources V2 and RBAC, though we jumped the gun a bit on implementation as we hadn\u2019t yet figured out what we wanted from the RFC process (we just needed a better way to plan things in the open). There are a few loose ends to tidy up with existing RFCs now that we have a full process in place.</p> <p>Credit where it\u2019s due: this process based pretty heavily on Rust\u2019s. Just about every detail seemed to apply just as appropriately to Concourse, and we\u2019re just as cautious about far-reaching changes, so it was a great match.</p>"},{"location":"blog/2018-12-20-the-great-process-update-of-2018/#switching-from-cla-to-dco","title":"Switching from CLA to DCO","text":"<p>Up until now, all pull request authors have had to sign off on the Pivotal CLA in order for their pull request to be accepted (unless it was an \u201cobvious fix\u201d).</p> <p>On occasion contributors would get caught in a corporate quagmire when trying to get their company to sign off on the CLA, and it was also kind of jarring for individuals. The need for something like the CLA hasn\u2019t gone away, but we felt it may have been hindering more than helping.</p> <p>So, we\u2019re abandoning the CLA process and instead adopting the Developer Certificate of Origin (\u201cDCO\u201d) process. This process is much more lightweight, only requiring pull request authors to include a \u201cSigned-off-by:\u201d line in each commit, which can be done via <code>git commit -s</code>. More information on this is available in <code>CONTRIBUTING.md</code>.</p>"},{"location":"blog/2018-12-20-the-great-process-update-of-2018/#completing-the-great-project-restructuring-of-2018","title":"Completing the Great Project Restructuring of 2018","text":"<p>The single biggest cause of the release slowdown has been The Great Project Restructuring of 2018, which was a massive revamp of how we develop, build, test, and ship Concourse. We knew this would be a \u201cstop-the-world\u201d transition that would prevent us from shipping for a while, but we really had to bite the bullet at some point.</p> <p>The focal point of this restructuring: almost all of Concourse\u2019s code now lives in one big <code>concourse</code>monorepo, using the new Go 1.11 module system to track dependencies. We\u2019ve replaced our BOSH-centric development and pipeline workflow with a Docker-based workflow which is more intuitive and has a much faster feedback cycle.</p> <p>This means you can now <code>git clone</code> the Concourse repo and get a cluster built from source and running in single command: <code>docker-compose up</code>. It\u2019s never been easier to make changes and test them out locally. Check out the new <code>CONTRIBUTING.md</code> for more information!</p> <p>This change kicked off a ripple effect that improved a ton of things about the developer, contributor, and operator experience:</p> <ul> <li>Now that all the code is together in one repo, cross-cutting changes can now be submitted as a single pull request! \ud83c\udf8a   Pull requests now trigger acceptance tests too, which is something we couldn\u2019t really do easily before.</li> <li>Resources are now versioned and shipped independently of Concourse versions. Each resource is published as   <code>concourse/&lt;name&gt;-resource</code> with appropriate tags (e.g. <code>1.2.3</code>, <code>1.2</code>, <code>1</code>, <code>latest</code>, <code>dev</code>). This means you can   refer to specific versions when necessary by using <code>resource_types:</code> in your pipeline. A core set of resource types   will still be shipped with Concourse, at whichever version they were when the release was frozen.</li> <li>The <code>concourse</code> repo is no longer a BOSH release; we\u2019ve split it out   into its own repository instead. The new BOSH release simply   wraps the binary distribution, rather than building from source. This reduces the surface area for support and removes   any discrepancies between the platforms \u2014 everything just uses the binary now! This also makes deploying the BOSH   release faster because there\u2019s not much to compile.</li> <li>We\u2019ve changed how the <code>concourse</code> executable is packaged. We\u2019re switching to a <code>.tgz</code> format containing the binary and   its dependencies, rather than a self-extracting \u201call-in-one\u201d binary. This results in way fewer moving parts and   dramatically reduces <code>concourse worker</code> start-up time.</li> </ul>"},{"location":"blog/2018-12-20-the-great-process-update-of-2018/#where-are-we-now","title":"Where are we now?","text":"<p>Overall, I think these recent changes may be the most important thing we\u2019ve done for the project since its inception, even if it meant not shipping for a while.</p> <p>The RFC process will make it easier to collaborate, switching to the DCO removes a hurdle for new contributors, and the the new project structure should dramatically improve the developer experience.</p> <p>I\u2019d like to give special thanks to everyone that has tried out and given feedback on this new development process, and all the users that have waited patiently for the next release. \ud83d\ude05</p>"},{"location":"blog/2018-12-20-the-great-process-update-of-2018/#whats-next","title":"What\u2019s next?","text":"<p>Well, now that the dust is settling it\u2019s time to actually start shipping software again. The next post will go over what\u2019s in store for 5.0 and peek ahead into what we\u2019re planning for 2019. See you then!</p>"},{"location":"blog/2019-01-11-concourse-update-jan-711/","title":"Concourse Update (Jan 7\u201311)","text":"<p>Photo Credit MomentsForZen</p> <p>...and we\u2019re back! The team\u2019s been pretty quiet over the past few weeks due to vacations and holidays. This was our first week back at full strength so we\u2019ve got some interesting updates for ya\u2019ll</p>"},{"location":"blog/2019-01-11-concourse-update-jan-711/#how-are-issues-managed","title":"How are issues managed?","text":"<p>This is an issue that comes up a lot in our open source community, and Alex Suraci has taken some time to clean up our issues backlog and add in some bots. You can read the full details here: How Issues are Managed</p> <p>In addition to the changes to how issues are labeled, we\u2019ve also changed how we used projects and milestones under <code>concourse/concourse</code>. Epics are now organized under projects in <code>concourse/concourse</code>, and release markers are managed under milestones in <code>concourse/concourse</code>. And as always, our \u201ctracks of work\u201d can be found at the org-level project page.</p>"},{"location":"blog/2019-01-11-concourse-update-jan-711/#updates","title":"Updates","text":""},{"location":"blog/2019-01-11-concourse-update-jan-711/#ux","title":"UX","text":"<p>Thanks to the hard work of the UX team, they were able to crank through a lot of nice UI issues over the past few weeks. This includes #2405 and #2881. We will also be scheduling a big track of work for transitioning to Elm 0.19.</p>"},{"location":"blog/2019-01-11-concourse-update-jan-711/#core","title":"Core","text":"<p>We\u2019re picking up from the global resource cache work from last year and picking off the remaining blockers to release. Specifically #2908 needs to be addressed otherwise everyone\u2019s <code>time-resource</code> will kick off at the same time; which may be very bad news for shared environments. In order to keep the release process on track we will be parallelizing #2874 performance testing to another pair.</p>"},{"location":"blog/2019-01-11-concourse-update-jan-711/#runtime","title":"Runtime","text":"<p>Having completed the placement strategy and testing it in prod, we\u2019re proceeding to do some refactoring in #2926</p>"},{"location":"blog/2019-01-18-concourse-update-jan-1418/","title":"Concourse Update (Jan 14\u201318)","text":"<p>Read GH issue #3003 for context\u2026or don\u2019t \ud83e\udd37</p> <p>Some updates worth bringing up this week. As I had mentioned last week we began to do a re-organization of projects and issues in our <code>concourse/concourse</code> repo; you can read more about it on our wiki page here. With that said, you can find the issues and PRs that are slated for Concourse 5.0.0\u2019s release in our 5.0.0 Milestones. If you\u2019d like to help us with documentation, we\u2019ve started a new branch in the docs repo under v5.0.</p> <p>One of the items we want to resolve before release is issue #3003 \u201cDetermine full set of core resources that we should bundle with Concourse\u201d. In this discussion we\u2019re going over the idea of removing the pre-baked resources in favour of slimming down the Concourse footprint and only shipping what is absolutely needed. We want to hear how this may impact you and your Concourse experience. We\u2019d like to wrap this up soon, so please drop in a comment at your earliest convenience!</p> <p>In other big news, the Concourse core engineering team has officially switched to a PR based workflow. That means we are no longer allowing direct commits to master and all issue \u201cacceptance\u201d will be conducted via the merging of pull requests. We hope this will make our development process even more transparent and further involve the community in day-to-day work!</p> <p>On to the update:</p>"},{"location":"blog/2019-01-18-concourse-update-jan-1418/#ux","title":"UX","text":"<ul> <li>Added a comments bar to indicate paused resources are now pinned (   PR#3064)</li> <li>You can now force check a resource from the web UI (PR #3051)</li> </ul>"},{"location":"blog/2019-01-18-concourse-update-jan-1418/#core","title":"Core","text":"<ul> <li>Completed #2908. This is one of the key blocking issues   preventing us from releasing Concourse 5.0.0</li> <li>Picked up #3013 as a way to address the two very clear use cases   where you might not want it: lots of time resources and resources that use IAM roles</li> <li>Picked up the performance test work #2874</li> </ul>"},{"location":"blog/2019-01-18-concourse-update-jan-1418/#runtime","title":"Runtime","text":"<ul> <li>In the first issue of many around runtime refactoring, we picked   up #3502 to break up the responsibilities of <code>containerProvider</code></li> </ul>"},{"location":"blog/2019-01-25-concourse-update-jan-2125/","title":"Concourse Update (Jan 21\u201325)","text":"<p>Photo credit Dennis Jarvis</p> <p>It's been a week since we switched over to the PR workflow and so far its been great! We\u2019re still working through some of the kinks with this process so please bear with us while we continue to burn down through the list of open PRs!</p> <p>And now...on to the update! I might have missed a few issues while I\u2019m still getting used to our new workflow. Completed issues now appear as closed PRs in concourse/concourse</p>"},{"location":"blog/2019-01-25-concourse-update-jan-2125/#docs","title":"Docs","text":"<ul> <li>Started to burn down our list of todos for Concourse docs pre-release. You can follow along   in #143</li> <li>Proposed a new structure for our docs in #136. Brace yourselves for   broken links.</li> </ul>"},{"location":"blog/2019-01-25-concourse-update-jan-2125/#ux","title":"UX","text":"<ul> <li>Added a \u201cnew version\u201d tooltip to versions in Resource page #3136</li> <li>Fixed a whole bunch of UX quirks in preparation for v5.0.0 release</li> <li>Beginning our Elm 0.19 refactor and upgrade</li> </ul>"},{"location":"blog/2019-01-25-concourse-update-jan-2125/#core","title":"Core","text":"<ul> <li>Upgrade and performance testing for Concourse 5.0 #2874</li> </ul>"},{"location":"blog/2019-01-25-concourse-update-jan-2125/#runtime","title":"Runtime","text":"<ul> <li>Decoupling container and volume creation in   FindOrCreateContainer #3052</li> </ul>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/","title":"An Overview of Authorization in Concourse 3, 4 and 5","text":"<p>Photo credit NASA HQ Photo</p> <p>With the release of Concourse 5.0.0 this week I thought it would be a good time to review the evolving implementation of authorization in Concourse. I\u2019ll also be covering some helpful debugging information for you to consider when configuring authorization in your own Concourse instance.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#read-the-docs","title":"Read the Docs","text":"<p>The revised Concourse Auth &amp; Teams docs is a great place to start when diving into Concourse 5.0.0. The docs will cover important steps around provider configuration and team configuration for your cluster. If you\u2019re more interested in how things used to work compared to how they now work; then read on!</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#how-authorization-works-in-3x","title":"How Authorization Works in 3.x","text":"<p>This section will only be useful to operators who are migrating into 4.x and beyond. Feel free to skip ahead if this does not apply to you.</p> <p></p> <p>Every Concourse instance starts with a <code>main</code> team that must be configured against an Authentication Provider on start-up. The <code>main</code> team is an admin team, meaning it can create teams, update other teams and view system-scoped details on workers, containers, etc.</p> <p>One of the tasks that only a <code>main</code> user can do is to create new teams via <code>set-team</code>. When creating the team, the operator must specify:</p> <ul> <li>An Authentication Provider e.g. Basic Auth, GitHub, OAuth</li> <li>The relevant configuration of the Authentication Provider e.g. secrets, tokens</li> <li>The user/group to authorize (if applicable)</li> </ul> <p>Some important notes to keep in mind:</p> <ul> <li>Authentication Provider configurations are attached to an individual team, and not shared across teams. As an   operator, you will have to repeat/resupply the Authentication Provider configuration for each team. If <code>Team 1</code> wanted   to change their own auth to add a member or group they would have to ask the Operator for the GitHub API token or   bring their own.</li> <li>Since Authentication Provider details are provided per-team, operators can set unique provders for each. A common   use-case is to provision <code>Team 1</code> to authenticate against the USA-East-1 OAuth server and <code>Team2</code> to authenticate   against the EMEA OAuth server.</li> <li>You can stack Authentication Providers by supplying multiple parameters when applying <code>set-team</code>; e.g. a team can   have both GitHub and Basic Auth configured to authenticate users.</li> <li>Users who are authorized to access more than one team can only see one team at a time.</li> </ul>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#concourse-40-users","title":"Concourse 4.0 \u2014 Users","text":"<p>Concourse 4 introduced Users and totally revamped the authorization flow:</p> <ul> <li>Identity providers must be specified when Concourse first starts up (this includes local users as well!)</li> <li>Identity providers are shared across teams and can no longer be customized per-team</li> <li>Adding/removing Identity Providers require a restart to the <code>web</code> node</li> <li>When specifying groups in provider configuration, administrators must use <code>:</code> as the separator instead of <code>/</code></li> <li>Users logging into Concourse are whitelisted into all teams that match their provider membership. More on this later</li> </ul>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#overview-of-authorization-flow","title":"Overview of Authorization Flow","text":"<ol> <li>Operator determines the Identity Providers they will allow in Concourse and configures their Concourse startup    scripts (Docker, BOSH, Helm, etc.) with the necessary parameters as described    in Configuring Auth.</li> <li>If there are any local users that have Basic Auth (username/password) identities, the operator will add them to the    startup scripts as outlined in Local Auth</li> <li>The Operator will start Concourse and begin creating teams using the <code>fly set-team</code> command. Keeping in mind the auth    providers that were added in step (1) the Operator can specify the allowed users/groups/teams from that provider.    See Configuring Team Auth for more details.</li> <li>When a User logs into Concourse, they are asked to login using one of the configured providers from (1).</li> <li>Once the User selects a provider, Concourse will redirect the User to the identity provider\u2019s authentication    mechanism and wait for a successful login response</li> <li>When a login success response is received, Concourse will examine all of the teams/orgs the User belongs to under    that provider. Concourse will then match the user\u2019s information against the internal list of Concourse teams and    their list of whitelisted users/teams/orgs. The resulting list will be the teams that the User can access</li> <li>The User is logged into Concourse and can access the teams they were whitelisted into</li> </ol>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#identity-providers","title":"Identity Providers","text":"<p>An Identity Provider is any external entity that creates, manages and maintains identity information to auth. Concourse 4 uses the OSS dex library to do most of the heavy lifting.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#specifying-identity-providers","title":"Specifying Identity Providers","text":"<p>You will need to provide the connection details for all the auth connectors you plan to use for teams up front. The full list of supported providers and their require parameters can be found on the Concourse docs site under Configuring Auth.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#local-users-the-special-case","title":"Local Users, The Special Case","text":"<p>Local Auth users are a bit of a special case because there\u2019s no external auth provider for them, and you can no longer \u201ccreate\u201d them on <code>set-team</code>.</p> <p>To add a local user you will need to add that user to the Concourse startup parameter list as described in the Local Auth docs.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#whitelisting-users-with-set-team","title":"Whitelisting Users with <code>set-team</code>","text":"<p>Once you have configured the providers you can freely add users/teams/orgs/groups/whatever to a team. This is as simple as using the parameters described in the <code>fly set-team</code> docs for Configuring Team Auth.</p> <p>As with most <code>fly</code> commands, you can actually attach multiple users/teams across providers to a team. For example: if you have GitHub and OAuth providers set up, a team owner could attach two teams (one from GitHub, one from OAuth) to the team.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#examples","title":"Examples","text":"<p>In this example we have a simple Concourse installation with two identity providers: GitHub and a single Local User.</p> <p>On the left we have two simple GitHub orgs: Pivotal and Concourse. Pivotal has three teams: cloud, billing and admin. Concourse has one team. Each team has a single user attached to them.</p> <p>On the right we have a map of the Concourse teams and their allowed users/groups.</p> <p>Let\u2019s go through a few scenarios to get a good understanding of how auth works in Concourse 4.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#local-user-logs-in","title":"Local User Logs In","text":"<p>A Concourse user uses the local user provider to login with <code>username:password</code> and only sees <code>Team Local</code>.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#alice-logs-in","title":"Alice Logs In","text":"<p>Alice logs into Concourse using the GitHub auth scheme. She finishes the flow and sees..two teams! Because she is a member of the <code>Pivotal</code> GitHub org she sees <code>Team All</code>, which is configured to allow all users under the <code>pivotal</code> org on GitHub. She also sees Team 1 because it allows all users who are also memebers of <code>pivotal:cloud</code> on GitHub.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#operator-logs-in","title":"Operator Logs In","text":"<p>The Operator logs in using GitHub auth and...can see everything! Because the Operator is part of the <code>main</code> team, they can see all teams. However, that does not mean the Operator can see all the team pipelines. In this scenario, the Operator can only see the <code>Main</code> and <code>Team All</code> team pipelines.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#a-non-member-logs-in","title":"A non-member logs in","text":"<p>Jama finds out about this cool Concourse thing and logs into Concourse using the GitHub auth provider. Since he has a GitHub account he is able to login successfully. However, once the login flow is completed he is returned to Concourse and a blank screen\u2026nothing is available to him! Jama is not a member of a GitHub team/organization that was specified in the Concourse team configurations.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#debugging-login-problems","title":"Debugging Login Problems","text":""},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#what-are-the-auth-settings-for-insert-team-name","title":"What are the auth settings for [insert team name]?","text":"<p>If you are an operator, and you need to figure out what the exact auth settings are, you can use the new <code>fly teams -d</code> command. This will list the teams with details, including the users and groups whitelisted into that team</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#help-i-logged-in-but-i-cant-see-my-team","title":"Help, I logged in but I can\u2019t see my team","text":"<ol> <li>Try using the search function on the dashboard. This is silly but for large Concourse clusters there are a LOT of    teams with exposed pipelines and it can be hard to find the team you need</li> <li>Logout and Log back in. Due to the implementation of the auth scheme, Users who are already logged into Concourse and    are added into a new team must refresh their token by logging out and logging in.    Yes, we know it sucks.</li> <li>Is the user a member of the org that was specified in <code>set-team</code>? For example, if GitHub team <code>pivotal:foo</code> was used,    make sure to ask if the user is a member of that team on GitHub!</li> <li>Was there a typo? Use <code>fly set-team -d</code> to look for the team in question and triple-check the spelling of usernames    and teams</li> <li>Did you use the correct separator? Concourse requires all group separators to use <code>:</code> and not <code>/</code>:<ul> <li><code>pivotal:foo</code> is OK</li> <li><code>pivotal/foo</code> will fail silently on <code>set-team</code></li> </ul> </li> </ol>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#i-have-two-identity-servers-how-do-i-add-them-both","title":"I have two Identity servers, how do I add them both?","text":"<p>Unfortunately, that is not possible in Concourse 4. You\u2019ll notice that you can only supply one set of credentials when providing auth providers. The side effect limitation is that a single Concourse installation can\u2019t be connected to more than one of the same provider. The operator will have to set up another Concourse if they absolutely must be able to connect to two different identity providers of the same type.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#concourse-50-rbac","title":"Concourse 5.0 \u2014 RBAC","text":"<p>Concourse 5.0 comes with 4 roles: Concourse Admin, Team Owner, Team Member, and Team Viewer.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#concourse-admin","title":"Concourse Admin","text":"<p>A Concourse Admin is the same as today\u2019s admin user. Members of <code>main</code> team will automatically be Concourse Admins* and have the ability to administrate teams with <code>fly</code>: <code>set-team</code>, <code>destroy-team</code>, <code>rename-team</code>, and <code>teams</code>. Given that all Concourse Admins must be a member of the <code>main</code> team, all Concourse Admins must have at least one other role; and that should typically be the Team Owner role.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#team-owner","title":"Team Owner","text":"<p>Team Owners have read, write and auth management capabilities within the scope of their team. For those familiar with Concourse today, the scope of allowed actions for a Team Owner is very closely aligned to today\u2019s Concourse team member. The new change is that you can no longer rename your own team or destroy your own team as an owner.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#team-member","title":"Team Member","text":"<p>Team Member is a new role that lets users operate within their teams in a read &amp; write fashion; but prevents them from changing the auth configurations of their team.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#team-viewer","title":"Team Viewer","text":"<p>Team Viewer is also a new role that gives users \u201cread-only\u201d access to a team. This locks everything down, preventing users from doing a <code>set-pipeline</code> or <code>intercept</code>.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#full-roles-breakdown","title":"Full Roles Breakdown","text":"<p>For a full list of each role\u2019s allowed actions you can reference our handy permission matrix on Google Sheets here.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#configuring-roles-with-fly","title":"Configuring Roles with fly","text":"<p>Now that we\u2019ve gone over the new roles, we can do a quick overview of how we can go about setting users &amp; roles on teams.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#default-behaviour","title":"Default Behaviour","text":"<p>By default, if no configuration is provided the user is given the <code>Team Owner</code> role:</p> <pre><code>fly -t dev set-team -n PowerRangers --local-user=Zordon\n\n# This behaviour also applies to groups as well, so be careful!\nfly -t dev set-team -n A-Team \\\n  --github-team=MightyMorphin:PowerRangers\n</code></pre>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#specifying-roles-with-c","title":"Specifying Roles with <code>-c</code>","text":"<p>Roles must be specified in a separate configuration file using the <code>-c</code></p> <pre><code>fly -t dev set-team -n A-Team -c ./team.yml\n</code></pre> <p><code>team.yml</code></p> <pre><code>roles:\n  - name: owner\n    local:\n      users: [ \"Zordon\" ]\n  - name: member\n    local:\n      users: [ \"RedRanger\", \"BlueRanger\", \"GreenRanger\" ]\n  - name: viewer\n    local:\n      users: [ \"Alpha\" ]\n</code></pre>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#inspecting-roles-configuration","title":"Inspecting Roles Configuration","text":"<p>Once you\u2019ve set the team configuration you can verify it using the details flag on <code>fly teams</code>:</p> <pre><code>fly -t dev teams -d\nname users groups\nA-Team/member local:RedRanger, BlueRanger, GreenRanger none\nA-Team/owner local:Zordon none\nA-Team/viewer local:Alpha none\n</code></pre> <p>... where you\u2019ll find the output is now updated to list each team/role combination and its associated users/groups.</p>"},{"location":"blog/2019-03-08-an-overview-of-authorization-in-concourse-3-4-and-5/#further-reading","title":"Further Reading","text":"<ul> <li>Oh, Auth by Josh Winters</li> <li>Concourse RBAC Preview.</li> <li>Concourse Auth &amp; Teams docs</li> </ul>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/","title":"Installing Concourse 5.0 on Kubernetes using Helm","text":"<p>A close up of the helm of a boat</p> <p>This article was written in conjunction with \u00c9amon Ryan, Advisory Solutions Architect at Pivotal.</p> <p>In early 2018, Pivotal and VMware came together to release Pivotal Container Service, known as PKS, to provide the power of Kubernetes with the reliability of BOSH and an alternative to Pivotal Application Service for software that needs it.</p> <p>The rise of Kubernetes (k8s) as a popular developer platform led me to wonder how easy it would be to install Concourse 5.0 on PKS. It turns out, it\u2019s very easy indeed. This tutorial will assume you\u2019ve already installed PKS 1.3 on vSphere that has NSX-T, VMware\u2019s software defined networking (SDN) solution, installed.</p> <p>While these instructions are based on PKS, the Helm chart you will use work on any k8s provider. If you already have your cluster provisioned, you can skip ahead to \u201cUsing the Helm Chart to install Concourse\u201d</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#prerequisites","title":"Prerequisites","text":"<p>PKS uses the Pivotal On-Demand Services SDK to provide k8s clusters on demand (it\u2019s not just a clever name!). The service broker that\u2019s created when you install PKS needs to have plans, so that you can create clusters of different sizes and production worthiness. You can install Concourse on the default <code>small</code> plan that is provided, with one small change: because Concourse will be running containers inside containers, you must enable privileged containers. In the Ops Manager page for your specific plan, look toward the bottom of the page, and you\u2019ll see this checkbox.</p> <p></p> <p>Checkbox labelled \u201cEnable Privileged Containers \u2014 Use with caution\u201d</p> <p>If that box is not checked, check it, and Apply Changes from the Ops Manager Installation Dashboard.</p> <p>It should be noted that privileged containers can introduce security risks, and very few cases actually need it. In this case, Concourse does require privileged containers because it will be running containers (tasks and resources) inside containers (the worker pods).</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#creating-a-cluster-for-concourse","title":"Creating a Cluster for Concourse","text":"<p>The <code>pks</code> CLI, which works in conjunction with <code>kubectl</code>, is used to create clusters. From a command prompt, run the following commands to create a k8s cluster:</p> <pre><code>$ pks login -a api.pks.haas-433.pez.pivotal.io -u pksadmin\nPassword: *******************\nAPI Endpoint: api.pks.haas-433.pez.pivotal.io\nUser: pksadmin\n$ pks create-cluster concourse --plan small --external-hostname concourse-km.haas-433.pez.pivotal.io\n</code></pre> <p>Note</p> <p>The DNS name does not need to exist when you create the cluster, but you must specify something. It is used to  generate TLS certificates for your k8s cluster, so it must be something you can create a DNS entry for when the  command succeeds.</p> <p>Because <code>pks</code> is creating a new BOSH deployment under the covers, and running a number of errands under the covers, this can take up to 30 minutes to complete. You can track process by running the <code>pks cluster</code> command:</p> <pre><code>$ pks cluster concourse\n\nName:                     concourse\nPlan Name:                small\nUUID:                     eaf1234f-d29c-4ad7-8103-ce0ae7777e11\nLast Action:              CREATE\nLast Action State:        succeeded\nLast Action Description:  Instance provisioning completed\nKubernetes Master Host:   concourse-km.haas-433.pez.pivotal.io\nKubernetes Master Port:   8443\nWorker Nodes:             3\nKubernetes Master IP(s):  10.197.59.136\nNetwork Profile Name:\n</code></pre> <p>When you see <code>Last Action State</code> as <code>succeeded</code>, you can take the <code>Kubernetes Master IP(s)</code> listed and create a DNS A record to the <code>Kubernetes Master Host</code> domain listed.</p> <p>Under the covers, the command has created a BOSH deployment that has created a Kubernetes Master VM, as well as the number of workers specified by your plan (the <code>small</code> plan creates 3 workers by default). It also created an NSX-T load balancer with multiple VIPs including the \u201cKubernetes Master IP\u201d you were provided, so regardless of whether you had deployed a k8s cluster with 1 or 3 masters, you would only see one IP listed here, since it is actually the load balancer IP with the backend already populated with your real master IP addresses.</p> <p>When you ask for PKS to create a k8s cluster, the following things occur:</p> <ul> <li>A network (NSX-T T1 router and logical switch) is created for the master and workers to sit on</li> <li>Networks are created for each default namespace (<code>default</code>, <code>kube-system</code>, <code>kube-public</code>, <code>pks-system</code>) for the pods   in those namespaces with accompanying SNAT rules</li> <li>Master and worker VMs are provisioned by BOSH</li> <li>An NSX-T load balancer is created, consuming 2 routable VIPs and 3 Virtual Servers initially. The first VIP is used to   load balancer the master VMs as stated above, and the second is explained below.</li> <li>Various post-deploy errands are executed</li> </ul> <p>Once your cluster is ready, back in <code>kubectl</code> land, it\u2019s important to note the following two behaviors:</p> <ol> <li>If you create a service of <code>type: LoadBalancer</code>, NSX-T will automatically create a new L4 Virtual Server entry on the    cluster load balancer with a new VIP.</li> <li>If you create a k8s <code>Ingress</code>, NSX-T will add an L7 HTTP forwarding rule to the second VIP that was pre-created on    the cluster load balancer at deploy time.</li> </ol> <p>Both methods allow you to access your workloads, it just depends on the method you want to use. The advantage here is you don\u2019t have to setup/configure nginx or similar to perform these services as the NSX-T integration handles that for you.</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#using-the-cluster-to-install-concourse","title":"Using the Cluster to install Concourse","text":"<p>Now that the cluster is created, we can (almost) go back to the <code>kubectl</code> and <code>helm</code> commands that you probably already know. You may be wondering how you authenticate against the cluster now, and that\u2019s where the final <code>pks</code> command comes in:</p> <pre><code>$ pks get-credentials concourse\n</code></pre> <p>That command creates a <code>$HOME/.kube/config</code> that <code>kubectl</code> uses for authentication.</p> <p>Now that you\u2019re authenticated against the cluster, you need to create a couple of resources ahead of time.</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#tiller","title":"Tiller","text":"<p>Helm is a client/server system with the <code>helm</code> CLI as the client, and a service called Tiller that runs in the cluster. To use Tiller in your cluster, you need to create a service account. Create a file on your machine called <code>tiller-config.yml</code> with the following contents:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system\n</code></pre> <p>And apply it to the cluster by running the command <code>kubectl create -f tiller-config.yml</code>. Once your service account is created, initialize it by running the command <code>helm init --service-account tiller</code>. Once this command completes successfully, your cluster will now be able to install helm charts.</p> <p>It should be noted that because this is a very simple example, you used the default configuration for Tiller, which is less secure for a number of reasons, including the fact that it does not use TLS. For information around creating a more secure Tiller deployment, read the Helm Documentation.</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#persistent-volumes","title":"Persistent Volumes","text":"<p>Concourse requires persistent volumes for both the workers (which run Concourse resources and tasks, not to be confused with k8s workers) and its database. Therefore, your cluster needs to have at least one <code>StorageClass</code> resource defined.</p> <p>Because PKS is IaaS-agnostic, it does not automatically provision a <code>StorageClass</code> for you like most public clouds\u2019 managed k8s offerings do. In vSphere, you can create both static and dynamic provisioned <code>StorageClass</code> resources, with dynamic being preferred in this case. As before, you\u2019ll create a config <code>yaml</code>, but call this one <code>storage-config.yml</code> and make it look like this:</p> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: concourse-storage-class\nprovisioner: kubernetes.io/vsphere-volume\nparameters:\n  datastore: LUN01\n  diskformat: thin\n  fstype: ext3\n</code></pre> <p>The name <code>concourse-storage-class</code> above can be whatever you want. The datastore is the name of an actual Datastore object in your vSphere datacenter, where virtual disks can be created to be used as homes for your persistent volumes. Apply as before by running <code>kubectl create -f storage-config.yml</code>.</p> <p>You\u2019ve now created all of the prerequisite resources necessary to install Concourse!</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#using-the-helm-chart-to-install-concourse","title":"Using the Helm Chart to install Concourse","text":"<p>A note for Pivotal customers</p> <p>The Concourse Helm chart is not yet supported by Pivotal so please proceed with caution. However, there is a very  active and vibrant community of operators who have deployed Concourse with the Helm chart and are very happy using  it day-to-day</p> <p>Now that you are ready to install Concourse, you can create the necessary values so that your installation is customized just for you. Helm charts live in a GitHub repository, so you can visit Concourse\u2019s in the official repository here. In particular, you will want to grab the <code>values.yaml</code> file from that directory, as you will need to make customizations to it. That file is large and well-documented, so it will be easier to create a new <code>yaml</code> file with the necessary customizations \u2014 it will be merged with the default <code>values.yaml</code> at install time. Call this <code>concourse-config.yml</code> and make it look like this:</p> <pre><code>concourse:\n  web:\n    auth:\n      mainTeam:\n        localUser: admin\n    externalUrl: https://concourse.haas-433.pez.pivotal.io\n    bindPort: 80\n    tls:\n      enabled: true\n      bindPort: 443\n  worker:\n    baggageclaim:\n      driver: btrfs\n  persistence:\n    worker:\n      storageClass: concourse-storage-class\n  postgresql:\n    persistence:\n      storageClass: concourse-storage-class\n  web:\n    service:\n      type: LoadBalancer\n  secrets:\n    localUsers: SEE BELOW\n    webTlsCert: |\n      -----BEGIN CERTIFICATE-----\n      data data data data\n      -----END CERTIFICATE-----\n    webTlsKey: |\n      -----BEGIN RSA PRIVATE KEY-----\n      data data data data\n      -----END RSA PRIVATE KEY-----\n    workerKey: |\n      -----BEGIN RSA PRIVATE KEY-----\n      data data data data\n      -----END RSA PRIVATE KEY-----\n    workerKeyPub: ssh-rsa datadatadata comment\n</code></pre> <p>Let\u2019s break down some of the big things here:</p> <p>First, <code>concourse.web.auth.mainTeam.localUser</code>. Because this is a relatively simple example, we\u2019ll create our main team with a local user. Make sure your user is also listed in <code>secrets.localUsers</code>, where the user is specified in the format <code>username:password</code>. Passwords can be plain text or <code>bcrypt</code>-hashed. Make sure your <code>bcrypt</code> hashing goes through at least 10 rounds. To do this on MacOS, use the following command:</p> <pre><code>htpasswd -bnBC 10 &lt;USERNAME&gt; &lt;PASSWORD&gt;\n</code></pre> <p>Next, <code>concourse.web.externalUrl</code>. It\u2019s critical that you set this to a valid DNS name, because it\u2019s used for callbacks. If you\u2019re enabling TLS like in the example above, it must be an <code>https://</code> URL. The DNS name need not be mapped to anything yet, because you won\u2019t get the IP of the Load Balancer until after you install.</p> <p>Setting <code>concourse.worker.baggageclaim.driver</code> to <code>btrfs</code> is a performance enhancement over the default <code>naive</code> driver, and Helm will warn you if you don\u2019t change it.</p> <p>I created a self-signed certificate for <code>secrets.webTlsCert</code> and <code>secrets.webTlsKey</code>, but you should probably use a signed certificate. The <code>LoadBalancer</code> resource created is Layer 4, and will pass traffic directly to the web pod, where it will terminate TLS.</p> <p>You should also create your own worker SSH key, as it used to communicate between the worker pod and the web pod. There is a default value in the Helm chart, but that would, of course, be very insecure. Concourse 5.0 introduces a command called <code>generate-key</code> to generate this SSH key that you can use in place of <code>ssh-keygen</code>, if you desire.</p> <pre><code>mkdir $HOME/ssh-keys\ndocker run -v $HOME/ssh-keys:/keys -it concourse/concourse generate-key -t ssh -f /keys/worker\n</code></pre> <p>This command will create two files in <code>$HOME/ssh-keys</code>: <code>worker</code> and <code>worker.pub</code>. You can copy and paste the contents of the file into the <code>secrets.workerKey</code> and <code>secrets.workerKeyPub</code> fields.</p> <p>Finally, you need to set the storage class name for PostgreSQL and the concourse workers to the name you created earlier so it can provision the necessary persistent volumes.</p> <p>To install, run <code>helm install --name concourse -f concourse-values.yml stable/concourse</code>. The output will look something like this:</p> <pre><code>NAME:   concourse\nLAST DEPLOYED: Wed Mar 20 11:39:17 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\nRESOURCES:\n==&gt; v1/ConfigMap\nNAME                  DATA  AGE\nconcourse-postgresql  0     1s\n==&gt; v1/Namespace\nNAME            STATUS  AGE\nconcourse-main  Active  1s\n==&gt; v1/PersistentVolumeClaim\nNAME                  STATUS   VOLUME                   CAPACITY  ACCESS MODES  STORAGECLASS  AGE\nconcourse-postgresql  Pending  concourse-storage-class  1s\n==&gt; v1/Pod(related)\nNAME                                   READY  STATUS             RESTARTS  AGE\nconcourse-postgresql-6d6688747b-426th  0/1    Pending            0         1s\nconcourse-web-56798b6694-mz2rl         0/1    ContainerCreating  0         1s\nconcourse-worker-0                     0/1    Init:0/1           0         1s\nconcourse-worker-1                     0/1    Init:0/1           0         1s\n==&gt; v1/Secret\nNAME                  TYPE    DATA  AGE\nconcourse-concourse   Opaque  8     1s\nconcourse-postgresql  Opaque  1     1s\n==&gt; v1/Service\nNAME                  TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                                    AGE\nconcourse-postgresql  ClusterIP     10.100.200.32  &lt;none&gt;       5432/TCP                                   1s\nconcourse-web         LoadBalancer  10.100.200.87  &lt;pending&gt;    80:32264/TCP,443:31098/TCP,2222:31294/TCP  1s\nconcourse-worker      ClusterIP     None           &lt;none&gt;       &lt;none&gt;                                     1s\n==&gt; v1/ServiceAccount\nNAME              SECRETS  AGE\nconcourse-web     1        1s\nconcourse-worker  1        1s\n==&gt; v1beta1/ClusterRole\nNAME           AGE\nconcourse-web  1s\n==&gt; v1beta1/Deployment\nNAME                  READY  UP-TO-DATE  AVAILABLE  AGE\nconcourse-postgresql  0/1    1           0          1s\nconcourse-web         0/1    1           0          1s\n==&gt; v1beta1/PodDisruptionBudget\nNAME              MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE\nconcourse-worker  1              N/A              0                    1s\n==&gt; v1beta1/Role\nNAME              AGE\nconcourse-worker  1s\n==&gt; v1beta1/RoleBinding\nNAME                AGE\nconcourse-web-main  1s\nconcourse-worker    1s\n==&gt; v1beta1/StatefulSet\nNAME              READY  AGE\nconcourse-worker  0/2    1s\nNOTES:\n* Concourse can be accessed:\n* Within your cluster, at the following DNS name at port 80:\nconcourse-web.default.svc.cluster.local\n* From outside the cluster, run these commands in the same shell:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\n           You can watch the status of by running 'kubectl get svc -w concourse-web'\nexport SERVICE_IP=$(kubectl get svc --namespace default concourse-web -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n    echo http://$SERVICE_IP:80\n* If this is your first time using Concourse, follow the tutorials at https://concourse-ci.org/tutorials.html\n</code></pre> <p>You can run <code>helm status concourse</code> to keep an eye on it. Eventually, the <code>v1/Service</code> block will look like this:</p> <pre><code>==&gt; v1/Service\nNAME                  TYPE          CLUSTER-IP     EXTERNAL-IP       PORT(S)                                    AGE\nconcourse-postgresql  ClusterIP     10.100.200.32  &lt;none&gt;            5432/TCP                                   27s\nconcourse-web         LoadBalancer  10.100.200.87  10.197.59.143...  80:32264/TCP,443:31098/TCP,2222:31294/TCP  27s\nconcourse-worker      ClusterIP     None           &lt;none&gt;            &lt;none&gt;                                     27s\n</code></pre> <p>You can then take that <code>EXTERNAL-IP</code> and create a DNS A record from your hostname defined in <code>concourse.web.externalUrl</code>.</p> <p>If you ever want to change any of the deployment values, including what auth mechanism you use for the main team, update your <code>concourse-config.yml</code> and run <code>helm upgrade -f concourse-config.yml concourse stable/concourse</code> and it will make your changes, except for secrets. See below on how to do that.</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#changing-secrets","title":"Changing Secrets","text":"<p>If you need to change your secrets \u2014 your certificate is expiring! you need to change your admin password \u2014 you can use standard <code>kubectl</code> commands to do this.</p> <p>First, you need to edit the secret itself. To do this, run <code>kubectl edit secret concourse-concourse</code>. Note that in that name, the first <code>concourse</code> is the name of the release you specified in <code>helm install</code>. The second <code>concourse</code> is static. This will open in your editor of choice. Under the <code>data:</code> block, edit your values. You need to base-64 encode the value before putting it into the file, and do so only on one line. Once you save the file and close the editor, the secrets will be updated.</p> <p>This is not enough, unfortunately, to trigger Concourse to pick up the new secrets. To do so, you need to edit the deployment. To do so, run <code>kubectl edit deployment concourse-web --record</code>, where <code>concourse</code> is your release name. It will also open in your editor. You need to change something under the <code>spec</code> block to trigger an update. I personally suggest adding a label under <code>spec.template.metadata.labels</code>. Mine looks like this:</p> <pre><code>spec:\n  progressDeadlineSeconds: 2147483647\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: concourse-web\n      release: concourse\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: concourse-web\n        last-updated: 2019-03-20T12.46.00 # My addition\n        release: concourse\n</code></pre> <p>When you save the file, because <code>spec.strategy.type</code> is set to <code>RollingUpdate</code>, it will trigger an update to the <code>concourse-web</code> deployment. You can watch the status of the update by running <code>kubectl rollout status deployment concourse-web</code>. Once it\u2019s complete, you can verify that the change was made by running <code>kubectl get pods</code>. You will see a pod named <code>concourse-web-xxxx-yyyy</code>. It should have a lower age than your other <code>concourse-*</code> pods.</p>"},{"location":"blog/2019-03-29-installing-concourse-50-on-kubernetes-using-helm/#conclusion","title":"Conclusion","text":"<p>It may look like a lot of work went into this, but I began this process as an absolute Kubernetes neophyte, but with the help of \u00c9amon, I went from an empty PKS cluster to a working Concourse in the span of about 2 hours, and at least half of that was spent with trial and error, fixing values and redeploying.</p> <p>Happy Concourse-ing!</p>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/","title":"Concourse Update (\ud83e\udd37-April 1, 2019)","text":"<p>Some airport somewhere...waiting</p> <p>Phew, it\u2019s been a while since I last wrote an update. For some background behind why I slowed down, hop on over to this thread on our forms: \u201cWhat would you like to see on our blog\u201d.</p> <p>That said, I do have a lot of interesting updates to share, so let\u2019s get started</p>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#concourse-500","title":"Concourse 5.0.0","text":"<p>In case you missed it, Concourse 5.0.0 and 5.0.1 came out a few weeks ago in March. This is a major version release with tons of new features, including:</p> <ul> <li>Role Based Access Control</li> <li>Global Resource Cache</li> <li>fewest-build-containers placement strategy</li> <li>Resource pinning</li> <li>Inputs on the put step of a pipeline</li> <li>UI tweaks</li> <li>and much much more!</li> </ul> <p>Be warned, there are some breaking changes in this release as well; so make sure you read all ofthe release notes before you upgrade!</p> <p>You\u2019ll also notice that we recently gave the Concourse homepage a small makeover as well. We\u2019ve tightened up the navigation and expanded some sections of our docs, check it out:</p> <ul> <li>Expanded docs onCredential Management with Vault and AWS SSM</li> <li>More info on the new Container Placement strategies</li> <li>A primer on the new Global Resources feature</li> <li>Our spiffy new Examples section, which gives you a side-by-side   comparison of a pipeline and the yml that made it</li> </ul>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#interesting-blog-posts","title":"Interesting Blog Posts","text":"<p>There\u2019s also been some interesting blog posts about Concourse from around the interwebs\u2026and not all of them were written by me!</p> <ul> <li>An Overview of Authorization in Concourse 3, 4 and 5   is a useful overview of auth across 3 major versions of Concourse</li> <li>Installing Concourse 5.0 on Kubernetes using Helm   is a great two-part overview of getting PKS installed and using the Concourse helm chart</li> <li>Building Go code, with and without Go modules, with Concourse</li> <li>Aptomi described how to   do CI/CD for Knative serverless apps on Kubernetes with Concourse</li> <li>Concourse-Up is now renamed   to \u201cControl Tower\u201d</li> <li>Someone compared us to Drone.io   in CI/CD tool showdown pits adoptability vs. adaptability</li> <li>We got a mention   on PorscheDev\u2019s Technology Radar vol 2 (I think   they like us :D)</li> </ul>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#concourse-swag","title":"Concourse Swag","text":"<p>We have swag! With the help of the team at Pivotal we\u2019ve listed our first Concourse-branded sweater under the official Pivotal apparel store. A few notes:</p> <ul> <li>The sweaters themselves are listed at-cost, so we\u2019re not making any profit off of them</li> <li>Apologies to anyone who\u2019s not in the United States because international shipping through this store is atrocious.   We\u2019re going to be working with our partners to see if we can find a better shipping solution.</li> <li>At the time of this writing we\u2019re relatively low on M and L sweaters, there\u2019s a new shipment of those sizes coming in   soon so the store should be updated in a week or so</li> <li>Once this batch of sweaters sell out we\u2019ll be planning on doing new designs to keep things fresh!</li> </ul>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#concourse-irl","title":"Concourse IRL","text":"<p>The Concourse team will be attending CF Summit NA 2019 this week in Philadelphia, so come by the Pivotal booth and say hi to the team!</p> <p>I\u2019ll also be attending the ConcourseCI Bay Area User Group meetup on April 11th in Palo Alto. The title of the meetup is \u201cKubernetes Deployments with Concourse CI and Spinnaker\u201d. Come check it out if you\u2019re in the Bay Area!</p>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#milestones-and-interesting-rfcs","title":"Milestones and Interesting RFCs","text":"<p>Alex Suraci has been experimenting with re-organizing our backlog of epics by using the GitHub Projects feature. You can see our current list of epics in the <code>concourse/concourse</code> project list. The big things we\u2019re working on are:</p> <ul> <li>Spatial Resource</li> <li>API refactoring</li> <li>Ephemeral check containers (Runtime)</li> <li>and Concourse + K8s runtime</li> </ul> <p>On the topic of k8s runtime situation, please take a second to review Topher Bullock\u2019s new RFC #22 How Do We Best Leverage K8s as Runtime?. The team is evaluating Concourse + Tekton CD vs Concourse + K8s our own way.</p>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#thanks-to-our-community","title":"Thanks to our Community \ud83d\ude4f","text":"<p>Finally, I wanted to give shout-outs to our growing community of Concourse fans and followers. In early 2019 the Concourse team made two changes to our contributor workflow: we switched over to a looser Contributors License Agreement (CLA) and the core team moved towards a PR-based workflow. Since then, we\u2019ve seen a lot more engagement on the work that we\u2019ve doing, and we\u2019ve also started to see a lot of new PRs coming in!</p> <p></p> <p># of PRs opened over time against concourse/concourse and other key resources</p> <p>In 2018, we saw 263 PRs opened against <code>concourse/concourse</code> and its core resources. As of today we already have more than 160 PRs opened by non-Pivots! Some notable PRs that I wanted to</p> <ul> <li>#3580 Add parallel Step</li> <li>#3163 [POC] Super nasty rendering of jobs that needs manual triggering</li> <li>#3560 Time based retention for build log collector</li> <li>#3430 Default the target if there is exactly one</li> <li>#3577 Auditor</li> <li>#3398 Make values starts with https or http clickable in build</li> <li>#3579 Display Task Duration on Finished Tasks</li> <li>#3475 web: add pause button to top bar of pipeline view</li> <li>#3248 Add option to prune all stalled workers instead of just one at a time</li> </ul>"},{"location":"blog/2019-04-01-concourse-update--april-1-2019/#the-future-of-weekly-updates","title":"The Future of Weekly Updates","text":"<p>I\u2019ll do my best to resume the weekly cadence of the project updates. In the meantime, if you have any specific opinions on what kind of blog posts we should right, I\u2019d suggest you check out this thread on our forums: \u201cWhat would you like to see on our blog\u201d</p>"},{"location":"blog/2019-04-05-concourse-update-april-15/","title":"Concourse Update (April 1\u20135)","text":"<p>Concourse in action at the CF Summit 2019 Grape Up booth</p> <p>Greetings from sunny Philadelphia! The team was there for the Cloud Foundry 2019 NA Summit for a few days; talking to Concourse users and attending talks. Recorded videos of the talks should be uploaded soon; so I\u2019ll point you to the interesting Concourse-related ones next week.</p> <p>On to the update.</p>"},{"location":"blog/2019-04-05-concourse-update-april-15/#for-active-discussion","title":"For Active Discussion","text":"<ul> <li>Please take some time to review and comment on the   latest Concourse + k8s Runtime RFC</li> <li>Regarding the runtime, there\u2019s been an active conversation around better build scheduling and load distribution. You   can catch up on the thread here. We\u2019d love for you to tell us   about your own experience in our meta-issue #3695</li> </ul>"},{"location":"blog/2019-04-05-concourse-update-april-15/#coming-soon-concourse-510","title":"Coming Soon: Concourse 5.1.0","text":"<p>Icons on resources</p> <p>We\u2019re in the process of polishing up some items that we weren\u2019t quite able to zfit into the 5.0.0 release. There\u2019s been also some interesting new features and new PRs that you can look forward to in 5.1.0 as well:</p> <ul> <li>Resource icons #3581 thanks to efforts of   contributor mockersf</li> <li>You can now pause the pipeline from the pipeline view #3475 thanks   to the efforts of contributor robwhitby</li> <li>There\u2019s been a great de-coupling of the API from the runtime &amp;   scheduler #3307. This is a refactor and cleanup that brings us   closer to an API that we\u2019d be happy to publish and support.</li> <li>Introduced an <code>on_error</code> option to allow outside sources to be notified of CI   failure #3652 thanks to   contributor amanw</li> </ul>"},{"location":"blog/2019-04-18-concourse-update-april-818/","title":"Concourse Update (April 8\u201318)","text":"<p>Roman Alekseenkov from Aptomi giving a talk on Concourse at the Bay Area User Group</p> <p>Sorry for missing the update last week. I was travelling out to the Bay Area to attend the ConcourseCI Bay Area User Group. For those who missed it, you can find a recording of the event here. On to the update.</p> <p></p> <p>In case you missed it, Concourse 5.1.0 is out! It\u2019s got icons on resources, better garbage collection, <code>on_error</code> on pipelines, and much more! As usual, you can read the full list of new features here.</p> <p>Other interesting developments:</p> <ul> <li>The runtime team has been looking into the administrative overhead of running tasks on workers. The results are pretty   sobering. More to come next week!</li> <li>We\u2019re still looking into the k8s Tekton integration. We expect things to pick up in pace starting next week, where   we\u2019ll have a few more Pivots lending a helping hand. Again, you can find our   RFC here</li> <li>The sidebar is coming back, and we\u2019re exploring how we can   extend the search and filtration capabilities across Concourse</li> <li>I added a section called \u201cConcourse Users\u201d on   our Community page. This is just some of the companies and   folks that have spoken about their Concourse usage in the past. If you\u2019d like to add to that list feel free to make   a PR here</li> </ul>"},{"location":"blog/2019-05-03-concourse-update-april-293/","title":"Concourse Update April 29\u20133","text":"<p>Sitewide search has been drastically improved, thanks to Alex Suraci</p> <p>In case you missed it, we\u2019ve made some tweaks to the structure of the website. I\u2019m happy to report that Alex Suraci drastically improved our site-wide search. This resolves #181, and we\u2019re all the better for it!</p> <p>Second, you\u2019ll notice that a lot of the community related comment that was on our homepage has now been moved to our Concourse GitHub Wiki. We hope this change will make contributor and community specific content more discoverable and more maintainable over time.</p> <p></p> <p>Concourse Wiki with more Contributor things!</p> <p>Notably, the Resource Types, Tutorials, and Tools page has moved over to the wiki. Content that may be new to some of you include the Project Management section: How Issues are Managed, How to Process PRs, and Release Process.</p> <p>On to the update.</p>"},{"location":"blog/2019-05-03-concourse-update-april-293/#k8s-runtime","title":"K8s Runtime","text":"<p>Bohen and Sameer have been doing some great write-ups on their research. You can get caught up with their latest research in two GitHub issues: What does k8s offer as a runtime and What does Tekton offer as a runtime. If you\u2019d like to track along with this project\u2019s movements you can bookmark the K8s Runtime project board here: https://github.com/concourse/concourse/projects/14</p>"},{"location":"blog/2019-05-03-concourse-update-april-293/#ux","title":"UX","text":"<p>The Sidebar is coming back! Check out our latest designs in #2440.</p> <p>We\u2019ve been looking into a few UI regressions in the web frontend as well. Specifically, #3745 and #3748 have been moved to the top of the backlog</p>"},{"location":"blog/2019-05-03-concourse-update-april-293/#runtime","title":"Runtime","text":"<p>We\u2019ve been working on #3607 and #3810 as sub-stories to help with Ephemeral Check Containers #3424</p> <p>Divya Dadlani has also been thinking a lot more about \u201cPerformance benchmarking for Concourse releases\u201d #3816. The idea is that we should be a bit more rigorous in tracking how Concourse improves with some of the runtime performance changes. Jump on over to the issue and drop a line if you have any ideas/opinions on this subject.</p>"},{"location":"blog/2019-06-07-concourse-update-june-7/","title":"Concourse Update June 7","text":"<p>YYZ on a cloudy day</p> <p>...and we\u2019re back! Apologies for the lack of updates lately. I\u2019ve just come back from some time off and work travel has taken up a lot of my time. I\u2019m back in Toronto now so let\u2019s get back into it.</p>"},{"location":"blog/2019-06-07-concourse-update-june-7/#release-engineering-concourse-530","title":"Release Engineering &amp; Concourse 5.3.0","text":"<p>In the past, we relied a lot on Alex Suraci to handle a lot of our release engineering work. Release Engineering is incredibly important and valuable work for the Concourse team, but it can also very time consuming. Thankfully, the UX track has volunteered some of their time to spin up our new Release Engineering track of work to help alleviate Alex from some of his responsibilities. This means a short-term slowdown in the throughput of the UX team, but we think its well worth the tradeoff.</p> <p>On that note, you can now follow along with our release plans for Concourse 5.3.0 by tracking our project note. Unfortunately, we were mostly blocked on some metrics instabilities in our production instance this week. Those issues have been mostly cleared up and we hope to be able to continue with our production and wings tests next</p>"},{"location":"blog/2019-06-07-concourse-update-june-7/#coreapi","title":"Core/API","text":"<p>The team\u2019s been making a lot of progress on two key issues:</p> <ul> <li>The Algorithm\u2122</li> <li>Resource check queue</li> </ul> <p>The team has been doing some preliminary performance tests with the new Algorithm and the results so far have been very promising. We\u2019ll be reporting more details on the performance improvements in the coming weeks; so keep an eye out for that!</p>"},{"location":"blog/2019-06-07-concourse-update-june-7/#runtime","title":"Runtime","text":"<p>Ephemeral check containers is back! We\u2019ve deployed our changes as-is on our test environment and are monitoring it for lower container counts in our environments</p> <p>Parallel Input Streaming was picked up by Krishna today, its amazing and there\u2019s totally lots of detail to be found on the linked issue.</p>"},{"location":"blog/2019-06-07-concourse-update-june-7/#k8s-concourse-koncourse","title":"K8s + Concourse == Koncourse?","text":"<p>We have two RFCs in flight, please take some time to read through the changes:</p> <ul> <li>Exploring Initial Run/Store interface: In order to port   Concourse on to non Garden/Baggageclaim runtimes (including Kubernetes!) we need to separate the two concepts of   containers as the unit of execution and volumes as the unit of storage. We\u2019re fleshing out the interface that these   components can implement in this RFC.</li> <li>Extract Core Resource Types: a proposal to not ship bundled base resource   types with concourse. The change would require Concourse to pull the base resource types on-demand at runtime. This is   required for moving establishing a more generic storage interface.</li> </ul> <p>You\u2019ll also note that we\u2019ve created a <code>architecture-rfcs</code> repo. This repository is reserved for internal RFCs that should not directly impact a Concourse user.</p>"},{"location":"blog/2019-06-07-concourse-update-june-7/#duty-free","title":"Duty Free","text":"<p>The proposal for a Concourse \u201cDuty Free\u201d was first reported in issue #191; its the idea of creating a separate site to highlight community resources and other re-usable Concourse artifacts for our community. Today, advertise Concourse resources through the Resources page in our wiki, but a dedicated Concourse Duty Free site would have a lot more pizzaz.</p> <p>We\u2019ve always wanted to build Duty Free, but we were never able to figure out how to slot it into our work schedule. Thankfully, the Pivotal team out in Dublin had some time and offered to help kick-start the project for us. We\u2019re still in the very early stages of development and design, but you can follow along the project on their GitHub repo here: <code>concourse/dutyfree</code></p>"},{"location":"blog/2019-06-21-concourse-update-july-21-2019/","title":"Concourse Update (July 21 2019)","text":"<p>I got a need, a need for speed (and parallelized input streams)</p> <p>The Concourse team had the opportunity to visit some Concourse users out in Montreal last week. We had a blast meeting everyone, including some folks from the Concourse OSS community. Thanks again for hosting us!</p> <p>I\u2019ll also be in Kansas City for two days next week to meet some other Concourse users as well, so give me a tap on Twitter or Discord (username jama) if you wanna meet up.</p>"},{"location":"blog/2019-06-21-concourse-update-july-21-2019/#parallel-input-streaming","title":"Parallel Input Streaming","text":"<p>initialization dropped from 1 hour 22 min to just over 4 min</p> <p>In addition to the work on Algorithm improvements from the Core track, the Runtime track tested out their new work on Parallel Input Streaming. By parallelizing the input streams we saw a massive improvement on the initialization of tasks in our test pipelines. In our test we saw Dwayne Forde\u2019s Strabo pipeline (which has over 100 input resources on a job) go from a 1 hour, 22 min initialization to just over 4 min. We were able to observe these results on both the BOSH and k8s deployment of Concourse. Exciting work!</p>"},{"location":"blog/2019-06-21-concourse-update-july-21-2019/#runtime-interface-track","title":"Runtime Interface Track","text":"<p>For those who are interested, you can follow along our swappable runtimes (including k8s) work in the Runtime Interface track. We\u2019ve been doing a lot of planning and research, but it's all come down to \u201clets just give it a shot\u201d. We\u2019ll probably have more to say on this next update.</p>"},{"location":"blog/2019-06-21-concourse-update-july-21-2019/#release-engineering","title":"Release Engineering","text":"<p>One of the big changes that have come out of our Release Engineering track is extracting our <code>ci</code> automation into its own repository. This was done to make our project more resilient and reusable. You can now track those changes under <code>concourse/ci</code></p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/","title":"Core roadmap: towards v10","text":"<p>A long-term roadmap for the core design of Concourse, a general-purpose CI/CD tool.</p> <p>Accompanying slides. Recommended viewing: episode 1 of Yu-Gi-Oh.</p> <p>Concourse's design philosophy is to be expressive, versatile, and safe while limited to a handful of simple, proven concepts. The design of these concepts should make good practices feel intuitive and bad practices feel uncomfortable.</p> <p>Coming up with these designs can be very challenging. There are many different workflows and patterns across the software industry, and they each have to be deeply understood in order to know what the good and bad practices are.</p> <p>This post provides a bit of insight into what we've been up to with Concourse's core design - chiefly regarding 'spaces', which has become a bit of a white whale on our roadmap.</p> <p>There are a lot of words here - sorry! If you just want to skim, I've added a single-paragraph summary under each roadmap entry.</p> <p>Each roadmap entry corresponds to an RFC or an issue, linked in their header. If you want to get involved in our design process or just provide feedback, please check them out and submit a PR review! (Thanks!)</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#table-of-contents","title":"Table of contents","text":"<ol> <li>Where is 'spaces'?</li> <li>Where are we now?<ul> <li>Issue #3602: a new algorithm</li> <li>Issue #413: build re-triggering</li> </ul> </li> <li>Where are we going?<ul> <li>RFC #24: resources v2</li> <li>RFC #26: artifact resources</li> <li>RFC #31: <code>set_pipeline</code> step</li> <li>RFC #32: projects</li> <li>RFC #33: archiving pipelines</li> <li>RFC #34: instanced pipelines</li> <li>RFC #29: spatial resources</li> <li>RFC #27: trigger resources</li> <li>RFC #28: notification resources</li> </ul> </li> <li>What comes after all this?</li> <li>Thanks!</li> </ol>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#where-is-spaces","title":"Where is 'spaces'?","text":"<p>For those of you not familiar with spaces, it was a big ol' feature that enabled the following workflows:</p> <ul> <li>Dynamically running against things like branches/pull requests, which change over time (i.e. commits to a branch)   and space (i.e. the set of branches themselves). Hence the name 'spaces.'</li> <li>Fanning in using <code>passed</code> constraints across spaces. This is currently impossible to do with separate pipelines,   because pipelines can't reference each other's resources.</li> <li>Automatically cleaning up spaces for closed PRs, etc. This is annoying to automate and requires keeping track of   state.</li> </ul> <p>These workflows still make sense, so why is 'spaces' dead?</p> <p>Well, I approached it the wrong way. To me, the idea of resources tracking change over time and space felt pretty solid from a theoretical standpoint. In hindsight, maybe it just sounded cool.</p> <p>I had no reservations baking 'spaces' in to every layer of the stack - it would add more depth to all the existing ideas. Everything was going to change: the resource interface, the web UI, how jobs work... It was all so exciting!</p> <p>But as time went on it became terrifying. It was a double-or-nothing bet. Either 'spaces' made sense everywhere, or ' spaces' didn't make sense at all. I tried to carve out work that could be done before fully committing to spaces, but it didn't make the monolithic feature any less monolithic.</p> <p></p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#where-are-we-now","title":"Where are we now?","text":"<p>First off, I want to give a quick update on a couple of big things that you can expect in v6.0:</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#issue-3602-a-new-algorithm","title":"Issue #3602: a new algorithm","text":"<p>We are re-designing the algorithm used for determining the candidate input versions for a job. The new approach will rely less on brute force and will perform better with large installations.</p> <p>This new algorithm fixes long-standing architectural issues with the old one, which loaded each pipeline's entire build and resource version history into memory and determined valid candidates using brute force.</p> <p>The key difference between the old and new algorithm is how <code>passed</code> constraints are implemented, specifically when multiple inputs depend on the same job:</p> <pre><code>plan:\n  - get: foo\n    passed: [ foo-unit, integration ]\n  - get: bar\n    passed: [ bar-unit, integration ]\n  - get: baz\n    passed: [ integration ]\n</code></pre> <p>In Concourse, this means \"give me versions of <code>foo</code>, <code>bar</code>, and <code>baz</code> that have passed through <code>integration</code> together in the same build, with the same version of <code>foo</code> having passed <code>foo-unit</code> and the same version of <code>bar</code> having passed <code>bar-unit</code>.\"</p> <p>How does this work? Well, it's hard to describe either algorithm succinctly, but I'll try:</p> <ul> <li>The old algorithm goes through resource versions, newest first, and checks whether each version satisfies the input's   own <code>passed</code> constraints. Next it checks that any other already-chosen input versions which mention the same job in   their <code>passed</code> constraints also came from the same build, recursing and walking through versions until everything is   satisfied. This process is brute-force, an uses a lot of CPU.</li> <li>The new algorithm instead loops over build output version sets via the jobs listed in each <code>passed</code> constraint,   assigning all the relevant versions for a given build at once as long as the versions match the other already-chosen   versions assigned via builds of prior jobs in the <code>passed</code> constraint.</li> </ul> <p>This new approach really simplifies things because the versions are inherently coming from the same build. Now that we don't have to do the extra cross-referencing, the new flow can just make a handful of cheap database queries instead of having to load the whole pipeline's dataset into memory.</p> <p>We've been testing the old and new algorithm in two separate environments, each at the scale of 1,000 jobs with varying <code>passed</code> constraints and a sprinkle of <code>version: every</code> across four <code>web</code> nodes.</p> <ul> <li>The old algorithm starts off very fast but grows slower and slower as the pipeline dataset grows, eventually   exhausting the <code>web</code> nodes of RAM and swap.</li> <li>The new algorithm starts off slightly slower than the old one - it's hard to beat an in-memory dataset - but it stays   stable, uses less CPU, and does not leak memory.</li> </ul> <p>We're making a few final touches as we to get as much performance out of the new algorithm as possible, since we don't tend to touch it often. Once we're finished, we'll jump straight to...:</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#issue-413-build-re-triggering","title":"Issue #413: build re-triggering","text":"<p>The new algorithm changes the behavior for today's pinning-based flow for re-triggering a build, so we're going to implement proper support for build re-triggering and ship these two features together in v6.0.</p> <p>Right now the only way to \"re-trigger\" a build is to pin each of the job's upstream resources to the version from the build, trigger a new build, and go back and un-pin them all. It's pretty cumbersome and error-prone.</p> <p>It also kind of breaks with the new algorithm. Now that the new algorithm is based on build output sets and not version history, once the new build succeeds its older versions will end up being the first set attempted for that job, potentially propagating them to downstream jobs.</p> <p>That's not what I would expect from a re-trigger. I would expect a re-trigger to act \"in-place,\" while preserving the logs of the original failure for posterity.</p> <p>To avoid this surprising change in behaviour, we're going to implement build re-triggering properly and stop abusing the version pinning feature, which was originally designed for temporarily pinning a broken upstream dependency to a \"known good\" one.</p> <p>Build re-triggering will be implemented in a way that preserves the order of the builds that the algorithm will go over. If the re-triggered build succeeds, its set of outputs will be available to downstream jobs based on the original build's order.</p> <p>Another benefit to implementing re-triggering soon is that folks using a pull request resource will have a much easier time re-triggering failed pull requests, without having to wait on the rest of the roadmap (i.e. 'spaces').</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#where-are-we-going","title":"Where are we going?","text":"<p>So, going back to the 'spaces' initiative. The pieces really started to fall into place over the past few months, and I think I've arrived at a roadmap that accomplishes all of the goals of 'spaces' but in a significantly more Concourse-y way.</p> <p>Instead of one monolithic feature, I have a bunch of smaller features to propose that are independently valuable and can be delivered in any order. As we complete them, a bigger picture will start to take shape.</p> <p>Let's jump right in!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-24-resources-v2","title":"RFC #24: resources v2","text":"<p>Resources v2 is the first major revision of the resource interface since Concourse's inception. It's a step to take very carefully. I think we're finally ready to go.</p> <p>**UPDATE: Just kidding! This proposal has been superceded by something even more general: Prototypes! (RFC #37) **</p> <p>The v2 interface brings long-awaited polish to the interface: it renames <code>in</code> and <code>out</code> to <code>get</code> and <code>put</code> to match their step names, introduces a <code>delete</code> action, standardises TLS configuration, and revises terminology so as to not be coupled to the 'versioned artifacts' use case.</p> <p>The latest proposal for Resources v2, RFC #24, is a lot like RFC #1 but with one big difference: 'spaces' is no longer a foundational piece of the interface. Instead, RFC #24 proposes that we generalize and simplify the interface to an extent that it can be used for various pipeline workflows, not just versioning artifacts.</p> <p>The new direction is to leverage composition between resources and pipelines via config fragments, which can be passed from one resource to another or used for <code>((vars))</code> in a pipeline template. 'Config fragments' replace 'versions' in the interface, and are used as versions for the 'versioned artifacts' flow (today's primary use of resources).</p> <p>By generalizing the resource concept we set the stage for proper pipeline-level support for notifications (e.g. Slack alerts, GitHub commit status), trigger-only resources (e.g. <code>time</code>), and spatial resources (e.g. branches, pull requests) without tying each use case into the interface itself.</p> <p>Now that 'spaces' is gone from the interface, the actual change in the interface protocol is somewhat cosmetic. As a result, Concourse pipelines will be able to use v1 and v2 resources side-by-side for all the same functionality. This way we can move forward with pipeline-level resource features without fragmenting the resource ecosystem!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-26-artifact-resources","title":"RFC #26: artifact resources","text":"<p>Artifact resources are an interpretation of the generic resource interface that maps to today's usage of the resource interface.</p> <p>UPDATE: this is now RFC #38, \" Resource Prototypes\"</p> <p>Artifact resources use config fragments as versions, modeling change to an external entity over time. This should sound familiar to anyone using Concourse today: they're the sole use case that Concourse resources were originally designed around.</p> <p>The 'artifact resources' proposal clarifies that this is now just one use case for the general resource interface, and outlines a few long-awaited features:</p> <ul> <li>Versions can be deleted using the <code>delete</code> action in the resource interface.</li> <li>The <code>put</code> action can emit multiple versions. Each will be recorded as an output of the build.</li> <li>The automatic <code>get</code> after the <code>put</code> step will be made opt-in. (Huzzah!)</li> </ul> <p>The automatic <code>get</code> after each <code>put</code> is something that has confused and occasionally frustrated users, but we didn't want to break backwards compatibility and we didn't want users to have to 'opt out' (that's too many knobs to turn).</p> <p>This RFC will provide a backwards-compatible transition path to artifact resources. Check out RFC #26 for more details!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-31-set_pipeline-step","title":"RFC #31: <code>set_pipeline</code> step","text":"<p>The first step on our journey towards 'spaces' is to introduce a simple, but critical piece of the puzzle: a <code>set_pipeline</code> step.</p> <p>The <code>set_pipeline</code> step is used like so:</p> <pre><code>jobs:\n  - name: bootstrap\n    plan:\n      - get: ci\n        trigger: true\n      - set_pipeline: concourse\n        file: ci/pipelines/concourse.yml\n</code></pre> <p>This job will configure a <code>concourse</code> pipeline within the job's team. The pipeline will be automatically unpaused, and no authentication is required.</p> <p>The first thing this lets us do is deprecate the <code>concourse-pipeline</code> resource, which has two pretty fundamental problems:</p> <ul> <li>Having to configure auth is really awkward - you have to set up a local user and give the resource the keys to the   kingdom.</li> <li>Keeping the version of <code>fly</code> within the resource in sync with your own Concourse's version is a bit clunky.</li> </ul> <p>With the <code>set_pipeline</code> step, both of these problems immediately go away and pipelines start to feel a more first-class rather than just being the tip of the abstraction iceberg.</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-32-projects","title":"RFC #32: projects","text":"<p>Ok, I promised to provide a tl;dr for each roadmap entry, but projects can't really be summed up that easily. This is the most impactful feature on this roadmap.</p> <ul> <li>A \"project\" is a new concept bootstrapped by two existing ones: a resource   from which to continuously load the project's config, which specifies   a build plan to execute whenever the project resource changes.</li> <li>Projects act as a namespace for pipelines, and provide a long-requested workflow for automating their configuration.   As the roadmap goes on, this workflow becomes more and more powerful.</li> <li>Projects allow you to define project-wide resources which let you clean up duplicate definitions across your   pipelines and support cross-pipeline <code>passed</code> constraints.</li> <li>Projects also define project-wide tasks, which remove the need to thread a resource through all your jobs just to   have the task configs to execute, and finally gives meaning to task names (the <code>x</code> in <code>task: x</code>).</li> </ul> <p>A project's build plan can be used for anything you want. Small projects could use the build plan to run tests and/or perform various steps in a single build - a workflow more familiar to users of other CI systems:</p> <pre><code>name: ci\nplan:\n  - get: booklit\n    trigger: true\n  - task: unit\n</code></pre> <p>Larger projects could use the build plan to execute <code>set_pipeline</code> steps. Concourse has long encouraged users to keep their pipelines under source control, but it never enforced it: <code>fly set-pipeline</code> was still a manual operation, and users would often forget to check in their changes. Projects will fix that:</p> <pre><code>name: ci\nplan:\n  - set_pipeline: booklit\n</code></pre> <p>Small projects may start without pipelines and start using pipelines as they grow. Our original slogan, 'CI that scales with your project,' is now pretty literal! The hope is that by introducing build plans without requiring knowledge of pipelines and jobs, we'll have made Concourse's learning curve more gradual and made Concourse feel less overkill for side-projects.</p> <p>This feature will have far-reaching implications for Concourse, so it won't be sneaking in quietly. I've opened RFC #32 and would really appreciate feedback!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-33-archiving-pipelines","title":"RFC #33: archiving pipelines","text":"<p>Archiving pipelines is a way to soft-delete a pipeline while still being able to peruse the build history for a pipeline you no longer want.</p> <p>Well, after that bombshell this one's pretty easy to explain. Let's take a look at our own Concourse team's pipelines:</p> <p></p> <p>Look at all that cruft! So many old, paused or bit-rotting pipelines which I really don't care about anymore but don't really have the heart to delete. That <code>old-concourse</code> pipeline served us well for years - it has sentimental value. In some cases you may also want to keep the history around for auditing purposes.</p> <p>Archiving pipelines will allow you to humanely retire a pipeline in a way that gets it out of your way while still allowing you to peruse the build history should you ever need to. Archived pipelines are no longer active and will allow you to re-use their name without bringing the old pipeline back.</p> <p>There's already an open pull request for this: #2518 - shout-out to @tkellen! The ball has been in our court for a while to figure out the UI/UX, so we're just going to submit a new RFC and work out all the details.</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-34-instanced-pipelines","title":"RFC #34: instanced pipelines","text":"<p>Instanced pipelines group together pipelines which share a common template configured with different <code>((vars))</code>. They provide a simple two-level hierarchy and automatic archiving of instances which are no longer needed.</p> <p>Instanced pipelines are an important piece of the 'spaces' puzzle: it's how users will navigate through their spatial pipelines, and it's what keeps no-longer-relevant spaces for e.g. merged PRs and deleted branches from piling up forever.</p> <p>Pipeline instances are created using the <code>set_pipeline</code> step like so:</p> <pre><code>plan:\n  - set_pipeline: branch\n    instance_vars:\n      branch: feature/projects\n  - set_pipeline: branch\n    instance_vars:\n      branch: feature/new-algorithm\n</code></pre> <p>At the end of a build which uses <code>set_pipeline</code>, all instances of the named pipelines which were not configured by the build will be automatically archived.</p> <p>Check out RFC #34 for more details!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-29-spatial-resources","title":"RFC #29: spatial resources","text":"<p>Spatial resources are resources whose <code>check</code> monitors spatial change, not change over time. Two common examples are the set of branches or open pull requests for a repo. The <code>across</code> step allows a build to process each 'space' and trigger on changes to the set.</p> <p>UPDATE: the syntax for this step has since been tweaked so that multi-var matrices don't require nesting <code>across</code> steps.</p> <p>When used with the new <code>across</code> step, the \u00a0<code>set_pipeline</code> step, and instanced pipelines, this enables dynamic pipeline configuration across spatial change.</p> <p>The final piece of the puzzle for 'spaces' is the addition of an <code>across</code> step. This step points to a resource and has a plan which will execute for every config fragment returned by the resource's <code>check</code>, all within one build.</p> <p>Let's first look at a simple use case, which is to execute a task across many variants:</p> <pre><code>plan:\n  # ...\n  - across: supported-go-versions\n    as: go\n    do:\n      - task: unit\n        image: go\n</code></pre> <p>In this case, imagine we have a <code>supported-go-versions</code> resource whose <code>check</code> returns a config fragment for each tag and digest based on a pre-configured list of supported tags (e.g. <code>1.10</code>, <code>1.11</code>, <code>1.12</code>), and whose <code>in</code>/<code>get</code> fetches the image.</p> <p>When nested, the <code>across</code> step enables dynamic build matrices:</p> <pre><code>plan:\n  # ...\n  - across: supported-go-versions\n    as: go\n    do: # needed so we can define another 'across'\n      - across: other-things\n        as: some-input\n        task: unit\n        image: go\n</code></pre> <p>When used with <code>set_pipeline</code> and instanced pipelines, it enables dynamic pipeline matrices:</p> <pre><code>plan:\n  - across: repo-branches\n    as: repo-branch\n    set_pipeline: branch\n    instance_vars:\n      branch_name: ((repo-branch.name))\n</code></pre> <p>(Assuming we provide the ability to access fields of an artifact with <code>((vars))</code>.)</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-27-trigger-resources","title":"RFC #27: trigger resources","text":"<p>Trigger resources allow jobs to specify parameters that can trigger new builds but don't have anything to fetch - they just propagate config fragments to the build.</p> <p>UPDATE: this has turned into a <code>get_var</code> step, rather than a <code>param</code> step</p> <p>This is also a relatively simple feature, but it will improve today's usage of the <code>time</code> resource by having per-job trigger semantics rather than having all jobs downstream of one <code>time</code> resource leading to a thundering herd of builds hitting your workers all at once.</p> <p>Rough sketch:</p> <pre><code>jobs:\n  - name: smoke-test\n    plan:\n      - param: 10m\n        trigger: true\n</code></pre> <p>Semantically, <code>param</code> is similar to <code>get</code> but with one key difference: there is no central version history. Rather than being used as an artifact, the resource is used solely for its config fragments. Concourse will <code>check</code> against the job's last used config fragment for the trigger resource, <code>10m</code>, and if a different fragment is returned the job will trigger with the new one.</p> <p>This skips the <code>get</code>, eliminates the thundering herd issue (because all jobs have their own interval), and could enable an interesting pattern for manually-parameterized builds: just write a resource type that can fetch user-provided config fragments from some external source (i.e. a repo).</p> <p>Here's one idea of what that may look like, where the config fragments returned by param are somehow usable with <code>((vars))</code> syntax in subsequent steps:</p> <pre><code>plan:\n  - param: environment\n  - task: smoke-test\n    vars:\n      environment: ((environment.name))\n</code></pre> <p>Another interesting use case would be to use it as a <code>instance_fragment</code> with the <code>set_pipeline</code> step.</p> <p>This idea is pretty half-baked - I've been mainly focusing on the 'spatial resources' idea. Follow along in the RFC #27 and help the idea develop!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#rfc-28-notification-resources","title":"RFC #28: notification resources","text":"<p>Notification resources will allow you to monitor the flow of a resource through your pipeline and emit build status notifications (e.g. Slack alerts, GitHub commit status) without having to sprinkle <code>put</code> steps all over your pipeline.</p> <p>Have you ever wanted to reflect your CI build status on your GitHub commits? Or send a Slack notification whenever the build is fixed or broken?</p> <p>If so, you're probably aware of how ugly it can make your pipelines, both in YAML and in the UI.</p> <p>A simple pipeline quickly turns into a mess of boxes and lines:</p> <p></p> <p></p> <p>a simple pipeline before and after notifications were added</p> <p>Not only is it a lot of manual work to copy-paste those <code>on_success</code> and <code>on_failure</code> hooks, when you finally configure it, it really ruins the signal-to-noise ratio of the pipeline UI.</p> <p>So, the plan for notification resources is to leverage composition, a pattern set forth in the Resources v2 RFC (#24). Instead of annotating every single job, you annotate a resource, and any time that resource is used in a build a notification will be fired, by executing the notification resource's <code>put</code> step with the config fragment of the original resource (e.g. <code>ref: abcdef</code>) and the status of the build.</p> <p>This way you don't have to update all of your jobs, and notifications don't clutter up the pipeline UI. Neato!</p> <p>This idea is also a bit half-baked - follow along in RFC #28 when you have time!</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#what-comes-after-all-this","title":"What comes after all this?","text":"<p>I dunno.</p> <p>I have a lot of respect for software that is eventually considered 'done.' I would really like Concourse's core design to achieve that someday.</p> <p>We'll always have things to improve, whether it's through better efficiency, better UX, support for new underlying technologies (Kubernetes, Nomad), or just making our codebase more accessible for contributors. But from a core design standpoint, I think the most important thing is stability.</p> <p>The software industry changes quickly. Hot new tools show up all the time and get replaced by newer and better tools. I don't want our users to have to keep re-doing their CI stack just to keep up.</p> <p>Concourse should insulate projects from the constant churn in the industry by providing a solid set of principles and abstractions that hold true regardless of the underlying technology.</p> <p>We will continue to listen to user feedback and improve Concourse. Our goal is for it to support good patterns and prevent anti-patterns that we can identify in workflows across the industry. Thankfully patterns don't change as frequently as tools do.</p>"},{"location":"blog/2019-07-17-core-roadmap-towards-v10/#thanks","title":"Thanks!","text":"<p>Everything I've outlined here comes from years of feedback through all of your GitHub issues, forum posts, and conversations in Discord (or Slack for you OGs). I'm very thankful for those of you that have stuck around and helped us understand your workflows, and I'm especially grateful for your patience.</p> <p>For those of you who couldn't wait and ultimately had to switch tools, I hope we accomplished one of our original goals, and I hope to see you back in the future!</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/","title":"Concourse Mid-year Update","text":"<p>Phew, it's been a while. I got lots of info to cover so let's just get right into it</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#concourse-oss-growth","title":"Concourse OSS Growth","text":"<p>As some of you may know, the Concourse team switched over to a PR-based workflow at the beginning of the year. This change is in line with our objectives of being open and transparent with our community of contributors. Plus, its just good thing to do because that's what most OSS projects do. Since then we've noticed a noticeable uptick in PRs opened by non-Concourse core contributors across our repos:</p> <p></p> <p>And while our peak period seemed to be concentrated at the beginning of the year, we're still seeing steady contributions through the summer months</p> <p></p> <p>The Concourse project also hit another big milestone: we now have over 4000 GitHub stars! As of today we're sitting at 4213 stars, a 52% increase in popularity from this time last year.</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#concourse-v10","title":"Concourse v10","text":"<p>In case you missed it, Alex wrote out a great blog post that outlines our long term vision for Concourse. It's got a breakdown of some exciting new features, from references to Spaces, the new Algorithm, Concourse Projects, etc. You can read more about it here 2019-07-17-core-roadmap-towards-v10.md</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#concourse-project-planning","title":"Concourse Project Planning","text":"<p>There's been some big changes to how we organize and visualize each track of work now. If you take a peek in the project board https://project.concourse-ci.org you'll see now that each of our tracks are clearly labeled as swimlanes, with each prioritized Epic as cards under each swimlane. We hope you'll find this new format easier to consume</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#increased-transparency-on-team-process","title":"Increased Transparency on Team Process","text":"<p>We recently held our first Concourse team offsite. We discussed topics such as:</p> <ul> <li>How can we tighten up our PR workflow?</li> <li>When do we release, and how often?</li> <li>Let's tackle the issue of Tech Quality</li> <li>Concourse Principles</li> </ul> <p>We've got a lot of action items and takeaways from that meetings, so look forward to updates from the team once they begin to formalize!</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#concourse-teams-in-the-wild","title":"Concourse Teams in the Wild","text":"<p>Members from the Concourse core team will be making a few more conference appearances before the end of year.</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#cf-summit-eu-2019","title":"CF Summit EU 2019","text":"<p>Taylor Silva and Scott Foerster will be giving a Concourse CI 102 talk at CF Summit EU 2019. I got a preview of the talk recently and its super informative for folks who are interested in learning more about the internals of the <code>web</code> node and how jobs/resources are scheduled</p>"},{"location":"blog/2019-08-30-concourse-mid-year-update/#springone-2019","title":"SpringOne 2019","text":"<p>The Concourse team will be hosting a 2 hour workshop in Austin during the workshop days before SpringOne 2019. The registration list is already full (sorry!) but if you're lucky you might get a spot if someone drops out. Or, you know, write a Concourse pipeline to watch the website and register yourself if the state changes! You can read more about the event here https://springoneplatform.io/2019/workshops/concourse</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/","title":"Re-inventing resource types","text":"<p>Before the paint completely dries on the v10 roadmap, there is one last big unknown I want to explore in case it brings more clarity to our direction: generic tasks.</p> <p>Resource types are a great way to share tools and integrations for others to use in their pipelines. Unfortunately, they're basically the only way, and because resources are a very opinionated concept, the resource type interface is not always a good fit.</p> <p>Concurrent to this problem, there's been a lot of talk about generic re-usable tasks. The idea is to make tasks just as easy to share and use as resource types. This would be a great alternative to resource types for workflows that don't really fit the resource model!</p> <p>I finally found the time to dig in to these problems, and I have two new RFCs that I'm excited to propose:</p> <ul> <li>RFC #37: Prototypes</li> <li>RFC #38: Resource Prototypes</li> </ul> <p>These proposals will have a lasting impact so I wanted to share some of my thought process here.</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#what-makes-a-resource","title":"What makes a resource?","text":"<p>If you'll humor me for a moment, I want to pin down what makes a resource a resource.</p> <p>Resources are the continuous part of Concourse. They represent inputs changing over time, passing different versions through jobs to form a pipeline. Resources are how the continuous thing-doer knows that there are things to do: pipelines converge on the latest available versions for each job's inputs, running builds until everything stabilizes.</p> <p>A resource is a single object with a linear version sequence. This assumption allows Concourse pipelines to skip ahead to the latest version by default instead of having to process every single version.</p> <p>Resources have an external source of truth ; the same resource definition will always yield the same versions, in the same order, in any pipeline, in any Concourse installation. This makes Concourse pipelines portable and self-contained, which is critical for disaster recovery.</p> <p>Resources are immutable ; fetching the same version will always give you the same bits. This allows <code>get</code> steps to be cached so that they don't have to be downloaded all the time.</p> <p>Resources are idempotent ; outputs will always result in the same external effect when given the same configuration and bits. This allows for builds to be safely re-run even if some of its <code>put</code> steps already ran.</p> <p>A resource definition looks something like this:</p> <pre><code>resources:\n- name: booklit\n  type: git\n  source:\n    uri: https://github.com/vito/booklit\n    branch: master\n</code></pre> <p>Every resource definition has a <code>type</code> and a <code>source</code>. The type denotes the resource type - i.e. the implementation of the Concourse resource interface to use. The source represents the location of the resource, i.e. the source of versions. This configuration is interpreted by the resource type, and is a black box to Concourse.</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#how-do-resource-types-work","title":"How do resource types work?","text":"<p>A resource type is packaged as a container image with 3 executables living under <code>/opt/resource</code>: <code>check</code>, used for finding versions, <code>in</code>, used for fetching versions, and <code>out</code>, used for writing versions. Each command reads a JSON request on <code>stdin</code> and emits a JSON response on <code>stdout</code>. These actions are run by Concourse during pipeline scheduling and build execution.</p> <p>Concourse comes with a few \"core\" resource types. Some are necessary for bootstrapping, like the <code>registry-image</code> or <code>docker-image</code> resource types. Some are included just to support common use cases, like <code>git</code> and <code>time</code>. We plan to remove most of them though; it's making the download size pretty big. (#4586)</p> <p>All other resource types must be configured in your pipeline under <code>resource_types:</code>. This makes the pipeline more self-contained, decoupling it from the resource types the Concourse installation happens to have installed.</p> <p>Pipelines define their own resource types by configuring a resource for the type's container image:</p> <pre><code>resource_types:\n- name: git\n  type: registry-image\n  source:\n    repository: concourse/git-resource\n    tag: 1\n</code></pre> <p>Technically, resource types work by using another resource type to fetch their container image. It's turtles all the way down!</p> <p>A resource type that fits the original design of resources implements the following semantics:</p> <ul> <li><code>check</code> queries the external source of truth to find new versions of the object.</li> <li><code>in</code> reads from the external source of truth and always produces the same bits for the same version and <code>params</code>.</li> <li><code>out</code> writes to the external source of truth if necessary based on the given bits and <code>params</code>. Any version emitted by   <code>out</code> can also be found by <code>check</code>.</li> </ul> <p>The easiest example of a 'proper' resource type is <code>git</code>. The <code>check</code> action consults <code>git log --first-parent</code> to return ordered commits for a single branch. The <code>in</code> action does a <code>git clone</code> to fetch the repo and check out the given commit; this is easily cached. The <code>out</code> action does a <code>git push</code>, optionally rebasing and returning a new version in the event of a conflict.</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#when-is-a-resource-type-not-a-resource-type","title":"When is a resource type not a resource type?","text":"<p>the treachery of container images</p> <p>Resource types should always implement <code>check</code> and <code>in</code>. Being able to find and fetch versions is what makes a resource a resource. Some resource types, however, only implement <code>out</code>. These resource types exist solely to be run as a <code>put</code> step - a form of \"generic tasks\" limited by the fact that it can't produce any outputs local to the build.</p> <p>Resource types should always represent a single object. This is pretty foundational to Concourse pipeline semantics. Some resource types, however, try to represent sets of objects. The easiest example is pull request resource types, which represent each pull request as a version so that you can use Concourse to run tests for all your PRs.</p> <p>This is fraught with peril:</p> <ul> <li>If you don't set <code>version: every</code> your builds will skip pull requests because all Concourse cares about is converging   on the latest version of each object. If each version is actually a different object, this assumption breaks.</li> <li>Because <code>version: every</code> allows versions to be skipped when used   with <code>passed:</code> constraints, now you have to cram everything into one monolithic job. You can try to work around this   by splitting it up and setting <code>serial: true</code> everywhere, but now you can't run PRs in parallel.</li> <li>Pull requests can skipped if the version history shifts around in a certain way. It's fundamentally impossible to try   to represent changes to all pull requests as one version stream with a stable order, so the order jumps around all the   time. If someone leaves a comment on a PR or pushes a new commit, it can get bumped to \"latest\" - and if a build has   already run for it, the other (\"older\") PRs won't run. Even with <code>version: every</code>, Concourse won't go back in time   to run old versions.</li> <li>Navigation is awkward. The pipeline UI is pretty meaningless since all the jobs just reflect the status of the most   recent PR that ran, and going through the build history of a job is pretty confusing because each build may be a   different pull request.</li> <li>Re-running builds for a pull request is annoying. You have to go to the PR resource, find the version for your PR, pin   it, trigger all the builds, wait for them all to start, and then you can unpin the resource, lest you forget and   your pipeline never runs another PR again. This will get slightly better in v6.0 as we've finally implemented build   re-triggering (#413), but that won't help with triggering builds   for an \"older\" PR that hasn't run yet.</li> </ul> <p>This pain is the main source of motivation for the v10 roadmap, which introduces all the required components to dynamically set a pipeline for each pull request instead - each with a resource representing only one pull request, as Concourse intended.</p> <p>In short, we have an interface being used for things beyond its original design. This results in surprising and unwanted behavior because Concourse functionality that is sensible for resources doesn't make sense for these other workflows. This hurts everyone: users have to deal with surprises and terrible UX, resource type authors have to deal with these limitations and workarounds, and the concept of 'resources' kind of erodes as these patterns spread.</p> <p>At this point it's pretty clear that there's a need to be able to share workflows and bespoke tools within the community, but it's also clear that resources aren't the best pipeline-level representation for all of them. So if resources aren't a good fit, what about tasks?</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#usability-of-generic-tasks","title":"Usability of generic tasks","text":"<p>Tasks can almost be packaged up and re-used as easily as resource types. I've been experimenting with this idea by writing a generic task for building OCI images. It works by configuring <code>vito/oci-build-task</code> as the task's image and configuring the rest of the task according to the README in the repo.</p> <p>So far, this UX doesn't sound that far off from using a resource type; you configure a resource type's image in <code>resource_types:</code> and figure out how to configure the rest using its README, too. On paper, the only difference is that a task's image is configured in <code>resources:</code> or with <code>image_resource:</code> instead.</p> <p>Let's compare what it looks like to take a <code>git</code> repo, build an OCI image from its <code>Dockerfile</code>, and push the image to a registry, using a generic task vs. using a resource type.</p> <p>We'll begin with two resources: one for my image source code, and one for the image repository on the registry:</p> <pre><code>resources:\n- name: my-image-src\n  type: git\n  source:\n    uri: # ...\n\n- name: my-image\n  type: registry-image\n  source:\n    repository: # ...\n    tag: latest\n</code></pre> <p>Next we'll add a job that does the build-and-push.</p> <p>Let's see how it looks to use a generic task:</p> <pre><code>jobs:\n- name: build-and-push\n  plan:\n  # fetch repository source (containing Dockerfile)\n  - get: my-image-src\n\n  # build using `oci-build` task\n  - task: build\n    image: oci-build-task\n    config:\n      platform: linux\n\n      image_resource:\n        type: registry-image\n        source:\n          repository: vito/oci-build-task\n\n      params:\n        CONTEXT: my-image-src\n\n      inputs:\n      - name: my-image-src\n\n      outputs:\n      - name: image\n\n      run:\n        path: build\n\n  # push using `registry-image` resource\n  - put: my-image\n    params: {image: image/image.tar}\n</code></pre> <p>Now let's see how it feels to use a resource type instead. If we switch the <code>my-repo</code> resource from <code>registry-image</code> to <code>docker-image</code>, we can leverage its (quite contentious) build-and-push behavior:</p> <pre><code>jobs:\n- name: build-and-push\n  plan:\n  # fetch repository source (containing Dockerfile)\n  - get: my-image-src\n\n  # build + push using `docker-image` resource\n  - put: my-image\n    params:\n      build: my-image-src\n</code></pre> <p>Resources clearly take a lot less effort to use in a pipeline. No wonder they're being used for everything!</p> <p>Providing a full task config is a lot of work. It allows for a lot of flexibility, but it feels verbose. Verbosity means wasting time on typos and forgotten boilerplate.</p> <p>Verbosity aside, tasks are also strictly worse at parameterization. Task <code>params</code> are really environment variables, so every value has to be a string. This is OK for simple values, but anything more complicated will need to be marshalled and unmarshalled. This is really crummy compared to resource types, which support complex YAML/JSON config structures like lists and objects.</p> <p>It seems like we need something in between tasks and resource types. We need something as versatile as tasks and as easy to use as resource types.</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#bridging-the-gap","title":"Bridging the gap","text":"<p>Let's hone in on the reason why resource types don't work for every use case: they have a particular set of actions which have particular semantics because they're built for a particular Concourse use case: resources.</p> <p>The v10 roadmap introduced RFC #24, a \"generalized resource\" interface which supports <code>check</code>, <code>get</code>, <code>put</code>, and <code>delete</code> actions while avoiding resource terminology like \"version\" and \"source\" so that it can be used for other workflows. It's kind of a strange middle ground: it's limited to resource-y actions while avoiding resource-y semantics.</p> <p>Aside from the resource-y actions, RFC #24 was pretty darn close to what I wanted out of generic tasks, so I decided to just fork it as RFC #37 and make one key change: instead of supporting <code>check</code>, <code>get</code>, <code>put</code>, and <code>delete</code>, support arbitrary actions instead.</p> <p>With <code>check</code> and <code>get</code> removed, the interface was definitely not a resource type interface anymore. And with its support of multiple actions, it definitely wasn't a task interface either, so I needed a new name for it.</p> <p>After much deliberation, I decided to call these things prototypes. This name is inspired by prototype-based object-oriented languages like JavaScript, Self, and Io. Conveniently enough, it still has \"type\" in the name, so all those <code>type:</code> fields on resources still make sense!</p> <p></p> <p>The next change in my fork of RFC #24 was to adjust the terminology. Now that the interface was so open-ended, I wanted to build a solid mental model so that prototype authors would have an idea of how prototypes are meant to be designed. I did this by stealing more terminology from prototype-based OOP.</p> <p>Here's where I landed: prototypes handle messages (previously 'actions') being sent to objects (previously 'config'). In response to a message, a prototype may emit more objects (previously 'config fragments').</p> <p>Thinking about Concourse as \"object-oriented CI/CD\" feels pretty compelling. This mental model can be easily used to describe how resource types work:</p> <ul> <li>The <code>check</code> message is sent to the <code>source</code> object to list <code>version</code> objects.</li> <li>The <code>get</code> message is sent to a <code>version</code> object (a clone   of the <code>source</code> object) to fetch its bits.</li> <li>The <code>put</code> message is sent to the <code>source</code> object to create <code>version</code> objects.</li> </ul> <p>Prototype implementations have full control over their domain of objects and the messages supported by those objects. For example, a <code>git</code> prototype could support multiple types of objects:</p> <ul> <li>a repo object, <code>{\"uri\":\"...\"}</code>, could support <code>branches</code> to find branch objects and <code>check</code> to find commit objects   in the \"default\" branch</li> <li>a branch object, <code>{\"uri\":\"...\",\"branch\":\"...\"}</code>, could support <code>check</code> to find commit objects on the branch or   <code>delete</code> to delete the branch</li> <li>a commit object, <code>{\"uri\":\"...\",\"branch\":\"...\",\"sha\":\"...\"}</code>, could support <code>get</code> to clone the repo and checkout   the commit</li> </ul> <p>Over time, we can start to identify patterns and implement pipeline semantics for certain interfaces, just like we have with <code>check</code>, <code>get</code>, and <code>put</code>. For example, when a build status changes, Concourse could run the <code>notify</code> message handler for any objects in the build which support it. A <code>git</code> prototype could implement this to automatically update commit status on GitHub. This would eliminate a whole class of <code>put</code>-only resource types and de-clutter everyone's pipelines.</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#prototypes-as-generic-tasks","title":"Prototypes as 'generic tasks'","text":"<p>Whereas a task is built around a single action, a prototype is built around objects which can handle messages. As such, the <code>oci-build</code> task would instead be an <code>oci-image</code> prototype supporting a <code>build</code> message.</p> <p>Here's how it could look to use a prototype for building an OCI image (note the use of <code>prototypes:</code> instead of <code>resource_types:</code>):</p> <pre><code>prototypes:\n- name: oci-image\n  type: registry-image\n  source:\n    repository: vito/oci-image-prototype\n\njobs:\n- name: build-and-push\n  plan:\n  # fetch repository source (containing Dockerfile)\n  - get: my-image-src\n\n  # build using `oci-image` prototype\n  - run: build\n    type: oci-image\n    inputs: [my-image-src]\n    params: {context: my-image-src}\n    outputs: [image]\n\n  # push using `registry-image` resource\n  - put: my-image\n    params: {image: image/image.tar}\n</code></pre> <p>Here we use a new <code>run</code> step to run the <code>oci-image</code> prototype and send the <code>build</code> message to an object, given as <code>params</code>. With the <code>run</code> step, <code>inputs</code> and <code>outputs</code> must be explicitly provided, though <code>inputs</code> can be automated in the future with #2692.</p> <p>All in all, this feels a whole lot better than the generic tasks of old. It's way less verbose, and feels a lot like using a <code>put</code> step, with no abstractions being abused and no surprising behavior. Mission accomplished?</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#how-does-this-impact-the-roadmap","title":"How does this impact the roadmap?","text":"<p>Through all of this, the only thing I've really added to the roadmap is the <code>run</code> step. Everything else is a lateral move; instead of using 'generalized resources' for spatial resources, notifications, and triggers, we would use  'prototypes' instead.</p> <p>I think the larger impact will be on future roadmaps. With a more flexible model at our disposal we can shorten the path from identifying a common workflow and implementing richer pipeline semantics for it. Concourse becomes a \"language of CI/CD,\" where the objects are provided at runtime and can be shared with the community.</p>"},{"location":"blog/2019-10-15-re-inventing-resource-types/#how-to-get-involved","title":"How to get involved","text":"<p>I'm still getting a grip on this idea myself but I'm excited to see the places we can go with it. If you'd like to get involved, I could use some feedback on the RFCs!</p> <ul> <li>RFC #37: Prototypes is based   on RFC #24, allowing implementations to support arbitrary messages and   switching everything over to prototype-based terminology. It also introduces the above <code>run</code> step for executing   arbitrary message handlers.</li> <li>RFC #38: Resource Prototypes shows that prototypes which implement   <code>check</code> and <code>get</code> messages can be used a resources in a pipeline, while maintaining backwards-compatibility for a   smooth migration to prototype-based resources over time.</li> </ul> <p>If everything goes well I plan to close RFC #24 and the other 'generalized resources' based RFCs in favor of these new prototype-based RFCs. (I still need to write up new prototype-based RFCs for the rest though: spatial resources, notification resources, trigger-only resources.)</p> <p>Special thanks to everyone that has helped me talk through ideas in Discord, on GitHub, and in person!</p>"},{"location":"blog/2020-01-24-a-new-hangar-for-resource-types/","title":"A New Hangar For Resource Types","text":"<p>The inside of an airplane hangar</p> <p>Photo: National Parks Service</p> <p>The idea to build a dedicated resource types catalog has been long-discussed. We\u2019d like to announce that the idea has come to fruition: the new Concourse resource types catalog is wheels up!</p> <p>The catalog lists Concourse resource types that have recently been submitted to the resource types GitHub repo. Originally, resource types were listed on a GitHub wiki page. While the wiki page listed resource types, it didn\u2019t provide much information about each resource. The resource types catalog will provide more information about each resource and enhanced search, both of which will make it easier to compare and find resource types.</p> <p>The addition of the resource types catalog means that the original resource types wiki page will be deprecated. If you have a resource listed on the wiki page, please migrate it over to the GitHub repo.</p>"},{"location":"blog/2020-01-24-a-new-hangar-for-resource-types/#contribution","title":"Contribution","text":"<p>As part of the effort to move resource types to a new home, we\u2019ve also spent some time thinking through the resource type submission process. This new process should make it easier for members of the community to contribute new resource types.</p> <p>The updated process consists of forking the existing resource types repository, adding your YAML file and submitting a pull request. After a quick review by community members, the resource type will be added to the repository and will be available on resource-types.concourse-ci.org. The process is described in more detail here. We\u2019re also working on automating some of this process using Concourse!</p> <p>If you\u2019ve gotten this far, have taken a quick look at the catalog, and are wondering why there is no \u201cresource type for x\u201d, it\u2019s a great opportunity to add your own! There are already some helpful walkthroughs from other community members on writing resource types (Implementing a Resource Type, Developing a Custom Concourse Resource, How to Make a Concourse Resource Type) which are a great place to start.</p>"},{"location":"blog/2020-01-24-a-new-hangar-for-resource-types/#whats-next","title":"What\u2019s Next?","text":"<p>We\u2019ve come a long way with Concourse resource types and are excited about the new catalog. We now have our sights set on adding more functionality on the page (check out the backlog). This includes displaying more information about each resource type on the cards (including GitHub stars and resource type actions), as well as improved search and sorting.</p> <p>We also have an eye on the V10 roadmap and can see prototypes on the horizon.</p> <p>In the spirit of the open-source project that it is, we\u2019d also love feedback to inform our roadmap. So if you have feedback, we\u2019d love to hear it. The best way to reach us is to either drop us a line in <code>#resource-types</code> on Discord or submit an issue against the GitHub repository.</p>"},{"location":"blog/2020-03-17-concourse-2020-community-survey/","title":"Concourse 2020 Community Survey","text":""},{"location":"blog/2020-03-17-concourse-2020-community-survey/#help-shape-the-future-of-concourse","title":"Help shape the future of Concourse","text":"<p>Since Concourse CI was created, thousands of users worldwide have helped the project by opening issues, committing code, and providing feedback to the team that develops the product. This community involvement is priceless - thank you, Concourse community! \ud83d\udc4f</p> <p>One of the ways the Concourse team collects feedback is through our annual Community Survey. This lets us gather crucial information about how users deploy Concourse, how different use cases scale, and various configuration patterns. We can also directly collect requests for new features and bug fixes. Past surveys have led us toward crucial features like the Dashboard UI, RBAC, new container placement strategies, and dozens of performance fixes.</p> <p>Each year\u2019s results are published in a blog post to help share out our most interesting findings.</p>"},{"location":"blog/2020-03-17-concourse-2020-community-survey/#the-state-of-concourse-in-2020","title":"The state of Concourse in 2020","text":"<p>Today we are launching the Concourse 2020 Community Survey. This survey is open to everyone who uses Concourse, whether it\u2019s your daily CI/CD tool, one of many automation tools your company uses, something you\u2019re experimenting with on the side, or something you\u2019re hoping to learn more about in 2020. The answers you provide will help ensure the product, community, and ecosystem fit the needs of the people closest to it.</p> <p>Please help us by participating in this 5-minute survey:</p> <p>Edit: Survey closed! Thank you to everyone who participated. Check out the results in the 2020 Community Report.</p>"},{"location":"blog/2020-03-17-concourse-2020-community-survey/#spread-the-word","title":"Spread the word!","text":"<p>We need as many users as possible to participate in this survey to help us better understand our global user base. We'd be grateful if you would spread the word by sharing this post on your social network feeds, around the office, at meet-ups, and in other communities.</p> <p>Thank you!</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/","title":"Developing Concourse (from home \ud83c\udfe1)","text":"<p>In March 2020, countless companies made a shift to have their employees work from home. For remote staff getting work done can be challenging enough, but staying connected to your team and company culture can be even more challenging. On the Concourse team, we\u2019re working hard to keep our product development running smoothly with some additions to our tech stack and day to day workflow.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#concourse-team-composition","title":"Concourse team composition","text":"<p>At VMware (formerly Pivotal Software), there are fourteen engineers and five product folks working on the Concourse CI project full time. We all care deeply about the software we're building, and put a great deal of effort and consideration into how we build software in order to be as effective as possible.</p> <p>Most of our team lives in Toronto, but in order to maintain velocity as the team expands to include engineering and product talent from other countries, and as we travel to meet with customers and talk about Concourse at conferences and meetups, we've refined our process to make ourselves resilient to occasions when a few employees might need to work from home or abroad.</p> <p>Then 2019's novel coronavirus arrived, and the ensuing pandemic in late February and early March 2020 forced us provided the right opportunity to test a fully-remote team experience.</p> <p>We thought we'd put together a quick post here to share how we're making it work. \ud83d\ude04</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#pre-covid19","title":"Pre-COVID19","text":"<p>The majority of the Concourse team is located in the greater Toronto area, with satellite members in a few cities in the United States. The Pivotal Software office in downtown Toronto is our home throughout the week. The team is set up with rows of computer workstations on the East side of the office, and gathers in various meeting rooms around the office for standups, prioritization meetings, discussions, and presentations.</p> <p>If you're a Pivotal/VMware customer, a Concourse contributor, or a user interview collaborator who's chatted with the team in the past, then you're probably familiar with a few of the meeting rooms from which we conduct Zoom meetings.</p> <p>The Toronto office is a great work environment. We're lucky to have the opportunity to get up to play a game of ping pong between engineering stories or meetings, and it's great to be in the same room to collaborate around a whiteboard or break down a problem with post-it notes.</p> <p>But what happens when your government enforces non-essential businesses to close their offices and asks employees to work from home instead?</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#post-covid19","title":"Post-COVID19","text":"<p>When the news and scale of the outbreak arrived in early March, we made the shift to collectively working from home as quickly as possible. Since the week of March 8th, all of us have been dutifully self-isolating, working out of our respective homes each day.</p> <p>Most of the solutions we've come up with are focused on maintaining our existing process (especially our use of IPMs, pairing, retrospectives, and a high degree of collaboration on every possible front) while making life remote friendly at the same time.</p> <p>With those goals in mind, our team has pulled from different experiences and techniques learned from past projects as well as new ideas that we're still iterating on daily. This effort has made for a novel mix of video, audio, and text applications that help us work in a comfortable, fun, efficient manner.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#zoom","title":"Zoom","text":"<p>First and foremost, we have a perpetual group Zoom meeting set up that everybody on the team hangs out in throughout the workday.</p> <p></p> <p>A typical workday in the Concourse hangar.</p> <p>Having one single room might be unconventional, but negates the need to constantly jump in and out of different Zoom meetings, and all of the confusion that can create. It's tied to a easy-to-remember URL that makes it painless to join each day - no more memorizing meeting IDs!</p> <p>Additionally, having everyone in the same place makes it feel more like we're all in the office together. Seeing each other's faces throughout the day makes working at home feel a lot more friendly and less isolated.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#discord","title":"Discord","text":"<p>The other major difference to Zoom meetings we held in the past is that now we remain fully muted. Zoom takes care of video communication, and we rely on Discord for audio and individual screen sharing.</p> <p>We were already using Discord for our open source community's text-based chat, but it also excels at fast, simple, and effective voice communications. The Concourse team uses a series of private voice channels (named after famous aircraft, of course \u2708\ufe0f) that we can join and depart with a single click.</p> <p></p> <p>Pairs of engineers working in audio channels on Discord</p> <p>This makes it easy to navigate for pair programming, impromptu meetings, or general chat and attention-getting. There\u2019s even a #water-cooler channel that acts as the defacto hangout spot! With this system you can see who is paired up in the respective rooms at a glance, adding a level of transparency and organization that isn't possible when everyone is pairing through separate Zoom meetings or other telecommunication products.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#slack","title":"Slack","text":"<p>The Concourse team has always used Slack to communicate internally, and Slack still plays a big role in organizing our work. Since moving to work from home and using Zoom and Discord as described above, however, we're using it less - the number of Slack messages simply doesn't need to be as high. If nothing else, it's great to have Slack as a backup option for screen sharing, especially since we can use it to pull in non-team members from the company as well.</p> <p>It still plays a large role in communicating with other product, engineering, and customer-facing teams in the company, and can't be beat for asynchronous messaging. However, when we need to chat within our team throughout the workday, we can just grab someone's attention and start talking immediately instead.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#remote-pairing","title":"Remote Pairing","text":"<p>Remote pairing is one area of our work process that is still up in the air. The Concourse team practices pair programming every day, and trying to do that remotely can be challenging at times. We\u2019ve tried the following methods of pair programming, with each having different strengths and weaknesses for different situations.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#toronto-workstations","title":"Toronto workstations","text":"<p>The pairing workstations in the Toronto office are set up so we can use OSX's screen sharing tool to securely connect to them over our company\u2019s VPN. This allows us to share everything (browser, IDE, terminal, and more) as we normally would if we were sitting side by side in the office to pair program.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#discord-go-live","title":"Discord Go Live","text":"<p>In a pinch we\u2019ve been able to share screens using Discord\u2019s Go Live feature. In mid-March Discord raised the viewing limit on Go Live and Go Live - Screen Share streams from 10 people at a time to 50 people, making this work well for mobbing as a group around one person's screen as well.</p> <p>One of the drawbacks is that it only streams in 720p resolution, making it hard to read text on the screen unless the text is enlarged. But there are a lot of times when a modest screen resolution is all you need, and we can switch to Slack's screen sharing in the rare case that we need higher fidelity.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#ssh-tmux","title":"SSH + tmux","text":"<p>Sometimes you just need a command line. In these situations, the team can also ssh onto any of the Toronto workstations, or any of the team's various VMs running linux on the cloud to share a tmux session.</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#attitude","title":"Attitude","text":"<p>This article has been heavy on technological solutions and workflow, but another thing worth mentioning is how positive and encouraging the team's attitude has been during the pandemic.</p> <p></p> <p>Concourse engineers, best engineers.</p> <p>Even isolated to our respective homes, we've seen everyone step up to keep each other happy and healthy, and to keep work moving at a sustainable pace. I won't go into detail about all the memes, inside jokes, guitar solos, Zoom virtual backgrounds, and pet cameos that have been shared among the team in the past couple weeks - you can use your imagination. \ud83d\ude02</p> <p>Don't underestimate the importance of taking breaks and having fun with your work!</p>"},{"location":"blog/2020-03-25-developing-concourse-from-home-/#what-next","title":"What next?","text":"<p>Nobody knows for certain how long we'll be working in full isolation, but since our goal is to flatten the curve of the outbreak, it's in our best interests to be prepared for a long wait.</p> <p>Our team is built around iterating on process and practices, and we plan to continue working on how we can collaborate to make sure we continue to deliver as much product value as possible.</p> <p>Join us on Discord to learn about new work in progress, report on bugs, collaborate on the codebase, or just keep us company! \ud83d\ude01</p>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/","title":"Community update: enter Discussions! \ud83c\udf89","text":"<p>Hasta la vista, stale bot.</p>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/#tldr","title":"tl;dr:","text":"<ul> <li>We've been granted access to GitHub's beta Discussions feature! \ud83c\udf89</li> <li>Discussions on the <code>concourse</code> repo will be used for questions   and technical support.</li> <li>Discussions on the <code>rfcs</code> repo will be for incubating ideas for new   workflows , which eventually turn into Pull Requests (also on the <code>rfcs</code> repo).</li> <li>From here on, Issues on the <code>concourse</code> repo are exclusively for project backlog and bug reports - i.e.   planned or emergent work.</li> <li>Creating an Issue directs you to these options, so there's   no need to change your muscle memory.</li> <li> <code>CONTRIBUTING.md</code> now covers this workflow   in addition to the more technical content.</li> <li>With these changes in place, the stale bot we all know and hate has been terminated.</li> <li>All Pull Requests will be assigned to someone as part of our daily process, and we will begin dedicating half of each   day to PR review.</li> <li>I am going to shift my focus from planning/prioritizing to shepherding RFCs and writing code. Expect more blog posts   in the future!</li> </ul>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/#an-update-on-triage","title":"An update on triage","text":"<p>With Concourse, there is always a lot of work to do. I personally would love to see some of the larger issues worked on today, but we (the Concourse team) have to choose our battles. A good chunk of our time is spent on upkeep, architectural improvements, and trying to identify the underlying needs across many feature requests so that we can make a lower volume of high-impact changes.</p> <p>The long and short of it is that the amount of work to do \u2013 both in code and in the community \u2013 greatly exceeds the number of people available to do it. Concourse is a product that has the entire software industry as its customer \u2013 including video game devs, mobile app devs, DevOps, and people who just want CI for personal side-projects. It's a lot to stay on top of, but it's something to embrace: it forces us to think in the abstract. It just takes time.</p> <p>The main goal of these changes is to promote healthier discourse by setting expectations about the status of an engagement more clearly. Issues are concrete; they will be prioritized and finished at some point, by the core team or \u2013 in a perfect world \u2013 by a volunteer from the community. Discussions on the other hand are at an earlier stage in the process.</p> <p>Discussions on the <code>concourse</code> repo will be used for questions and support. These can be more open-ended than bug reports \u2013 there may indeed be a bug, but there might also just be an answer or a better approach. The outcome of these discussions may be a bug report, an improvement to the docs, an answer to the question, or perhaps a new Discussion on the <code>rfcs</code> repo.</p> <p>Discussions on the <code>rfcs</code> repo will be used for incubating new ideas. By eliminating the \"solution-first\" framing of feature request issues, we can begin to focus on the problems instead. The hope is that we can all more easily identify underlying patterns and try to form broader solutions \u2013 whether they're ones we need to plan, whether they're already on the roadmap, or whether there's simply an existing solution that needs to be easier to discover.</p> <p>With these changes, we no longer have any need for the 'stale bot' as Discussions can just keep trucking along at their own pace. The bot has been terminated. Unfortunately, I removed its configuration before uninstalling it, causing it to assume the default settings and unleash its annoying comments across a slew of issues and pull requests, going out in one last blaze of glory. Sorry about that.</p>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/#improving-rfc-engagement","title":"Improving RFC engagement","text":"<p>Some of you have submitted RFCs and haven't received much feedback yet. I'm really sorry about that.</p> <p>With v6.0 out and with the dust settling on the \"v10\" roadmap,  I am going to shift my role towards shepherding RFCs and getting back to writing code rather than endlessly planning and prioritizing. It's been a long time! This will also eliminate the conflict-of-interest where I author RFCs and then prioritize them while neglecting others. Definitely not a trend that I want to continue.</p> <p>Expect more RFC update blog posts soon!</p>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/#improving-pull-request-engagement","title":"Improving Pull Request engagement","text":"<p>Another area we're always trying to improve on is Pull Request engagement. We've been tried a lot of things, but in the end it's been hard to integrate into our day-to-day pairing process and escape the single-point-of-failure (cough me).</p> <p>We're going to start assigning each and every PR to someone on the team and dedicate half of each day to PR review. Our goal is to dramatically shorten the feedback cycle time and not leave anyone hanging.</p>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/#what-about-discussconcourse-ciorg","title":"What about discuss.concourse-ci.org?","text":"<p>These changes make our forums a little (ok a lot) redundant. Once the Discussions feature feels solid I plan to shut the forums down and centralize our community in GitHub (in addition to Discord for real-time chat).</p>"},{"location":"blog/2020-04-16-community-update-enter-discussions-/#whats-happening-with-vmware","title":"What's happening with VMware?","text":"<p>Some of you may be wondering what the future holds for Concourse through VMware's acquisition of Pivotal, the company that has supported Concourse's development since 2015.</p> <p>VMware is heavily invested in Concourse \u2013 in fact some of our recent significant contributions originated from VMware pre-acquisition. Concourse is already being used internally, and there is work underway planning Concourse's integration into VMware's product ecosystem. We ain't going anywhere!</p> <p>Thanks and stay safe everyone!</p>"},{"location":"blog/2020-05-06-rfc-round-up-may-6th-2020/","title":"RFC round-up: May 6th, 2020","text":"<p>Howdy, and welcome to our first RFC round-up! \ud83e\udd20</p> <p></p> <p>For those unaware, Concourse RFCs are a process for proposing and collaborating on improvements to core Concourse functionality, including pipeline behavior, new step types, new operator capabilities, etc.</p> <p>In short, RFCs are where all the cool new stuff is planned. \ud83d\ude0e</p> <p>My goal is to provide an update at least every few weeks on the status of RFCs and shepherd them through the process via blog posts like this one. Each post will be limited to a handful of RFCs in order to focus our energy and not overwhelm readers.</p>"},{"location":"blog/2020-05-06-rfc-round-up-may-6th-2020/#rfcs-ready-to-merge","title":"RFCs ready to merge","text":"<p>The following RFCs have been given the <code>resolution/merge</code> label:</p> <ul> <li>RFC #33: archiving pipelines proposes that pipelines can be \"archived\" -   effectively a soft-delete, or perhaps a long-pause. This RFC is ready to go, and in fact we've already started to   implement it. It will be an experimental opt-in feature until this RFC is merged.</li> <li>RFC #34: pipeline instances proposes a mechanism for grouping related   pipelines together under a single identifier, further breaking down each instance by a set of associated vars.</li> </ul> <p>Both of these RFCs are key components to our plan for Git branch/PR pipeline automation, as described in the v10 blog post.</p> <p>Per the resolution process, if there are no objections or significant changes in the 2 weeks after this post is published, they will be merged! \ud83d\ude80</p>"},{"location":"blog/2020-05-06-rfc-round-up-may-6th-2020/#rfcs-in-need-of-specific-feedback","title":"RFCs in need of specific feedback","text":"<p>These two RFCs are nearing completion, but have some outstanding questions:</p> <ul> <li>RFC #39: var sources is the RFC behind the experimental   <code>var_sources:</code> feature introduced in v5.8.0. The main question is   around whether and how it may be used to replace the cluster-wide credential manager configuration.</li> <li>RFC #31: <code>set_pipeline</code> step is mostly implemented already,   also shipped experimentally in   v5.8.0. The remaining question is around whether to support <code>set_pipeline: self</code> - this is a point of contention as   there may be a better pattern for that sort of thing in the   future (hint).</li> </ul> <p>Lend us your opinions!</p>"},{"location":"blog/2020-05-06-rfc-round-up-may-6th-2020/#rfcs-in-need-of-attention","title":"RFCs in need of attention","text":"<p>These ones just need more eyes on'em:</p> <ul> <li>RFC #43: task queue proposes a \"Resource Pool\" mechanism with the end   goal of fixing the age-old problem of Concourse overloading workers. If you've run into this before and you'd like to   see it fixed, this is your chance to get involved!</li> <li>RFC #41: OPA integration proposes support for policy enforcement   through Open Policy Agent, which would allow access control to be delegated to an   external OPA endpoint. Neat!</li> </ul>"},{"location":"blog/2020-05-06-rfc-round-up-may-6th-2020/#wrapping-up","title":"Wrapping up...","text":"<p>Thanks to everyone who has gotten involved already, and special thanks to the RFC authors for your patience!</p> <p>Sorry if you had an RFC that didn't make the cut. \ud83d\ude15 We have a backlog of 23 RFCs at the moment, and I'll be going through all of them through the next few posts.</p> <p>Happy trails! \ud83d\udc0e</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/","title":"Concourse 2020 Community Report","text":"<p>A little over a month ago, the Concourse team sent a survey out to the community. The purpose of this survey was to gain insight into our users as well as measure our year-over-year growth. In the process of learning about how you all deploy and manage Concourse, we also received tons of great feedback about what's working well and what needs work in order to make Concourse even better. We\u2019re excited to share our findings!</p> <p>A huge thank you to everyone who responded. At the time of this writing, we\u2019ve received over 100 responses and that number is still climbing. Your contributions are valuable, and learning about how different segments of our user base works with our product is going to help us make Concourse even better in 2020!</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#feature-requests-areas-to-improve","title":"Feature requests, areas to improve","text":""},{"location":"blog/2020-05-14-concourse-2020-community-report/#configuration","title":"Configuration \u2699\ufe0f","text":"<p>This is a big one. The community wants more control for administrators and operators, more options for integrations, and more power over resource types configuration. We also learned a lot about the specific ways Concourse is making life more difficult than it needs to be in terms of configuring tasks, pipelines, teams, and the product itself.</p> <p>Code and configuration duplication is a serious issue, and our users want more powerful templating tools to help them split pipeline configuration into more manageable chunks that will be easier to reason about and maintain.</p> <p>In addition, there's a lot of support for concepts covered by our Instanced Pipelines, Spatial Resources, and other major architectural ideas that we have prioritized for 2020.</p> <p>We're also paying particular attention to the number of responses that were focused on git integration and GitOps workflows. If you have a way of using Concourse that you feel isn\u2019t well represented by the current featureset or CLI/UI, please @mention us on Twitter or drop by Discord and tell us about it.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#web-ui","title":"Web UI \ud83d\udda5","text":"<p>Concourse\u2019s web UI is a hot topic! While most of the web UI feedback is positive, there are lots of suggestions on how to improve it or what to add next. Feedback from the survey about the web UI could make for its own blog post, so in the interest of being brief, I\u2019m just going to touch lightly on the strongest signals/insights that were generated.</p> <p>A number of respondents called to attention the ease of use and clarity of information in the current UI. While we\u2019ve been continuously iterating to add text labels instead of just icons where possible, and to add clarifying tooltips elsewhere, there\u2019s clearly a need for more. In addition to several smaller tweaks, we have work underway around adding the minimum viable Favorite Pipelines functionality that will be built upon to extend the Archiving Pipelines functionality introduced in v6.1.0 to the front end. Hopefully these fixes will make a big impact, decluttering Concourse dashboards and making it a lot faster to find what you want at the same time.</p> <p>Another area where we can clearly improve is by adding more detail to the dashboard. Users are requesting more options for adding notes, tracking an audit trail of actions in the UI, clearer and more detailed error messages, and more statistical information like build duration and lead times. We\u2019ll be looking at the possibilities in this space over the coming months. If you have ideas, start a Discussion on GitHub.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#runtime-improvements","title":"Runtime Improvements \ud83d\udcc8","text":"<p>In addition to more stability and performance, the community puts a high level of importance and value on improving the efficient use of check containers, global locks on resource checking, and the ability to clear cached resource versions of a worker on demand with fly.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#docker-enhancements-and-performance","title":"Docker Enhancements and Performance \ud83d\udca8","text":"<p>We hear you. \ud83d\ude00</p> <p>Comments from the community emphasized optimizing docker-image resources, facilitating docker in worker containers, and better reporting on docker image status. There are a number of different voices in this conversation all with very different strategies for how they use Concourse, and we're sorting through feedback to help us prioritize low hanging fruit and high value enhancements that the team can prioritize.</p> <p>Additionally, we're actively monitoring issues and continuously collecting data on Docker performance so that we can make more improvements - we understand that every last bit of performance we can squeeze out of Docker interactions results in a huge benefit to many of our users.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#stability-kubernetes-documentation","title":"Stability, Kubernetes, Documentation \u2696\ufe0f \ud83d\udea2 \ud83d\udcda","text":"<p>These issues have remained top of mind in the community for the past few years, and this year's survey is no exception. From a stability perspective, the team has made great strides with the release of the new algorithm in version 6.0.0. The team has also taken further steps into being more k8s native by beginning an ongoing track of work dedicated to running K8s workloads. And lastly, our documentation work is ongoing - we hope to prioritize more \u2018getting started\u2019 materials for \u00a0beginners in order to enable new users to climb the learning curve faster than before. For more advanced users, we also plan more documentation around topics like autoscaling, tracing, and build statistics, among others.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#demographic-data","title":"Demographic Data","text":""},{"location":"blog/2020-05-14-concourse-2020-community-report/#how-long-have-you-used-concourse","title":"How long have you used Concourse?","text":"<p>Most of the people who responded indicated they had been using Concourse for one year or less. It's great to see that more people are picking up and experimenting with Concourse with each new release, and it's just as exciting to see that people stick around: more than 45% of respondents said they have been using Concourse for 2+ years. Whenever we interpret feedback from the community, we want to make sure we're taking into account the experiences of both newcomers, established users, and very experienced power users. Each segment experiences different challenges, and prioritizes different parts of the product.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#other-cicd-tools-used","title":"Other CI/CD tools used","text":"<p>Another dimension that's helpful to understand is the related experiences that each survey respondent is equipped with. When looking at other CI/CD tools that our community employs, Jenkins is still the top dog, accounting for nearly 30% of the tools mentioned. GitHub Actions has seen a rise in adoption since its initial release, and Travis, Gitlab, Bitbucket, and CircleCI are all fairly common options as well.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#how-did-you-find-out-about-concourse","title":"How did you find out about Concourse?","text":"<p>Pivotal Software (now VMware) has been Concourse's largest supporter since the project's inception. In previous years, it was common to see more than half of respondents say they were introduced to Concourse CI through a Pivotal Labs engagement, or through Concourse\u2019s role in automation of the Pivotal Platform, Pivotal Cloud Foundry. Now the community has started to branch out, with only 22% of people reporting that they learned about the product through Pivotal.</p> <p>The majority of users seem to have found Concourse organically, through search engines or social media. We're hoping to expand the use of our blog this year to help support the number of people hunting for Concourse content. Be on the lookout for more tutorials, advanced operations articles, and general updates about the Concourse product development and roadmap.</p> <p>We'd love to grow that Conference or Meetup section in 2020 - who's up for a remote meetup over Zoom? \ud83d\ude4c</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#why-use-concourse","title":"Why use Concourse?","text":"<p>When asked about the very important why behind their Concourse usage, concerns about Open Source tooling and flexibility were top of mind. The special emphasis that Concourse put on reproducibility and user interface also ranked highly, along with Concourse's scalability and overall feature set. Scalability is always a huge concern for the team, as we see enterprise customers frequently testing the limits of their tooling (sometimes with hundreds of Concourse clusters, many thousands of teams, and many hundreds of thousands of pipelines). Likewise, reproducibility is a commitment we're not planning on straying from any time soon.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#concourse-versions","title":"Concourse Versions","text":"<p>We released the survey just as v6.0.0 of Concourse was being finalized, so it was only close to the end that we started to see people upgrading to v6. We're thrilled, nonetheless, to see so many people had already upgraded to v5.8.x. Together, versions v5.8.x and v5.7.x represented the majority of survey respondents, with a low (&gt;10) rate of responses for any other version.</p> <p>To those 12 users who are still on v4.x.x and 7 users still on v3.x.x, feel free to get in touch on the Concourse Discord if you need any help upgrading! You can find all of the wonderful reasons to upgrade in the release notes, and we'll write blog articles in the coming months highlighting some of the latest and greatest new features and optimizations, as well as some upcoming enhancements on our roadmap.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#scale","title":"Scale","text":"<p>The data gathered shows that the majority of respondents are working with Concourses organized with fewer teams. And when it comes to users...</p> <p></p> <p>... we see a lot of smaller Concourse instances of under 10 users. There are also a few examples of large, enterprise scale deployments of 100+ users over 50+ teams. On the Concourse team, we frequently reach out to enterprise customers for special feedback on more massive implementation concerns. We also survey and interview members of the open source community to make sure we're building solutions that scale down to single users and small teams.</p> <p>If you'd like to add your voice, feel free to join in on the Concourse Discussions board.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#deployment-method","title":"Deployment Method","text":"<p>Docker remains the most frequently used deployment method, but the margins are slowly shrinking, and there's more even distribution across other popular options than we've seen in past years.</p> <p>Nearly identical numbers of responses came in citing Kubernetes (via the Helm chart), BOSH , and VM deployment strategies, reinforcing both our interest in facilitating K8s workflows and supporting our substantial BOSH user base.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#concourse-usage-style","title":"Concourse Usage Style","text":"<p>This year we asked about our users' usage style - specifically, what sort of development scenarios they were using Concourse to facilitate. Concourse remains an Infrastructure Automation powerhouse, and a similar number of users are using it to perform CI for web development and deploying software as part of their path to production.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#workloads","title":"Workloads","text":"<p>Linux workloads represent the vast majority for the Concourse community. We're also paying attention to special concerns for those running Windows and Darwin workloads, however this knowledge will help us prioritize fixes to help the largest group of users possible.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#preferred-iaas","title":"Preferred IAAS","text":"<p>Note: RMDH is remotely-managed dedicated hardware</p> <p>Finally, when asked about their preferred IAAS, AWS takes the top position again for the third year in a row. We consistently see a strong vSphere presence from enterprise customers, but it's really interesting to see the variety of setups that the open source community as a whole employs when deploying Concourse.</p>"},{"location":"blog/2020-05-14-concourse-2020-community-report/#summary","title":"Summary","text":"<p>A recurring topic that comes up in conversations with customers and internal teams at VMware is the sheer variety of ways that Concourse can be set up and put to work. Running this survey further reinforces that idea, giving us insight into an even larger number of configurations and implementations than what we see during our day to day enterprise development and support.</p> <p>It\u2019s also interesting to reflect on how far the project has come since Concourse CI was first introduced. We\u2019re nearing 22k commits from over 318 contributors adding up to 333 releases as of the typing of this sentence, and we\u2019re looking forward to speeding up even further in 2020.</p> <p>Of course, we want to make sure that we\u2019re developing the right features, prioritizing the right fixes and enhancements, and validating that each step we take has been made in the right direction. In the upcoming months we\u2019ll be consolidating all of these ideas into a new high level roadmap that sets out quarterly milestones for the team.</p> <p>Keep watch on the GitHub discussions page, this blog, and the Concourse Twitter feed for more updates, and don\u2019t forget to join the conversation on Discord.</p>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/","title":"Introduction to Task Inputs and Outputs","text":"<p>Understanding how task inputs and outputs work in Concourse can be a little confusing initially. This post will walk you through a few example pipelines to show you how inputs and outputs work within a single Concourse job. By the end you should understand how inputs and outputs work within the context of a single job.</p> <p>Let's define some jargon first.</p> <ul> <li>step : A step is a container running code within the context of a   Concourse job. A step may have inputs and/or outputs, or neither.</li> <li>Job plan : A list of steps that a job will execute when triggered.</li> <li>Inputs and Outputs : These are directories. Within Concourse, they're generically referred to as artifacts.   These artifacts are mounted in a step's container under a directory with some-name. You, as a writer of   Concourse pipelines, have control over what the name of your artifacts will be. If you're coming from the Docker   world, artifact is synonymous with volumes.</li> </ul> <p>To run the pipelines in the following examples yourself you can get your own Concourse running locally by following the Quick Start guide. Then use <code>fly set-pipeline</code> to see the pipelines in action.</p> <p>Concourse pipelines contain a lot of information. Within each pipeline YAML there are comments to help bring specific lines to your attention.</p>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/#example-one-two-tasks","title":"Example One - Two Tasks","text":"<p>This pipeline will show us how to create outputs and pass outputs as inputs to the next step(s) in a job plan.</p> <p>This pipeline has two tasks. The first task outputs a file with the date. The second task reads and prints the contents of the file from the first task.</p> <pre><code>jobs:\n  - name: a-job\n    plan:\n      - task: create-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: alpine }\n          outputs:\n            # Concourse will make an empty dir with this name\n            # and save the contents for later steps\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file\n      - task: read-ouput-from-previous-step\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: alpine }\n          # You must explicitly name the inputs you expect\n          # this task to have.\n          # If you don't then outputs from previous steps\n          # will not appear in the step's container.\n          # The name must match the output from the previous step.\n          # Try removing or renaming the input to see what happens!\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./the-output/file\n</code></pre> <p>Here's a visual graphic of what happens when the above job is executed.</p> <p></p>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/#example-two-two-tasks-with-the-same-output-who-wins","title":"Example Two - Two tasks with the same output, who wins?","text":"<p>This example is to satisfy the curiosity cat inside all of us! Never do this in real life because you're definitely going to hurt yourself!</p> <p>There are two jobs in this pipeline. The first job has two steps; both steps will produce an artifact named <code>the-output</code> in parallel. If you run the <code>writing-to-the-same-output-in-parallel</code> job multiple times you'll see the file in <code>the-output</code> folder changes depending on which of the parallel tasks finished last. Here's a visualization of the first job.</p> <p></p> <p>The second job is a serial version of the first job. In this job the second task always wins because it's the last task that outputs <code>the-output</code>, so only <code>file2</code> will be in <code>the-output</code> directory in the last step in the job plan.</p> <p></p> <p>This pipeline illustrates that you could accidentally overwrite the output from a previous step if you're not careful with the names of your outputs.</p> <pre><code>jobs:\n  - name: writing-to-the-same-output-in-parallel\n    plan:\n      # running two tasks that output in parallel?!?\n      # who will win??\n      - in_parallel:\n          - task: create-the-output\n            config:\n              platform: linux\n              image_resource:\n                type: registry-image\n                source: { repository: busybox }\n              outputs:\n                - name: the-output\n              run:\n                path: /bin/sh\n                args:\n                  - -cx\n                  - |\n                    ls -lah\n                    date &gt; ./the-output/file1\n          - task: also-create-the-output\n            config:\n              platform: linux\n              image_resource:\n                type: registry-image\n                source: { repository: busybox }\n              outputs:\n                - name: the-output\n              run:\n                path: /bin/sh\n                args:\n                  - -cx\n                  - |\n                    ls -lah\n                    date &gt; ./the-output/file2\n      # run this job multiple times to see which\n      # previous task wins each time\n      - task: read-ouput-from-previous-step\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./the-output\n                echo \"Get ready to error!\"\n                cat ./the-output/file1 ./the-output/file2\n\n  - name: writing-to-the-same-output-serially\n    plan:\n      - task: create-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file1\n      - task: create-another-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file2\n      - task: read-ouput-from-previous-step\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./the-output\n                echo \"Get ready to error!\"\n                cat ./the-output/file1 ./the-output/file2\n</code></pre>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/#example-three-inputoutput-name-mapping","title":"Example Three - Input/Output Name Mapping","text":"<p>Sometimes the names of inputs and outputs don't match, or they do match, and you don't want them overwriting each other, like in the previous example. That's when <code>input_mapping</code> and <code>output_mapping</code> become helpful. Both of these features map the inputs/outputs in the task's config to some artifact name in the job plan.</p> <p>This pipeline has one job with four tasks.</p> <p>The first task outputs a file with the date to the <code>the-output</code> directory. <code>the-output</code> is mapped to the new name <code>demo-disk</code>. The artifact <code>demo-disk</code> is now available in the rest of the job plan for future steps to take as inputs. The remaining steps do this in various ways.</p> <p>The second task reads and prints the contents of the file under the new name <code>demo-disk</code>.</p> <p>The third task reads and prints the contents of the file under another name, <code>generic-input</code>. The <code>demo-disk</code> artifact in the job plan is mapped to <code>generic-input</code>.</p> <p>The fourth task tries to use the artifact named <code>the-output</code> as its input. This task fails to even start because there was no artifact with the name <code>the-output</code> available in the job plan; it was remapped to <code>demo-disk</code>.</p> <p>Here's a visualization of the job.</p> <p></p> <p>Here's the pipeline YAML for you to run on your local Concourse.</p> <pre><code>jobs:\n  - name: a-job\n    plan:\n      - task: create-one-output\n        # The task config has the artifact `the-output`\n        # output_mapping will rename `the-output` to `demo-disk`\n        # in the rest of the job's plan\n        output_mapping:\n          the-output: demo-disk\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file\n      # this task expects the artifact `demo-disk` so no mapping is needed\n      - task: read-ouput-from-previous-step\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          inputs:\n            - name: demo-disk\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./demo-disk/file\n      - task: rename-and-read-output\n        # This task expects the artifact `generic-input`.\n        # input_mapping will map the tasks `generic-input` to\n        # the job plans `demo-disk` artifact\n        input_mapping:\n          generic-input: demo-disk\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          inputs:\n            - name: generic-input\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./generic-input/file\n      - task: try-and-read-the-output\n        input_mapping:\n          generic-input: demo-disk\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          # `the-output` is not available in the job plan\n          # so this task will error while initializing\n          # since there's no artiact named `the-output` in\n          # the job's plan\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./generic-input/file\n</code></pre>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/#example-four-can-you-add-files-to-an-existing-output-artifact","title":"Example Four - Can you add files to an existing output artifact?","text":"<p>This pipeline will also have two jobs in order to illustrate this point. What happens if we add a file to an output? If you think back to example two you may already know the answer.</p> <p>The first task will create <code>the-output</code> with <code>file1</code>. The second task will add <code>file2</code> to the <code>the-output</code>. The last task will read the contents of <code>file1</code> and <code>file2</code>.</p> <p>As long as you re-declare the input as an output in the second task you can modify any of your outputs.</p> <p>This means you can pass something between a bunch of tasks and have each task add or modify something in the artifact.</p> <pre><code>jobs:\n  - name: add-file-to-output\n    plan:\n      - task: create-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file1\n      - task: add-file-to-previous-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          # this task lists the same artifact as\n          # its input and output\n          inputs:\n            - name: the-output\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file2\n      - task: read-ouput-from-previous-step\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./the-output\n                cat ./the-output/file1 ./the-output/file2\n</code></pre> <p>Here's a visualization of the job.</p> <p></p>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/#example-five-multiple-outputs","title":"Example Five - Multiple Outputs","text":"<p>What happens if you have a task that has multiple outputs and a second task that only lists one of the outputs? Does the second task get the extra outputs from the first task?</p> <p>The answer is no. A task will only get the artifacts that match the name of the inputs listed in the task's config.</p> <pre><code>jobs:\n  - name: multiple-outputs\n    plan:\n      - task: create-three-outputs\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          outputs:\n            - name: the-output-1\n            - name: the-output-2\n            - name: the-output-3\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output-1/file\n                date &gt; ./the-output-2/file\n                date &gt; ./the-output-3/file\n      - task: take-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          # only one of the three outputs are\n          # listed as inputs\n          inputs:\n            - name: the-output-1\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./\n                cat ./the-output-1/file\n      - task: take-two-outputs\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          # this task pulls in the other\n          # two outputs, just for fun!\n          inputs:\n            - name: the-output-2\n            - name: the-output-3\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./\n                cat ./the-output-2/file\n                cat ./the-output-3/file\n</code></pre> <p>Here's a visualization of the above job.</p> <p></p>"},{"location":"blog/2020-05-25-introduction-to-task-inputs-and-outputs/#example-six-get-steps","title":"Example Six - Get Steps","text":"<p>The majority of Concourse pipelines have at least one resource, which means they have at least one get step. Using a get step in a job makes an artifact with the name of the get step available for later steps in the job plan to consume as inputs.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    source: { uri: \"https://github.com/concourse/examples\" }\n\njobs:\n  - name: get-step\n    plan:\n      # there will be an artifact named\n      # \"concourse-examples\" available in the job plan\n      - get: concourse-examples\n      - task: take-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: busybox }\n          inputs:\n            - name: concourse-examples\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./\n                cat ./concourse-examples/README.md\n</code></pre> <p>Here's a visualization for the above job.</p> <p></p> <p>I hope you found these example helpful with figuring out how inputs and outputs work within a single Concourse job.</p>"},{"location":"blog/2020-06-10-rfc-round-up-june-10th-2020/","title":"RFC round-up: June 10th, 2020","text":"<p>First off: sorry, I immediately failed to keep my target pace for these. \ud83d\ude13 I got wrapped up in a deadline, and since I alternate weeks between engineering and community duties like this post, when I miss a week for RFC updates the 2-week interval can quickly turn into 4 or 5.</p> <p>Owing to the missed round-up, and in hopes of burning through the backlog more quickly so that interested contributors may volunteer for merged RFCs, I'm going to expand the scope of this post to include more RFCs than the last one - primarily by proposing that we merge ones that are nearly certain for the v10 roadmap.</p>"},{"location":"blog/2020-06-10-rfc-round-up-june-10th-2020/#merged-rfcs","title":"Merged RFCs","text":"<ul> <li>RFC #33 (pipeline archiving)   and RFC #34 (pipeline instances) have both been merged! \ud83c\udf89</li> </ul>"},{"location":"blog/2020-06-10-rfc-round-up-june-10th-2020/#rfcs-ready-to-merge","title":"RFCs ready to merge","text":"<p>The following RFCs have been given the <code>resolution/merge</code> label:</p> <ul> <li>RFC #31: <code>set_pipeline</code> step is the RFC corresponding to the   <code>set_pipeline</code> step that was introduced experimentally in v5.8. Once this is merged, the step itself will no longer be   experimental, but there are a couple of experimental features for the step that are now outlined in the RFC - <code>self</code>   and <code>team:</code>. These features will result in warnings when used.</li> <li>RFC #40: valid identifiers proposes that we restrict the set of allowed   characters in Concourse identifiers such as pipeline names, job names, and resource names. Existing pipelines and   objects will be grandfathered in to ease the transition. Note: if you're worried about this change you may be   interested in RFC #34.</li> <li>RFC #39: var sources is the RFC corresponding to the <code>var_sources</code>   feature, which was also introduced experimentally in v5.8. This feature is a key component to v10 - it unblocks   spatial pipelines, per-job timed triggers, and per-pipeline credential management configuration.</li> <li>RFC #27: var steps is behind the  <code>load_var</code> step (shipped experimentally in v6.0), and also introduces a <code>get_var</code>   step which can theoretically be used to implement per-job trigger intervals. This RFC builds on the var sources   concept described in RFC #39.</li> </ul> <p>Per the resolution process, if there are no objections or significant changes in the 2 weeks after this post is published, they will be merged! \ud83d\ude80</p>"},{"location":"blog/2020-06-10-rfc-round-up-june-10th-2020/#rfcs-in-need-of-attention","title":"RFCs in need of attention","text":"<p>Quite a few RFCs have had some pretty interesting discussions or developments since the last round-up:</p> <ul> <li>RFC #36: manual step has had some juicy conversation around how things   like approval and manual gating in a pipeline should be expressed in a Concoursey way - if you have thoughts on this,   please chime in!</li> <li>RFC #37: prototypes is the RFC for the \"Prototypes\" concept introduced in   the Re-inventing resource types blog post. The latest   revision introduces encryption, which will enable Prototypes to implement credential managers. If you are a resource   type author or if you have a security background, please give it a look!</li> <li>RFC #32: projects now has a pretty radical new question: can Projects   replace Teams in order to provide more complete cluster config automation? If you've ever had a need for automating   team configuration, or if you have a thirst for GitOps, this should be a pretty interesting conversation!</li> </ul>"},{"location":"blog/2020-06-10-rfc-round-up-june-10th-2020/#new-rfcs","title":"New RFCs","text":"<ul> <li>RFC #53: configurable build event stores proposes a pluggable   architecture for build event storage as an alternative to storing them in the database.</li> <li>RFC #59: static configuration proposes a method for configuring Concourse   with a config file that prescribes the teams and projects, in addition to the regular config that would previously   have been set in flags or env vars. It also proposes disallowing the use of <code>fly set-team</code> at runtime so that the   config is the source of truth.</li> </ul>"},{"location":"blog/2020-06-10-rfc-round-up-june-10th-2020/#thanks","title":"Thanks!","text":"<p>Giving feedback on RFCs is critical to our ability to move forward more quickly and with higher confidence. Any and all comments and questions we receive are deeply appreciated. Thanks to everyone who's been involved, and thanks in advance to everyone else! \ud83d\ude42</p> <p>(Stay safe!)</p>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/","title":"How To Build and Publish a Container Image","text":"<p>In this blog post we are going to show how to build and publish container images using the oci-build task and registry-image resource. This post assumes you understand how to build container images with <code>Dockerfile</code>'s and publish to Docker Hub or another image registry using the <code>docker</code> cli.</p> <p>If you just want to see the pipeline, scroll to the bottom or click here. What follows is a detailed explanation of what each part of the pipeline does.</p> <p>First we need a Dockerfile. You can store this in your own repo or reference the github.com/concourse/examples repo. The rest of this post assumes you use the examples repo. All files in this blog post can be found in the examples repo.</p>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#the-dockerfile","title":"The Dockerfile","text":"<p>We are going to use a very basic Dockerfile so we can focus on building the Concourse pipeline.</p> <pre><code>FROM busybox\n\nRUN echo \"I'm simple!\"\nCOPY ./stranger /stranger\nRUN cat /stranger\n</code></pre>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#defining-pipeline-resources","title":"Defining Pipeline Resources","text":"<p>Now we can start building out our pipeline. Let's declare our resources first. We will need one resource to pull in the repo where our Dockerfile is located, and a second resource pointing to where we want to push the built container image to.</p> <p>There are some variables in this file that we will fill out later.</p> <pre><code>resources:\n  # The repo with our Dockerfile\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n      branch: master\n\n  # Where we will push the image to\n  - name: simple-image\n    type: registry-image\n    icon: docker\n    source:\n      repository: ((image-repo-name))/simple-image\n      username: ((registry-username))\n      password: ((registry-password))\n</code></pre>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#create-a-job","title":"Create a Job","text":"<p>Next we will create a job that will build and push our container image.</p> <pre><code>jobs:\n  - name: build-and-push\n</code></pre>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#retrieve-the-dockerfile","title":"Retrieve the Dockerfile","text":"<p>The first step in the job plan will be to retrieve the repo where our Dockerfile is.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n</code></pre>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#build-the-container-image","title":"Build the Container Image","text":"<p>The second step in our job will build the container image.</p> <p>To build the container image we are going to use the oci-build-task. The oci-build-task is a container image that is meant to be used in a Concourse task to build other container images. Check out the  <code>README</code> in the repo for more details on how to configure and use the oci-build-task in more complex build scenarios.</p> <p>Let's add a task to our job plan and give it a name.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n</code></pre> <p>All configuration of the <code>oci-build-task</code> is done through a task config. Viewing the <code>README</code> from the repo we can see that the task needs to be run as a privileged task on a linux worker.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n</code></pre> <p>To use the <code>oci-build-task</code> container image we specify the <code>image_resource</code> that the task should use.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n</code></pre> <p>Next we will add <code>concourse-examples</code> as an input to the build task to ensure the artifact from the get step (where our <code>Dockerfile</code> is fetched) is mounted in our<code>build-task-image</code> step.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n          inputs:\n            - name: concourse-examples\n</code></pre> <p>The <code>oci-build-task</code> outputs the built container image in a directory called <code>image</code>. Let's add <code>image</code> as an output artifact of our task so we can publish it in a later step.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n</code></pre> <p>Next we need to tell the <code>oci-build-task</code> what the build context of our Dockerfile is. The  <code>README</code> goes over a few other methods of creating your build context. We are going to use the simplest use-case. By specifying <code>CONTEXT</code> the <code>oci-build-task</code> assumes a <code>Dockerfile</code> and its build context are in the same directory.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n</code></pre> <p>The last step is specifying what our <code>build-task-image</code> should execute. The <code>oci-build-task</code> container image has a binary named  <code>build</code> located in its <code>PATH</code> in the  <code>/usr/bin</code> directory. We'll tell our task to execute that binary, which will build our container image.</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          run:\n            path: build\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n</code></pre> <p>At this point in our job the container image is built! The <code>oci-build-task</code> has saved the container image as a tarball named <code>image.tar</code> in the <code>image</code> artifact specified in the task outputs. This tar file is the same output you would get if you built the container image using Docker and then did  <code>docker save</code>.</p>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#publish-the-container-image","title":"Publish the Container Image","text":"<p>Now let's push the container image to an image registry! For this example we're pushing to Docker Hub using the  <code>registry-image</code> resource. You can use the <code>registry-image</code> resource to push to any image registry, private or public. Check out the  <code>README.md</code> for more details on using the resource.</p> <p>To push the container image add a put step to our job plan and tell the registry-image resource where the tarball of the container image is.</p> <p>The put step will push the container image using the information defined in the resource's source, when we defined the pipeline's resources.</p> <p>This is where you'll need to replace the three variables found under <code>resource_types</code>. You can define them statically using <code>fly</code>'s <code>--var</code> flag when setting the pipeline. (In production make sure to use a credential management system to store your secrets!)</p> <pre><code>jobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n          run:\n            path: build\n      - put: simple-image\n        params:\n          image: image/image.tar\n</code></pre>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#the-entire-pipeline","title":"The Entire Pipeline","text":"<p>Putting all the pieces together, here is our pipeline that builds and pushes (publishes) a container image.</p> <pre><code>resources:\n  # The repo with our Dockerfile\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n      branch: master\n\n  # Where we will push the image\n  - name: simple-image\n    type: registry-image\n    icon: docker\n    source:\n      repository: ((image-repo-name))/simple-image\n      username: ((registry-username))\n      password: ((registry-password))\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: vito/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n          run:\n            path: build\n      - put: simple-image\n        params:\n          image: image/image.tar\n</code></pre> <p>You can set the pipeline with the following <code>fly</code> command, updating the variable values with real values the pipeline can use. The behaviour is similar to <code>docker push</code>:</p> <pre><code>fly -t &lt;target&gt; set-pipeline -p build-and-push-image \\\n    -c ./examples/pipelines/build-and-push-simple-image.yml \\\n    --var image-repo-name=&lt;repo-name&gt; \\\n    --var registry-username=&lt;user&gt; \\\n    --var registry-password=&lt;password&gt;\n</code></pre> <p></p> <p>Build and Push Pipeline</p>"},{"location":"blog/2020-06-19-how-to-build-and-publish-a-container-image/#further-readings","title":"Further Readings","text":"<p>Understanding what the build context is important when building container images. You can read Dockerfile Best Practices for more details about build contexts.</p> <p>The inputs section of the oci-build-task's <code>README</code> has examples on how to create a build context with multiple inputs and other complex build scenarios.</p> <p>Read the <code>README</code>'s in the oci-build-task and registry-image resource to learn more about their other configuration options.</p> <p>If you had trouble following how the artifacts get passed between the steps of a job then read our other blog post about task inputs and outputs.</p>"},{"location":"blog/2020-06-24-rfc-round-up-june-24th-2020/","title":"RFC round-up: June 24th, 2020","text":"<p>With the four(!) RFCs from the last round-up now merged, it's time to move on to the next RFC milestone: Prototypes!</p>"},{"location":"blog/2020-06-24-rfc-round-up-june-24th-2020/#merged-rfcs","title":"Merged RFCs \ud83c\udf89","text":"<ul> <li>RFC #31: <code>set_pipeline</code> step</li> <li>RFC #40: valid identifiers</li> <li>RFC #39: var sources</li> <li>RFC #27: var steps</li> </ul>"},{"location":"blog/2020-06-24-rfc-round-up-june-24th-2020/#rfcs-ready-to-merge","title":"RFCs ready to merge \ud83e\udd1e","text":"<ul> <li>RFC #37: Prototypes is finally ready to go! For (much) further reading,   check out the Re-inventing resource types blog post. The   importance of this RFC really cannot be overstated; it will be the most significant change to Concourse since its   creation.</li> <li>RFC #38: Resource Prototypes demonstrates how the Prototype protocol may   be used to implement the next generation of resource prototypes (formerly resource types) and gain long-requested   functionality along the way.</li> </ul>"},{"location":"blog/2020-06-24-rfc-round-up-june-24th-2020/#shiny-new-rfcs","title":"Shiny new RFCs \u2728","text":"<ul> <li>RFC #62: worker pools introduces a \"worker pool\" which will allow a   many-to-many relationship between workers and teams.</li> <li>RFC #63: API auth flow for applications is a conversation-starter around   adding a token-based auth flow for read-only APIs.</li> <li>RFC #61: add \"watch\" parameter for API endpoints introduces a   long-polling approach to API requests to reduce the load from constant polling from the web UI.</li> </ul>"},{"location":"blog/2020-06-24-rfc-round-up-june-24th-2020/#open-call-for-contributors","title":"Open call for contributors \ud83d\udce2","text":"<p>The following is a list of issues for each merged RFC that still has work to be done.</p> <ul> <li>Pipeline instances: #5808</li> <li>Valid identifiers: #5810</li> <li>Finishing var sources: #5813</li> <li>Finishing the <code>set_pipeline</code> step: #5814</li> <li>Implementing the <code>get_var</code> step: #5815</li> </ul> <p>If anyone's interested in throwing their hat into the ring and getting involved with Concourse development, let us know by replying to one of the issues linked above! Future RFC round-ups will include the same list, presumably with more entries.</p> <p>The v10 roadmap is highly parallelizeable, so if more people get involved we can make it to v10 much more quickly and with a healthier project that has more people able to contribute. We're super keen to give guidance and help out. \ud83d\udc4c</p> <p>Thanks!</p>"},{"location":"blog/2020-07-10-rfc-round-up-july-10th-2020/","title":"RFC round-up: July 10th, 2020","text":"<p>Happy Friday! This one's brief.</p>"},{"location":"blog/2020-07-10-rfc-round-up-july-10th-2020/#merged-rfcs","title":"Merged RFCs","text":"<ul> <li>RFC #37: prototypes has landed! ...but it probably could have use more   detail regarding the <code>run</code> step, which is the only immediately actionable part of it. \ud83e\udd14 I'll draft another RFC for   that; #37 mainly covered the protocol.</li> <li>RFC #38: resource prototypes is in! Its associated issue for   implementation is #5870.</li> </ul>"},{"location":"blog/2020-07-10-rfc-round-up-july-10th-2020/#rfcs-to-merge","title":"RFCs to merge","text":"<ul> <li>n/a - taking a breather for this round-up to focus on the below RFCs and \"reset\" the 2 week merge window so I can   start publishing these posts earlier in the week. \ud83d\ude05</li> </ul>"},{"location":"blog/2020-07-10-rfc-round-up-july-10th-2020/#rfcs-in-need-of-feedback","title":"RFCs in need of feedback","text":"<ul> <li>RFC #43: tasks queue still needs some love! The goal is to introduce a   queuing mechanism to resolve the long-running issue of Concourse over-working workers.</li> <li>RFC #29: <code>across</code> step introduces the special sauce for build matrices   and branch/PR pipeline automation, and the proposal has been heavily revised. Check it out!</li> </ul>"},{"location":"blog/2020-07-10-rfc-round-up-july-10th-2020/#open-call-for-contributors","title":"Open call for contributors","text":"<p>Valid identifiers (#5810) is now spoken for - thanks @mouellet! \ud83c\udf7b</p> <p>The following issues are up for grabs:</p> <ul> <li>Pipeline instances: #5808</li> <li>Finishing var sources: #5813</li> <li>Finishing the <code>set_pipeline</code> step: #5814</li> <li>Implementing the <code>get_var</code> step: #5815</li> </ul> <p>If anyone's interested in helping out, or just learning how, let us know by replying to any of the issues linked above or asking in Discord!</p> <p>This section will be repeated in each RFC round-up - the goal is to get to the finish line on the v10 roadmap by tackling items in parallel while improving project health by enabling more people to make significant contributions.</p> <p>Thanks everyone!</p>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/","title":"GitOps For Your Pipelines","text":"<p>In this blog post we're going to cover how to use git and Concourse to automatically set, update, and archive your pipelines using the <code>set_pipeline</code> step. No longer will you need to use <code>fly set-pipeline</code> to update any of your pipelines!</p> <p>For consistency, we will refer to the pipeline that contains all the <code>set_pipeline</code> steps as the parent pipeline. The pipelines created by the <code>set_pipeline</code> steps will be called child pipelines.</p> <p>Scroll to the bottom to see the final pipeline template or click here. What follows is a detailed explanation of how the parent pipeline works along with git and automatic archiving.</p>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#prerequisites","title":"Prerequisites","text":"<p>To run the pipelines in this blog post for yourself you can get your own Concourse running locally by following the Quick Start guide.</p> <p>You will also need to fork the github.com/concourse/examples repo and replace <code>USERNAME</code> with your GitHub username in the below examples. We will continue to refer to the repo as <code>concourse/examples</code>. Once you have forked the repo clone it locally onto your machine and <code>cd</code> into the repo.</p> <pre><code>git clone git@github.com:USERNAME/examples.git\ncd examples\n</code></pre>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#create-the-parent-pipeline","title":"Create the Parent Pipeline","text":"<p>Inside your fork of <code>concourse/examples</code> that you have cloned locally, create a file named <code>reconfigure-pipelines.yml</code> inside the <code>pipelines</code> folder. This is the pipeline that we are going to be building. We will refer to this pipeline as the parent pipeline.</p> <pre><code>touch ./pipelines/reconfigure-pipelines.yml\n</code></pre> <p>Like the <code>fly set-pipeline</code> command, the <code>set_pipeline</code> step needs a YAML file containing a pipeline configuration. We will use the concourse/examples repo as the place to store our pipelines, and thankfully it already contains many pipelines! Let's add the repo as a resource to our parent pipeline.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/examples.git\n</code></pre> <p>Now we will add a job that will set our pipelines. The first step in the job will fetch the <code>concourse/examples</code> repo, making it available to future steps as the <code>concourse-examples</code> artifact. We will also add the <code>trigger</code> parameter to ensure that the job will run whenever a new commit is pushed to the <code>concourse/examples</code> repo.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/examples.git\n\njobs:\n  - name: configure-pipelines\n    public: true\n    plan:\n      - get: concourse-examples\n        trigger: true\n</code></pre> <p>Next we will add the <code>set_pipeline</code> step to set one of the pipelines in the <code>concourse/examples</code> repo. We will set the <code>hello-world</code> pipeline first.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/examples.git\n\njobs:\n  - name: configure-pipelines\n    public: true\n    plan:\n      - get: concourse-examples\n        trigger: true\n      - set_pipeline: hello-world\n        file: concourse-examples/pipelines/hello-world.yml\n</code></pre> <p>Let's commit what we have so far and push it to GitHub.</p> <pre><code>$ git add pipelines/reconfigure-pipelines.yml\n$ git commit -m \"add reconfigure-pipelines\"\n$ git push -u origin head\n</code></pre>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#setting-the-parent-pipeline","title":"Setting the Parent Pipeline","text":"<p>Now we have a chicken or the egg problem, except in this case we know our parent pipeline comes first! Let's set our pipeline with <code>fly</code> and execute the <code>configure-pipelines</code> job.</p> <pre><code>$ fly -t local set-pipeline \\\n  -p reconfigure-pipelines \\\n  -c pipelines/reconfigure-pipelines.yaml\n\n...\napply configuration? [yN]: y\n\n$ fly -t local unpause-pipeline \\\n  -p reconfigure-pipelines\n\nunpaused 'reconfigure-pipelines'\n\n$ fly -t local trigger-job \\\n  -j reconfigure-pipelines/configure-pipelines \\\n  --watch\n</code></pre> <p>Once the job is done running you should see two pipelines, <code>reconfigure-pipelines</code> and <code>hello-world</code>.</p> <p></p> <p>Concourse dashboard showing two pipelines</p> <p>Now any changes you make to the <code>hello-world</code> pipeline will be updated automatically in Concourse once it picks up the commit with your changes.</p>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#pipelines-setting-themselves","title":"Pipelines Setting Themselves","text":"<p>Our parent pipeline is setting and updating one other pipeline now, but it has one glaring limitation: it doesn't set itself. We have to <code>fly set-pipeline</code> every time we want to add a new pipeline to the <code>configure-pipelines</code> job.</p> <p>To resolve this we can do the following to our parent pipeline:</p> <ul> <li>Add a job before the <code>configure-pipelines</code> job that self-updates the parent pipeline. We'll name the job   <code>configure-self</code>.</li> <li>Add a <code>passed</code> constraint to the <code>configure-pipelines</code> job to only run once the <code>concourse-examples</code> resource has   passed the new <code>configure-self</code> job.</li> </ul> <p>By doing the above we will never have to use <code>fly</code> to update the parent pipline again. Every commit to the <code>concourse/examples</code> repo will cause the parent pipeline to update itself and then all of its child pipelines. Now our pipelines are following a GitOps type of workflow!</p> <p>Here is what the above changes look like when implemented:</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/examples.git\n\njobs:\n  - name: configure-self\n    plan:\n      - get: concourse-examples\n        trigger: true\n      - set_pipeline: reconfigure-pipelines\n        file: concourse-examples/pipelines/reconfigure-pipelines.yml\n  - name: configure-pipelines\n    plan:\n      - get: concourse-examples\n        trigger: true\n        passed: [ configure-self ]\n      - set_pipeline: hello-world\n        file: concourse-examples/pipelines/hello-world.yml\n</code></pre> <p>Side-note : for the <code>configure-self</code> job, you could also use the  <code>self</code> keyword, though this is labelled as experimental and may disappear in the future.</p> <p>Let's set the parent pipeline one more time with <code>fly</code> and then we'll make commits to the repo to make all future changes.</p> <pre><code>$ fly -t local set-pipeline \\\n  -p reconfigure-pipelines \\\n  -c pipelines/reconfigure-pipelines.yaml\n\n...\napply configuration? [yN]: y\n</code></pre> <p>The parent pipeline should now look like this. Now the pipeline will first update itself and then update any existing child pipelines.</p> <p></p> <p>parent pipeline with config-self job</p> <p>Let's commit our changes, which will be a no-op since we've already updated the pipeline with the latest changes.</p> <pre><code>$ git add pipelines/reconfigure-pipelines.yml\n$ git commit -m \"add configure-self job\"\n$ git push\n</code></pre> <p>Now comes the real fun! To add a pipeline to Concourse all we need to do is add a <code>set_pipeline</code> step to the parent pipeline, commit it to the <code>concourse/examples</code> repo, and let the parent pipeline pick up the new commit and make the changes for us.</p> <p>Let's add the <code>time-triggered</code> pipeline to our <code>reconfigure-pipelines.yml</code> file.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/examples.git\n\njobs:\n  - name: configure-self\n    plan:\n      - get: concourse-examples\n        trigger: true\n      - set_pipeline: reconfigure-pipelines\n        file: concourse-examples/pipelines/reconfigure-pipelines.yml\n  - name: configure-pipelines\n    plan:\n      - get: concourse-examples\n        trigger: true\n        passed: [ configure-self ]\n      - set_pipeline: hello-world\n        file: concourse-examples/pipelines/hello-world.yml\n      - set_pipeline: time-triggered\n        file: concourse-examples/pipelines/time-triggered.yml\n</code></pre> <p>Commit and push the changes to GitHub.</p> <pre><code>$ git add pipelines/reconfigure-pipelines.yml\n$ git commit -m \"add time-triggered pipeline\"\n$ git push\n</code></pre> <p>Once Concourse picks up the commit (may take up to a minute by default) you should see three pipelines on the dashboard. Now you never need to use <code>fly</code> to set pipelines!</p> <p></p> <p>parent and child pipelines</p>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#detour-a-future-alternative-of-setting-pipelines","title":"Detour: A Future Alternative of Setting Pipelines","text":"<p>In the future there will be a different solution to setting parent pipelines: no more parent pipelines! How will Concourse eliminate the current need to start with a parent pipeline in order to set child pipelines? The answer is RFC 32: Projects.</p> <p>If RFC 32 is implemented as currently described then you won't have to ever use <code>fly set-pipeline</code> to create pipelines, you'll simply create a Project , which involves pointing Concourse to a repo where you code lives. In the proposed <code>project.yml</code> you can then define all of your child pipelines with <code>set_pipeline</code> steps. No need to create a parent pipeline; the <code>project.yml</code> replaces the parent pipeline and no longer requires you to have a separate job that does <code>set_pipeline: self</code>.</p> <p>The RFC is still open and looking for feedback. Check out the PR and leave your thoughts for the community to discuss!</p> <p>Now let's get back on track and talk about the last step in a pipeline's lifecycle: archiving.</p>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#automatically-archiving-pipelines","title":"Automatically Archiving Pipelines","text":"<p>Having Concourse automatically set pipelines for you is great but that only covers half of the lifecycle that a pipeline can go through. Some pipelines stay around forever and get continuously updated. Other pipelines may only be around for a small amount of time and then be deleted or archived.</p> <p>Thanks to RFC #33 you can now archive pipelines and have Concourse * automatically archive* pipelines for you as well. You've been able to archive pipelines using <code>fly</code> since Concourse 6.1.0. Automatic archiving was added in 6.5.0.</p> <p>A pipeline will only be considered for automatic archiving if it was previously set by a <code>set_pipeline</code> step. It will be archived if one of the following is true:</p> <ul> <li>the <code>set_pipeline</code> step is removed from the job</li> <li>the job that was setting the child pipeline is deleted</li> <li>the parent pipeline is deleted or archived</li> </ul> <p>We can test this out with the parent pipeline we were just using. Let's remove the <code>hello-world</code> pipeline.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/examples.git\n\njobs:\n  - name: configure-self\n    plan:\n      - get: concourse-examples\n        trigger: true\n      - set_pipeline: reconfigure-pipelines\n        file: concourse-examples/pipelines/reconfigure-pipelines.yml\n  - name: configure-pipelines\n    plan:\n      - get: concourse-examples\n        trigger: true\n        passed: [ configure-self ]\n      - set_pipeline: time-triggered\n        file: concourse-examples/pipelines/time-triggered.yml\n</code></pre> <p>Commit and push the changes to GitHub.</p> <pre><code>$ git add pipelines/reconfigure-pipelines.yml\n$ git commit -m \"remove hello-world pipeline\"\n$ git push\n</code></pre> <p>After a few seconds the pipeline should disappear from the dashboard (unless you toggle \"show archived\" on).</p> <p>With automatic archiving the entire lifecycle of your pipelines can now be managed with a git repo and a few commits.</p> <p>I suggest checking out the documentation for <code>set_pipeline</code> to see all the other fields available for the step, like <code>team</code> and <code>vars</code>!</p>"},{"location":"blog/2020-08-24-gitops-for-your-pipelines/#the-parent-pipeline-template-tldr","title":"The Parent Pipeline Template (tl;dr)","text":"<pre><code>resources:\n  - name: ci\n    type: git\n    icon: github\n    source:\n      uri: git@github.com:USERNAME/repo-where-pipelines-live.git\n\njobs:\n  - name: configure-self\n    plan:\n      - get: ci\n        trigger: true\n      - set_pipeline: self\n        file: ci/path/to/parent-pipeline.yml\n  - name: configure-pipelines\n    plan:\n      - get: ci\n        trigger: true\n        passed: [ configure-self ]\n      - set_pipeline: some-pipeline\n        file: ci/path/to/some-pipeline.yml\n      - set_pipeline: another-pipeline\n        file: ci/path/to/another-pipeline.yml\n</code></pre>"},{"location":"blog/2020-12-29-running-docker-in-concourse/","title":"Running Docker in Concourse","text":"<p>So you want to run Docker in Concourse? Well this is the guide for you!</p> <p>Let's clarify what it is we want to do. We want to be able to run <code>docker-compose</code> inside a task in Concourse to bring up our application alongside some other services (i.e. Redis, Postgres, MySQL, etc.).</p> <p>Thankfully this challenge has been solved by the community! There are a few \"Docker-in-Docker\" images designed to run in Concourse that are maintained by the community. Here's a short list made from a cursory search, in no particular order:</p> <ul> <li>github.com/meAmidos/dcind</li> <li>github.com/karlkfi/concourse-dcind</li> <li>github.com/fhivemind/concourse-dind</li> <li>github.com/taylorsilva/dcind</li> </ul> <p>You can also opt to build your own fork of the above images.</p> <p>All of the above repositories have their own example pipelines that you can use to get started. What follows are some bits of information that are useful to know when using these task images.</p>"},{"location":"blog/2020-12-29-running-docker-in-concourse/#privileged-tasks","title":"Privileged Tasks","text":"<p>Running Docker inside Concourse requires the task step to be privileged because Docker needs access to the hosts cgroup filesystem in order to create containers.</p> <p>You can verify this by looking at the bash scripts for each of the above images which all take inspiration from the docker-image resource. Read the  <code>sanitize_cgroups</code> function to see what exactly is being mounted from the host. (tldr: mount all cgroups as read-write)</p>"},{"location":"blog/2020-12-29-running-docker-in-concourse/#externalize-all-images","title":"Externalize All Images","text":"<p>You should avoid having Docker fetch any images from inside your task step where you are running <code>docker-compose</code>. You should externalize these as image resources if they're a dependency of your application (e.g. Postgres, MySQL).</p> <p>For the container image that contains your application you should have that built in a previous step or job. You can build and publish an image using the oci-build task.</p> <p>To ensure Docker doesn't try to fetch the images itself you can use  <code>docker load</code> and  <code>docker tag</code> to load your externalized images into Docker. meAmidos's has a great example pipeline that does exactly that.</p> <p>meAmidos also makes two great points about why you should externalize your image:</p> <ul> <li>If the image comes from a private repository, it is much easier to let Concourse pull it, and then pass it through to   the task.</li> <li>When the image is passed to the task, Concourse can often get the image from its cache.</li> </ul> <p>That's all you need to know to run Docker inside Concourse!</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/","title":"Concourse 2024 in Review","text":"<p>Hey Concourse community, it\u2019s been a while since a blog post was made. My name is Taylor Silva, and I\u2019m the current lead maintainer of the project. I have fallen into this role for historical (was on the Concourse team at Pivotal starting in 2019) and personal reasons (still using Concourse at my day job). I really like using Concourse and I haven't found another tool that works as well as Concourse does, that's why I've stuck around.</p> <p>This post isn't about me though, it's about the project and what's happening with it. I'm going to talk about the last year of the project to recap where the project is at. Then I'll discuss where I see the project going over the next year.</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/#concourse-is-10-years-old","title":"Concourse is 10 Years Old!","text":"<p>The first commit for Concourse was made April 13th, 2014. That's over 10 years ago! Not sure how all that time flew by, but I guess it means Concourse is \"legacy software\", especially on the timescale of the internet. Concourse is well on its way to getting its pilot license, only four more years to go ( in Canada at least).</p> <p>Concourse has changed so much over the years. Overall I would say Concourse's development has been about slow and thoughtful improvements. I think it's paid off well so far for the project and will continue to do so in the future. There are a lot of exciting things we can still add onto Concourse, and I think there are a lot of existing things we can continue to refine and improve.</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/#v7120","title":"v7.12.0","text":"<p>2024 was a very slow on the release front. We had one Minor release, v7.12.0 and two earlier patch releases for v7.11.0.</p> <p>There was quite a bit of turbulence this year in Concourse's development. Broadcom has unfortunately scaled down the engineering time they dedicate towards Concourse. I picked up the slack mid-way through the year and got v7.12.0 pushed out. I was able to get Broadcom to reaffirm their commitment to providing the infrastructure behind ci.concourse-ci.org.</p> <p>There weren't many features added in v7.12.0. I think the biggest one that's worth shouting out is IPv6 support being added by Qjammer in #8801. This feature only works for the Containerd runtime. This feature is a big push in making sure Concourse is future-proofed as more people build out networks with IPv6 or dual-stack setups.</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/#project-leadership","title":"Project Leadership","text":"<p>Overall it kinda sucked how little development happened this year. Lots of folks where posting messages on GitHub and Discord, asking what was going on with the project. There's definitely been a leadership gap since Alex (vito) left Pivotal/VMware, and therefore the project. (Please don't message him about Concourse stuff, he's moved on to other things).</p> <p>Earlier this year I decided I would try to start filling that leadership gap. This was hard for me do while working a full-time job, but I was able to push out v7.12.0 using my evenings and weekends. It was rewarding but also very draining. I learned that I could not do my full-time job and properly steward Concourse at the same time. Concourse is too large of a project to manage in one's spare time.</p> <p>My reward for pushing out v7.12.0 is that I got people seeing me as the leader of the project. Yay! Goal accomplished! This lead to people in the community reaching out to me and one group came to me with an interesting offer, which I'll talk about more in a bit.</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/#concourse-in-2025","title":"Concourse in 2025","text":"<p>Where is Concourse going in 2025?</p> <p>Right now I'm planning to focus my efforts on refining what we have right now. There are some annoying bugs that I'd like to get fixed, and PR's I want to merge in. I've updated the milestones to reflect what I'm prioritizing: https://github.com/concourse/concourse/milestones</p> <p>There might also be a v8 release this year to bundle together some larger changes, like changing the default worker runtime to containerd. There's also a milestone for this.</p> <p>I am also going to declare Issue Bankruptcy. There are over 700 issues in the main <code>concourse/concourse</code> repo which is completely unmanageable. I want to get the number of open issues down to less than 100. I will keep more recent ones open and anything that's an obvious feature request or bug that could reasonably be done open as well. If I can't grok the issue within a few seconds of looking at it, it's getting closed.</p> <p>I also really want to get an ARM64 version of Concourse officially built. The work on this has already been started by Rui, I just need to pick up the thread from where he left off. I am very confident that we will have official ARM64 builds this year!</p> <p>Now you might be wondering:</p> <p>* Taylor, you just said you can't do your full-time job AND steward Concourse at the same time. How will you find time for all of this?!*</p> <p>My answer to that is: I'm not doing both. I quit my job. I'm doing Concourse full-time.</p> <p>Hooray, problem solved, everything is good now, Concourse has a full-time maintainer again!</p> <p>Okay, of course this problem is not fully solved. How am I going to afford to live?!</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/#a-concourse-company","title":"A \"Concourse\" Company","text":"<p>Some folks in the community reached out to me offering to try starting a company centered around Concourse. I of course said yes! Right now we are a small, dedicated team with a combined 10+ years of experience running Concourse clusters both small and large. The goal of this commercial venture is to advance and sustain then open-source Concourse project. We think Concourse is still the best CI/CD tool out there and that we can make a compelling commercial offering around it.</p> <p>~~If your company may be interested in what a \"Concourse\" company has to offer then please share your email with us here: LINK REMOVED~~</p> <p>May 2025 Update - If you're looking for someone to run a managed Concourse (SaaS) for you please reach out to the folks at CentralCI. If you're looking for commercial support for your on-premise Concourse please reach out to Pixel Air.</p> <p>One last thing I want to mention about this company we're building, because it's important to us, is that we are not taking any kind of VC funding. We are not creating a company that will be focused on \"growth at all costs\". Our focus will be on developing the product (Concourse) and using the product to solve CI/CD problems for customers, and finally catching up to the rest of the CI/CD world with a managed/SaaS version of Concourse. We want to build a sustainable business. We are not building a business to eventually sell; we are building a business that will advance and sustain Concourse and the entire community surrounding it.</p>"},{"location":"blog/2025-01-06-concourse-2024-in-review/#2025","title":"2025","text":"<p>So that's everything about the project right now. If you have any thoughts or comments you can leave them in the discussion thread for this blog post: https://github.com/concourse/concourse/discussions/9048</p> <p>Here's to a better 2025 \u2708\ufe0f\ud83e\udd42</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/","title":"v7.13.0 Release and Project Update","text":"<p>First minor release of 2025! This release is FULL of bug fixes, performance improvements, and even a few new features. I'll start with a quick project update and then dive into the exciting things you can find in this release.</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#concourse-joins-the-cloud-foundry-foundation","title":"Concourse Joins the Cloud Foundry Foundation","text":"<p>Concourse officially joined the Cloud Foundry Foundation back in February. Derek Richard from Broadcom deserves the credit for making this happen. This was one of those tasks that so many folks have wanted to happen over the last few years, but through all the acquisitions (Pivotal -&gt; VMware -&gt; Broadcom) it kept getting started and stopped over and over and over. A HUGE thanks to Derek for finally making this happen.</p> <p>There are still some small stuff transitioning behind the scenes that Derek is taking care of. You can watch our monthly working group meetings for more details or chat with us on Discord if you have any questions about this.</p> <p>Now let's get into the details about 7.13.0!</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#v7130","title":"\ud83c\udf89 v7.13.0","text":"<p>Like any release, I recommend reading the release notes for all the details. I'm going to call out some of the big ticket items here though.</p> <p>Overall this release is light on new features, but is packed full of bug fixes, optimizations, and upgrades.</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#breaking-change-for-pgbouncer-users","title":"Breaking Change for PgBouncer users","text":"<p>We've migrated our database driver from lib/pq to jackc/pgx. <code>lib\\pq</code> has been in maintenance mode for years now and the majority of the Go community now uses <code>pgx</code> as the preferred Postgresql driver.</p> <p>This means we had to remove the <code>CONCOURSE_POSTGRES_BINARY_PARAMETERS</code> flag because it was exposing a feature specific to <code>lib/pq</code>. As far as I know, this flag was introduced and used by PgBouncer users. There is a similar flag that pgx exposes, but based on recent pgx discussions and the PgBouncer release notes, it seems there shouldn't be any issues for PgBouncer users of Concourse as long as you're using PgBouncer &gt;1.21.0. Concourse has never made any promises about compatibility with PgBouncer, but if PgBouncer users have any issues I'll gladly review a PR to improve the situation for you folks.</p> <p>Thankfully this is the only breaking change and I don't plan to make any new breaking changes outside of a Major version bump. The scope of this one seemed quite small which is why I decided to push this out in a Minor version bump.</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#new-features","title":"New Features","text":"<ul> <li>@A1kmm added <code>CONCOURSE_CONTAINERD_PRIVILEGED_MODE</code> to the <code>concourse worker</code> command.   This is useful for the security-conscious operator and limits the permissions that privileged containers can get while   still allowing tools like Podman and Buildah to work. You can also use this flag to disable the use of privileged   containers completely. PR 9017</li> <li>@analytically added a <code>background_filter</code> option to the <code>display</code> field, which   allows you to specify CSS filters on your pipeline   background images. Useful if you're tired of grey backgrounds and want more colour in your pipelines   \ud83c\udf08 PR 9117</li> <li>@IvanChalukov added the <code>--team</code> flag to the <code>containers</code> and   <code>clear-resource-cache</code> fly commands.   PRs 9106, 9107</li> </ul> <p>Like I said, this release is light on new features. Most of the work went into fixing long-standing bugs and updating the code base.</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#uuid-collisions","title":"\ud83d\udcab UUID Collisions?!","text":"<p>If you search \"what are the chances of a UUID collision?\" in your preferred search engine, you'll find many comments stating \"You're more likely to be killed by a gamma ray than get a UUID collision\", usually with a caveat that you're generating v4 UUID's. Well, I have another caveat to add to that statement: the library you're using to generate UUID' s implements RFC 9562 correctly!</p> <p>If you want one reason to upgrade to 7.13.0, I'd say this is why. All versions prior to this release will occasionally have UUID collisions when creating containers and volumes. I'm not sure how often this happens, it could be every 1,000 or 100,000,000, calls to <code>uuid.New()</code>, but it definitely does happen. On a low-usage cluster we only saw it once. The error we saw was this one:</p> <pre><code>container \"bba06975-46f6-4bbe-73f5-9e39a869719a\": already exists\n</code></pre> <p>The UUID library we were using was the most popular library at the time. Concourse has been using this library from the very beginning. I'm sure it's been the source of a handful of weird \"ghost in the machine\" type of errors over the years. We can now lay this ghost to rest! We are now using the github.com/google/uuid library instead.</p> <p>PR 9083 for full details.</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#fly-login-with-chrome","title":"Fly Login With Chrome","text":"<p>If you use a Chromium web browser and have tried to <code>fly login</code> you've definitely run into this bug where Chrome says \" your token could not be sent to fly\", but saw that it was sent to fly. What gives? Preflight requests is what gives! You write your web app to send one HTTP request and SURPRISE, Chrome sends a bonus second request first!</p> <p>The <code>fly</code> CLI will now handle this \u2b50special bonus\u2b50 preflight request from Chrome, give Chrome the thumbs up, and then wait until the real request from Concourse actually comes through. You'll now see a \"login successful!\" message when you <code>fly login</code> now.</p> <p>PR 9051 for full details.</p> <p>There is of course another browser bug that's been annoying users for the last few years...</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#preserve-existing-browser-session","title":"Preserve Existing Browser Session","text":"<p>After you did that janky <code>fly login</code> with Chrome, you'd then run into another issue. You had a tab open and logged into Concourse before logging in with <code>fly</code>. Now when you go back to that tab and click around in the UI you get told to login AGAIN. What gives?!</p> <p>@IvanChalukov dove into this issue and fixed it. I can only imagine the amount of hours he spent debugging this issue. The TL;DR is that a CSRF token got wrapped in quotes and the quotes were then considered part of the token, resulting in an invalid CSRF check server-side. Thank you Ivan for fixing this mild but annoying bug. Many browser sessions will now be saved! \ud83d\ude4f</p> <p>PR 9109 for full details.</p>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#everything-else","title":"Everything Else","text":"<p>Those are the big bugs that I think most people will be excited to see fixed. There's also been a lot of chore-level changes as well. Concourse is quite a big application and has a lot of dependencies. I think we've done a good job of dusting things off and upgrading things. Here's a quick rundown of the other exciting things in this release:</p> <ul> <li>This release is built with Go 1.24, gaining the performance improvements from that   release, which boasts a 2-3% decrease in CPU</li> <li>@analytically upgraded a bunch of stuff in the web frontend:<ul> <li>Upgraded Graphviz from v2.47.1 to v12.2.1</li> <li>Upgraded D3js from v3.5.5 to v7</li> <li>Updated Material Design Icons from v5.0.45 to 7.4.47, adding 2,350 new icons to use in your pipelines!</li> <li>Reduced the size of all SVG's using svgo</li> <li>Replaced the unmaintained NYTimes/gziphandler   with klauspost/compress, giving a performance boost to all HTTP   endpoints.</li> </ul> </li> <li>@IvanChalukov and @Kump3r upgraded our fork of Dex to   use the CF v3 API, ensuring users of CF Auth aren't locked out of their Concourse when the CF v2 API goes away.</li> <li>The experimental warning seen during the <code>set_pipeline</code> step is now gone. It's interface and current behaviour are   considered stable.</li> <li>Modernization of the Go codebase, mostly removing usage of deprecated Go functions and other similar improvements.</li> <li>The registry-image, s3,   and semver resources have been updated to use v2 of the AWS Go SDK.   While making this change I've also changed the authentication behaviour when these resources are interacting with AWS.   They will now use the default authentication chain that the AWS SDK uses. This means these resource types can now use   the Concourse Worker's IAM role and static credentials are no longer required to use these resources.</li> <li>@mariash enabled cgroupv2 support for the Guardian runtime, enabling the use of the   Guardian runtime on newer kernel versions.</li> <li>I updated Concourse's seccomp profile to include newer syscalls, bringing it inline with the default seccomp profile   from Docker and Containerd.</li> <li>Shout-out to @aliculPix4D and the folks at Pix4D for testing and finding some last   minute critical bugs before the release went out.</li> <li>Shout-out to @IvanChalukov and @Kump3r from SAP for   their help testing Concourse on Bosh.</li> </ul> <p>Again, read the release notes for everything that's been fixed or improved.</p> <p>Thank you everyone that contributed to this release. If you find any bugs, open an issue on GitHub.</p> <ul> <li>GitHub Release</li> <li>Docker Hub Image</li> <li>Helm Chart</li> <li>Bosh Release</li> </ul>"},{"location":"blog/2025-04-03-v7130-release-and-project-update/#one-more-thing","title":"\ud83c\udf4e One More Thing...","text":"<p>Myself and my co-founders are happy to officially launch CentralCI, a company providing managed Concourse clusters. We run Concourse for you, owning the operational overhead of running Concourse. We've put a lot of effort into solving some common pains that operators of Concourse have, allowing you to focus on writing your pipelines.</p> <p>You can learn more about CentralCI in our introduction blog post.</p>"},{"location":"blog/2025-05-08-may-2025-project-update/","title":"May 2025 Project Update","text":"<p>It's the May 2025 project update! I'm going to try and do this at a somewhat regular cadence. Once a month seems like a good idea, though I may skip a month here and there if there's nothing interesting to write about.</p>"},{"location":"blog/2025-05-08-may-2025-project-update/#administrative","title":"Administrative","text":"<p>I've opened up a discussion thread about the breaking changes currently planned to go into v8 of Concourse. Nothing is set in stone, all feedback is welcome! Likely the most disruptive one so far is to finally enforce the valid identifiers instead of warning when they're detected. See the link for full details and reasoning.</p> <p>The Concourse Working Group has been meeting once per month. You can view past meetings on YouTube and view meeting notes here.</p> <p>The last thing I want to mention is that I decided to step away from CentralCI. They are still operating and working at making a fantastic managed Concourse that you can go and buy right now!!! If you want Concourse but don't want the overhead of running it, I highly recommend reaching out to them. I will be focusing my time on the community and those currently running Concourse on-premise.</p> <p>If you want to support my work stewarding Concourse you can sponsor me on GitHub. Thank you to those who already found the page and are sponsoring me \ud83d\udc99</p>"},{"location":"blog/2025-05-08-may-2025-project-update/#roadmap","title":"Roadmap","text":"<p>I have two project boards that I'm using to track work right now.</p> <ul> <li>Roadmap</li> <li>Pull Requests</li> </ul> <p>The Roadmap board is tracking work that I am actively working on and planning to work on next. Right now the ARM build stuff is taking up most of my time. I have to update some of the tooling we use to build images (the registry-image resource and oci-build task) to make it easier to build and push multi-arch images. I'm also taking the time to clean up a bunch of stuff in the <code>concourse/ci</code> repo. It's a lot of work, but it's slowly happening. I see no major roadblocks with this work right now.</p> <p>The Pull Requests board is a place for me to see all open pull requests across all repositories. I've added most open PR's to this board. If you see your PR on this board, that means I'm aware of it, and I'll get to it eventually.</p>"},{"location":"blog/2025-05-08-may-2025-project-update/#s3-resource-type","title":"S3 Resource Type","text":"<p>I made a few breaking changes with the S3 resource type that came out with 7.13. The v2.0.0 release notes go over those. I did not hit the mark with these changes sadly. Users found that they couldn't pull down items from public S3 buckets. Resources pulling from public buckets were using no credentials previously, but with v2 they now tried to use any credentials the default SDK authentication chain could find, which usually ended up being an EC2 instance profile. I then made a PR to fix this, but then that made it, again, impossible for users to use the EC2 instance profile. Fritz then made a PR that adds a new flag to the S3 resource, <code>enable_aws_creds_provider</code>, which allows the behaviour introduced in v2 to happen if you want it to.</p> <p>I published a new version of the resource type with this PR, v2.2.0. If you want to use that in your pipelines you can do so with this snippet:</p> <pre><code>resource_types:\n  - name: s3\n    type: registry-image\n    source:\n      repository: docker.io/concourse/s3-resource\n      tag: 2.2.0\n</code></pre> <p>That will override the S3 resource type found on the worker for pipelines that you add this to. Apologies again for the blunder and thank you to those who helped get the resource back in a state that works for everyone.</p>"},{"location":"blog/2025-05-08-may-2025-project-update/#vs-code-extension","title":"VS Code Extension","text":"<p>Shouting this out as it looks handy. A user made a VS code extension for previewing Concourse pipelines. You can see their post where that shared that here: https://github.com/concourse/concourse/discussions/9193</p>"},{"location":"blog/2025-05-08-may-2025-project-update/#cloud-foundry-day-north-america","title":"Cloud Foundry Day North America","text":"<p>Cloud Foundry Day is happening next week on May 14th. Derek (my co-lead of the Concourse working group) has a talk about Concourse's journey into the Cloud Foundry foundation. Make sure you check it out and say hi to Derek if you'll be there.</p>"},{"location":"blog/2025-05-08-may-2025-project-update/#thats-all-folks","title":"That's All Folks!","text":"<p>And that's the update! If anyone wants to chat I'm always hanging around on Discord. You can also reach me at <code>dev</code> at <code>taydev.net</code>.</p>"},{"location":"blog/2025-08-11-v7140---the-first-arm-build-of-concourse/","title":"v7.14.0 - The First ARM Build of Concourse","text":"<p>v7.14.0 is out and with it also comes the first ARM build of Concourse! There were a lot of behind the scenes changes that were required to make this happen, so let's dive into everything that's in this release.</p>"},{"location":"blog/2025-08-11-v7140---the-first-arm-build-of-concourse/#pipeline-identity-tokens","title":"\ud83e\udec6Pipeline Identity Tokens","text":"<p>I figure we'll start with the new features, because that's always fun. #9035 added a whole new <code>var_source</code> called \"Identity Tokens\". These are JWT's that Concourse can generate for you and that you can then use to authenticate to third-party systems that support \"identity federation\", such as Vault, AWS, and Azure.</p> <p>@dbaumgarten did everything here: wrote the RFC, made the PR, AND wrote a comprehensive set of docs with examples on how to use it. Huge thank you to him for bringing this feature to the community. I think a lot of users will find it useful and help them migrate away from using static credentials.</p>"},{"location":"blog/2025-08-11-v7140---the-first-arm-build-of-concourse/#the-road-to-arm","title":"\ud83e\uddbe The Road to ARM","text":"<p>This has been a longgggggg time coming. There was a community fork of Concourse out there for a while, specifically for building an ARM version of Concourse. I remember when Ciro, a co-worker from Pivotal, did a little exploration running Concourse on his Raspberry Pi. He showed it was possible, though the road wasn't completely smooth.</p> <p>Thankfully, the container ecosystem has continued to develop and mature these last few years while Concourse was in limbo. More workloads are running on ARM now and as a result more of our tools and libraries just work when trying to do cross-compilation or ARM stuff. The big win for us was Docker making multi-platform builds a thing. This saved us from having to manually build an ARM Concourse worker, like Ciro did, just so we could build ARM images of Concourse and all the resource-types that we ship with Concourse.</p> <p>At this point, all we needed to do was update our pipelines to support building and releasing ARM versions of everything!</p> <p>The journey went like this:</p> <ul> <li>Update all 12 base resource-types to use   Wolfi as their base image because the   previous base image did not have an ARM variant. The <code>concourse/concourse</code>   image also uses Wolfi as its base image now.</li> <li>Update the OCI Build task and   Registry Image   resource to better   support multi-platform images and workflows.</li> <li>Update the pipeline used to build, test, and release the resource-types, and   release ARM versions of all the resource-types.</li> <li>Update the main Concourse pipeline and the release pipeline to build and test the ARM variant of Concourse.</li> <li>Added a <code>/download-fly</code>   page to support all variants of <code>fly</code>, replacing the three static download   links previously located in the footer</li> </ul> <p>Each of those steps took many hours to complete. I had to touch every repository we own and got to do a little clean-up everywhere.</p> <p>One nice side effect of moving to Wolfi for the base image is that the size of all the container images we produce dropped. Some by a lot, some by just a little. For example, the Git resource went from 218MB to 37MB, a massive drop in size! Some other resources dropped by only a few MB's. The size drops weren't 100% due to Wolfi. I took some extra time to ensure we were only adding what was necessary for each resource-type to function.</p> <p>Collectively, this results in us shipping a much smaller <code>concourse/concourse</code> image. v7.14.0 clocks in at 928MB (x86_64) and 883MB (ARM), down from 1.41GB (x86_64). That's a 34% drop in size for the x86_64 image and 37% for the ARM image. The ARM-based images are also always smaller, so you save a bit more disk space if you go for a fully ARM-based Concourse deployment.</p> <p>Now there is finally an ARM version of Concourse and fly that folks can run on their Raspberry Pi's, M-Series macs, and ARM cloud servers. I'm excited to see what Concourse will end up running on now \ud83c\udf89</p>"},{"location":"blog/2025-08-11-v7140---the-first-arm-build-of-concourse/#cloud-foundry-foundation","title":"\u2601\ufe0f Cloud Foundry Foundation","text":"<p>As a project member of the Cloud Foundry Foundation (CFF), Concourse has had two tasks assigned to it.</p> <ol> <li>Reduce project cost: We've reduced project costs by    40%,    leaving us in a decent state. There's some smaller things we can tackle, but    urgency from CFF is gone.</li> <li>Running a shared Concourse    cluster for CFF member    projects. This is related to the CFF's wider goal of reducing costs across    all projects. This is something myself and Derek plan to work on over the    next few months.</li> </ol>"},{"location":"blog/2025-08-11-v7140---the-first-arm-build-of-concourse/#whats-next","title":"\ud83e\udded What's Next","text":"<p>My goal with Concourse right now is to continue to refine and improve what we currently have. There are plenty of little bugs littered throughout the code base that I want to resolve to help make Concourse feel even more stable and reliable than it currently is.</p> <p>I try to keep this GitHub project board up to date with what I plan to work on. Folks are free to look at the board, and if it isn't in the \"In Progress\" column, feel free to pick up the issue and work on it.</p> <p>I'll be putting a lot of my attention to the breaking changes planned for v8. See this discussion post for details and leave any thoughts or comments there.</p> <p>That's all I have for everyone now. See everyone over on Discord or GitHub Discussions. Enjoy the new release!</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/","title":"v8 - New Year, New Concourse Release","text":"<p>v8.0.0 of Concourse is out! I'm going to explain why a new major was needed and flag anything that pipeline writers and Concourse operators should keep an eye out for.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#new-website","title":"\u2728 New Website","text":"<p>First, huge thanks to kcbimonte for doing the tedious work of migrating the website off of booklit and onto MkDocs. We will eventually move it all over to Zensical once they have all the features we need.</p> <p>My hope with the new site is that it's easier for users to contribute to the docs now. Booklit had its pros, particularly with how internal links were handled, but it was definitely a bit of a challenge to extend and change. It was also quite a hurdle for new users to understand.</p> <p>Now, onto the new release!</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#why-a-new-major-version","title":"Why a New Major Version?","text":"<p>As a project, Concourse avoids making actually breaking changes. For example, a pipeline from v4 of Concourse should still continue to work on v8 of Concourse. That said, we have made major version bumps over the years, but they're usually not for \"breaking changes\" in the traditional sense.</p> <p>With Concourse, major version bumps have been motivated by large/long-running database migrations. Those were the reasons behind the v5, v6, and v7 major bumps. That was also the initial reason for this new major, kicked off by PR #9165.</p> <p>#9165 changes how Concourse stores resource versions, using SHA256 instead of MD5 to create and store digests. This helps operators running Concourse in secure environments, where algorithms like MD5 are not allowed.</p> <p>The migration from this PR alone will take a few minutes to run. I recommend scaling down your web nodes to one node and then upgrading that single node. I've tested running this migration on ci.concourse-ci.org and it took ~5 minutes on a Postgresql instance with 4 vCPU and 5GB of RAM. We have <code>4,358,045</code> records in our <code>resource_config_versions</code> table, which is the main table modified in the migration. You can run the following query against your Concourse database to check how many records your table has:</p> <pre><code>SELECT COUNT(*) FROM resource_config_versions;\n</code></pre> <p>This is very much a \"behind the scenes\" change, as users shouldn't see anything different happening with regards to how their pipelines behave. This change future-proofs Concourse and enables Concourse to continue running in secure environments around the world.</p> <p>Not wanting to let a good major go to waste, I decided to also tackle a lot of outstanding chore issues to improve and refresh Concourse a bit. Let's go over all the big changes that landed in v8. Full release notes can be found on Github: https://github.com/concourse/concourse/releases/tag/v8.0.0</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#changing-defaults","title":"\ud83d\udee0\ufe0f Changing Defaults","text":""},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#container-runtime","title":"\ud83d\udce6 Container Runtime","text":"<p>We've changed the default container runtime to <code>containerd</code>! Guardian is still a supported runtime but is no longer the default. The main motivation for this change was the issues we'd see from users trying to run Guardian on newer kernel versions, specifically systems only using cgroups v2. <code>containerd</code> has proven very stable for us and many users in the community.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#secret-redaction","title":"\ud83d\udd12 Secret Redaction","text":"<p>Secret redaction is now always enabled and its corresponding feature flag has no effect. This feature was initially feature flagged to monitor for any performance issues. We have seen none over the many years that this feature has been available. We consider this feature safe to use in all production environments.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#instance-pipelines-and-across-step","title":"\ud83d\udd00 Instance Pipelines and <code>across</code> Step","text":"<p>Instance pipelines and <code>across</code> steps are now always enabled, and their feature flags have no effect. Their \"experimental\" labels and warnings have also been removed. Both features have been available for years now and I see no reason to change their current behaviour. Users can consider these features stable.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#inputs-for-put-steps","title":"\u27a1\ufe0f Inputs for <code>put</code> Steps","text":"<p>The default value for <code>put.inputs</code> is changed from <code>all</code> to <code>detect</code>. Streaming all volumes into a <code>put</code> step has always been overkill and results in long initialization times for <code>put</code> steps. <code>detect</code> works by reviewing all values in <code>put.params</code> and only mounting volumes with matching names.</p> <p>This is a breaking change if you have a custom resource type that has hard-coded the expected names of volumes into your scripts. I haven't seen any resources like this personally, but if they do exist, simply set <code>put.inputs</code> to <code>all</code> or an array of the volumes to mount in your put step.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#new-features-and-enhancements","title":"\u2708\ufe0f New Features and Enhancements","text":"<p>It wouldn't be a major release without a couple new features and enhancements.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#broadcast-message-system","title":"\ud83d\udce2 Broadcast Message System","text":"<p>Operators can now broadcast messages to their Concourse cluster and it will appear as a banner in the Concourse web UI. Thank you to ceco556 for finishing the implementation.</p> <p>There are three <code>fly</code> commands used to manage this system:</p> <ul> <li><code>fly set-wall</code><ul> <li>Can only be used by users of the <code>main</code> team</li> </ul> </li> <li><code>fly get-wall</code><ul> <li>Can be used by all users</li> </ul> </li> <li><code>fly clear-wall</code><ul> <li>Can only be used by users of the <code>main</code> team</li> </ul> </li> </ul> <p>Fun Fact!</p> <p>\"Wall\" is a reference to the Unix <code>wall</code> CLI.</p> <p>When setting the message with <code>fly set-wall</code>, you can include emojis and links. Links will be parsed and turned into HTML links that users can click. We made sure that no XSS vulnerabilities were introduced.</p> <p></p> <p>Concourse dashboard showing a broadcast message</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#build-urls-for-step-metadata","title":"\ud83c\udff7\ufe0f Build URLs for Step Metadata","text":"<p>There are two new handy env vars made available to <code>get</code> and <code>put</code> steps:</p> <ul> <li><code>BUILD_URL</code> - Same URL as what you see in the web UI that includes team, pipeline, and job name</li> <li><code>BUILD_URL_SHORT</code> - Shorter URL that uses the build's internal ID instead of the pipeline scoped ID</li> </ul> <p>See the docs for more details.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#team-name-in-set_pipeline-steps","title":"\ud83d\udc65 Team Name in <code>set_pipeline</code> Steps","text":"<p>Users of the <code>main</code> team have been able to set pipelines for other teams for quite a while now. In the web UI though, it wasn't obvious that a pipeline was being set for another team. The team name is now displayed when setting a pipeline for another team.</p> <p></p> <p>Concourse build logs show <code>set_pipeline</code> steps with the team name</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#highlighted-group-tabs","title":"\ud83d\udea5 Highlighted Group Tabs","text":"<p>To make finding failed builds even easier, especially on large pipelines with lots of jobs and groups, group tabs will be highlighted red/orange.</p> <p></p> <p>Concourse group tabs highlighted red</p> <p>Thank you to analytically for these last three enhancements! He's made many other useful contributions this release that you can find in the release notes.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#tasks-can-use-entrypointcmd","title":"\u25b6\ufe0f Tasks Can Use <code>ENTRYPOINT</code>/<code>CMD</code>","text":"<p>If you don't define a <code>run.path</code> in your Task's config, Concourse will now fall back to running the <code>ENTRYPOINT</code>/<code>CMD</code> defined in the container image. If you define any <code>run.args</code> in your Task's config, those will be appended to the <code>ENTRYPOINT</code>/<code>CMD</code> from the container. This is the same behaviour you get with <code>[docker/podman] run</code> today.</p> <p>To be clear, Concourse still uses its custom rootfs format behind-the-scenes. I assume the majority of users use the registry-image to pull in their images in rootfs format and can therefore leverage this feature. If you have some tooling that creates its own rootfs formatted images, you can review this PR for how to pass that info along to Concourse.</p>"},{"location":"blog/2026-01-15-v8---new-year-new-concourse-release/#2026-and-support","title":"2026 and Support","text":"<p>Thank you to everyone who contributed to this new release. There are already more PRs open that will land in v8.1.0 in a few months. Moving forward, I'm planning to make new minor releases approximately every quarter. Patches will be released for any critical bugs and security fixes as needed. My main focus will be on continuing to make Concourse more robust and stable.</p> <p>I am still looking for more commercial users of Concourse that are interested in supporting the project. If your company uses Concourse and is interested in seeing the continued development and maintenance of the project, please reach out to me. You get direct access to me to address any Concourse issues you have, as well as training on Concourse best practices.</p> <p>Thank you to all the individuals and the organization (SAP\ud83d\udc99) that support me over on GitHub as well. They, along with my commercial customers, have made this first year of Concourse development possible! \u2708\ufe0f</p>"},{"location":"docs/","title":"Docs","text":"<p>Concourse is a pipeline-based continuous thing-doer.</p> <p>The term \"pipeline\" has become widespread in CI discussions, so being precise about what this means is important; Concourse's pipelines differ significantly from others.</p> <p>Pipelines are built around Resources, which represent all external state, and Jobs, which interact with them. Concourse pipelines function as dependency flows, similar to distributed <code>Makefile</code>s. Pipelines are designed to be self-contained to minimize server-wide configuration. Maximizing portability also reduces risk, making it simpler for projects to recover from CI disruptions.</p> <p>Resources like the <code>git</code> resource and  <code>s3</code> resource are used to express source code, dependencies, deployments, and other external states. This interface also models more abstract concepts like scheduled or interval triggers, via the <code>time</code> resource.</p> <p>Resource Types are defined within the pipeline itself, making the pipelines more self-sufficient while keeping Concourse lean and versatile without needing a complex plugin system.</p> <p>Jobs are sequences of <code>get</code>, <code>put</code>, and <code>task</code> steps to execute. These steps determine the job's inputs and outputs. Jobs are designed to be idempotent and loosely coupled, allowing the pipeline to evolve with project needs without requiring engineers to maintain too much context simultaneously.</p> <p>Everything in Concourse runs in a container. Instead of modifying workers to install build tools, Tasks define their own container image (typically using Docker images via the  <code>registry-image</code> resource).</p>"},{"location":"docs/#what","title":"...What?","text":"<p>Concourse admittedly has a steeper learning curve initially, and depending on your background it might seem like a lot to grasp. A key goal of this project is for that curve to flatten out shortly after and lead to greater productivity and reduced stress over time.</p> <p>If this all sounds confusing, that's OK - you may want to simply continue onward, start experimenting a bit, and use the above as a quick reference of the \"big picture\" as your understanding develops.</p>"},{"location":"docs/builds/","title":"Builds","text":"<p>A build is an execution of a build plan, which is either</p> <ul> <li>configured as a sequence of steps in a job</li> <li>generated by the Resource Checker to run   a <code>check</code></li> <li>submitted directly to Concourse as a one-off build via <code>fly execute</code></li> </ul> <p>Containers and volumes are created as <code>get</code> steps, <code>put</code> steps, and <code>task</code> steps run. When a build completes successfully, these containers go away.</p> <p>A failed build's containers and volumes are kept around so that you can debug the build via <code>fly intercept</code>. If the build belongs to a job, the containers will go away when the next build starts. If the build is a one-off, its containers will be removed immediately, so make sure you intercept while it's running, if you want to debug.</p>"},{"location":"docs/builds/#rerunning-a-build","title":"Rerunning a Build","text":"<p>Concourse supports build rerunning, which means to run a new build using the exact same set of input versions as the original build. There are two ways to rerun a build: through the web UI on the builds page and through the <code>fly rerun-build</code>.</p> <p>When a build is rerun, it will create a new build using the name of the original build with the rerun number appended to it, e.g. <code>3.1</code> for the first rerun of build <code>3</code>.</p> <p>Rerun builds are ordered chronologically after the original build, rather than becoming a new \"latest\" build. Similarly, when the scheduler is resolving <code>passed</code> constraints that reference a job with rerun builds, those rerun builds are processed in this same order. This ensures that the versions, which made it through a rerun build, do not become the new \"latest versions\". Instead, they act as if the original build had succeeded at its point in the build history.</p> <p>This may sound a little confusing, but the summary is that reruns should behave as if they replace the original failed build.</p>"},{"location":"docs/builds/#current-caveats-with-rerunning","title":"Current caveats with rerunning","text":"<p>The current implementation of rerunning is an early iteration with one key limitation: a rerun build will use the current state of the job config, instead of running the exact build plan the original build ran with.</p> <p>This means that if the <code>job.plan</code> has changed in a way that is backwards-incompatible, the rerun build may error. For example, if a new input is added, its version will not be available as the original build did not use it.</p>"},{"location":"docs/builds/#fly-builds","title":"<code>fly builds</code>","text":"<p>To list the most recent builds, run:</p> <pre><code>fly -t example builds\n</code></pre> <p>To list the builds of a job, run:</p> <pre><code>fly -t example builds -j pipeline-name/job-name\n</code></pre> <p>This can be useful for periodically monitoring the state of a job. The output also works well with tools like <code>awk</code> and <code>grep</code>.</p> <p>By default, the most recent 50 builds are shown. To see more builds, use the <code>-c</code> flag, like so:</p> <pre><code>fly -t example builds -c 100\n</code></pre> <p>To see builds within a certain time range, you can use <code>--since</code> and <code>--until</code>. You can use one or both flags to filter builds. The flags accept a time format of <code>yyyy-mm-dd HH:mm:ss</code>.</p> <pre><code>fly -t example builds --since '2006-02-06 00:00:00'\n</code></pre>"},{"location":"docs/builds/#fly-intercept","title":"<code>fly intercept</code>","text":"<p>Sometimes it's helpful to connect to the machine where tasks run. This way you can either profile or inspect tasks, or see the state of the machine at the end of a run. Due to Concourse running tasks in containers on remote machines this would typically be hard to access.</p> <p>To this end, there is a <code>fly intercept</code> command that will give you an interactive shell inside the specified container. Containers are identified by a few things, so you may need to specify a few flags to hone down the results. If there are multiple containers that the flags could refer to, an interactive prompt will show up allowing you to disambiguate.</p> <p>For example, running the following will run a task and then enter the finished task's container:</p> <pre><code>fly -t example execute\nfly -t example intercept --step build\n</code></pre> <p>Windows Workers</p> <p>When intercepting a task running on a Windows worker, you will need to specifically tell fly to run <code>powershell</code>:</p> <pre><code>fly -t example intercept powershell\n</code></pre> <p>Containers are around for a short time after a build finishes in order to allow people to intercept them.</p> <p>You can also intercept builds that were run in your pipeline. By using <code>--job</code>, <code>--build</code>, and <code>--step</code> you can intercept a specific step from a build of a job in your pipeline. These flags also have short forms, like so:</p> <pre><code>fly -t example intercept -j some-pipeline/some-job -b some-build -s some-step\n</code></pre> <p>Note that <code>--build</code> can be omitted, and will default to the most recent build of the job. One-off builds can be reached by passing in their build ID to <code>--build</code> which can be found on the build list page.</p> <p>The <code>--step</code> flag can also be omitted; this will let you pick the step interactively, if you don't know the exact name.</p> <p>Resource checking containers can also be intercepted with <code>--check</code> or <code>-c</code>:</p> <pre><code>fly -t example intercept --check some-pipeline/some-resource\n</code></pre> <p>A specific command can also be given, e.g. <code>fly intercept ps auxf</code> or <code>fly intercept htop</code>. This allows for patterns such as <code>watch fly intercept ps auxf</code>, which will continuously show the process tree of the current build's task, even as the \"current build\" changes.</p> <p>The working directory and any relevant environment variables (e.g. those having come from <code>task</code> step <code>params</code>) used by the original process will also be used for the process run by intercept.</p>"},{"location":"docs/builds/#fly-abort-build","title":"<code>fly abort-build</code>","text":"<p>To abort a build of a job, run:</p> <pre><code>fly -t example abort-build --job my-pipeline/my-job --build 3\n</code></pre> <p>This will cancel build <code>3</code> of the <code>my-job</code> job in the <code>my-pipeline</code> pipeline.</p>"},{"location":"docs/builds/#fly-watch","title":"<code>fly watch</code>","text":"<p>Concourse emits streaming colored logs on the website, but it can be helpful to have the logs available to the command line (e.g. so that they can be processed by other commands).</p> <p>The <code>watch</code> command can be used to do just this. You can also view builds that are running in your pipeline, or builds that have already finished.</p> <p>Note that unlike <code>fly execute</code>, killing <code>fly watch</code> via <code>SIGINT</code> or <code>SIGTERM</code> will not abort the build.</p> <p>To watch the most recent one-off build, just run <code>fly watch</code> with no arguments. To watch a specific build (one-off or not), pass <code>--build</code> with the ID of the build to watch. This ID is available at the start of <code>fly execute</code>'s output or by browsing to the builds list in the web UI.</p> <p>By using the <code>--job</code> and <code>--build</code> flags you can pick out a specific build of a job to watch. For example, the following command will either show the archived logs for an old build, if it has finished running, or it will stream the current logs, if the build is still in progress.</p> <pre><code>fly -t example watch --job my-pipeline/tests --build 52\n</code></pre> <p>If the <code>--job</code> flag is specified and <code>--build</code> is omitted, the most recent build of the specified job will be selected.</p> <p>If there is a mismatch between the <code>fly</code> and <code>web</code> versions, it is possible to run into <code>failed to parse next event: unknown event type</code> error. The <code>--ignore-event-parsing-errors</code> flag can be passed to ignore such errors.</p>"},{"location":"docs/config-basics/","title":"Config Basics","text":"<p>Concourse configuration for things like Pipelines and Tasks is done through declarative YAML files.</p> <p>Concourse configuration supports basic variable substitution by way of <code>((vars))</code>. There is no built-in support for fancier templating constructs, e.g. loops and conditionals; users are free to use whatever templating system they like.</p>"},{"location":"docs/config-basics/#intro-to-yaml","title":"Intro to YAML","text":"<p>YAML is a human-friendly syntax for writing structured documents. You can think of it as JSON without the sharp edges.</p> <p>If you want a slightly more in-depth overview of YAML compared to what we provide below, we recommend reading Learn YAML in Y Minutes.</p> <p>Here's a quick example demonstrating common YAML syntax:</p> <pre><code># commented lines are prefixed with the '#' character\n\n# strings\nquoted_string: \"bar\"\nunquoted_string: hello world!\nmultiline_string: |\n  hello, world!\n  this is one big string with a trailing linebreak!\n\n# arrays\narray: [ hello, world ]\nmultiline_array:\n  - hello\n  - world\n\n# objects\nobject: { one: uno, two: dos }\nmultiline_object:\n  one: uno\n  two: dos\n\n# boolean values\nbooleans: [ true, false ]\n\n# numeric values\nnumeric: [ 1234, 12.34 ]\n</code></pre>"},{"location":"docs/config-basics/#yaml-tips-tricks","title":"YAML Tips &amp; Tricks","text":"<p>YAML anchor syntax can be used to avoid repetition within configuration.</p> <p>For example, the following YAML document...:</p> <pre><code>large_value: &amp;my_anchor\n  do_the_thing: true\n  with_these_values: [ 1, 2, 3 ]\n\nduplicate_value: *my_anchor\n</code></pre> <p>...is exactly equivalent to:</p> <pre><code>large_value:\n  do_the_thing: true\n  with_these_values: [ 1, 2, 3 ]\n\nduplicate_value:\n  do_the_thing: true\n  with_these_values: [ 1, 2, 3 ]\n</code></pre> <p>If you find yourself repeating configuration throughout your pipeline, it may be a sign that Concourse is missing some kind of abstraction to make your pipeline less verbose. If you have the time and are interested in helping out with Concourse's design, feedback of this sort is welcome in GitHub Discussions!</p> <p>We do want to avoid implementing an entire YAML templating engine within Concourse. We encourage you to reach for your favourite templating tool if you're eager about DRYing up your pipelines as much as possible.</p>"},{"location":"docs/config-basics/#yaml-quirks","title":"YAML Quirks","text":"<p>YAML has some weird parts. For example, all the following terms are acceptable boolean values: <code>true</code>, <code>false</code>, <code>yes</code>, <code>no</code>, <code>on</code>, <code>off</code>.</p> <p>YAML is also whitespace-sensitive. For the most part, this is really handy because it keeps you from having to count curly-braces in deeply nested parts of configuration such as <code>job.plan</code>. Sometimes, though, it can be hard to keep track of the correct indentation level.</p>"},{"location":"docs/config-basics/#basic-schemas","title":"Basic Schemas","text":"<p>Throughout the Concourse documentation you will come across schema definitions for each API.</p> <p>The following are basic schema definitions that the other schemas refer to. You can probably skip past this and just make assumptions along the way; this is just here for completeness!</p>"},{"location":"docs/config-basics/#number-schema","title":"<code>number</code> schema","text":"<p>Any integer, i.e. <code>1000</code>.</p>"},{"location":"docs/config-basics/#string-schema","title":"<code>string</code> schema","text":"<p>An arbitrary string value with no content restrictions.</p>"},{"location":"docs/config-basics/#config-schema","title":"<code>config</code> schema","text":"<p>An arbitrary object representing configuration that is not directly interpreted by Concourse - typically given to a resource type.</p> <pre><code>uri: https://github.com/vito/booklit\nbranch: master\n</code></pre> <p>All object keys must be strings, preferably <code>snake_cased</code>.</p>"},{"location":"docs/config-basics/#vars-schema","title":"<code>vars</code> schema","text":"<p>An arbitrary object representing key-value definitions for <code>((vars))</code>.</p> <p>As with <code>config</code> schema, all object keys must be strings, preferably <code>snake_cased</code>.</p>"},{"location":"docs/config-basics/#env-vars-schema","title":"<code>env-vars</code> schema","text":"<p>An object containing string keys and string values. Each pair represents an environment variable to set to the given value.</p> <pre><code>SOME_SIMPLE_VAR: simple-var\nSOME_LONG_VAR: |\n  This is an example of using YAML multi-line string syntax to set a very\n  long environment variable value.\nSOME_NUMERIC_VALUE: \"1\"\n</code></pre> <p>Note that in the last example we took special care to quote the number.</p>"},{"location":"docs/config-basics/#boolean-schema","title":"<code>boolean</code> schema","text":"<p><code>true</code> or <code>false</code>.</p> <p>YAML also supports the alias <code>yes</code>, <code>no</code>, <code>on</code>, or <code>off</code>, but ... please don't.</p>"},{"location":"docs/config-basics/#value-schema","title":"<code>value</code> schema","text":"<p>An arbitrary YAML value. It may be a <code>number</code> schema, <code>string</code> schema,  <code>boolean</code> schema, <code>config</code> schema, or a <code>[value</code> schema<code>]</code>.</p> <pre><code>values:\n  - 123\n  - bar\n  - true\n  - key1: abc\n    key2: def\n  - [ hello, world ]\n</code></pre>"},{"location":"docs/config-basics/#identifier-schema","title":"<code>identifier</code> schema","text":"<p>An identifier is a string value. The following defines the allowed character set for an identifier:</p> <ul> <li>Unicode lowercase letters, while still supporting languages that don't have any casing (e.g. Japanese).</li> <li>Decimal numbers.</li> <li>Hyphens <code>-</code> and underscores <code>_</code> as word separators.</li> <li>Periods <code>.</code> in order to support domain names and version numbers.</li> </ul> <p>The validation rule is as follows:</p> <pre><code>^[\\p{Ll}\\p{Lt}\\p{Lm}\\p{Lo}\\d][\\p{Ll}\\p{Lt}\\p{Lm}\\p{Lo}\\d\\-_.]*$\n</code></pre> <p>Where all identifier must start with a Unicode lowercase letter or number, followed by any number of allowed characters.</p> <p>Currently, the validation will only show as warnings. For the sake of future-proofing, you may want to conform to it.</p>"},{"location":"docs/config-basics/#dir-path-schema","title":"<code>dir-path</code> schema","text":"<p>A string value specifying a (typically relative) path of a directory.</p>"},{"location":"docs/config-basics/#file-path-schema","title":"<code>file-path</code> schema","text":"<p>A string value specifying a (typically relative) path of a file.</p>"},{"location":"docs/config-basics/#duration-schema","title":"<code>duration</code> schema","text":"<p>A string value in Go <code>time.ParseDuration</code> format. <code>1h</code> for one hour, <code>5m</code> for 5 minutes.</p>"},{"location":"docs/config-basics/#version-schema","title":"<code>version</code> schema","text":"<p>An object with string keys and string values.</p> <p>The following is an array of versions:</p> <pre><code>- { \"ref\": \"33042e15e930b6786fc9b0a9ea5dec78689c5e4b\" }\n- ref: v1.2.0,\n  sha: 33042e15e930b6786fc9b0a9ea5dec78689c5e4b\n- foo: \"0\"\n</code></pre> <p>Note that in the last example we took special care to quote the number.</p> <p>In many scenarios where a version can be specified, e.g. <code>get</code> step <code>version</code>, only a subset of the full set of fields is necessary. The latest version matching the fields specified will be chosen.</p>"},{"location":"docs/fly/","title":"The fly CLI","text":""},{"location":"docs/fly/#the-fly-cli","title":"The <code>fly</code> CLI","text":"<p>The first step to getting started with Concourse is to install the <code>fly</code> CLI tool. You can download <code>fly</code> from any Concourse installation by clicking the download link in the bottom-right corner of the web UI.</p> <p>Throughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult <code>fly -h</code> to learn the short forms.</p>"},{"location":"docs/fly/#fly-login","title":"<code>fly login</code>","text":"<p>The first thing you'll want to do is authenticate with your target. This is done with the <code>fly login</code> command. This is also useful to save targets under a more convenient alias, so you don't have to type out the URL all the time:</p> <p>The <code>login</code> command serves double duty: it authenticates with a given endpoint, and saves it under a more convenient name. The name and token are stored in <code>~/.flyrc</code> (though you shouldn't really edit the file manually).</p> <p>Concourse deployments can be occupied by multiple teams. To specify the team to which to log in, specify the <code>--team-name</code> or <code>-n</code> flag. If not specified, this defaults to the  <code>main</code> team.</p> <p>So, to log in to a team <code>my-team</code> an endpoint served at <code>https://ci.example.com</code> and save it as the more convenient name <code>example</code>, you would run:</p> <pre><code>fly --target example login --team-name my-team \\\n    --concourse-url https://ci.example.com\n</code></pre> <p>The <code>login</code> command will see which authentication methods are available for the specified team and prompt you to choose one. For basic auth, it will ask your username and password and use them to acquire a token. For OAuth, it will give you a link to click, and after you've gone through the OAuth flow it will print an OAuth token on the page that you can then copy and paste into the prompt.</p> <p>Note that if no authentication methods are configured, <code>fly</code> will acquire a token without any prompting. You can then use the alias like normal.</p> <p>In any case, a token is saved in your <code>~/.flyrc</code>, which will expire after one day.</p> <p>If your Concourse uses SSL but does not have a certificate signed by a trusted CA, you can use the <code>--ca-cert</code> flag so that <code>fly</code> can trust the connection, like so:</p> <pre><code>fly -t example login -c https://ci.example.com --ca-cert ./ca.crt\n</code></pre> <p>This will read the value out of the file <code>./ca.crt</code> and save it into <code>~/.flyrc</code> so you don't have to pass it on every <code>login</code> invocation.</p> <p>If your Concourse instance is protected by a proxy server requiring client certificates, you can use <code>--client-cert</code> and <code>--client-key</code> to point to where your certificate is stored. These paths will be stored in <code>.flyrc</code> and the certificate will by attached to every request made to that target.</p> <pre><code>fly -t example login -c https://ci-example.com \\\n    --client-cert ./client.pem \\\n    --client-key ./client.key\n</code></pre> <p>After you've logged in you can use <code>--target example</code> (or <code>-t example</code> for short) to run a command against the saved target <code>example</code>. For example, <code>fly -t example builds</code> will list the last few builds on the <code>example</code> Concourse instance.</p> <p>The <code>-t</code> flag is intentionally stateless and must be explicitly added to each command. This reduces the risk of accidentally running a command against the wrong environment when you have multiple targets defined.</p>"},{"location":"docs/fly/#fly-targets","title":"<code>fly targets</code>","text":"<p>To see what targets are currently known to <code>fly</code>, run:</p> <pre><code>fly targets\n</code></pre> <p>This will show each target's name, URL, and when its token expires.</p>"},{"location":"docs/fly/#fly-status","title":"<code>fly status</code>","text":"<p>To check your current authentication status with a given target, run:</p> <pre><code>fly -t example status\n</code></pre> <p>This will let you know if the token has expired.</p>"},{"location":"docs/fly/#fly-userinfo","title":"<code>fly userinfo</code>","text":"<p>To check what user you're logged in as, as well as which teams you are currently authenticated to and which roles within each team you have, run:</p> <pre><code>fly -t example userinfo\n</code></pre>"},{"location":"docs/fly/#fly-logout","title":"<code>fly logout</code>","text":"<p>To clear out your token for a given target, run:</p> <pre><code>fly -t example logout\n</code></pre> <p>To clear out your token for all targets, run:</p> <pre><code>fly logout -a\n</code></pre> <p>Note</p> <p>These two variations are mutually exclusive. If the target parameter <code>-t</code> and all parameter <code>-a</code> are both  specified, an error will occur.</p>"},{"location":"docs/fly/#fly-edit-target","title":"<code>fly edit-target</code>","text":"<p>To modify a target's name, team, or URL, run:</p> <pre><code>fly -t example edit-target \\\n    --target-name new-name \\\n    --concourse-url https://ci.example.com \\\n    --team-name my-team\n</code></pre> <p>Each flag is optional - only the specified flags will be changed.</p>"},{"location":"docs/fly/#fly-delete-target","title":"<code>fly delete-target</code>","text":"<p>When logging out just isn't enough, a target can be completely removed from <code>~/.flyrc</code> by running:</p> <pre><code>fly -t example delete-target\n</code></pre> <p>To delete all targets, run:</p> <pre><code>fly delete-target -a\n</code></pre> <p>Note</p> <p>These two variations are mutually exclusive. If the target parameter <code>-t</code> and all parameter <code>-a</code> are both  specified, an error will occur.</p>"},{"location":"docs/fly/#fly-sync","title":"<code>fly sync</code>","text":"<p>Occasionally we add additional features to <code>fly</code> or make changes to the communication between it and Concourse's API server. To make sure you're running the latest and greatest version that works with the Concourse you are targeting we provide a command called <code>sync</code> that will update your local <code>fly</code>. It can be used like so:</p> <pre><code>fly -t example sync\n</code></pre> <p>The <code>fly</code> command will also warn you if it notices that your CLI version is out of sync with the server.</p>"},{"location":"docs/fly/#fly-completion","title":"<code>fly completion</code>","text":"<p>Fly can output autocomplete configuration for some shells. For example, you can add an entry to your <code>.bashrc</code> like this:</p> <pre><code>source &lt;(fly completion --shell bash)\n</code></pre> <p>or, using the <code>/etc/bash_completion.d</code> directory:</p> <pre><code>fly completion --shell bash &gt; /etc/bash_completion.d/fly\n</code></pre> <p>Note that, unlike other fly commands, this command does not interact with a remote server so you do not need to provide the <code>-t</code> or <code>--target</code> flag.</p>"},{"location":"docs/jobs/","title":"Jobs","text":"<p>Jobs determine the actions of your pipeline. They determine how resources progress through it, and how the pipeline is visualized.</p> <p>The most important attribute of a job is its build plan, configured as job.plan. This determines the sequence of Steps to execute in any builds of the job.</p> <p>A pipeline's jobs are listed under <code>pipeline.jobs</code> with the following schema:</p>"},{"location":"docs/jobs/#job-schema","title":"<code>job</code> schema","text":"<code>name</code>: <code>identifier</code> (required) <code>steps</code>: <code>[step]</code> (required) <code>old_name</code>: <code>identifier</code> <code>serial</code>: <code>boolean</code> <code>serial_groups</code>: <code>[identifier]</code> <code>max_in_flight</code>: <code>number</code> <code>build_log_retention</code>: <code>build_log_retention_policy</code> <code>public</code>: <code>boolean</code> <code>disable_manual_trigger</code>: <code>boolean</code> <code>interruptible</code>: <code>boolean</code> <code>on_success</code>: <code>step</code> <code>on_failure</code>: <code>step</code> <code>on_error</code>: <code>step</code> <code>on_abort</code>: <code>step</code> <code>ensure</code>: <code>step</code>"},{"location":"docs/jobs/#name","title":"<code>name</code>","text":"<p>The name of the job. This should be short; it will show up in URLs. If you want to rename a job, use <code>job.old_name</code>.</p>"},{"location":"docs/jobs/#steps","title":"<code>steps</code>","text":"<p>The sequence of steps to execute.</p>"},{"location":"docs/jobs/#old_name","title":"<code>old_name</code>","text":"<p>The old name of the job. If configured, the history of old job will be inherited to the new one. Once the pipeline  is set, this field can be removed as the builds have been transfered.</p> Renaming a job <p>This can be used to rename a job without losing its history, like so:</p> <pre><code>jobs:\n  - name: new-name\n    old_name: current-name\n    plan:\n      - get: 10m\n</code></pre> <p>After the pipeline is set, because the builds have been inherited, the job can have the field removed:</p> <pre><code>jobs:\n  - name: new-name\n    plan:\n      - get: 10m\n</code></pre>"},{"location":"docs/jobs/#serial","title":"<code>serial</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, builds will queue up and execute one-by-one, rather than executing in parallel.</p>"},{"location":"docs/jobs/#serial_groups","title":"<code>serial_groups</code>","text":"<p>Default <code>[]</code>. When set to an array of arbitrary tag-like strings, builds of this job and other jobs referencing  the same tags will be serialized.</p> Limiting parallelism <p>This can be used to ensure that certain jobs do not run at the same time, like so:</p> <pre><code>jobs:\n  - name: job-a\n    serial_groups:\n      - some-tag\n\n  - name: job-b\n    serial_groups:\n      - some-tag\n      - some-other-tag\n\n  - name: job-c\n    serial_groups:\n      - some-other-tag\n</code></pre> <p>In this example, <code>job-a</code> and <code>job-c</code> can run concurrently, but neither job can run builds at the same time as  <code>job-b</code>.</p> <p>The builds are executed in their order of creation, across all jobs with common tags.</p>"},{"location":"docs/jobs/#max_in_flight","title":"<code>max_in_flight</code>","text":"<p>If set, specifies a maximum number of builds to run at a time. If <code>serial</code> or <code>serial_groups</code> are set, they take  precedence and force this value to be <code>1</code>.</p>"},{"location":"docs/jobs/#build_log_retention","title":"<code>build_log_retention</code>","text":"<p>Configures the retention policy for build logs. This is useful if you have a job that runs often but after some  amount of time the logs aren't worth keeping around.</p> <p>Builds which are not retained by the configured policy will have their logs reaped. If this configuration is  omitted, logs are kept forever (unless Build log retention is  configured globally).</p> A complicated example <p>The following example will keep logs for any builds that have completed in the last 2 days, while also keeping  the last 1000 builds and at least 1 succeeded build.</p> <pre><code>jobs:\n  - name: smoke-tests\n    build_log_retention:\n      days: 2\n      builds: 1000\n      minimum_succeeded_builds: 1\n    plan:\n      - get: 10m\n      - task: smoke-tests\n        # ...\n</code></pre> <p>If more than 1000 builds finish in the past 2 days, all of them will be retained thanks to the <code>days</code>  configuration. Similarly, if there are 1000 builds spanning more than 2 days, they will also be kept thanks to  the <code>builds</code> configuration. And if they all happened to have failed, the <code>minimum_succeeded_builds</code> will keep  around at least one successful build. All policies operate independently.</p>"},{"location":"docs/jobs/#build_log_retention_policy-schema","title":"<code>build_log_retention_policy</code> schema","text":"<code>days</code>: <code>number</code> <p>Keep logs for builds which have finished within the specified number of days.</p> <code>builds</code>: <code>number</code> <p>Keep logs for the last specified number of builds.</p> <code>minimum_succeeded_builds</code>: <code>number</code> <p>Keep a minimum number of successful build logs that would normally be reaped.</p> <p>Requires <code>builds</code> to be set to an integer higher than 0 in order to work. For example, if <code>builds</code> is set to 5,  and this attribute to 1, say a job has the following build history: 7(f), 6(f), 5(f), 4(f), 3(f), 2(f), 1(s),  where f means failed and s means succeeded, then builds 2 and 3 will be reaped, because it retains 5 build logs, and at least 1 succeeded build log. Default is 0.</p>"},{"location":"docs/jobs/#public","title":"<code>public</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, the build log of this job will be viewable by unauthenticated users.  Unauthenticated users will always be able to see the inputs, outputs, and build status history of a job. This is  useful if you would like to expose your pipeline publicly without showing sensitive information in the build log.</p> <p>Note</p> <p>When this is set to <code>true</code>, any <code>get</code> step and <code>put</code> steps will show the  metadata for their resource version, regardless of whether the resource itself has set  <code>resource.public</code> to <code>true</code>.</p>"},{"location":"docs/jobs/#disable_manual_trigger","title":"<code>disable_manual_trigger</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, manual triggering of the job (via the web UI or  <code>fly trigger-job</code>) will be disabled.</p>"},{"location":"docs/jobs/#interruptible","title":"<code>interruptible</code>","text":"<p>Default <code>false</code>. Normally, when a worker is shutting down it will wait for builds with containers running on that  worker to finish before exiting. If this value is set to <code>true</code>, the worker will not wait on the builds of this job. You may want this if you have a self-deploying Concourse or long-running-but-low-importance jobs.</p>"},{"location":"docs/jobs/#on_success","title":"<code>on_success</code>","text":"<p>Step to execute when the job succeeds. Equivalent to the <code>on_success</code>  hook.</p>"},{"location":"docs/jobs/#on_failure","title":"<code>on_failure</code>","text":"<p>Step to execute when the job fails. Equivalent to the <code>on_failure</code>  hook.</p>"},{"location":"docs/jobs/#on_error","title":"<code>on_error</code>","text":"<p>Step to execute when the job errors. Equivalent to the <code>on_error</code>  hook.</p>"},{"location":"docs/jobs/#on_abort","title":"<code>on_abort</code>","text":"<p>Step to execute when the job aborts. Equivalent to the <code>on_abort</code>  hook.</p>"},{"location":"docs/jobs/#ensure","title":"<code>ensure</code>","text":"<p>Step to execute regardless of whether the job succeeds, fails, errors, or aborts. Equivalent to the  <code>ensure</code> hook.</p>"},{"location":"docs/jobs/#managing-jobs","title":"Managing Jobs","text":""},{"location":"docs/jobs/#fly-jobs","title":"<code>fly jobs</code>","text":"<p>To list the jobs configured in a pipeline, run:</p> <pre><code>fly -t example jobs -p my-pipeline\n</code></pre>"},{"location":"docs/jobs/#fly-trigger-job","title":"<code>fly trigger-job</code>","text":"<p>To immediately queue a new build of a job, run:</p> <pre><code>fly -t example trigger-job --job my-pipeline/my-job\n</code></pre> <p>This will enqueue a new build of the <code>my-job</code> job in the <code>my-pipeline</code> pipeline.</p> <p>To start watching the newly created build, append the <code>--watch</code> flag like so:</p> <pre><code>fly -t example trigger-job --job my-pipeline/my-job --watch\n</code></pre> <p>You can also queue new builds by clicking the <code>+</code> button on the job or build pages in the web UI.</p>"},{"location":"docs/jobs/#fly-rerun-build","title":"<code>fly rerun-build</code>","text":"<p>To queue a new build of a job with exactly the same inputs as a given build of the same job, run:</p> <pre><code>To queue a new build of a job with exactly the same inputs as a given build of the same job, run:\n</code></pre> <p>This will enqueue a new build of the <code>my-job</code> job in the <code>my-pipeline</code> pipeline, using the same input versions as build number 4.</p> <p>To start watching the newly created build, append the <code>--watch</code> flag like so:</p> <pre><code>fly -t example rerun-build --job my-pipeline/my-job --build 4 --watch\n</code></pre> <p>You can also rerun builds by visiting the build page for the build in question in the web UI and clicking the rerun button.</p>"},{"location":"docs/jobs/#fly-pause-job","title":"<code>fly pause-job</code>","text":"<p>To prevent scheduling and running builds of a job, run:</p> <pre><code>fly -t example pause-job --job my-pipeline/my-job\n</code></pre> <p>This will prevent pending builds of the job from being scheduled, though builds that are in-flight will still run, and pending builds will still be created as normal.</p>"},{"location":"docs/jobs/#fly-unpause-job","title":"<code>fly unpause-job</code>","text":"<p>To resume scheduling of a job, run:</p> <pre><code>fly -t example unpause-job --job my-pipeline/my-job\n</code></pre> <p>This will resume scheduling of builds queued for the job.</p>"},{"location":"docs/jobs/#fly-clear-task-cache","title":"<code>fly clear-task-cache</code>","text":"<p>If you've got a task cache that you need to clear out for whatever reason, this can be done like so:</p> <pre><code>fly -t example clear-task-cache --job my-pipeline/my-job --step my-step-name\n</code></pre> <p>This will immediately invalidate the caches - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.</p> <p>You can also clear out a particular path for the given step's cache, using <code>--cache-path</code>:</p> <pre><code>fly -t example clear-task-cache \\\n    --job my-pipeline/my-job \\\n    --step my-step-name \\\n    --cache-path go/pkg\n</code></pre> <p>If <code>--cache-path</code> is not specified, all caches for the given step will be cleared.</p>"},{"location":"docs/observation/","title":"Observation","text":"<p>This section outlines everything you need to know for observing the state of your pipelines.</p>"},{"location":"docs/observation/#the-dashboard","title":"The Dashboard","text":"<p>The dashboard, available at the default route (<code>/</code>), provides a bird's-eye view of the Concourse cluster. All visible pipelines across all teams are listed here. A high-density (HD) view is available at <code>/hd</code>.</p>"},{"location":"docs/observation/#ccxml","title":"<code>cc.xml</code>","text":"<p>The Concourse API can return the status of a team's pipelines in a format compatible with tools like CCMenu. This endpoint is available at the following route:</p> <pre><code>/api/v1/teams/{team}/cc.xml\n</code></pre>"},{"location":"docs/observation/#badges","title":"Badges","text":"<p>The Concourse API supports returning SVG badges indicating the status of a job:</p> <pre><code>/api/v1/teams/{team}/pipelines/{pipeline}/jobs/{job}/badge\n</code></pre> <p>... and for an entire pipeline:</p> <pre><code>/api/v1/teams/{team}/pipelines/{pipeline}/badge\n</code></pre> <p>This can be used to annotate your READMEs with a build status badge like so:</p> <pre><code>![CI](https://ci.concourse-ci.org/api/v1/teams/main/pipelines/concourse/jobs/unit/badge)\n</code></pre> <p>... which should render the following:</p> <p></p> <p>You can also specify a custom title for the badge with a title query parameter:</p> <pre><code>/api/v1/teams/{team}/pipelines/{pipeline}/badge?title=hello\n</code></pre> <p>or</p> <pre><code>/api/v1/teams/{team}/pipelines/{pipeline}/jobs/{job}/badge?title=hello\n</code></pre> <p>... which should render the following:</p> <p></p> <p>If you want to have the image link to your pipeline/job in a README, you can make it like so:</p> <pre><code>&lt;a href=\"https://ci.concourse-ci.org/teams/main/pipelines/concourse/jobs/unit/badge\"&gt;\n    &lt;img src=\"https://ci.concourse-ci.org/api/v1/teams/main/pipelines/concourse/jobs/unit/badge\"\n         alt=\"Concourse Pipeline Status\"&gt;\n&lt;/a&gt;\n</code></pre>"},{"location":"docs/observation/#pipeline-visibility","title":"Pipeline Visibility","text":"<p>Pipelines may be exposed so that they can be monitored without having to authenticate. For more information, see Pipeline &amp; Build Visibility.</p>"},{"location":"docs/tasks/","title":"Tasks","text":"<p>The smallest configurable unit in a Concourse pipeline is a single task. A task can be thought of as a function from <code>task-config.inputs</code> to <code>task-config.outputs</code> that can either succeed or fail.</p> <p>Going a bit further, ideally tasks are pure functions: given the same set of inputs, it should either always succeed with the same outputs or always fail. This is entirely up to your script's level of discipline, however. Flaky tests or dependencies on the internet are the most common source of impurity.</p> <p>Once you have a running Concourse deployment, you can start configuring your tasks and executing them interactively from your terminal with the Fly command line tool.</p> <p>Once you've figured out your task's configuration, you can reuse it for a Job in your Pipeline.</p> <p>Conventionally a task's configuration is placed in the same repository as the code it's testing, possibly under some <code>ci</code> directory.</p> <p>A task's configuration specifies the following:</p>"},{"location":"docs/tasks/#task-config-schema","title":"<code>task-config</code> schema","text":"<code>platform</code>: <code>linux</code> | <code>darwin</code> | <code>windows</code> (required) <p>The platform the task should run on. This determines the pool of workers that the task can run against.</p> <p>Technically any string value is allowed so long as a worker advertises the same platform, but in practice only  <code>linux</code>, <code>darwin</code>, and <code>windows</code> are in use.</p> <code>image_resource</code>: <code>anonymous_resource</code> <p>The container image to run with, as provided by an anonymous resource definition.</p> <p>Whenever the task runs, the anonymous resource will be <code>check</code>ed to discover the latest version available. The image will then be fetched onto the worker, if necessary, just prior to running the task.</p> <p>To use an image provided by a previous step within your build plan, set <code>task</code> step <code>image</code> on the  <code>task</code> step instead.</p> <p>Note</p> <p>This field is only required for tasks targeting the Linux platform. This field will be ignored for Windows and  Darwin workers. Windows containers are currently not supported and Darwin does not have native containers. The  task will run inside a clean temporary directory on the Windows/Darwin worker with any inputs and outputs copied into the same directory. Any dependencies should be pre-installed on the worker.</p> Using the <code>golang</code> Docker image <p>The following task config will use the <code>golang</code> Docker image to run  <code>go version</code>:</p> <pre><code>platform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: golang\n\nrun:\n  path: go\n  args:\n    - version\n</code></pre> <code>inputs</code>: <code>[input]</code> <p>The set of artifacts used by task, determining which artifacts will be available in the current directory when the  task runs.</p> <p>These are satisfied by <code>get</code> steps or <code>task-config.outputs</code> of a previous task. These can also be  provided by <code>-i</code> with <code>fly execute</code>.</p> <p>If any required inputs are missing at run-time, then the task will error immediately.</p> <code>outputs</code>: <code>[output]</code> <p>The artifacts produced by the task.</p> <p>Each output configures a directory to make available to later steps in the build plan. The  directory will be automatically created before the task runs, and the task should place any artifacts it wants to  export in the directory.</p> <code>caches</code>: <code>[cache]</code> <p>The cached directories shared between task runs.</p> <p>On the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those  artifacts.</p> <p>Caches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on  different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.</p> <p>Caches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off  builds.</p> <code>params</code>: <code>env-vars</code> <p>A key-value mapping of string keys and values that are exposed to the task via environment variables.</p> <p>Pipelines can override these params by setting <code>task</code> step <code>params</code> on the <code>task</code> step. This is a  common method of providing credentials to a task.</p> <code>run</code>: <code>command</code> <p>The command to execute in the container. If not specified, Concourse will try running any <code>ENTRYPOINT</code>/<code>CMD</code> commands found in the container, following the same logic as Docker. Any <code>args</code> specified in the task config will be appended to <code>ENTRYPOINT</code>/<code>CMD</code>.</p> <code>ENTRYPOINT</code>/<code>CMD</code> execution is only supported on &gt;= v8 of Concourse. <p>If you're on an older version of Concourse you must specify <code>run.path</code> in your task config.</p> <p>Note</p> <p>This is not provided as a script blob, but explicit <code>path</code> and <code>args</code> values; this allows <code>fly</code> to forward  arguments to the script, and forces your config <code>.yml</code> to stay fairly small.</p> <code>rootfs_uri</code>: <code>string</code> <p>A string specifying the rootfs uri of the container, as interpreted by your worker's Garden backend.</p> <p><code>task-config.image_resource</code> is the preferred way to specify base image. You should only use this if you have no  other option and you really know what you're doing.</p> <code>container_limits</code>: <code>container_limits</code> <p>CPU and memory limits to enforce on the task container.</p> <p>Note</p> <p>These values, when specified, will override any limits set by passing the <code>--default-task-cpu-limit</code> or  <code>--default-task-memory-limit</code> flags to the <code>concourse web</code> command.</p>"},{"location":"docs/tasks/#anonymous_resource-schema","title":"<code>anonymous_resource</code> schema","text":"<code>type</code>: <code>resource_type.name</code> <p>The type of the resource. Usually <code>registry-image</code>.</p> <p>You can use any resource type that returns a filesystem in the correct format: a <code>/rootfs</code> directory containing  a full filesystem, and a <code>metadata.json</code> file containing.</p> <code>source</code>: <code>config</code> <p>The configuration for the resource; see <code>resource.source</code>.</p> <code>params</code>: <code>config</code> <p>A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see  what it supports.</p> <code>version</code>: <code>version</code> <p>A specific version of the resource to fetch. This should be a map with string keys and values. If not specified, the latest version will be fetched.</p>"},{"location":"docs/tasks/#input-schema","title":"<code>input</code> schema","text":"<code>name</code>: <code>identifier</code> <p>The name of the input.</p> <code>path</code>: <code>dir-path</code> <p>The path where the input will be placed. If not specified, the input's <code>name</code> is used.</p> <p>Paths are relative to the working directory of the task unless an absolute path is given. An absolute path is  any path that starts with a forward slash <code>/</code>. We recommend only using relative paths unless you have a strong  technical reason to use absolute paths.</p> <p>Any parent directory references (<code>../</code>) in the path will be removed.</p> <code>optional</code>: <code>boolean</code> <p>Default <code>false</code>. If <code>true</code>, then the input is not required by the task. The task may run even if this input is missing.</p> <p>An <code>optional</code> input that is missing will not appear in the current directory of the running task.</p>"},{"location":"docs/tasks/#output-schema","title":"<code>output</code> schema","text":"<code>name</code>: <code>identifier</code> <p>The name of the output. The contents under <code>path</code> will be made available to the rest of the plan under this name.</p> <code>path</code>: <code>dir-path</code> <p>The path to a directory where the output will be taken from. If not specified, the output's <code>name</code> is used.</p> <p>Paths are relative to the working directory of the task unless an absolute path is given. An absolute path is  any path that starts with a forward slash <code>/</code>. We recommend only using relative paths unless you have a strong  technical reason to use absolute paths.</p> <p>Any parent directory references (<code>../</code>) in the path will be removed.</p>"},{"location":"docs/tasks/#cache-schema","title":"<code>cache</code> schema","text":"<code>path</code>: <code>dir-path</code> <p>The path to a directory to be cached.</p> <p>Paths are relative to the working directory of the task. Absolute paths are not respected.</p>"},{"location":"docs/tasks/#command-schema","title":"<code>command</code> schema","text":"<code>path</code>: <code>file-path</code> <p>The name of or path to the executable to run found inside the container.</p> <p><code>path</code> is relative to the working directory. If <code>dir</code> is specified to set the working directory, then <code>path</code> is  relative to it.</p> <p>This is commonly a path to a script provided by one of the task's inputs, e.g. <code>my-resource/scripts/test</code>. It  could also be a command like <code>bash</code> (respecting standard <code>$PATH</code> lookup rules), or an absolute path to a file to execute, e.g. <code>/bin/bash</code>.</p> <code>args</code>: <code>[string]</code> <p>Arguments to pass to the command. Note that when executed with <code>fly</code>, any arguments passed to  <code>fly execute</code> are appended to this array.</p> <code>dir</code>: <code>dir-path</code> <p>A directory, relative to the initial working directory, to set as the working directory when running the script.</p> <code>user</code>: <code>string</code> <p>Explicitly set the user to run as. If not specified, this defaults to the user configured by the task's image.  If not specified there, it's up to the Garden backend, and may be e.g. <code>root</code> on Linux.</p>"},{"location":"docs/tasks/#container_limits-schema","title":"<code>container_limits</code> schema","text":"<code>cpu</code>: <code>number</code> <p>The maximum amount of CPU available to the task container, measured in shares. 0 means unlimited.</p> <p>CPU shares are relative to the CPU shares of other containers on a worker. For example, if you have two  containers both with a CPU limit of 2 shares then each container will get 50% of the CPU's time.</p> <pre><code>Container A: 2 shares - 50% CPU\nContainer B: 2 shares - 50% CPU\nTotal CPU shares declared: 4\n</code></pre> <p>If you introduce another container then the number of CPU time per container changes. CPU shares are  relative to each other.</p> <pre><code>Container A: 2 shares - 25% CPU\nContainer B: 2 shares - 25% CPU\nContainer C: 4 shares - 50% CPU\nTotal CPU shares declared: 8\n</code></pre> <code>memory</code>: <code>number</code> <p>The maximum amount of memory available to the task container, measured in bytes. 0 means unlimited. Can use  units such as <code>KB/MB/GB</code>.</p>"},{"location":"docs/tasks/#examples","title":"Examples","text":"Testing a Ruby app <p>This configuration specifies that the task must run with the <code>ruby:2.1</code> Docker image with a <code>my-app</code> input, and when  the task is executed it will run the <code>scripts/test</code> script in the same repo.</p> <pre><code>---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: ruby\n    tag: '2.1'\n\ninputs:\n  - name: my-app\n\nrun:\n  path: my-app/scripts/test\n</code></pre> Producing outputs from a task <p>A task can configure <code>task-config.outputs</code> to produce artifacts that can then be propagated to a <code>put</code>  step or another <code>task</code> step in the same plan. They can also be downloaded with fly  execute by passing <code>-o</code>.</p> <pre><code>---\nplatform: linux\n\nimage_resource: # ...\n\ninputs:\n  - name: project-src\n\noutputs:\n  - name: built-project\n\nrun:\n  path: project-src/ci/build\n</code></pre> <p>... assuming <code>project-src/ci/build</code> looks something like:</p> <pre><code>#!/bin/bash\n\nset -e -u -x\n\nexport GOPATH=$PWD/project-src\n\ngo build -o built-project/my-project \\\n  github.com/concourse/my-project\n</code></pre> <p>... this task could then be used in a build plan like so:</p> <pre><code>plan:\n  - get: project-src\n  - task: build-bin\n    file: project-src/ci/build.yml\n  - put: project-bin\n    params:\n      file: built-project/my-project\n</code></pre> Caching ephemeral state <p>The following task and script could be used by a Node project to cache the <code>node_modules</code> directory:</p> <pre><code>---\nplatform: linux\n\nimage_resource: # ...\n\ninputs:\n- name: project-src\n\ncaches:\n- path: project-src/node_modules\n\nrun:\n  path: project-src/ci/build\n</code></pre> <p>... assuming <code>project-src/ci/build</code> looks something like:</p> <pre><code>#!/bin/bash\n\nset -e -u -x\n\ncd project-src\nnpm install\n\n# ...\n</code></pre> <p>... this task would cache the contents of <code>project-src/node_modules</code> between runs of this task on the same worker.</p> Using an image from a private Docker registry <p>The following external task uses an image from a private registry. Assuming the CA is configured properly on the  workers, SSL should Just Work\u2122.</p> <p>External tasks are now fully interpolated using credential manager variables and  <code>task</code> step <code>vars</code>, so you can use template variables in an external task:</p> <pre><code>---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\ninputs:\n  - name: my-app\n\nrun:\n  path: my-app/scripts/test\n  args: [ \"Hello, world!\", \"((myparam))\" ]\n</code></pre>"},{"location":"docs/tasks/#running-tasks-with-fly-execute","title":"Running tasks with <code>fly execute</code>","text":"<p>One of the most common use cases of <code>fly</code> is taking a local project on your computer and setting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.</p> <p>You can execute a task like this:</p> <pre><code>fly -t example execute --config tests.yml\n</code></pre> <p>Your files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify <code>-i name=</code>. Instead, where <code>name</code> is the input name from the task configuration.</p> <p>Fly will automatically capture <code>SIGINT</code> and <code>SIGTERM</code> and abort the build when received. This allows it to be transparently composed with other toolchains.</p> <p>By default, <code>fly execute</code> will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the <code>--include-ignored</code> flag in order to send ignored files to Concourse along with those that are not ignored.</p> <p>If your task needs to run as <code>root</code>, then you can specify the <code>-p</code> or <code>--privileged</code> flag.</p> Providing multiple inputs <p>Tasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current  working directory) that has the same name as the directory.</p> <p>Tasks must specify the inputs that they require as <code>task-config.inputs</code>. For <code>fly</code> to upload these inputs you can  use the <code>-i</code> or <code>--input</code> arguments with name and path pairs. For example:</p> <pre><code>fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\n</code></pre> <p>This would work together with a <code>build-stemcell.yml</code> if its inputs: section was as follows:</p> <pre><code>inputs:\n  - name: code\n  - name: stemcells\n</code></pre> <p>If you specify an input, then the default input will no longer be added automatically, and you will need to  explicitly list it (as with the <code>code</code> input above).</p> <p>This feature can be used to mimic other resources and try out input combinations that would normally not be possible  in a pipeline.</p> Basing inputs on a job in your pipeline with <code>inputs-from</code> <p>If the <code>--inputs-from</code> flag is given, the specified job will be looked up in the pipeline, and the one-off build  will base its inputs on those currently configured for the job.</p> <p>If any <code>--input</code> flags are given (see above), they will override the base set of inputs.</p> <p>For example:</p> <pre><code>fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\n</code></pre> <p>This will trigger a one-off-build using the <code>task.yml</code> task config, basing its inputs on the latest candidates for  the <code>integration</code> job in the <code>main</code> pipeline, with the <code>foo</code> input overridden to specify local code to run.</p> <p>This can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local  development so that you don't have to upload every single resource from your local machine.</p> Using an image from a job in your pipeline with <code>--image</code> <p>When using <code>--inputs-from</code> as above, you can additionally specify which input to use as the task's image by passing  <code>--image input-name</code>.</p> <p>For example, the following pipeline fetches an image via a <code>get</code> step and uses it for <code>task</code> step  <code>image</code>:</p> <pre><code>resources:\n  - name: my-repo\n    type: git\n    source: { uri: https://example.com }\n\n  - name: some-image\n    type: registry-image\n    source: { repository: ubuntu }\n\njobs:\n  - name: integration\n    plan:\n      - get: my-repo\n      - get: some-image\n      - task: my-task\n        file: my-repo/task.yml\n        image: some-image\n</code></pre> <p>... so to run the same task with the same image in a one-off build, you would run:</p> <pre><code>fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --image some-image\n</code></pre> Taking artifacts from the build with <code>--output</code> <p>If a task specifies outputs, then you're able to extract these back out of the build and back to your local system.  For example:</p> <pre><code>fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\n</code></pre> <p>This would work together with a <code>build-stemcell.yml</code>, if its <code>outputs:</code> section was as follows:</p> <pre><code>outputs:\n  - name: stemcell\n</code></pre> <p>This feature is useful to farm work out to your Concourse server to build things in a repeatable manner.</p> Providing values for <code>params</code> <p>Any params listed in the task configuration can be specified by using environment variables.</p> <p>So, if you have a task with the following params:</p> <pre><code>params:\n  FOO: fizzbuzz\n  BAR:\n</code></pre> <p>... and you run:</p> <pre><code>BAR=hello; fly execute\n</code></pre> <p>The task would then run with <code>BAR</code> as <code>\"hello\"</code>, and <code>FOO</code> as <code>\"fizzbuzz\"</code> (its default value).</p> Providing values for vars <p>Task config files can contain Vars which can can be set during <code>fly execute</code> by using the <code>-v</code>, <code>-y</code> and  <code>-l</code> flags:</p> <pre><code>fly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\n</code></pre> <p>Any variables not satisfied via the above flags will be deferred to the configured credential  manager.</p> <p>To satisfy these vars when running the task in a pipeline, see <code>task</code> step <code>vars</code>.</p> Targeting a specific worker with <code>--tag</code> <p>If you want to execute a task on a worker that has a specific tag, you can do so by passing <code>--tag</code>:</p> <pre><code>fly -t example execute --config task.yml --tag bar\n</code></pre> <p>This will execute the task specified by <code>task.yml</code> on a worker that has been tagged <code>bar.</code></p>"},{"location":"docs/tasks/#task-runtime-environment","title":"Task runtime environment","text":"<p>A task runs in a new container every time, using the image provided by <code>task-config.image_resource</code> as its base filesystem (i.e. <code>/</code>).</p> <p>The command specified by <code>task-config.run</code> will be executed in a working directory containing each of the <code>task-config.inputs</code>. If any input is missing, the task will not run (and the container will not even be created).</p> <p>The working directory will also contain empty directories for each of the <code>task-config.outputs</code>. The task must place artifacts in the output directories for them to be exported. This meshes well with build tools with configurable destination paths.</p> <p>Tip</p> <p>If your build tools don't support output paths, you can configure an input and output with the same path. The  directory will be populated by the input, and any changes made to the directory will propagate downstream as an  output.</p> <p>Any <code>task</code> step <code>params</code> configured will be set in the environment for the task's command, along with any environment variables provided by the task's image (i.e. <code>ENV</code> rules from your <code>Dockerfile</code>).</p> <p>The user the command runs as is determined by the image. If you're using a Docker image, this will be the user set by a <code>USER</code> rule in your <code>Dockerfile</code>, or <code>root</code>, if not specified.</p> <p>Another relevant bit of configuration is <code>task</code> step <code>privileged</code>, which determines whether the user the task runs as will have full privileges (primarily when running as <code>root</code>). This is intentionally not configurable by the task itself, to prevent privilege escalation by submitting pull requests to repositories that contain task configs.</p> <p>Putting all this together, the following task config:</p> <pre><code>---\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: golang\n    tag: '1.6'\n\nparams:\n  SOME_PARAM: some-default-value\n\ninputs:\n  - name: some-input\n  - name: some-input-with-custom-path\n    path: some/custom/path\n\noutputs:\n  - name: some-output\n\nrun:\n  path: sh\n  args:\n    - -exc\n    - |\n      whoami\n      env\n      go version\n      find .\n      touch some-output/my-built-artifact\n</code></pre> <p>... will produce the following output:</p> <pre><code>$ whoami\nroot\n$ env\nUSER=root\nHOME=/root\nGOLANG_DOWNLOAD_SHA256=5470eac05d273c74ff8bac7bef5bad0b5abbd1c4052efbdbc8db45332e836b0b\nPATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nGOPATH=/go\nPWD=/tmp/build/e55deab7\nGOLANG_DOWNLOAD_URL=https://golang.org/dl/go1.6.linux-amd64.tar.gz\nGOLANG_VERSION=1.6\nSOME_PARAM=some-default-value\n$ go version\ngo version go1.6 linux/amd64\n$ find .\n.\n./some-input\n./some-input/foo\n./some\n./some/custom\n./some/custom/path\n./some/custom/path/bar\n./some-output\n$ touch some-output/my-built-artifact\n</code></pre> <p>... and propagate <code>my-built-artifact</code> to any later <code>task</code> steps or <code>put</code> steps that reference the <code>some-output</code> artifact, in the same way that this task had <code>some-input</code> as an input.</p>"},{"location":"docs/vars/","title":"Vars","text":"<p>Concourse supports value substitution in YAML configuration by way of <code>((vars))</code>.</p> <p>Automation entails the use of all kinds of credentials. It's important to keep these values separate from the rest of your configuration by using vars instead of hardcoding values. This allows your configuration to be placed under source control and allows credentials to be tucked safely away into a secure credential manager like Vault instead of the Concourse database.</p> <p>Aside from credentials, vars may also be used for generic parameterization of pipeline configuration templates, allowing a single pipeline config file to be configured multiple times with different parameters - e.g. <code>((branch_name))</code>.</p>"},{"location":"docs/vars/#var-syntax","title":"<code>((var))</code> syntax","text":"<p>The full syntax for vars is <code>((</code><code>source-name:secret-path.secret-field</code><code>))</code>.</p> <p>The optional source-name identifies the var source from which the value will be read. If omitted (along with the <code>:</code> delimiter), the cluster-wide credential manager will be used, or the value may be provided statically. The special name <code>.</code> refers to the local var source, while any other name refers to a var source.</p> <p>The required secret-path identifies the location of the credential. The interpretation of this value depends on the var source type. For example, with Vault this may be a path like <code>path/to/cred</code>. For the Kubernetes secret manager this may just be the name of a secret. For credential managers which support path-based lookup, a secret-path without a leading / may be queried relative to a predefined set of path prefixes. This is how the Vault credential manager currently works; <code>foo</code> will be queried under <code>/concourse/(team name)/(pipeline name)/foo</code>.</p> <p>The optional secret-field specifies a field on the fetched secret to read. If omitted, the credential manager may choose to read a 'default field' from the fetched credential if the field exists. For example, the Vault credential manager will return the value of the <code>value</code> field if present. This is useful for simple single-value credentials where typing <code>((foo.value))</code> would feel verbose.</p> <p>The secret-path and secret-field may be surrounded by double quotes <code>\"...\"</code> if they contain special characters like <code>.</code> and <code>:</code>. For instance, <code>((source:\"my.secret\".\"field:1\"))</code> will set the secret-path to <code>my.secret</code> and the secret-field to <code>field:1</code>.</p>"},{"location":"docs/vars/#local-var","title":"The \"<code>.</code>\" var source","text":"<p>The special var source name <code>.</code> refers to a \"local var source.\"</p> <p>The precise scope for these \"local vars\" depends on where they're being used. Currently, the only mechanism that uses the local var source is the <code>load_var</code> step, which sets a var in a local var source provided to all steps executed in the build.</p>"},{"location":"docs/vars/#interpolation","title":"Interpolation","text":"<p>Values for vars are substituted structurally. That is, if you have <code>foo: ((bar))</code>, whatever value <code>((bar))</code> resolves to will become the value of the <code>foo</code> field in the object. This can be a value of any type and structure: a boolean, a simple string, a multiline credential like a certificate, or a complicated data structure like an array of objects.</p> <p>This differs from text-based substitution in that it's impossible for a value to result in broken YAML syntax, and it relieves the template author from having to worry about things like whitespace alignment.</p> <p>When a <code>((var))</code> appears adjacent to additional string content, e.g. <code>foo: hello-((bar))-goodbye</code>, its value will be concatenated with the surrounding content. If the <code>((var))</code> resolves to a non-string value, an error will be raised.</p> <p>If you are using the YAML operator for merging <code>&lt;&lt;</code>, you will need to wrap it in double quotes like so <code>\"&lt;&lt;\": ((foobars))</code>, to avoid a cryptic error message such as \"error: yaml: map merge requires map or sequence of maps as the value\". This will allow you to merge in values from various vars. See YAML merge specification for more information on how this normally works.</p>"},{"location":"docs/vars/#static-vars","title":"Static vars","text":"<p>Var values may also be specified statically using the <code>set_pipeline</code> step and <code>task</code> step.</p> <p>When running the <code>fly</code> CLI equivalent commands (fly set-pipeline and fly execute), var values may be provided using the following flags:</p> <ul> <li><code>-v</code> or <code>--var NAME=VALUE</code> sets the string <code>VALUE</code> as the value for the var <code>NAME</code>.</li> <li><code>-y</code> or <code>--yaml-var NAME=VALUE</code> parses <code>VALUE</code> as YAML and sets it as the value for the var <code>NAME</code>.</li> <li><code>-i</code> or <code>--instance-var NAME=VALUE</code> parses <code>VALUE</code> as YAML and sets it as the value for the instance var <code>NAME</code>.   See Grouping Pipelines to learn more about instance vars.</li> <li><code>-l</code> or <code>--load-vars-from FILE</code> loads <code>FILE</code>, a YAML document containing mapping var names to values, and sets them   all.</li> </ul> <p>When used in combination with <code>-l</code>, the <code>-y</code> and <code>-v</code> flags take precedence. This way a vars file may be re-used, overriding individual values by hand.</p> Setting values with the <code>task</code> step <p>Let's say we have a task config like so:</p> <pre><code>platform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: golang\n    tag: ((tag))\n\ninputs:\n  - name: booklit\n\nrun:\n  path: booklit/ci/unit\n</code></pre> <p>We could use vars to run this task against different versions of Go:</p> <pre><code>jobs:\n  - name: unit\n    plan:\n      - get: booklit\n        trigger: true\n      - task: unit-1.13\n        file: booklit/ci/unit.yml\n        vars: { tag: 1.13 }\n      - task: unit-1.8\n        file: booklit/ci/unit.yml\n        vars: { tag: 1.8 }\n</code></pre> Setting values with <code>-v</code> and <code>-y</code> <p>With a pipeline template like so:</p> <pre><code>resources:\n- name: booklit\n  type: booklit\n  source:\n    uri: https://github.com/concourse/booklit\n    branch: ((branch))\n    private_key: ((\"github.com\".private_key))\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: ((trigger))\n  - task: unit\n    file: booklit/ci/unit.yml\n</code></pre> <p>Let's say we have a private key in a file called <code>private_key</code>.</p> <p>The fly validate-pipeline command may be used to test how  interpolation is applied, by passing the <code>--output</code> flag.</p> <pre><code>fly validate-pipeline \\\n  -c pipeline.yml \\\n  -y trigger=true \\\n  -v \\\"github.com\\\".private_key=\"$(cat private_key)\" \\\n  -v branch=master \\\n  --output\n</code></pre> <p>The above incantation should print the following:</p> <pre><code>jobs:\n  - name: unit\n    plan:\n      - get: booklit\n        trigger: true\n      - file: booklit/ci/unit.yml\n        task: unit\nresources:\n  - name: booklit\n    type: booklit\n    source:\n      branch: master\n      private_key: |\n        -----BEGIN RSA PRIVATE KEY-----\n        # ... snipped ...\n        -----END RSA PRIVATE KEY-----\n      uri: https://github.com/concourse/booklit\n</code></pre> <p>Note that we had to use <code>-y</code> so that the <code>trigger: true</code> ends up with a boolean value instead of the  string <code>\"true\"</code>.</p> Loading values from files with <code>-l</code> <p>With a pipeline template like so:</p> <pre><code>resources:\n  - name: booklit\n    type: booklit\n    source:\n      uri: https://github.com/concourse/booklit\n      branch: ((branch))\n      private_key: ((\"github.com\".private_key))\n\njobs:\n  - name: unit\n    plan:\n      - get: booklit\n        trigger: ((trigger))\n      - task: unit\n        file: booklit/ci/unit.yml\n</code></pre> <p>Let's say I've put the <code>private_key</code> var in a file called <code>vars.yml</code>, since it's quite large and hard to pass  through flags:</p> <pre><code>github.com:\n  private_key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    # ... snipped ...\n    -----END RSA PRIVATE KEY-----\n</code></pre> <p>The fly validate-pipeline command may be used to test how  interpolation is applied, by passing the <code>--output</code> flag.</p> <pre><code>fly validate-pipeline \\\n  -c pipeline.yml \\\n  -l vars.yml \\\n  -y trigger=true \\\n  -v branch=master \\\n  --output\n</code></pre> <p>The above incantation should print the following:</p> <pre><code>jobs:\n  - name: unit\n    plan:\n      - get: booklit\n        trigger: true\n      - task: unit\n        file: booklit/ci/unit.yml\nresources:\n  - name: booklit\n    type: booklit\n    source:\n      branch: master\n      private_key: |\n        -----BEGIN RSA PRIVATE KEY-----\n        # ... snipped ...\n        -----END RSA PRIVATE KEY-----\n      uri: https://github.com/concourse/booklit\n</code></pre> <p>Note that we had to use <code>-y</code> so that the <code>trigger: true</code> ends up with a boolean value instead of the  string <code>\"true\"</code>.</p>"},{"location":"docs/vars/#dynamic-vars","title":"Dynamic vars","text":"<p>Concourse can read values from \"var sources\" - typically credential managers like Vault - at runtime. This keeps them out of your configuration and prevents sensitive values from being stored in your database. Values will be read from the var source and optionally cached to reduce load on the var source.</p> <p>The following attributes can be parameterized through a var source:</p> <ul> <li>resource.source under pipeline.resources</li> <li>resource_type.source   under pipeline.resources</li> <li>resource.webhook_token   under pipeline.resources</li> <li>task step params on a task step in a pipeline</li> <li>tasks configuration in their entirety - whether from task step file or task step config in a pipeline, or   a config executed with <code>fly execute</code></li> </ul> <p>Concourse will fetch values for vars as late as possible - i.e. when a step using them is about to execute. This allows the credentials to have limited lifetime and rapid rotation policies.</p>"},{"location":"docs/vars/#across-step-dynamic-vars","title":"Across Step &amp; Dynamic Vars","text":"<p>For the across step, more fields can be dynamically interpolated during runtime:</p> <ul> <li>set_pipeline step identifier and file field</li> <li>task step identifier, input_mapping, and output_mapping, in   addition to the all other fields mentioned above for the task step</li> </ul>"},{"location":"docs/vars/#var-sources-experimental","title":"Var sources (experimental)","text":"<p>Experimental Feature</p> <p><code>var_sources</code> was introduced in Concourse v5.8.0. It is considered an experimental feature until its implementation is complete. See concourse/concourse#5229.</p> <p>Var sources can be configured for a pipeline via <code>pipeline.var_sources</code>.</p> <p>Each var source has a name which is then referenced as the source-name in var syntax, e.g. <code>((my-vault:test-user.username))</code> to fetch the <code>test-user</code> var from the <code>my-vault</code> var source. See <code>((var))</code> syntax for a detailed explanation of this syntax.</p> <p>Currently, only these types are supported:</p> <ul> <li><code>vault</code></li> <li><code>dummy</code></li> <li><code>ssm</code></li> <li><code>secretmanager</code> (since v7.7.0)</li> <li><code>idtoken</code> (since v7.14.0)</li> </ul> <p>In the future we want to make use of something like the Prototypes (RFC #37) so that third-party credential managers can be used just like resource types.</p>"},{"location":"docs/vars/#var_source-schema","title":"<code>var_source</code> schema","text":"name: string <p>The name of the <code>((var))</code> source. This should be short and simple. This name will be referenced  <code>((var))</code> syntax throughout the config.</p> <p>one of ...</p> VaultDummySSMSecrets ManagerID Token <code>type</code>: <code>vault</code> <p>The <code>vault</code> type supports configuring a Vault server as a <code>((var))</code> source.</p> <code>config</code>: <code>vault_config</code> <code>type</code>: <code>dummy</code> <p>The <code>dummy</code> type supports configuring a static map of vars to values.</p> <p>This is really only useful if you have no better alternative for credential management but still have  sensitive values that you would like to redact them from build output.</p> <code>config</code>: <code>dummy_config</code> <code>type</code>: <code>ssm</code> <p>The <code>SSM</code> type supports configuring an AWS Systems Manager  in a single region as a <code>((var))</code> source.</p> <code>config</code>: <code>ssm_config</code> <code>type</code>: <code>secretsmanager</code> <p>The <code>secretsmanager</code> type supports configuring an AWS Secrets  Manager in a single region as a <code>((var))</code> source.</p> <code>config</code>: <code>secretsmanager_config</code> <code>type</code>: <code>idtoken</code> <p>The <code>idtoken</code> type issues JWTs which are signed by concourse and contain information about the currently  running pipeline/job.</p> <p>These JWTs can be used to authenticate with external services.</p> <code>config</code>: <code>idtoken_config</code>"},{"location":"docs/vars/#vault_config-schema","title":"<code>vault_config</code> schema","text":"<code>url</code>: <code>string</code> <p>The URL of the Vault API.</p> <code>ca_cert</code>: <code>string</code> <p>The PEM encoded contents of a CA certificate to use when connecting to the API.</p> <code>path_prefix</code>: <code>string</code> <p>Default <code>/concourse</code>. A prefix under which to look for all credential values.</p> <p>See Changing the path prefix for more information.</p> <code>lookup_templates</code>: <code>[string]</code> <p>Default <code>[\"/{{.Team}}/{{.Pipeline}}/{{.Secret}}\", \"/{{.Team}}/{{.Secret}}\"]</code>.</p> <p>A list of path templates to be expanded in a team and pipeline context subject to the <code>path_prefix</code> and  <code>namespace</code>.</p> <p>See Changing the path templates for more  information.</p> <code>shared_path</code>: <code>string</code> <p>An additional path under which credentials will be looked up.</p> <p>See Configuring a shared path for more  information.</p> <code>namespace</code>: <code>string</code> <p>A Vault namespace to operate under.</p> <code>client_cert</code>: <code>string</code> <p>A PEM encoded client certificate, for use with TLS based auth.</p> <p>See Using the <code>cert</code> auth backend for more  information.</p> <code>client_key</code>: <code>string</code> <p>A PEM encoded client key, for use with TLS based auth.</p> <p>See Using the <code>cert</code> auth backend for more  information.</p> <code>server_name</code>: <code>string</code> <p>The expected name of the server when connecting through TLS.</p> <code>insecure_skip_verify</code>: <code>boolean</code> <p>Skip TLS validation. Not recommended. Don't do it. No really, don't.</p> <code>client_token</code>: <code>string</code> <p>Authenticate via a periodic client token.</p> <p>See Using a periodic token for more information.</p> <code>auth_backend</code>: <code>string</code> <p>Authenticate using an auth backend, e.g. <code>cert</code> or <code>approle</code>.</p> <p>See Using the <code>approle</code> auth backend or  Using the <code>cert</code> auth backend for more  information.</p> <code>auth_params</code>: <code>env-vars</code> <p>A key-value map of parameters to pass during authentication.</p> <p>See Using the <code>approle</code> auth backend for more information.</p> <code>auth_max_ttl</code>: <code>duration</code> <p>Maximum duration to elapse before forcing the client to log in again.</p> <code>auth_retry_max</code>: <code>duration</code> <p>When failing to authenticate, give up after this amount of time.</p> <code>auth_retry_initial</code>: <code>duration</code> <p>When retrying during authentication, start with this retry interval. The interval will increase  exponentially until <code>auth_retry_max</code> is reached.</p>"},{"location":"docs/vars/#dummy_config-schema","title":"<code>dummy_config</code> schema","text":"<code>vars</code>: <code>vars</code> <p>A mapping of var name to var value.</p>"},{"location":"docs/vars/#ssm_config-schema","title":"<code>ssm_config</code> schema","text":"<code>region</code>: <code>string</code> <p>The AWS region to read secrets from.</p>"},{"location":"docs/vars/#secretsmanager_config-schema","title":"<code>secretsmanager_config</code> schema","text":"<code>region</code>: <code>string</code> <p>The AWS region to read secrets from.</p>"},{"location":"docs/vars/#idtoken_config-schema","title":"<code>idtoken_config</code> schema","text":"<code>audience</code>: <code>[string]</code> <p>A list of audience-values to place into the token's aud-claim.</p> <code>subject_scope</code>: <code>team</code> | <code>pipeline</code> | <code>instance</code> | <code>job</code> | <code>string</code> <p>Default <code>pipeline</code>.</p> <p>Determines what is put into the token's sub-claim. See  Subject Scope for a detailed explanation.</p> <code>expires_in</code>: <code>duration</code> <p>Default <code>1h</code>. Cannot be longer than <code>24h</code>.</p> <p>How long the token should be valid.</p> <code>algorithm</code>: <code>RS256</code> | <code>ES256</code> | <code>string</code> <p>Default <code>RS256</code>.</p> <p>The signature algorithm to use for the token.</p>"},{"location":"docs/vars/#the-cluster-wide-credential-manager","title":"The cluster-wide credential manager","text":"<p>Concourse can be configured with a single cluster-wide credential manager, which acts as a source for any vars which do not specify a source name.</p> <p>See Credential Management for more information.</p> <p>Note</p> <p>In the future we would like to introduce support for multiple cluster-wide var sources, configured using the  <code>var_source</code> schema, and begin deprecating the cluster-wide credential  manager.</p>"},{"location":"docs/auth-and-teams/","title":"Auth & Teams","text":"<p>A single Concourse installation can accommodate many projects and users.</p> <p>Pipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the <code>concourse</code> GitHub organization to be a member.</p> <p>When a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.</p>"},{"location":"docs/auth-and-teams/caveats/","title":"Security Caveats","text":"<p>At present, teams only provide trusted multi-tenancy. This means it should be used for cases where you know and trust who you're allowing access into your Concourse cluster.</p> <p>There are a few reasons it'd be a bad idea to do otherwise:</p> <ul> <li>Any team can run builds with <code>task</code> step <code>privileged</code> tasks. A bad actor in the mix   could easily use this to harm your workers and your cluster. You   can lock down privileged mode if you   use the containerd runtime and avoid this issue all together.</li> <li>There are no networking restrictions in place, and traffic to and from the worker's Garden and Baggageclaim endpoints   is currently unencrypted and unauthorized. Anyone could run a task that does horrible things to your worker's   containers, possibly stealing sensitive information.       This can be remedied with configuration specified on Garden to restrict access to the internal network, but this is   not detailed in our docs, and we'll probably want to find a better answer than configuration in the future.       You could put firewall rules in place between workers to mitigate this issue as well.</li> </ul>"},{"location":"docs/auth-and-teams/exposing/","title":"Pipeline & Build Visibility","text":"<p>Every newly configured pipeline is hidden to anyone but the pipeline's team. To make a pipeline publicly viewable, both by other teams and unauthenticated users, see  <code>fly expose-pipeline</code>.</p> <p>Even with a pipeline exposed, all build logs are hidden by default. This is because CI jobs are prone to leaking credentials and other ... unsavory information. After you've determined that a job's builds should be safe for public consumption, you can set <code>public: true</code> on the job in your pipeline.</p>"},{"location":"docs/auth-and-teams/main-team/","title":"The main team","text":""},{"location":"docs/auth-and-teams/main-team/#the-main-team","title":"The <code>main</code> team","text":"<p>Out of the box, Concourse comes with a single team called <code>main</code>.</p> <p>The <code>main</code> team is an admin team, meaning members (specifically users with the owner role) can create and update other teams. Currently, there is no way to promote a team to become an admin team, so <code>main</code> is a special-case.</p> <p>The <code>main</code> team is different in that all flags normally passed to  <code>fly set-team</code> are instead passed to the <code>concourse web</code> command, prefixed with <code>--main-team-</code>. The values set in these flags take effect whenever the <code>web</code> node starts up. This is done so that you can't get locked out.</p> <p>To learn how to configure your <code>main</code> team, continue on to the appropriate section for your auth provider of choice under Configuring Auth.</p>"},{"location":"docs/auth-and-teams/managing-teams/","title":"Managing Teams","text":""},{"location":"docs/auth-and-teams/managing-teams/#fly-set-team","title":"<code>fly set-team</code>","text":"<p>Once you've logged in as the <code>main</code> team with <code>fly</code>, you can run  <code>fly set-team</code> to create or update other teams. Users with a  <code>owner</code> role can also update their own configuration with the same command.</p> <p>For example, to create a new team that authorizes the local <code>foo</code> user, you would run:</p> <pre><code>fly -t example set-team --team-name my-team --local-user foo\n</code></pre> <p>Note that each time <code>set-team</code> is run, the team's authorization config is set as a whole - it is not a stateful operation.</p> <p>There are many different ways to configure team auth; see Configuring Auth for more information.</p> <p>Once the team has been created, you can use <code>fly login</code> to log in:</p> <pre><code>fly -t example login -n my-team\n</code></pre> <p>Any newly configured pipelines (via  <code>fly set-pipeline</code>) and one-off builds (via  <code>fly execute</code>) will be owned by the authorized team. Commands that list content will be scoped to the current team by default, such as  <code>fly pipelines</code> and  <code>fly builds</code>. The web UI will reflect the same state.</p> <p>Newly configured pipelines are hidden by default, meaning other teams and unauthorized visitors cannot view them. To make them publicly viewable, see Pipeline &amp; Build Visibility.</p>"},{"location":"docs/auth-and-teams/managing-teams/#setting-user-roles","title":"Setting User Roles","text":"<p>By default, authorization config passed to <code>set-team</code> configures the  <code>owner</code> role.</p> <p>More advanced roles configuration can be specified through the <code>--config</code> or <code>-c</code> flag.</p> <p>The <code>-c</code> flag expects a <code>.yml</code> file with a single field, <code>roles:</code>, pointing to a list of role authorization configs.</p> <p>All the attributes in each config will vary by provider. Consult the appropriate section for your provider under Configuring Auth for specifics.</p> <p>For example, the following config sets three roles with different auth config for each role's provider:</p> <pre><code>roles:\n  - name: owner\n    github:\n      users: [ \"admin\" ]\n  - name: member\n    github:\n      teams: [ \"org:team\" ]\n  - name: viewer\n    github:\n      orgs: [ \"org\" ]\n    local:\n      users: [ \"visitor\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/managing-teams/#fly-active-users","title":"<code>fly active-users</code>","text":"<p>To list all users that have logged into your instance in the last two months, run:</p> <pre><code>fly -t example active-users\n</code></pre> <p>The output will include the username, connector (which method they used to authenticate) and the date of their last login.</p> <p>You can list users whose last login was within a different range by using:</p> <pre><code>fly -t example active-users --since yyyy-MM-dd\n</code></pre> <p>This can be helpful to get a sense of how active your cluster is.</p>"},{"location":"docs/auth-and-teams/managing-teams/#fly-teams","title":"<code>fly teams</code>","text":"<p>To list all the teams, run:</p> <pre><code>fly -t example teams\n</code></pre> <p>This can be useful if you've forgotten your team name.</p>"},{"location":"docs/auth-and-teams/managing-teams/#fly-teams-d-with-details","title":"<code>fly teams -d</code>: With Details","text":"<p>To list all the teams with authentication details and members, run:</p> <pre><code>fly -t example teams -d\n</code></pre> <p>This can be helpful when debugging OAuth, OIDC groups or listing all individual members.</p>"},{"location":"docs/auth-and-teams/managing-teams/#fly-get-team","title":"<code>fly get-team</code>","text":"<p>To show a team's configuration, run:</p> <pre><code>fly -t example get-team -n some-team\n</code></pre>"},{"location":"docs/auth-and-teams/managing-teams/#fly-rename-team","title":"<code>fly rename-team</code>","text":"<p>To rename a team, run:</p> <pre><code>fly -t example rename-team --old-name my-team --new-name cool-team\n</code></pre> <p>This can only be run by the <code>main</code> team.</p>"},{"location":"docs/auth-and-teams/managing-teams/#fly-destroy-team","title":"<code>fly destroy-team</code>","text":"<p>To remove a team, including all of its pipelines and one-off builds, first log in as the <code>main</code> team, and then run:</p> <pre><code>fly -t example destroy-team --team-name my-team\n</code></pre> <p>Currently, if there were any workers assigned specifically to this team, they'll be orphaned, without having their containers or volumes cleaned up.</p>"},{"location":"docs/auth-and-teams/user-roles/","title":"User Roles & Permissions","text":"<p>Concourse comes with five roles:</p> <ol> <li>Concourse Admin</li> <li>Team Owner</li> <li>Team Member</li> <li>Pipeline Operator</li> <li>Team Viewer</li> </ol> <p>These roles are strictly ordered, so that each role always has all the permissions of any other role lower on the list. This means that a Pipeline Operator can always do anything a Team Viewer can, and so on.</p> <p>In this document we say an action is assigned to a role if that role is capable of performing the action, but any less-privileged role is not. For example, the <code>SaveConfig</code> action is assigned to the <code>member</code> role, so owners and members can set a pipeline config, but pipeline operators and viewers cannot.</p>"},{"location":"docs/auth-and-teams/user-roles/#concourse-admin","title":"Concourse Admin","text":"<p><code>Admin</code> is a special user attribute granted only to owners of the <code>main</code> team.</p> <p>Admins have the ability to administrate teams using <code>fly set-team</code>,  <code>fly destroy-team</code>, <code>fly rename-team</code>, etc.</p> <p>Admins always have permission to perform any action on any team. You cannot assign actions to the admin role using the <code>--config-rbac</code> flag.</p> <p>The following actions are also assigned to admins, and cannot be reconfigured:</p> <pre><code>- GetLogLevel\n- ListActiveUsersSince\n- SetLogLevel\n- GetInfoCreds\n- SetWall\n- ClearWall\n</code></pre>"},{"location":"docs/auth-and-teams/user-roles/#owner-role","title":"<code>owner</code> role","text":"<p>Team owners have read, write and auth management capabilities within the scope of their team, and can rename or destroy the team.</p> <p>Actions assigned to the <code>owner</code> role by default:</p> <pre><code>owner:\n  - SetTeam\n  - RenameTeam\n  - DestroyTeam\n</code></pre>"},{"location":"docs/auth-and-teams/user-roles/#member-role","title":"<code>member</code> role","text":"<p>Team members can operate within their team in a read &amp; write fashion, but they can not change the configuration of their team.</p> <p>Actions assigned to the <code>member</code> role by default:</p> <pre><code>member:\n  - SaveConfig\n  - CreateBuild\n  - DeletePipeline\n  - OrderPipelines\n  - OrderPipelinesWithinGroup\n  - ExposePipeline\n  - HidePipeline\n  - RenamePipeline\n  - ArchivePipeline\n  - CreatePipelineBuild\n  - RegisterWorker\n  - LandWorker\n  - RetireWorker\n  - PruneWorker\n  - HeartbeatWorker\n  - DeleteWorker\n  - HijackContainer\n  - ReportWorkerContainers\n  - ReportWorkerVolumes\n  - CreateArtifact\n  - GetArtifact\n</code></pre>"},{"location":"docs/auth-and-teams/user-roles/#pipeline-operator-role","title":"<code>pipeline-operator</code> role","text":"<p>Team pipeline operators can perform pipeline operations such as triggering builds and pinning resources, however they cannot update pipeline configurations.</p> <p>Actions assigned to the <code>pipeline-operator</code> role by default:</p> <pre><code>pipeline-operator:\n  - AbortBuild\n  - RerunJobBuild\n  - CreateJobBuild\n  - PauseJob\n  - UnpauseJob\n  - ClearTaskCache\n  - UnpinResource\n  - SetPinCommentOnResource\n  - CheckResource\n  - CheckResourceWebHook\n  - CheckResourceType\n  - EnableResourceVersion\n  - DisableResourceVersion\n  - PinResourceVersion\n  - PausePipeline\n  - UnpausePipeline\n  - ClearResourceCache\n</code></pre>"},{"location":"docs/auth-and-teams/user-roles/#viewer-role","title":"<code>viewer</code> role","text":"<p>Team viewers have \"read-only\" access to a team and its pipelines. This locks everything down, preventing users from doing a <code>fly set-pipeline</code> or  <code>fly intercept</code>.</p> <p>Actions assigned to the <code>viewer</code> role by default:</p> <pre><code>viewer:\n  - GetConfig\n  - GetCC\n  - GetBuild\n  - GetCheck\n  - GetBuildPlan\n  - ListBuilds\n  - BuildEvents\n  - BuildResources\n  - GetBuildPreparation\n  - GetJob\n  - ListAllJobs\n  - ListJobs\n  - ListJobBuilds\n  - ListJobInputs\n  - GetJobBuild\n  - GetVersionsDB\n  - JobBadge\n  - MainJobBadge\n  - ListAllResources\n  - ListResources\n  - ListResourceTypes\n  - GetResource\n  - ListResourceVersions\n  - GetResourceVersion\n  - ListBuildsWithVersionAsInput\n  - ListBuildsWithVersionAsOutput\n  - GetResourceCausality\n  - ListAllPipelines\n  - ListPipelines\n  - GetPipeline\n  - ListPipelineBuilds\n  - PipelineBadge\n  - ListWorkers\n  - DownloadCLI\n  - GetInfo\n  - ListContainers\n  - GetContainer\n  - ListDestroyingContainers\n  - ListVolumes\n  - ListDestroyingVolumes\n  - ListTeams\n  - GetTeam\n  - ListTeamBuilds\n  - ListBuildArtifacts\n</code></pre>"},{"location":"docs/auth-and-teams/user-roles/#action-matrix","title":"Action Matrix","text":"<p>In this table, an action is marked as customizable if it is possible to change its permissions by providing the <code>--config-rbac</code> flag, documented below. Assigning an action to a role that is not customizable will have no effect on its permissions.</p> Action <code>fly</code> commands affected UI actions affected can be performed unauthenticated? customizable GetBuild n/a view one-off build page BuildResources n/a view build page GetBuildPreparation n/a view build page BuildEvents <code>fly watch</code>,<code>fly execute</code> view build page GetBuildPlan n/a view build page ListBuildArtifacts n/a n/a AbortBuild <code>fly abort-build</code> abort button on build page PruneWorker <code>fly prune-worker</code> n/a LandWorker <code>fly land-worker</code> n/a RetireWorker n/a n/a ListDestroyingVolumes n/a n/a ListDestroyingContainers n/a n/a ReportWorkerContainers n/a n/a ReportWorkerVolumes n/a n/a GetPipeline n/a view pipeline page GetJobBuild n/a view build page PipelineBadge n/a n/a JobBadge n/a n/a ListJobs <code>fly jobs</code> view pipeline page GetJob n/a view job page ListJobBuilds <code>fly builds</code> view job page ListPipelineBuilds <code>fly builds</code> n/a GetResource n/a view resource page ListBuildsWithVersionAsInput n/a expand version on resource page ListBuildsWithVersionAsOutput n/a expand version on resource page GetResourceCausality n/a n/a GetResourceVersion n/a n/a ListResources <code>fly resources</code> view pipeline page ListResourceTypes n/a n/a ListResourceVersions <code>fly resource-versions</code>,<code>fly pin-resource</code> view resource page CreateBuild <code>fly execute</code> n/a GetContainer n/a n/a HijackContainer <code>fly intercept</code> n/a ListContainers <code>fly containers</code> n/a ListWorkers <code>fly workers</code> n/a RegisterWorker n/a n/a HeartbeatWorker n/a n/a DeleteWorker n/a n/a GetTeam <code>fly get-team</code> n/a SetTeam <code>fly set-team</code> n/a ListTeamBuilds <code>fly builds</code> n/a RenameTeam <code>fly rename-team</code> n/a DestroyTeam <code>fly destroy-team</code> n/a ListVolumes <code>fly volumes</code> n/a DownloadCLI <code>fly sync</code> icons on dashboard and pipeline pages CheckResourceWebHook n/a n/a GetInfo n/a n/a GetCheck <code>fly check-resource</code>,<code>fly check-resource-type</code> check button on resource page ListTeams <code>fly teams</code> view dashboard page ListAllPipelines n/a view dashboard page ListPipelines <code>fly pipelines</code> n/a ListAllJobs <code>fly teams</code> view dashboard page ListAllResources n/a view dashboard page ListBuilds <code>fly builds</code> n/a MainJobBadge n/a n/a GetLogLevel n/a n/a SetLogLevel n/a n/a GetWall n/a n/a SetWall n/a n/a ClearWall n/a n/a ListActiveUsersSince <code>fly active-users</code> n/a GetInfoCreds n/a n/a CheckResource <code>fly check-resource</code> check button on resource page CheckResourceType <code>fly check-resource-type</code> n/a CreateJobBuild <code>fly trigger-job</code> trigger button on job and build pages RerunJobBuild <code>fly rerun-build</code> rerun button on build page CreatePipelineBuild <code>fly execute</code> n/a DeletePipeline <code>fly destroy-pipeline</code> n/a DisableResourceVersion <code>fly disable-resource-version</code> version disable widget on resource page EnableResourceVersion <code>fly enable-resource-version</code> version enable widget on resource page PinResourceVersion <code>fly pin-resource</code> pin buttons on resource page UnpinResource <code>fly unpin-resource</code> pin buttons on resource page SetPinCommentOnResource <code>fly pin-resource</code> comment overlay on resource page GetConfig <code>fly get-pipeline</code> n/a GetCC n/a n/a GetVersionsDB n/a n/a ListJobInputs n/a n/a OrderPipelines <code>fly order-pipelines</code> drag and drop on dashboard OrderPipelinesWithinGroup <code>fly order-instanced-pipelines</code> drag and drop within instance group on dashboard PauseJob <code>fly pause-job</code> pause button on job page PausePipeline <code>fly pause-pipeline</code> pause button on pipeline or dashboard RenamePipeline <code>fly rename-pipeline</code> n/a UnpauseJob <code>fly unpause-job</code> play button on job page UnpausePipeline <code>fly unpause-pipeline</code> play button on pipeline or dashboard ExposePipeline <code>fly expose-pipeline</code> eyeball button on dashboard HidePipeline <code>fly hide-pipeline</code> slashed eyeball button on dashboard SaveConfig <code>fly set-pipeline</code> n/a ClearTaskCache <code>fly clear-task-cache</code> n/a CreateArtifact <code>fly execute</code> n/a GetArtifact <code>fly execute</code> n/a ClearResourceCache <code>fly clear-resource-cache</code> n/a"},{"location":"docs/auth-and-teams/user-roles/#configuring-rbac","title":"Configuring RBAC","text":"<p>It is possible to promote or demote the roles to which actions are assigned by passing the <code>--config-rbac</code> to the <code>concourse web</code> command with a path to a <code>.yml</code> file, like the following:</p> <pre><code>concourse web --config-rbac=/path/to/rbac/config.yml\n</code></pre> <p>This file should be a YAML map where the keys are role names (<code>owner</code>, <code>member</code>, <code>pipeline-operator</code>, and <code>viewer</code> are valid). For each role, the value should be a list of actions. On startup, Concourse will assign each role to its associated list of actions.</p> <p>For example, in the default configuration only pipeline-operators and above can abort builds. To restrict aborting builds to only members and above, you could pass this as a <code>--config-rbac</code> file:</p> <pre><code>member:\n  - AbortBuild\n</code></pre> <p>On the other hand, only members and above can order pipelines by default. To extend this privilege down to pipeline-operators, you can use a <code>--config-rbac</code> file like the following:</p> <pre><code>pipeline-operator:\n  - OrderPipelines\n</code></pre> <p>You do not need to specify a role for every possible action; if an action does not appear in the file, then the default role (as described in the sections above) will be assigned to that action. Also, please avoid specifying the same action under multiple roles in this file - it can have unpredictable results.</p>"},{"location":"docs/auth-and-teams/configuring/","title":"Configuring Auth","text":"<p>The very first thing to configure with Concourse is how users will log in, and what those users should be able to do.</p> <p>This is configured in two separate tiers:</p> <ul> <li>Authentication, how users identify themselves, is configured on the <code>web</code> node.</li> <li>Authorization, how user access is determined, is configured on each team.</li> </ul> <p>Concourse currently supports the following auth methods:</p> <ul> <li> <p> Local Auth</p> <p> Configure</p> </li> <li> <p> GitHub Auth</p> <p> Configure</p> </li> <li> <p> GitLab Auth</p> <p> Configure</p> </li> <li> <p> BitBucket Cloud Auth</p> <p> Configure</p> </li> <li> <p> CF / UAA Auth</p> <p> Configure</p> </li> <li> <p> LDAP Auth</p> <p> Configure</p> </li> <li> <p> Microsoft Auth</p> <p> Configure</p> </li> <li> <p> Generic OIDC Auth</p> <p> Configure</p> </li> <li> <p> Generic OAuth</p> <p> Configure</p> </li> <li> <p> Generic SAML Auth</p> <p> Configure</p> </li> </ul> <p>Any number of providers can be enabled at any one time. Users will be given a choice when logging in as to which one they would like to use.</p> <p>Concourse uses a fork of Dex for its authentication. You can find additional documentation on the supported auth providers in the Dex connectors documentation.</p> <p>Adding a new auth provider to Concourse is as simple as submitting a pull request to our fork of Dex and then adding a bit of configuration to the  <code>skymarshal</code> component.</p>"},{"location":"docs/auth-and-teams/configuring/bitbucket-cloud/","title":"BitBucket Cloud Auth","text":"<p>A Concourse server can authenticate against BitBucket Cloud to leverage its permission model.</p>"},{"location":"docs/auth-and-teams/configuring/bitbucket-cloud/#authentication","title":"Authentication","text":"<p>First, you'll need to create an OAuth consumer on Bitbucket Cloud.</p> <p>The consumer will need the following permissions:</p> <ul> <li>Account:<ul> <li>Email</li> <li>Read</li> </ul> </li> <li>Team membership:<ul> <li>Read</li> </ul> </li> </ul> <p>The \"Callback URL\" must be the URL of your Concourse server with <code>/sky/issuer/callback</code> appended. This address must be reachable by BitBucket Cloud - it can't be <code>localhost</code>.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>You will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the <code>web</code> node by setting the following env:</p> <pre><code>CONCOURSE_BITBUCKET_CLOUD_CLIENT_ID=myclientid\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_SECRET=myclientsecret\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/bitbucket-cloud/#authorization","title":"Authorization","text":"<p>BitBucket users and teams can be authorized for a team by passing the following flags to  <code>fly set-team</code>:</p> <ul> <li><code>--bitbucket-cloud-user=LOGIN</code> - Authorize an individual user.</li> <li><code>--bitbucket-cloud-team=TEAM_NAME</code> - Authorize an entire organization's members.</li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --bitbucket-cloud-user my-bitbucket-login \\\n    --bitbucket-cloud-team my-bitbucket-team\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    bitbucket-cloud:\n      users: [ \"my-bitbucket-login\" ]\n      teams: [ \"my-bitbucket-team\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/bitbucket-cloud/#configuring-main-team-authorization","title":"Configuring main Team Authorization","text":"<p>BitBucket users and teams can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_BITBUCKET_CLOUD_USER=my-bitbucket-login\nCONCOURSE_MAIN_TEAM_BITBUCKET_CLOUD_TEAM=my-bitbucket-team\n</code></pre> <p>Multiple teams and users may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/cf-uaa/","title":"CF / UAA Auth","text":"<p>Cloud Foundry (CF) auth can be used for operators who wish to authenticate their users configured against their Cloud Foundry instance via the UAA auth component.</p>"},{"location":"docs/auth-and-teams/configuring/cf-uaa/#authentication","title":"Authentication","text":"<p>You'll need to configure your UAA with a <code>concourse</code> client by setting the following under  <code>uaa.clients</code>:</p> <pre><code>concourse:\n  id: myclientid\n  secret: myclientsecret\n  scope: openid,cloud_controller.read\n  authorized-grant-types: \"authorization_code,refresh_token\"\n  access-token-validity: 3600\n  refresh-token-validity: 3600\n  redirect-uri: https://concourse.example.com/sky/issuer/callback\n</code></pre> <p>The value for <code>redirect-uri</code> must be the external URL of your Concourse server with <code>/sky/issuer/callback</code> appended.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>Next, you'll need to take the same client ID and secret and configure it on the  <code>web</code> node by setting the following env:</p> <pre><code>CONCOURSE_CF_API_URL=http://mycf.example.com\nCONCOURSE_CF_CLIENT_ID=myclientid\nCONCOURSE_CF_CLIENT_SECRET=myclientsecret\n</code></pre> <p>Note: if you're integrating with Cloud Foundry, you're probably also deploying Concourse via BOSH - in which case you'll want to set the  <code>cf_auth.*</code> properties in your manifest instead of setting the above env.</p>"},{"location":"docs/auth-and-teams/configuring/cf-uaa/#authorization","title":"Authorization","text":"<p>CloudFoundry users and org/space members can be authorized for a team by passing the following flags to fly set-team:</p> <ul> <li><code>--cf-user=USERNAME</code> - Authorize an individual user.</li> <li><code>--cf-org=ORG_NAME</code> - Authorize an entire organization's members. Members will need to be part of a Space inside the   organization.</li> <li><code>--cf-space=ORG_NAME:SPACE_NAME</code> - Deprecated in favor of <code>--cf-space-with-developer-role</code>. Authorize the members with   <code>developer</code> role of a space within an organization.</li> <li><code>--cf-space-with-any-role=ORG_NAME:SPACE_NAME</code> - Authorize the members with any role of a space within an   organization.</li> <li><code>--cf-space-with-developer-role=ORG_NAME:SPACE_NAME</code> - Authorize the members with <code>developer</code> role of a space within   an organization.</li> <li><code>--cf-space-with-auditor-role=ORG_NAME:SPACE_NAME</code> - Authorize the members with <code>auditor</code> role of a space within an   organization.</li> <li><code>--cf-space-with-manager-role=ORG_NAME:SPACE_NAME</code> - Authorize the members with <code>manager</code> role of a space within an   organization.</li> <li><code>--cf-space-guid=SPACE_GUID</code> - Authorize the members with any role of a space within an organization by space GUID.</li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --cf-user my-username \\\n    --cf-org my-org \\\n    --cf-space my-other-org:my-space\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    cf:\n      users: [ \"my-username\" ]\n      orgs: [ \"my-org\" ]\n      spaces: [ \"my-other-org:my-space\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/cf-uaa/#adding-cf-users-to-the-main-team","title":"Adding CF Users to the <code>main</code> Team","text":"<p>CloudFoundry users and org/space members can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_CF_USER=username\nCONCOURSE_MAIN_TEAM_CF_ORG=org-name\nCONCOURSE_MAIN_TEAM_CF_SPACE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_ANY_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_DEVELOPER_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_AUDITOR_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_MANAGER_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_GUID=SPACE_GUID\n</code></pre> <p>Multiple users, spaces, etc. may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oauth/","title":"Generic OAuth Auth","text":"<p>A Concourse server can authenticate against any valid OAuth auth provider, though it's a bit \"closer to the metal\" as you'll need to explicitly configure the auth, token, and user-info URLs. You may want to see if you can use Generic OIDC auth if your auth provider is compatible with OIDC.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oauth/#authentication","title":"Authentication","text":"<p>First you'll need to create a client with your oAuth provider.</p> <p>The callback URL must be the URL of your Concourse server with <code>/sky/issuer/callback</code> appended. This address must be reachable by your oAuth provider - it can't be <code>localhost</code>.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>The Generic oAuth provider has many values to set - for a full list consult <code>concourse web --help</code>.</p> <p>A typical web node env config may look something like this:</p> <pre><code>CONCOURSE_OAUTH_DISPLAY_NAME=Acme\nCONCOURSE_OAUTH_CLIENT_ID=myclientid\nCONCOURSE_OAUTH_CLIENT_SECRET=myclientsecret\nCONCOURSE_OAUTH_AUTH_URL=https://oauth.example.com/oauth2/auth\nCONCOURSE_OAUTH_TOKEN_URL=https://oauth.example.com/oauth2/token\nCONCOURSE_OAUTH_USERINFO_URL=https://oauth.example.com/oauth2/userinfo\n</code></pre> <p>Consult <code>concourse web --help</code> for a full list of flags with descriptions.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oauth/#authorization","title":"Authorization","text":"<p>OAuth users and groups can be authorized for a team by passing the following flags to  <code>fly set-team</code>:</p> <ul> <li><code>--oauth-user=USERNAME</code> - Authorize an individual user.</li> <li><code>--oauth-group=GROUP_NAME</code> - Authorize anyone from the group.<ul> <li>You may only configure groups if the auth provider exposes this information in either the token itself, or in the   contents of the userinfo endpoint.</li> <li>You can configure which claim points to the groups information by specifying <code>CONCOURSE_OAUTH_GROUPS_KEY</code> on the  <code>web</code> node.</li> </ul> </li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --oauth-user my-username \\\n    --oauth-group my-group\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    oauth:\n      users: [ \"my-username\" ]\n      groups: [ \"my-group\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/generic-oauth/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>OAuth users and groups can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_OAUTH_USER=my-user\nCONCOURSE_MAIN_TEAM_OAUTH_GROUP=my-group\n</code></pre> <p>Multiple users and groups may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oidc/","title":"Generic OIDC Auth","text":"<p>A Concourse server can authenticate against any valid OIDC auth provider. This provider is similar to Generic oAuth except it only requires an issuer URL rather than auth/token/userinfo URLs.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oidc/#authentication","title":"Authentication","text":"<p>First you'll need to create a client with your oAuth provider.</p> <p>The callback URL must be the URL of your Concourse server with <code>/sky/issuer/callback</code> appended. This address must be reachable by your OIDC provider - it can't be <code>localhost</code>.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>A typical <code>web</code> node env config may look something like this:</p> <pre><code>CONCOURSE_OIDC_DISPLAY_NAME=Acme\nCONCOURSE_OIDC_CLIENT_ID=myclientid\nCONCOURSE_OIDC_CLIENT_SECRET=myclientsecret\nCONCOURSE_OIDC_ISSUER=https://oidc.example.com\n</code></pre> <p>Consult <code>concourse web --help</code> for a full list of flags with descriptions.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oidc/#a-note-about-user-lookup","title":"A note about user lookup","text":"<p>When determining the user identity, Concourse will first look at the <code>preferred_username</code> claim. If this claim is empty or missing, it will then look at the claim specified by <code>CONCOURSE_OIDC_USER_NAME_KEY</code> (which defaults to <code>username</code>).</p> <p>Let's say that you want to tie each user to their email by using <code>CONCOURSE_OIDC_USER_NAME_KEY=email</code>.</p> <p>If your OIDC provider returns the following claims, Concourse will still resolve the user to <code>Jane Doe</code>:</p> <pre><code>{\n  \"sub\": \"248289761001\",\n  \"username\": \"j.doe\",\n  \"preferred_username\": \"Jane Doe\",\n  \"email\": \"janedoe@example.com\"\n}\n</code></pre> <p>However, if the <code>preferred_username</code> claim is empty or missing, Concourse will respect the key and resolve the user to <code>janedoe@example.com</code>:</p> <pre><code>{\n  \"sub\": \"248289761001\",\n  \"username\": \"j.doe\",\n  \"preferred_username\": \"\",\n  \"email\": \"janedoe@example.com\"\n}\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/generic-oidc/#authorization","title":"Authorization","text":"<p>Warning</p> <p>When authorizing individual users, it's up to you to ensure that the <code>preferred_username</code> claim and/or the claim  specified by <code>CONCOURSE_OIDC_USER_NAME_KEY</code> is unique. If they're not, then it's possible for users to impersonate  each other</p> <p>OIDC users and groups can be authorized for a team by passing one or more of the following flags to fly set-team:</p> <ul> <li><code>--oidc-user=USERNAME</code> - Authorize an individual user.</li> <li><code>--oidc-group=GROUP_NAME</code> - Authorize anyone from the group.<ul> <li>You may only configure groups if the auth provider exposes this information in either the token itself, or in the   contents of the userinfo endpoint.</li> <li>You can configure which claim points to the groups information by specifying <code>CONCOURSE_OIDC_GROUPS_KEY</code> on the  <code>web</code> node.</li> </ul> </li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --oidc-user my-username \\\n    --oidc-user another-username \\\n    --oidc-group my-group \\\n    --oidc-group my-other-group\n</code></pre> <p>...or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    oidc:\n      users: [ \"my-username\", \"another-username\" ]\n      groups: [ \"my-group\", \"my-other-group\" ]\n</code></pre> <p>Both users and groups are optional. You may opt to only provide privileges based on membership to a group and not to any user explicitly and vice versa.</p>"},{"location":"docs/auth-and-teams/configuring/generic-oidc/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>OIDC users and groups can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_OIDC_USER=my-user\nCONCOURSE_MAIN_TEAM_OIDC_GROUP=my-group\n</code></pre> <p>Multiple users and groups may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/generic-saml/","title":"Generic SAML Auth","text":"<p>A Concourse server can authenticate against any valid SAML auth provider.</p>"},{"location":"docs/auth-and-teams/configuring/generic-saml/#authentication","title":"Authentication","text":"<p>First you'll need to create an application with your SAML provider. Note that the terminology used for configuring an application may vary between SAML providers - this document uses Okta's terminology.</p> <p>SAML Assertion Consumer Service (ACS) URL must be the URL of your Concourse server with <code>/sky/issuer/callback</code> appended.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>Audience URI (SP Entity ID) must match <code>CONCOURSE_SAML_ENTITY_ISSUER</code>, which defaults to the URL of your Concourse server with <code>/sky/issuer/callback</code> appended.</p> <p>Attribute statements that you define in the SAML provider can be remapped in Concourse:</p> <pre><code>CONCOURSE_SAML_USERNAME_ATTR=name   # default\nCONCOURSE_SAML_EMAIL_ATTR=email     # default\nCONCOURSE_SAML_GROUPS_ATTR=groups   # default\n</code></pre> <p>Finally, the SAML provider will generate a SSO URL, a CA certificate, and an Identity Provider Issuer. These values correspond with <code>CONCOURSE_SAML_SSO_URL</code>, <code>CONCOURSE_SAML_CA_CERT</code>, and <code>CONCOURSE_SAML_SSO_ISSUER</code> respectively.</p> <p>A typical web node env config may look something like this:</p> <pre><code>CONCOURSE_SAML_DISPLAY_NAME=Okta\nCONCOURSE_SAML_SSO_URL=https://acme.okta.com/app/Y/Z/sso/saml\nCONCOURSE_SAML_CA_CERT=/path/to/ca_cert\nCONCOURSE_SAML_SSO_ISSUER=http://www.okta.com/X\n</code></pre> <p>Consult <code>concourse web --help</code> for a full list of flags with descriptions.</p>"},{"location":"docs/auth-and-teams/configuring/generic-saml/#authorization","title":"Authorization","text":"<p>OAuth users and groups can be authorized for a team by passing the following flags to  <code>fly set-team</code>:</p> <ul> <li><code>--saml-user=USERNAME</code> - Authorize an individual user.</li> <li><code>--saml-group=GROUP_NAME</code> - Authorize anyone from the group.<ul> <li>You may only configure groups if the auth provider exposes this information in either the token itself, or in the   contents of the userinfo endpoint.</li> <li>You can configure which claim points to the groups information by specifying <code>CONCOURSE_SAML_GROUPS_ATTR</code> on the  <code>web</code> node.</li> </ul> </li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --saml-user my-username \\\n    --saml-group my-group\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    saml:\n      users: [ \"my-username\" ]\n      groups: [ \"my-groups\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/generic-saml/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>SAML users and groups can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_SAML_USER=my-user\nCONCOURSE_MAIN_TEAM_SAML_GROUP=my-group\n</code></pre> <p>Multiple users and groups may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/github/","title":"GitHub Auth","text":"<p>A Concourse server can authenticate against GitHub to leverage their permission model and other security improvements in their infrastructure.</p>"},{"location":"docs/auth-and-teams/configuring/github/#authentication","title":"Authentication","text":"<p>First, you'll need to create an OAuth application on GitHub.</p> <p>The \"Authorization callback URL\" must be the URL of your Concourse server. This address must be reachable by GitHub - it can't be <code>localhost</code>.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>You will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the <code>web</code> node by setting the following env:</p> <pre><code>CONCOURSE_GITHUB_CLIENT_ID=myclientid\nCONCOURSE_GITHUB_CLIENT_SECRET=myclientsecret\n</code></pre> <p>Note that the client must be created under an organization if you want to authorize users based on organization/team membership. In addition, the GitHub application must have at least read access on the organization's members. If the client is created under a personal account, only individual users can be authorized.</p> <p>If you're configuring GitHub Enterprise, you'll also need to set the following env:</p> <pre><code>CONCOURSE_GITHUB_HOST=github.example.com\nCONCOURSE_GITHUB_CA_CERT=/path/to/ca_cert\n</code></pre> <p>The GitHub Enterprise host must not contain a scheme, or a trailing slash.</p>"},{"location":"docs/auth-and-teams/configuring/github/#authorization","title":"Authorization","text":"<p>Users, teams, and entire organizations can be authorized for a team by passing the following flags to  <code>fly set-team</code>:</p> <ul> <li><code>--github-user=LOGIN</code> - Authorize an individual user.</li> <li><code>--github-org=ORG_NAME</code> - Authorize an entire organization's members.</li> <li><code>--github-team=ORG_NAME:TEAM_NAME</code> - Authorize a team's members within an organization.</li> </ul> <pre><code>fly set-team -n my-team \\\n    --github-user my-github-login \\\n    --github-org my-org \\\n    --github-team my-other-org:my-team\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    github:\n      users: [ \"my-github-login\" ]\n      orgs: [ \"my-org\" ]\n      teams: [ \"my-other-org:my-team\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/github/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>GitHub users, teams, and organizations can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_GITHUB_ORG=org-name\nCONCOURSE_MAIN_TEAM_GITHUB_TEAM=org-name:team-name\nCONCOURSE_MAIN_TEAM_GITHUB_USER=some-user\n</code></pre> <p>Multiple orgs, teams, and users may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/gitlab/","title":"GitLab Auth","text":"<p>A Concourse server can authenticate against GitLab to leverage their permission model.</p>"},{"location":"docs/auth-and-teams/configuring/gitlab/#authentication","title":"Authentication","text":"<p>First you need to create an OAuth application on GitLab with the following scopes:</p> <ul> <li>read_user</li> <li>openid</li> </ul> <p>The \"Authorization callback URL\" must be the URL of your Concourse server with <code>/sky/issuer/callback</code> appended. This address must be reachable by GitLab - it can't be <code>localhost</code>.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>You will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the <code>web</code> node by setting the following env:</p> <pre><code>CONCOURSE_GITLAB_CLIENT_ID=myclientid\nCONCOURSE_GITLAB_CLIENT_SECRET=myclientsecret\n</code></pre> <p>If you're configuring a self-hosted GitLab instance, you'll also need to set the following flag:</p> <pre><code>CONCOURSE_GITLAB_HOST=https://gitlab.example.com\n</code></pre> <p>The GitLab host must contain a scheme and not a trailing slash.</p>"},{"location":"docs/auth-and-teams/configuring/gitlab/#authorization","title":"Authorization","text":"<p>Users and groups can be authorized for a team by passing the following flags to fly set-team:</p> <ul> <li><code>--gitlab-user=USERNAME</code> - Authorize an individual user.</li> <li><code>--gitlab-group=GROUP_NAME</code> - Authorize an entire group's members.</li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --gitlab-user my-gitlab-user \\\n    --gitlab-group my-group\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    gitlab:\n      users: [ \"my-gitlab-login\" ]\n      groups: [ \"my-gitlab-group\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/gitlab/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>GitLab users and groups can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_GITLAB_GROUP=group-name\nCONCOURSE_MAIN_TEAM_GITLAB_USER=some-user\n</code></pre> <p>Multiple groups and users may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/ldap/","title":"LDAP Auth","text":"<p>The LDAP provider can be used for operators who wish to authenticate their users against an LDAP server.</p>"},{"location":"docs/auth-and-teams/configuring/ldap/#authentication","title":"Authentication","text":"<p>The LDAP provider is configured by pointing it to an LDAP host with a read-only bind DN and password. This bind DN and password is used for authenticating with the LDAP host and querying the users.</p> <p>Additionally, the base DN under which users are searched as well as the attribute of the users to associate to ' usernames' must also be configured.</p> <p>These can be specified via env to the <code>web</code> node like so:</p> <pre><code>CONCOURSE_LDAP_DISPLAY_NAME=Acme # optional; default \"LDAP\"\nCONCOURSE_LDAP_HOST=ldap.example.com # port defaults to 389 or 636\nCONCOURSE_LDAP_BIND_DN='cn=read-only-admin,dc=example,dc=com'\nCONCOURSE_LDAP_BIND_PW=read-only-admin-password\nCONCOURSE_LDAP_USER_SEARCH_BASE_DN='cn=users,dc=example,dc=com'\nCONCOURSE_LDAP_USER_SEARCH_USERNAME=uid\n</code></pre> <p>To configure TLS, you may need to set a CA cert:</p> <pre><code>CONCOURSE_LDAP_CA_CERT=/path/to/ca_cert\n</code></pre> <p>If your LDAP host does not use TLS, you must set:</p> <pre><code>CONCOURSE_LDAP_INSECURE_NO_SSL=true\n</code></pre> <p>To fine-tune which users are queried, you can specify a user search filter like so:</p> <pre><code>CONCOURSE_LDAP_USER_SEARCH_FILTER='(objectClass=person)'\n</code></pre> <p>To set which user attributes map to the token claims, you can set the following:</p> <pre><code>CONCOURSE_LDAP_USER_SEARCH_ID_ATTR=uid         # default\nCONCOURSE_LDAP_USER_SEARCH_EMAIL_ATTR=mail     # default\nCONCOURSE_LDAP_USER_SEARCH_NAME_ATTR=some-attr # no default\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/ldap/#configuring-ldap-group-search","title":"Configuring LDAP group search","text":"<p>The LDAP provider can also be configured with group search configuration, so that users can be configured for team authorization by their 'group' in LDAP.</p> <p>For example, to find groups and identify them by their <code>ou</code> attribute, you would configure:</p> <pre><code>CONCOURSE_LDAP_GROUP_SEARCH_BASE_DN='cn=groups,dc=example,dc=com'\nCONCOURSE_LDAP_GROUP_SEARCH_NAME_ATTR=ou\n</code></pre> <p>The attributes correlating a user to a group must be specified like so:</p> <pre><code>CONCOURSE_LDAP_GROUP_SEARCH_USER_ATTR=uid\nCONCOURSE_LDAP_GROUP_SEARCH_GROUP_ATTR=members\n</code></pre> <p>This specifies that the <code>uid</code> attribute of the user must be present in the <code>members</code> attribute of the group.</p> <p>An additional filter may be specified, just like with users:</p> <pre><code>CONCOURSE_LDAP_GROUP_SEARCH_FILTER='(objectClass=posixGroup)'\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/ldap/#authorization","title":"Authorization","text":"<p>LDAP users and groups can be authorized for a team by passing the following flags to  <code>fly set-team</code>:</p> <ul> <li><code>--ldap-user=USERNAME</code> - Authorize an individual user.</li> <li><code>--ldap-group=GROUP_NAME</code> - Authorize anyone from the group.</li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --ldap-user my-username \\\n    --ldap-group my-group\n</code></pre> <p>... or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    ldap:\n      users: [ \"my-username\" ]\n      groups: [ \"my-groups\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/ldap/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>LDAP users and groups can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_LDAP_USER=my-user\nCONCOURSE_MAIN_TEAM_LDAP_GROUP=my-group\n</code></pre> <p>Multiple users and groups may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/local-user/","title":"Local User Auth","text":"<p>Local User auth is a primitive username/password-based auth mechanism. All users and passwords are configured statically.</p> <p>In general, we recommend configuring one of the other providers instead, but for small deployments with only a few users, local user auth may be all you need.</p>"},{"location":"docs/auth-and-teams/configuring/local-user/#authentication","title":"Authentication","text":"<p>Local users are configured on the <code>web</code> node by setting the following env:</p> <pre><code>CONCOURSE_ADD_LOCAL_USER=myuser:mypass,anotheruser:anotherpass\n</code></pre> <p>This configures two users, <code>myuser</code> and <code>anotheruser</code>, with their corresponding passwords. The literal password can be provided, or a bcrypt hash of the password.</p> <p>When local users are configured, the log-in page in the web UI will show a username/password prompt.</p> <p>Local users can also log in via <code>fly login</code> with the <code>--username</code> and <code>--password</code> flags.</p>"},{"location":"docs/auth-and-teams/configuring/local-user/#bcrypt-hashing-passwords","title":"Bcrypt Hashing Passwords","text":"<p>Instead of passing in user passwords in plaintext, you can provide Concourse with a bcrypt hash of the passwords.</p> <p>There aren't any great CLI tools for quickly hashing passwords with bcrypt. Here's a simple Go program that can do the hashing for you.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"golang.org/x/crypto/bcrypt\"\n)\n\nfunc main() {\n    password := []byte(\"mypass\")\n    hash, _ := bcrypt.GenerateFromPassword(password, 12)\n    fmt.Println(string(hash))\n}\n</code></pre> <p>Put that in a <code>main.go</code> then run go run <code>main.go</code> and it will output a hash for your password. You can run this program in the Go Playground if you want to avoid installing Go.</p> <p>Hashing the passwords for the previous example, you would then set <code>CONCOURSE_ADD_LOCAL_USER</code> to the following:</p> <pre><code>CONCOURSE_ADD_LOCAL_USER='myuser:$2a$12$L8Co5QYhD..S1l9mIIVHlucvRjfte4tuymMCk9quln0H/eol16d5W,anotheruser:$2a$12$VWSSfrsTIisf96q7UVsvyOBbrcP88kh5CLtuXYSXGwnSnM3ClKxXu'\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/local-user/#authorization","title":"Authorization","text":"<p>Local users are granted access to teams via <code>fly set-team</code>, using the <code>--local-user</code> flag:</p> <pre><code>fly set-team -n my-team --local-user some_username\n</code></pre> <p>...or via --config for setting user roles:</p> <pre><code>roles:\n  - name: member\n    local:\n      users: [ \"some_username\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/local-user/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>Local users can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_LOCAL_USER=myuser\n</code></pre> <p>Multiple users may be specified by comma-separating them.</p>"},{"location":"docs/auth-and-teams/configuring/microsoft/","title":"Microsoft Auth","text":"<p>A Concourse server can authenticate against Microsoft Azure AD to leverage its permission model.</p>"},{"location":"docs/auth-and-teams/configuring/microsoft/#authentication","title":"Authentication","text":"<p>You'll need to register a new application on Azure.</p> <p>The \"Callback URL\" must be the URL of your Concourse server with <code>/sky/issuer/callback</code> appended. This address must be reachable by Microsoft - it can't be <code>localhost</code>.</p> <p>For example, Concourse's own CI server's callback URL would be:</p> <pre><code>https://ci.concourse-ci.org/sky/issuer/callback\n</code></pre> <p>You will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the <code>web</code> node by setting the following env:</p> <pre><code>CONCOURSE_MICROSOFT_CLIENT_ID=myclientid\nCONCOURSE_MICROSOFT_CLIENT_SECRET=myclientsecret\n</code></pre> <p>Consult <code>concourse web --help</code> for a full list of flags with descriptions.</p>"},{"location":"docs/auth-and-teams/configuring/microsoft/#authorization","title":"Authorization","text":"<p>Warning</p> <p>Individual user auth is disabled due to a quirk with with Microsoft returning unique IDs but non-unique usernames</p> <p>Groups can be authorized for a team by passing the following flags to fly set-team:</p> <ul> <li><code>--microsoft-group=GROUP_NAME</code> - Authorize an entire group's members.</li> </ul> <p>For example:</p> <pre><code>fly set-team -n my-team \\\n    --microsoft-group my-group\n</code></pre> <p>...or via <code>--config</code> for setting user roles:</p> <pre><code>roles:\n  - name: member\n    microsoft:\n      groups: [ \"my-groups\" ]\n</code></pre>"},{"location":"docs/auth-and-teams/configuring/microsoft/#configuring-main-team-authorization","title":"Configuring <code>main</code> Team Authorization","text":"<p>Microsoft groups can be added to the <code>main</code> team authorization config by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_MAIN_TEAM_MICROSOFT_GROUP=my-group\n</code></pre> <p>Multiple teams may be specified by comma-separating them.</p>"},{"location":"docs/how-to/","title":"How-To Guides","text":"<p>The following pages are guides that show how to accomplish certain workflows within Concourse. Most of the guides will use specific images, but you are in no way limited to or forced to use these images to accomplish the same task. There are many ways to accomplish the same thing in Concourse, so don't let these guides limit you in what you think is possible with Concourse.</p>"},{"location":"docs/how-to/container-image-guides/","title":"Container Image Guides","text":"<p>The contained guides are meant to demonstrate various workflows using and creating container images within Concourse.</p>"},{"location":"docs/how-to/container-image-guides/build-push/","title":"Building and Pushing an Image","text":"<p>In this guide we are going to show how to build and publish container images using the oci-build task and registry-image resource. This guide assumes you understand how to build container images with Dockerfile's and publish to Docker Hub or another image registry using the docker cli.</p> <p>Note</p> <p>This is one way of building and pushing images. There are many other ways to accomplish this same task in Concourse.</p> <p>First we need a Dockerfile. You can store this in your own repo or reference the github.com/concourse/examples repo. The rest of this post assumes you use the examples repo. All files in this blog post can be found in the examples repo.</p>"},{"location":"docs/how-to/container-image-guides/build-push/#the-dockerfile","title":"The Dockerfile","text":"<p>The <code>Dockerfile</code>:</p> Dockerfile<pre><code>FROM busybox\n\nRUN echo \"I'm simple!\"\nCOPY ./stranger /stranger\nRUN cat /stranger\n</code></pre> <p>The <code>stanger</code> text file:</p> stranger<pre><code>Alien #1: A stranger.\nAlien #2: From the outside.\nAliens: Oooooooooooooooh.\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#defining-pipeline-resources","title":"Defining Pipeline Resources","text":"<p>Now we can start building out our pipeline. Let's declare our Resources first. We will need one resource to pull in the repo where our Dockerfile is located, and a second resource pointing to where we want to push the built container image to.</p> <p>There are some Variables in this file that we will fill out when setting the pipeline.</p> build-push.yml<pre><code>---\nresources:\n  # The repo with our Dockerfile\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n      branch: main\n\n  # Where we will push the image\n  - name: simple-image\n    type: registry-image\n    icon: docker\n    source:\n      # You need to provide these variables when\n      # setting the pipeline\n      repository: ((image-repo-name))/simple-image\n      username: ((registry-username))\n      password: ((registry-password))\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#create-the-job","title":"Create the Job","text":"<p>Next we will create a job that will build and push our container image.</p> <p>To build the job we will need to pull in the repo where the <code>Dockerfile</code> is.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#build-the-image","title":"Build the Image","text":"<p>The second step in our job will build the container image.</p> <p>To build the container image we are going to use the oci-build-task. The oci-build-task is a container image that is meant to be used in a Concourse task to build other container images. Check out the <code>README.md</code> in the repo for more details on how to configure and use the oci-build-task in more complex build scenarios.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n</code></pre> <p>Next we will add concourse-examples as an  <code>input</code> to the build task to ensure the artifact from the  <code>get</code> step (where our <code>Dockerfile</code> is fetched) is mounted in our <code>build-image</code> step.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n</code></pre> <p>The oci-build-task outputs the built container image in a directory called <code>image</code>. Let's add image as an output of our task so we can publish it in a later step.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#defining-the-build-context","title":"Defining the Build Context","text":"<p>Next we need to tell the <code>oci-build-task</code> what the build context of our <code>Dockerfile</code> is. The README goes over a few other methods of creating your build context. We are going to use the simplest use-case. By specifying <code>CONTEXT</code> the <code>oci-build-task</code> assumes a <code>Dockerfile</code> and its build context are in the same directory.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n            UNPACK_ROOTFS: \"true\" # only needed if using image in a future step\n          run:\n            path: build\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#publish-the-container-image","title":"Publish the Container Image","text":"<p>To push the container image add a <code>put</code> step to our job plan and tell the registry-image resource where the tarball of the container image is.</p> <p>The <code>put</code> step will push the container image using the information defined previously in the resource's source.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n            UNPACK_ROOTFS: \"true\" # only needed if using image in a future step\n          run:\n            path: build\n      - put: simple-image\n        params:\n          image: image/image.tar\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#the-entire-pipeline","title":"The Entire Pipeline","text":"<p>Putting all the pieces together, here is our pipeline that builds and pushes a container image.</p> build-push.yml<pre><code>---\nresources:\n  # The repo with our Dockerfile\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n      branch: main\n\n  # Where we will push the image\n  - name: simple-image\n    type: registry-image\n    icon: docker\n    source:\n      # You need to provide these variables when\n      # setting the pipeline\n      repository: ((image-repo-name))/simple-image\n      username: ((registry-username))\n      password: ((registry-password))\n\njobs:\n  - name: build-and-push\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n            UNPACK_ROOTFS: \"true\" # only needed if using image in a future step\n          run:\n            path: build\n      - put: simple-image\n        params:\n          image: image/image.tar\n</code></pre> <p>You can set the pipeline with the following fly command, updating the variable values with real values the pipeline can use to run.</p> <pre><code>fly -t &lt;target&gt; set-pipeline -p build-and-push-image \\\n  -c ./examples/pipelines/build-and-push-simple-image.yml \\\n  --var image-repo-name=&lt;repo-name&gt; \\\n  --var registry-username=&lt;user&gt; \\\n  --var registry-password=&lt;password&gt;\n</code></pre>"},{"location":"docs/how-to/container-image-guides/build-push/#further-readings","title":"Further Readings","text":"<p>Understanding what the build context is important when building container images. You can read Dockerfile Best Practices for more details about build contexts.</p> <p>The inputs section of the oci-build-task's <code>README</code> has examples on how to create a build context with multiple inputs and other complex build scenarios.</p> <p>Read the <code>README</code>'s in the oci-build-task and registry-image resource to learn more about their other configuration options.</p>"},{"location":"docs/how-to/container-image-guides/build-use/","title":"Building an Image and Using it in a Task","text":"<p>This guide will show you how to build and use an image within one job without pushing the image to an external image registry like Docker Hub.</p>"},{"location":"docs/how-to/container-image-guides/build-use/#build-the-image","title":"Build The Image","text":"<p>To avoid repeating ourselves we're going to use the pipeline made in the other guide Building and Pushing an Image. We will start with the pipeline from the Defining the Build Context section.</p> <p>We will add the <code>UNPACK_ROOTFS</code> parameter to the task. This parameter tells the oci-build-task to include the image in a special format that Concourse's container runtime uses.</p> <p>Note</p> <p>In the future this may not be necessary if Concourse starts using the OCI image format.</p> build-and-use-image.yml<pre><code>---\nresources:\n  # The repo with our Dockerfile\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n      branch: main\n\njobs:\n  - name: build-and-use-image\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n            UNPACK_ROOTFS: \"true\" # only needed if using image in a future step\n          run:\n            path: build\n</code></pre> <p>The above pipeline will build a container image and also output it in Concourse's rootfs image format.</p>"},{"location":"docs/how-to/container-image-guides/build-use/#use-the-image","title":"Use the Image","text":"<p>Next we want to add a second task to this job which will use the image generated from the first task as its container image. To use the image from the previous step add the top-level <code>image</code> key to the <code>task</code> step.</p> build-push.yml<pre><code>resources: ... # omitting resource section from above\n\njobs:\n  - name: build-and-use-image\n    plan:\n      - get: concourse-examples\n      - task: build-task-image\n        privileged: true\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              # Check out the README for oci-build-task at\n              # https://github.com/concourse/oci-build-task\n              repository: concourse/oci-build-task\n          inputs:\n            - name: concourse-examples\n          outputs:\n            - name: image\n          params:\n            CONTEXT: concourse-examples/Dockerfiles/simple\n            UNPACK_ROOTFS: \"true\" # only needed if using image in a future step\n          run:\n            path: build\n      - task: use-built-image-in-task\n        image: image\n        config:\n          platform: linux\n          run:\n            path: cat\n            args: [\"/stranger\"]\n</code></pre> <p>You can set the pipeline with the following fly command.</p> <pre><code>fly -t &lt;target&gt; set-pipeline -p build-and-use-image \\\n  -c ./build-and-use-image.yml\n</code></pre>"},{"location":"docs/how-to/git-guides/","title":"Git Guides","text":"<p>The contained guides are meant to demonstrate various git workflows within Concourse such as basic retrieval of repos.</p>"},{"location":"docs/how-to/git-guides/basic/","title":"Basic Git Operations","text":"<p>All of these examples use the concourse/git-resource image. That image is probably the most popular git resource for Concourse since it is shipped in the concourse/concourse image and in the tarball on the GitHub release page. It is not the only resource available for working with git-related resources. If you don't see your use-case on this page then there is probably another resource that you can use. For example, Pull Request workflows can be accomplished with the teliaoss/github-pr-resource.</p> <p>Check out the docs for the git resource for all configuration options.</p>"},{"location":"docs/how-to/git-guides/basic/#fetching-a-repository","title":"Fetching a Repository","text":"<p>Here is how you fetch the contents of a git repository and use it in a task.</p> <pre><code>resources:\n  - name: concourse-examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples\n\njobs:\n  - name: read-the-readme\n    plan:\n      - get: concourse-examples\n      - task: cat-readme\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          inputs: # pass concourse-examples into this task step\n            - name: concourse-examples\n          run:\n            path: cat\n            args: [ \"concourse-examples/README.md\" ]\n</code></pre>"},{"location":"docs/how-to/git-guides/basic/#creating-commits-and-tags","title":"Creating Commits and Tags","text":"<p>Here's a simple way to create a commit using a bash script.</p> <pre><code>resources:\n  - name: repo-main\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/user/my-repo\n      branch: main\n\njobs:\n  - name: create-a-commit\n    plan:\n      - get: repo-main\n      - task: commit-and-tag\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: gitea/gitea # use any image that has the git cli\n          inputs:\n            - name: repo-main\n          outputs:\n            # to pass the commit to the following steps specify\n            # the \"repo-main\" as an output as well\n            - name: repo-main\n          run:\n            path: sh\n            args:\n              - -cx\n              # this is just a bash script\n              - |\n                cd repo-main\n                # edit a file / make a change\n                date +%Y-%m-%d &gt; todays-date\n                git add ./todays-date\n                git commit -m \"Add todays date\"\n                git tag v0.1.6\n      # push commit and tag\n      - put: repo-main\n        params:\n          # specify the \"repo-main\" artifact as the location\n          repository: repo-main\n</code></pre>"},{"location":"docs/how-to/git-guides/basic/#merging-branches","title":"Merging Branches","text":"<p>Here is how you can merge two branches. Common if you are using gitflow and need to merge a <code>dev</code> branch into <code>main</code> every so often.</p> <pre><code>resources:\n  - name: repo-main\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/user/my-repo\n      branch: main\n\n  - name: repo-dev\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/user/my-repo\n      branch: dev\n\njobs:\n  - name: merge-dev-into-main\n    plan:\n      - get: repo-dev\n      - put: repo-main\n        params:\n          repository: repo-dev\n          merge: true\n</code></pre>"},{"location":"docs/how-to/git-guides/multi-branch/","title":"Multi-Branch Workflows","text":"<p>Teams may make use of multiple branches for their development. For instance, some teams create feature branches while working on new functionality - once this functionality is ready, the branch will be merged into the main branch and the feature branch will be deleted.</p> <p>While a feature is under development, you'll often want to run tests against the feature branch and possibly deploy to a staging environment. To model this in Concourse, you'll need to have a pipeline for each active feature branch. Manually setting (and eventually archiving) a pipeline for each feature branch would be quite a burden. For this type of workflow, Concourse has a few important tools to help you out: the <code>set_pipeline</code> step,  <code>across</code>, and instanced pipelines.</p> <p>In this guide, we'll cover:</p> <ol> <li>Writing a pipeline to Test, Build &amp; Deploy a branch to a staging environment. We'll    use Terraform for our deployment</li> <li>Tracking Branches in a repository; for each branch, we'll set a pipeline (using the  <code>set_pipeline</code> step and across)</li> <li>Automatically Cleaning Up Old Workspaces after branches get merged or deleted</li> </ol>"},{"location":"docs/how-to/git-guides/multi-branch/#test-build-deploy","title":"Test, Build &amp; Deploy","text":"<p>We'll start out by defining the pipeline that should run for each active branch. For this example, we'll be working with the following sample Go application.</p> <p>Our pipeline will have three stages:</p> <ol> <li>Run unit tests</li> <li>Build and upload a binary to a blobstore (in our case, we'll    use Google Cloud Storage)</li> <li>Trigger a <code>terraform apply</code> to deploy our app to a staging environment.    The Terraform module we'll use here    doesn't actually provision any infrastructure, and is just used as an example</li> </ol> <p>Since the pipeline config is intended to be used as a template for multiple different branches, we can use Vars to parameterize the config. In particular, we'll use the vars <code>((feature))</code> and <code>((branch))</code>, which represent the name of the feature and the name of the branch, respectively.</p> <p>Below is the full pipeline config from the Examples Repo:</p> template.yml<pre><code>resource_types:\n- name: terraform\n  type: registry-image\n  source:\n    repository: ljfranklin/terraform-resource\n\n- name: gcs\n  type: registry-image\n  source:\n    repository: frodenas/gcs-resource\n\nresources:\n- name: branch\n  type: git\n  source:\n    uri: https://github.com/concourse/examples\n    branch: ((branch))\n\n- name: examples\n  type: git\n  source:\n    uri: https://github.com/concourse/examples\n\n- name: build-artifact\n  type: gcs\n  source:\n    bucket: concourse-examples\n    json_key: ((gcp_service_account_key))\n    regexp: multi-branch/features/((feature))/my-app-(.+)\\.tgz\n\n- name: staging-env\n  type: terraform\n  source:\n    env_name: ((feature))\n    backend_type: gcs\n    backend_config:\n      bucket: concourse-examples\n      prefix: multi-branch/terraform\n      credentials: ((gcp_service_account_key))\n\njobs:\n- name: test\n  build_log_retention:\n    builds: 50\n  plan:\n  - in_parallel:\n    - get: branch\n      trigger: true\n    - get: examples\n  - task: unit\n    file: examples/tasks/go-test.yml\n    input_mapping: {repo: branch}\n    params: {MODULE: apps/golang}\n\n- name: build\n  build_log_retention:\n    builds: 50\n  plan:\n  - in_parallel:\n    - get: branch\n      passed: [test]\n      trigger: true\n    - get: examples\n  - task: build\n    file: examples/tasks/go-build.yml\n    params:\n      MODULE: apps/golang\n      BINARY_NAME: my-app\n    input_mapping: {repo: branch}\n  - put: build-artifact\n    params: {file: \"binary/my-app-*.tgz\"}\n\n- name: deploy\n  build_log_retention:\n    builds: 50\n  plan:\n  - in_parallel:\n    - get: build-artifact\n      passed: [build]\n      trigger: true\n    - get: examples\n  - load_var: bundle_url\n    file: build-artifact/url\n  - put: staging-env\n    params:\n      terraform_source: examples/terraform/staging\n      vars: {bundle_url: ((.:bundle_url))}\n</code></pre>"},{"location":"docs/how-to/git-guides/multi-branch/#tracking-branches","title":"Tracking Branches","text":"<p>In addition to the branch pipeline template, we'll also need a pipeline to track the list of branches and set a pipeline for each one.</p> <p>To track the list of branches in a repository, we can use  <code>aoldershaw/git-branches-resource</code>. This  <code>resource_type</code> emits a new resource version whenever a branch is created or deleted. It also lets us filter the list of branches by a regular expression. In this case, let's assume our feature branches match the regular expression <code>feature/.*</code>.</p> <p>Below is the current pipeline config for this tracker pipeline:</p> tracker.yml<pre><code>resource_types:\n  - name: git-branches\n    type: registry-image\n    source:\n      repository: aoldershaw/git-branches-resource\n\nresources:\n  - name: feature-branches\n    type: git-branches\n    source:\n      uri: https://github.com/concourse/examples\n      # The \"(?P&lt;name&gt;pattern)\" syntax defines a named capture group.\n      # aoldershaw/git-branches-resource emits the value of each named capture\n      # group under the `groups` key.\n      #\n      # e.g. feature/some-feature ==&gt; {\"groups\": {\"feature\": \"some-feature\"}}\n      branch_regex: 'feature/(?P&lt;feature&gt;.*)'\n\n  - name: examples\n    type: git\n    source:\n      uri: https://github.com/concourse/examples\n\njobs:\n  - name: set-feature-pipelines\n    plan:\n      - in_parallel:\n          - get: feature-branches\n            trigger: true\n          - get: examples\n      - load_var: branches\n        file: feature-branches/branches.json\n      - across:\n          - var: branch\n            values: ((.:branches))\n        set_pipeline: dev\n        file: examples/pipelines/multi-branch/template.yml\n        instance_vars: { feature: ((.:branch.groups.feature)) }\n        vars: { branch: ((.:branch.name)) }\n</code></pre> <p>We set each pipeline as an instanced pipeline - this will result in Concourse grouping all the related <code>dev</code> pipelines in the UI.</p>"},{"location":"docs/how-to/git-guides/multi-branch/#cleaning-up-old-workspaces","title":"Cleaning Up Old Workspaces","text":"<p>With the setup described in Tracking Branches, Concourse will automatically archive any pipelines for branches that get removed. However, Concourse doesn't know that it should destroy Terraform workspaces when a branch is removed. To accomplish this, we can yet again make use of the Terraform resource to destroy these workspaces. We'll add another job to the tracker pipeline that figures out which workspaces don't belong to an active branch and destroy them.</p> template.yml<pre><code>resource_types:\n- name: git-branches\n  type: registry-image\n  source:\n    repository: aoldershaw/git-branches-resource\n\n- name: terraform\n  type: registry-image\n  source:\n    repository: ljfranklin/terraform-resource\n\nresources:\n- name: feature-branches\n  type: git-branches\n  source:\n    uri: https://github.com/concourse/examples\n    # The \"(?P&lt;name&gt;pattern)\" syntax defines a named capture group.\n    # aoldershaw/git-branches-resource emits the value of each named capture\n    # group under the `groups` key.\n    #\n    # e.g. feature/some-feature ==&gt; {\"groups\": {\"feature\": \"some-feature\"}}\n    branch_regex: 'feature/(?P&lt;feature&gt;.*)'\n\n- name: examples\n  type: git\n  source:\n    uri: https://github.com/concourse/examples\n\n- name: staging-env\n  type: terraform\n  source:\n    backend_type: gcs\n    backend_config: &amp;terraform_backend_config\n      bucket: concourse-examples\n      prefix: multi-branch/terraform\n      credentials: ((gcp_service_account_key))\n\njobs:\n- name: set-feature-pipelines\n  build_log_retention:\n    builds: 50\n  plan:\n  - in_parallel:\n    - get: feature-branches\n      trigger: true\n    - get: examples\n  - load_var: branches\n    file: feature-branches/branches.json\n  - across:\n    - var: branch\n      values: ((.:branches))\n    set_pipeline: dev\n    file: examples/pipelines/multi-branch/template.yml\n    instance_vars: {feature: ((.:branch.groups.feature))}\n    vars: {branch: ((.:branch.name))}\n\n- name: cleanup-inactive-workspaces\n  build_log_retention:\n    builds: 50\n  plan:\n  - in_parallel:\n    - get: feature-branches\n      passed: [set-feature-pipelines]\n      trigger: true\n    - get: examples\n  - task: find-inactive-workspaces\n    config:\n      platform: linux\n      image_resource:\n        type: registry-image\n        source: {repository: hashicorp/terraform}\n      inputs:\n      - name: feature-branches\n      outputs:\n      - name: extra-workspaces\n      params:\n        TERRAFORM_BACKEND_CONFIG:\n          gcs: *terraform_backend_config\n      run:\n        path: sh\n        args:\n        - -c\n        - |\n          set -euo pipefail\n\n          apk add -q jq\n\n          active_features=\"$(jq '[.[].groups.feature]' feature-branches/branches.json)\"\n\n          jq -n \"{terraform: {backend: $TERRAFORM_BACKEND_CONFIG}}\" &gt; backend.tf.json\n          terraform init\n\n          # List all active workspaces, ignoring the default workspace\n          active_workspaces=\"$(terraform workspace list | grep -v '^[*]' | tr -d ' ' | jq --raw-input --slurp 'split(\"\\n\") | map(select(. != \"\"))')\"\n\n          jq -n \"$active_workspaces - $active_features\" &gt; extra-workspaces/workspaces.json\n  - load_var: extra_workspaces\n    file: extra-workspaces/workspaces.json\n  - across:\n    - var: workspace\n      values: ((.:extra_workspaces))\n    put: staging-env\n    params:\n      terraform_source: examples/terraform/staging\n      env_name: ((.:workspace))\n      action: destroy\n    get_params:\n      action: destroy\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/","title":"Pipeline Guides","text":"<p>The contained guides are meant to demonstrate various pipeline workflows within Concourse such as Gated and Time  Triggered Pipelines.</p>"},{"location":"docs/how-to/pipeline-guides/common-pipeline/","title":"Common Pipeline Practices","text":"<p>The following are practices that we see a lot of people use in their pipelines. These are by no means \"Best\" practices, they are simply common and may or may not work for you and your team.</p>"},{"location":"docs/how-to/pipeline-guides/common-pipeline/#parallelizing-get-steps-in-jobs","title":"Parallelizing Get Steps in Jobs","text":"<p>All jobs usually have <code>get</code> steps as their first set of steps.</p> <pre><code>jobs:\n  - name: awesome-job\n    plan:\n      - get: cool-code\n      - get: funny-binary\n      - get: the-weather\n      - task: business-stuff\n</code></pre> <p>To reduce the waiting time to the length of the longest running get step, put all <code>get</code> steps under an  <code>in_parallel</code> step.</p> <pre><code>jobs:\n  - name: awesome-job\n    plan:\n      - in_parallel:\n          - get: cool-code\n          - get: funny-binary\n          - get: the-weather\n      - task: business-stuff\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/common-pipeline/#specify-inputs-for-put-steps","title":"Specify Inputs for Put Steps","text":"<p>By default, <code>put</code> step's have all artifacts from a job mounted in their resource container. This can result in long initialization times for put steps. It's likely that a <code>put</code> step only needs a subset of all available artifacts generated throughout the job.</p> <p>There are two ways to specify which artifacts to send to a <code>put</code> step. You can specify <code>detect</code> as the <code>put</code> step <code>inputs</code>, or you can pass in an exact list of all artifacts the  <code>put</code> step needs.</p> <p>Using <code>detect</code>:</p> <pre><code>jobs:\n  - name: the-job\n    plan: # Get some artifacts\n      - in_parallel:\n          - get: apples\n          - get: apple-basket\n          - get: monkeys\n      # using detect will result in \"apples-location\" and \"basket\" being passed in\n      # \"monkeys\" will not be passed in\n      - put: apples-in-basket\n        inputs: detect\n        params:\n          apples-location: apples/location # matches the first get step\n          basket: apple-basket # matches the second get step\n</code></pre> <p>Specifying the exact inputs needed for the <code>put</code> step:</p> <pre><code>jobs:\n  - name: the-job\n    plan: # Get some artifacts\n      - in_parallel:\n          - get: apples\n          - get: apple-basket\n          - get: monkeys\n      - put: apples-in-basket\n        inputs: [ apples, apple-basket ] # specify the exact inputs needed\n        params:\n          apples-location: apples/location\n          basket: apple-basket\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/common-pipeline/#putting-task-configs-in-files","title":"Putting Task Configs in Files","text":"<p>A lot of the pipeline examples that you will find on this site and in resource repos will embed a <code>task</code> step <code>config</code> directly in the pipeline. This is a nice way of clearly seeing what inputs/outputs the task uses. Tasks are usually designed to be used in multiple places, maybe with slightly different configuration. To support this scenario, most users store task configs in files instead of embedding the config directly in the pipeline.</p> <p>Here's what this looks like in practice:</p> print-date.yml, stored in some git repo under a folder named tasks<pre><code>platform: linux\n\nimage_resource: # define a default image for the task to use\n  type: registry-image\n  source:\n    repository: busybox\n\nrun:\n  path: date\n  args: [ \"+%Y-%m-%d\" ]\n</code></pre> <p>Using the task in a pipeline:</p> pipeline.yml<pre><code>resources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n\njobs:\n  - name: the-best-job\n    plan:\n      - get: ci\n      - task: today\n        file: ci/tasks/print-date.yml\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/common-pipeline/#get-images-for-tasks-instead-of-using-anonymous-image-resources","title":"<code>Get</code> Images for Tasks Instead of using Anonymous Image Resources","text":"<p>It is easy to let Concourse fetch images for tasks right when they are needed by using the <code>task-config.image_resource</code> field in a task config. It's the easy out-of-the-box solution. Another way is to pass the image for a task as an input to the job by setting the <code>task</code> step <code>image</code> field. This also allows you to track the version of the image being used by the task and also avoid getting rate-limited by configuring the resource with credentials.</p> BeforeAfter Anonymous Image Fetching for Tasks<pre><code>jobs:\n  - name: job\n    plan:\n      - task: simple-task\n        config:\n          platform: linux\n          image_resource: # anonymous image resource\n            type: registry-image\n            source:\n              repository: busybox\n          run:\n            path: echo\n            args: [ \"Hello world!\" ]\n</code></pre> Passing Task Image as Job Inputs<pre><code>resources:\n  - name: busybox\n    type: registry-image\n    source:\n      repository: busybox\n      username: ((docker.user))\n      password: ((docker.password))\n\njobs:\n  - name: job\n    plan:\n      - get: busybox # pass image into job\n      - task: simple-task\n        image: busybox # use image for task. Overrides anonymous image\n        config:\n          platform: linux\n          run:\n            path: echo\n            args: [ \"Hello world!\" ]\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/gated-pipelines/","title":"Gated Pipeline Patterns","text":"<p>Gated pipelines provide control for administrators and release managers on when a given software release is deployed to a tightly protected environment (e.g. production).</p> <p>The execution of jobs that perform certain tasks (e.g. deployment) targeting the downstream environment beyond the \" gate\" step is done only upon either an approval coming from an external Change Control system or an explicit manual trigger of such step.</p>"},{"location":"docs/how-to/pipeline-guides/gated-pipelines/#1-a-simple-gated-pipeline","title":"1) - A Simple Gated Pipeline","text":"<p>By default, all Jobs only run when manually triggered. That means a user has to run  <code>fly trigger-job</code> or click the plus button in the web interface for a job to run. A job only runs automatically if one of its resources has the <code>trigger: true</code> parameter set.</p> <p>Therefore, in order to create a gated job in a pipeline you simply need to create a job that can only be manually triggered. That means not setting <code>trigger: true</code> for any of the jobs' <code>get</code> steps.</p> <pre><code>jobs:\n  - name: run-automatically\n    plan:\n      - get: my-repo\n        trigger: true  # has trigger:true so automatically triggers\n    # can include more steps to run other things before hitting the gate\n\n  - name: the-gate  # manually trigger this job\n    plan:\n      - get: my-repo\n        trigger: false  # redundant but guarantees the job won't run automatically\n        passed:\n          - run-automatically\n\n  # runs immediately after the gate is triggered\n  - name: do-more-stuff-after-the-gate\n    plan:\n      - get: my-repo\n        passed:\n          - the-gate\n        trigger: true\n    # can include more steps to run other things\n\nresources:\n  - name: my-repo\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre> <p></p>"},{"location":"docs/how-to/pipeline-guides/gated-pipelines/#2-gated-pipeline-fanning-in-and-out","title":"2) - Gated Pipeline Fanning In and Out","text":"<p>You can also use a gate as way to fan-in from multiple jobs and/or fan-out to multiple jobs as well.</p> <pre><code>jobs:\n  # three pre-gate jobs\n  - name: job-a\n    plan:\n      - get: my-repo\n        trigger: true\n  - name: job-b\n    plan:\n      - get: my-repo\n        trigger: true\n  - name: job-c\n    plan:\n      - get: my-repo\n        trigger: true\n\n  - name: the-gate  # manually trigger this job\n    plan:\n      - get: my-repo\n        trigger: false\n        passed: # fan-in from the three pre-gate jobs\n          - job-a\n          - job-b\n          - job-c\n\n  # fan-out to three post-gate jobs\n  - name: post-gate-job-a\n    plan:\n      - get: my-repo\n        trigger: true\n        passed: [ the-gate ]\n  - name: post-gate-job-b\n    plan:\n      - get: my-repo\n        trigger: true\n        passed: [ the-gate ]\n  - name: post-gate-job-c\n    plan:\n      - get: my-repo\n        trigger: true\n        passed: [ the-gate ]\n\nresources:\n  - name: my-repo\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre> <p></p>"},{"location":"docs/how-to/pipeline-guides/gated-pipelines/#3-a-gated-pipeline-with-notifications","title":"3) - A Gated Pipeline With Notifications","text":"<p>This pipeline shows you how you can send a notification, like an email, to notify someone that a new build of your application is ready to be shipped.</p> <pre><code>jobs:\n  - name: build-it\n    plan:\n      - get: my-repo\n        trigger: true\n    # can add steps to build your app\n\n  - name: test-it\n    plan:\n      - get: my-repo\n        trigger: true\n        passed: [ build-it ]\n      # can add steps to run tests\n      - put: email-release-manager\n        params:\n          subject: \"Ready to ship\"\n          body_text: |\n            A build is ready to be shipped!\n            Build to be shipped: ${ATC_EXTERNAL_URL}/teams/${BUILD_TEAM_NAME}/pipelines/${BUILD_PIPELINE_NAME}/jobs/${BUILD_JOB_NAME}/builds/${BUILD_NAME}\n            Link to pipeline: ${ATC_EXTERNAL_URL}/teams/${BUILD_TEAM_NAME}/pipelines/${BUILD_PIPELINE_NAME}\n\n  - name: ship-it\n    plan:\n      - get: my-repo\n        trigger: false\n        passed: [ test-it ]\n\nresources:\n  - name: my-repo\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n  - name: email-release-manager\n    type: email\n    source:\n      # other required fields for this resource have been omitted\n      from: pipeline@example.com\n      to: release-manager@example.com\n\nresource_types:\n  - name: email\n    type: registry-image\n    source:\n      repository: pcfseceng/email-resource\n</code></pre> <p></p>"},{"location":"docs/how-to/pipeline-guides/managing-pipeline-configs/","title":"Managing Pipeline Configurations","text":"<p>When first starting with Concourse, it is common to write the pipeline and set it with <code>fly set-pipeline</code>. This works fine when initially building a pipeline or testing out some new changes. Once you're past the initial building phase though, you probably want to store your pipeline YAML files somewhere that you and others on your team can access and update the pipeline without worrying about remembering to do <code>fly set-pipeline</code> every time. A Git repository is a good place to store your pipeline YAML files, so our pipeline management conventions start with a Git repository.</p> <p>Most other CI/CD tools will tell you to store your \"pipeline\" or \"workflow\" configuration files in a <code>.something</code> directory in your app's git repository. You could do that with Concourse too and create a <code>.concourse</code> directory, but Concourse does not force any convention onto you. There are so many ways that you could store and manage your pipeline configuration files. The most common way is to use a Git repository, but there's nothing stopping you from using a versioned S3 bucket either.</p> <p>Let's cover the two most common conventions for storing pipeline configurations.</p>"},{"location":"docs/how-to/pipeline-guides/managing-pipeline-configs/#1-in-your-apps-git-repository","title":"1) In Your App's Git Repository","text":"<p>This is what most users will be familiar with coming from other CI/CD tools. Simply make a folder in the same repository as your code. Name the folder whatever you want. Some possible names if you need inspiration:</p> <ul> <li><code>ci</code></li> <li><code>concourse</code></li> <li><code>pipelines</code></li> </ul> <p>Then store your pipeline YAML files in that directory. Again, there's no \"special way\" that Concourse expects you to store your pipeline YAML files. Do whatever makes sense to you!</p> <p>To automatically update your pipeline in Concourse with what's stored in your Git repository, use the  <code>set_pipeline</code> step in a job. You can view an example of a pipeline updating itself in the examples section. There are also examples on the  <code>set_pipeline</code> step page.</p>"},{"location":"docs/how-to/pipeline-guides/managing-pipeline-configs/#2-in-a-different-git-repository","title":"2) In a Different Git Repository","text":"<p>This is the most common convention that Concourse users follow, especially if your pipelines interact with multiple Git repositories. The Concourse project does this; we store all of our pipeline and task YAML files in  <code>github.com/concourse/ci</code>, completely separate from the main repository at  <code>github.com/concourse/concourse</code> and all the resource type repositories.</p>"},{"location":"docs/how-to/pipeline-guides/managing-pipeline-configs/#2a-parent-child-pipeline-relationships","title":"2.a) Parent-Child Pipeline Relationships","text":"<p>Tip</p> <p>The following is also described on the <code>set_pipeline</code> step page.</p> <p>If you are setting multiple pipelines, or multiple instances of the same pipeline, it can be helpful to manage them from one place. Concourse allows you to use the <code>set_pipeline</code> step to create other pipelines. The set_pipeline step is not limited to updating the current pipeline.</p> <p>When you use one pipeline to create other pipelines, this creates a parent-child relationship that Concourse tracks. You can see an example of this here in <code>set-pipelines.yml</code>.</p> <p>As long as the parent pipeline continues to set/update the child pipeline(s), the child pipeline(s) will remain active. If the parent pipeline stops updating the child pipeline(s) (e.g. you updated the parent pipeline to not set/update the child pipeline(s) anymore), Concourse will archive the pipeline. This pauses the child pipeline( s) and hides them from the web UI. The child pipeline configuration is deleted, but its build logs are retained.</p> <p>If you want to fully delete a pipeline, use  <code>fly destroy-pipeline</code>.</p>"},{"location":"docs/how-to/pipeline-guides/manual-approval/","title":"Manual Approval Step","text":"<p>This is an example of a <code>task</code> step you can add to your Jobs that requires a human to approve or reject the job from running. This is probably the most minimal version of a manual approval step you can have in Concourse that doesn't require pulling in a bunch of other tech into your stack. It's definitely not the best UX since you need to use the <code>fly</code> CLI to approve the step.</p> <p>Task configuration, <code>config.yml</code>:</p> <pre><code>platform: linux\n\ninputs:\n  - name: repo\n\nparams:\n  APPROVAL_TIMEOUT: 600 #default of 10mins\n\nrun:\n  path: repo/tasks/manual-approval/run.sh\n</code></pre> <p>Task script, <code>run.sh</code>:</p> <pre><code>#!/usr/bin/env bash\n\nset -euo pipefail\n\ntimeout=$((EPOCHSECONDS+APPROVAL_TIMEOUT))\necho -n \"Waiting for manual approval...\"\nuntil [[ -f /tmp/approved || $EPOCHSECONDS -gt $timeout ]]; do\n    sleep 5\n    echo -n \".\"\ndone\n\nif [[ -f /tmp/approved ]]; then\n    echo \"Step approved!\"\nelse\n    echo \"Approval timeout reached. Aborting job.\"\n    exit 1\nfi\n</code></pre> <p>To approve the job when it gets to this step you have to create <code>/tmp/approved</code> on the step's container. You can do that user <code>fly</code>'s <code>intercept</code> command, like so (replace <code>PIPELINE/JOB</code> with the name of your pipeline and job that the step resides in):</p> <pre><code>fly -t ci intercept --job PIPELINE/JOB --step manual-approval touch /tmp/approved\n</code></pre> <p>Here's the step added in-line to a pipeline so you can see how it works on its own.</p> <pre><code>jobs:\n  - name: approval\n    plan:\n      - task: manual-approval\n        params:\n          APPROVAL_TIMEOUT: 600 #10mins\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          run:\n            path: bash\n            args:\n              - -c\n              - |\n                #!/usr/bin/env bash\n\n                set -euo pipefail\n\n                timeout=$((EPOCHSECONDS+APPROVAL_TIMEOUT))\n                echo -n \"Waiting for manual approval...\"\n                until [[ -f /tmp/approved || $EPOCHSECONDS -gt $timeout ]]; do\n                    sleep 5\n                    echo -n \".\"\n                done\n\n                if [[ -f /tmp/approved ]]; then\n                    echo \"Step approved!\"\n                else\n                    echo \"Approval timeout reached. Aborting job.\"\n                    exit 1\n                fi\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/","title":"Exploring Task Input and Output Scenarios","text":"<p>Understanding how task inputs and outputs work in Concourse can be a little confusing initially. This guide will walk you through a few example pipelines to show you how inputs and outputs work within a single Concourse job. By the end you should understand how inputs and outputs work within the context of a single job.</p> <p>To run the pipelines in the following examples yourself you can get your own Concourse running locally by following the Quick Start guide. Then use  <code>fly set-pipeline</code> to see the pipelines in action.</p>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/#1-passing-inputs-between-tasks","title":"1) - Passing Inputs Between Tasks","text":"<p>This pipeline will show us how to create outputs and pass outputs as inputs to the next step in a job plan.</p> <p>This pipeline has two tasks. The first task outputs a file with the date. The second task reads and prints the contents of the file from the first task.</p> <p>Here's a visualization of the job.</p> <p></p> passing-artifacts.yml<pre><code>busybox: &amp;busybox #YAML anchor\n  type: registry-image\n  source:\n    repository: busybox\n\njobs:\n  - name: the-job\n    plan:\n      - task: create-one-output\n        config:\n          platform: linux\n          image_resource: *busybox\n          outputs:\n            # Concourse will make an empty dir with this name\n            # and save the contents for later steps\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file\n      - task: read-output-from-previous-step\n        config:\n          platform: linux\n          image_resource: *busybox\n          # You must explicitly name the inputs you expect\n          # this task to have.\n          # If you don't then outputs from previous steps\n          # will not appear in the step's container.\n          # The name must match the output from the previous step.\n          # Try removing or renaming the input to see what happens!\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./the-output/file\n</code></pre> <p>Set and run this pipeline to see the results yourself. Save the pipeline in a file called <code>passing-artifacts.yml</code>.</p> <pre><code>fly -t tutorial set-pipeline -p passing-artifacts -c passing-artifacts.yml\nfly -t tutorial unpause-pipeline -p passing-artifacts\nfly -t tutorial trigger-job --job passing-artifacts/the-job --watch\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/#2-two-tasks-with-the-same-output-who-wins","title":"2) - Two tasks with the same output, who wins?","text":"<p>This scenario is to satisfy the curiosity cat inside all of us. Never do this in real life because you're definitely going to hurt yourself!</p> <p>There are two Jobs in this pipeline. The first job, <code>writing-in-parallel</code>, has two Steps; both steps will produce an artifact named <code>the-output</code> in parallel. If you run the <code>writing-to-the-same-output-in-parallel</code> job multiple times you'll see the file in <code>the-output</code> folder changes depending on which of the parallel tasks finished last. Here's a visualization of the first job.</p> <p></p> <p>The second job, <code>writing-to-the-same-output-serially</code>, is a serial version of the first job. In this job the second task always wins because it's the last task that outputs <code>the-output</code>, so only <code>file2</code> will be in <code>the-output</code> directory in the last step in the job plan.</p> <p></p> <p>The lesson to take away from this example is that last to write wins when it comes to the state of any particular artifact in your job.</p> parallel-artifacts.yml<pre><code>busybox: &amp;busybox #YAML anchor\n  type: registry-image\n  source:\n    repository: busybox\n\njobs:\n  - name: writing-to-the-same-output-in-parallel\n    plan:\n      # running two tasks that output in parallel?!?\n      # who will win??\n      - in_parallel:\n          - task: create-the-output\n            config:\n              platform: linux\n              image_resource: *busybox\n              outputs:\n                - name: the-output\n              run:\n                path: /bin/sh\n                args:\n                  - -cx\n                  - |\n                    ls -lah\n                    date &gt; ./the-output/file1\n          - task: also-create-the-output\n            config:\n              platform: linux\n              image_resource: *busybox\n              outputs:\n                - name: the-output\n              run:\n                path: /bin/sh\n                args:\n                  - -cx\n                  - |\n                    ls -lah\n                    date &gt; ./the-output/file2\n      # run this job multiple times to see which\n      # previous task wins each time\n      - task: read-output-from-previous-step\n        config:\n          platform: linux\n          image_resource: *busybox\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./the-output\n                echo \"Get ready to error!\"\n                cat ./the-output/file1 ./the-output/file2\n\n  - name: writing-to-the-same-output-serially\n    plan:\n      - task: create-the-output\n        config:\n          platform: linux\n          image_resource: *busybox\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file1\n      - task: also-create-the-output\n        config:\n          platform: linux\n          image_resource: *busybox\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file2\n      - task: read-output-from-previous-step\n        config:\n          platform: linux\n          image_resource: *busybox\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./the-output\n                echo \"Get ready to error! file1 will never exist\"\n                cat ./the-output/file1 ./the-output/file2\n</code></pre> <p>Set and run this pipeline to see the results yourself. Save the pipeline in a file called <code>parallel-artifacts.yml</code>.</p> <pre><code>fly -t tutorial set-pipeline -p parallel-artifacts -c parallel-artifacts.yml\nfly -t tutorial unpause-pipeline -p parallel-artifacts\nfly -t tutorial trigger-job --job parallel-artifacts/writing-to-the-same-output-in-parallel --watch\nfly -t tutorial trigger-job --job parallel-artifacts/writing-to-the-same-output-serially --watch\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/#3-mapping-the-names-of-inputs-and-outputs","title":"3) - Mapping the Names of Inputs and Outputs","text":"<p>Sometimes the names of inputs and outputs don't match between multiple task configs, or they do match, and you don't want them overwriting each other, like in the previous example. That's when <code>input_mapping</code> and <code>output_mapping</code> become helpful. Both of these features rename the inputs/outputs in the task's config to some other name in the job plan.</p> <p>This pipeline has one job with four tasks.</p> <p>The first task outputs a file with the date to the <code>the-output</code> directory. <code>the-output</code> is mapped to the new name <code>demo-disk</code>. The artifact <code>demo-disk</code> is now available in the rest of the job plan for future steps to take as inputs.</p> <p>The second task reads and prints the contents of the file under the new name <code>demo-disk</code>.</p> <p>The third task reads and prints the contents of the file under another name, <code>generic-input</code>. The <code>demo-disk</code> artifact in the job plan is mapped to <code>generic-input</code>.</p> <p>The fourth task tries to use the artifact named <code>the-output</code> as its input. This task fails to even start because there was no artifact with the name <code>the-output</code> available in the job plan; it was remapped to <code>demo-disk</code>.</p> <p>Here's a visualization of the job.</p> <p></p> mapping-artifacts.yml<pre><code>busybox: &amp;busybox #YAML anchor\n  type: registry-image\n  source:\n    repository: busybox\n\njobs:\n  - name: the-job\n    plan:\n      - task: create-one-output\n        # The task config has the artifact `the-output`\n        # output_mapping will rename `the-output` to `demo-disk`\n        # in the rest of the job's plan\n        output_mapping:\n          the-output: demo-disk\n        config:\n          platform: linux\n          image_resource: *busybox\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file\n      # this task expects the artifact `demo-disk` so no mapping is needed\n      - task: read-output-from-previous-step\n        config:\n          platform: linux\n          image_resource: *busybox\n          inputs:\n            - name: demo-disk\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./demo-disk/file\n      - task: rename-and-read-output\n        # This task expects the artifact `generic-input`.\n        # input_mapping will map the task's `generic-input` to\n        # the job plans `demo-disk` artifact.\n        # `demo-disk` is renamed to `generic-input`.\n        input_mapping:\n          generic-input: demo-disk\n        config:\n          platform: linux\n          image_resource: *busybox\n          inputs:\n            - name: generic-input\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./generic-input/file\n      - task: try-to-read-the-output\n        input_mapping:\n          generic-input: demo-disk\n        config:\n          platform: linux\n          image_resource: *busybox\n          # `the-output` is not available in the job plan\n          # so this task will error while initializing\n          # since there's no artiact named `the-output` in\n          # the job's plan\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                cat ./generic-input/file\n</code></pre> <p>Set and run this pipeline to see the results yourself. Save the pipeline in a file called <code>mapping-artifacts.yml</code>.</p> <pre><code>fly -t tutorial set-pipeline -p mapping-artifacts -c mapping-artifacts.yml\nfly -t tutorial unpause-pipeline -p mapping-artifacts\nfly -t tutorial trigger-job --job mapping-artifacts/the-job --watch\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/#4-adding-files-to-an-existing-artifact","title":"4) - Adding Files to an Existing Artifact","text":"<p>This pipeline will also have two jobs in order to illustrate this point. What happens if we add a file to an output? If you think back to example two you may already know the answer.</p> <p>The first task will create <code>the-output</code> with <code>file1</code>. The second task will add <code>file2</code> to the <code>the-output</code>. The last task will read the contents of <code>file1</code> and <code>file2</code>.</p> <p>As long as you re-declare the input as an output in the second task you can modify any of your outputs.</p> <p>This means you can pass something between a bunch of tasks and have each task add or modify something in the artifact.</p> <p>Here's a visualization of the job.</p> <p></p> existing-artifact.yml<pre><code>jobs:\n  - name: add-file-to-output\n    plan:\n      - task: create-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file1\n      - task: add-file-to-previous-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          # this task lists the same artifact as\n          # its input and output\n          inputs:\n            - name: the-output\n          outputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output/file2\n      - task: read-output-from-previous-step\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          inputs:\n            - name: the-output\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./the-output\n                cat ./the-output/file1 ./the-output/file2\n</code></pre> <p>Set and run this pipeline to see the results yourself. Save the pipeline in a file called <code>existing-artifact.yml</code>.</p> <pre><code>fly -t tutorial set-pipeline -p existing-artifact -c existing-artifact.yml\nfly -t tutorial unpause-pipeline -p existing-artifact\nfly -t tutorial trigger-job --job existing-artifact/add-file-to-output --watch\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/#5-task-with-multiple-inputs-and-outputs","title":"5) - Task With Multiple Inputs and Outputs","text":"<p>What happens if you have a task that has multiple outputs and a second task that only lists one of the outputs? Does the second task get the extra outputs from the first task?</p> <p>The answer is no. A task will only get the artifacts that match the name of the inputs listed in the task's config.</p> <p>Here's a visualization of the job.</p> <p></p> multiple-artifacts.yml<pre><code>jobs:\n  - name: multiple-outputs\n    plan:\n      - task: create-three-outputs\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          outputs:\n            - name: the-output-1\n            - name: the-output-2\n            - name: the-output-3\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah\n                date &gt; ./the-output-1/file\n                date &gt; ./the-output-2/file\n                date &gt; ./the-output-3/file\n      - task: take-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          # only one of the three outputs are\n          # listed as inputs\n          inputs:\n            - name: the-output-1\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./\n                cat ./the-output-1/file\n      - task: take-two-outputs\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          # this task pulls in the other\n          # two outputs, just for fun!\n          inputs:\n            - name: the-output-2\n            - name: the-output-3\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./\n                cat ./the-output-2/file\n                cat ./the-output-3/file\n</code></pre> <p>Set and run this pipeline to see the results yourself. Save the pipeline in a file called <code>multiple-artifacts.yml</code>.</p> <pre><code>fly -t tutorial set-pipeline -p multiple-artifacts -c multiple-artifacts.yml\nfly -t tutorial unpause-pipeline -p multiple-artifacts\nfly -t tutorial trigger-job --job multiple-artifacts/multiple-outputs --watch\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/task-inputs-outputs/#6-get-steps-generate-artifacts","title":"6) - Get Steps Generate Artifacts","text":"<p>The majority of Concourse pipelines have at least one resource, which means they have at least one <code>get</code> step. Using a <code>get</code> step in a job makes an artifact with the name of the get step available for later steps in the job plan to consume as inputs.</p> <p>Here's a visualization of the job.</p> <p></p> get-artifact.yml<pre><code>resources:\n  - name: concourse-examples\n    type: git\n    source:\n      uri: \"https://github.com/concourse/examples\"\n\njobs:\n  - name: get-job\n    plan:\n      # there will be an artifact named\n      # \"concourse-examples\" available in the job plan\n      - get: concourse-examples\n      - task: take-one-output\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          inputs:\n            - name: concourse-examples\n          run:\n            path: /bin/sh\n            args:\n              - -cx\n              - |\n                ls -lah ./\n                cat ./concourse-examples/README.md\n</code></pre> <p>Set and run this pipeline to see the results yourself. Save the pipeline in a file called <code>get-artifact.yml</code>.</p> <pre><code>fly -t tutorial set-pipeline -p get-artifact -c get-artifact.yml\nfly -t tutorial unpause-pipeline -p get-artifact\nfly -t tutorial trigger-job --job get-artifact/get-job --watch\n</code></pre>"},{"location":"docs/how-to/pipeline-guides/time-triggered-pipelines/","title":"Time Triggered Pipeline Patterns","text":"<p>The time resource produces a new version for the time interval that was declared in its definition in the pipeline configuration file.</p> <p>The two most common usages are having the time resource trigger on an interval:</p> <pre><code>resources:\n  - name: trigger-every-3-minutes\n    type: time\n    source:\n      interval: 3m\n</code></pre> <p>Or trigger once within a certain time range:</p> <pre><code>resources:\n  - name: trigger-daily-between-1am-and-2am\n    type: time\n    source:\n      start: 1:00 AM\n      stop: 2:00 AM\n      location: America/Toronto\n</code></pre> <p>Check the README of the time resource for more details.</p>"},{"location":"docs/how-to/pipeline-guides/time-triggered-pipelines/#1-single-time-trigger","title":"1) - Single Time Trigger","text":"<p>The following is an example of a pipeline that is triggered by a time resource on a pre-determined interval.</p> <pre><code>resources:\n  - name: trigger-every-3-minutes\n    type: time\n    source:\n      interval: 3m\n\njobs:\n  - name: run-forrest-run\n    plan:\n      - get: trigger-every-3-minutes\n        trigger: true\n    # can add other steps to run in this job\n\n  - name: run-bubba-run\n    plan:\n      - get: trigger-every-3-minutes\n        trigger: true\n        passed:\n          - run-forrest-run\n    # can add other steps to run in this job\n</code></pre> <p></p>"},{"location":"docs/how-to/pipeline-guides/time-triggered-pipelines/#2-multiple-time-triggers","title":"2) - Multiple Time Triggers","text":"<p>As an enhancement to the previous sample with a single time trigger, this pipeline example implements two time resource triggers and the ability to manually kick it off outside the time resources schedules.</p> <p>The first time you set up a pipeline like this you will need to manually trigger it in order to satisfy the passed constraint of the <code>manual-trigger</code> resource. Once one version is available that satisfies the passed constraint all future triggers by the other resources will work as expected.</p> <pre><code>resources:\n  - name: trigger-every-4-minutes\n    type: time\n    source:\n      interval: 4m\n  - name: trigger-every-10-minutes\n    type: time\n    source:\n      interval: 10m\n  - name: manual-trigger\n    type: time\n    source:\n      interval: 1m\n\njobs:\n  - name: manual-trigger\n    plan:\n      - put: manual-trigger\n\n  - name: run-forrest-run\n    plan:\n      - get: trigger-every-4-minutes\n        trigger: true\n      - get: trigger-every-10-minutes\n        trigger: true\n      - get: manual-trigger\n        trigger: true\n        passed:\n          - manual-trigger\n    # can add other steps to run in this job\n\n  - name: run-bubba-run\n    plan:\n      - get: trigger-every-4-minutes\n        trigger: true\n        passed:\n          - run-forrest-run\n      - get: trigger-every-10-minutes\n        trigger: true\n        passed:\n          - run-forrest-run\n      - get: manual-trigger\n        trigger: true\n        passed:\n          - run-forrest-run\n    # can add other steps to run in this job\n</code></pre> <p></p>"},{"location":"docs/install/","title":"Install","text":"<p>A Concourse installation is composed of a <code>web</code> node, a <code>worker</code> node, and a PostgreSQL node.</p> <p>There are many ways to deploy Concourse, depending on your personal preference. The Quick Start guide shows how to get Concourse up and running quickly via Docker Compose, and there is also an official Concourse Helm chart.</p> <p>The documentation found here will primarily focus on the <code>concourse</code> CLI, which is the lowest common denominator, and can also be directly used if you want to just run Concourse yourself on real hardware or your own managed VMs.</p> <p>The high-level steps to follow for installing Concourse are:</p> <ol> <li>Setup a Postgres database</li> <li>Generate Secrets for the web and worker nodes</li> <li>Install the web node</li> <li>Install the worker node</li> </ol> <p>Note</p> <p>We don't document every configuration option for the <code>web</code> and <code>worker</code> commands. To view all flags you can  run the following <code>docker</code> commands.</p> <pre><code>docker run -t concourse/concourse web --help\ndocker run -t concourse/concourse worker --help\n</code></pre>"},{"location":"docs/install/concourse-cli/","title":"The concourse CLI","text":""},{"location":"docs/install/concourse-cli/#the-concourse-cli","title":"The <code>concourse</code> CLI","text":"<p>The <code>concourse</code> CLI can be downloaded from the latest GitHub release - make sure to grab the appropriate archive for your platform. Each <code>concourse-*</code> archive contains the following files:</p> <pre><code>concourse/bin/concourse\nconcourse/bin/gdn            # Linux only\nconcourse/fly-assets/...\nconcourse/resource-types/... # Linux only\n</code></pre> <p>The Linux release is the largest among all the platforms because it is prepackaged with a bundle of resource types like the git, time, and registry-image resources. Resources only run on Linux workers, that's why the other platforms are not bundled with resources; resources don't currently exist for non-linux platforms.</p> <p>When extracted, the <code>concourse</code> binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere. On Linux a typical install location is <code>/usr/local/concourse</code>:</p> <pre><code>tar -zxf concourse-*.tgz -C /usr/local\n</code></pre> <p>From there, you can either add <code>/usr/local/concourse/bin</code> to your <code>$PATH</code>, or just execute <code>/usr/local/concourse/bin/concourse</code> directly.</p>"},{"location":"docs/install/concourse-cli/#configuring-concourse","title":"Configuring <code>concourse</code>","text":"<p>All Concourse <code>web</code> and <code>worker</code> node configuration is defined statically via flags. For a full list of flags, you can pass <code>--help</code> to any command.</p> CLI Commands<pre><code>concourse web --help\nconcourse worker --help\nconcourse quickstart --help\nconcourse migrate --help\nconcourse generate-key --help\nconcourse land-worker --help\nconcourse retire-worker --help\n</code></pre> <p>Each flag can also be set via an environment variable. The env var for each flag is based on the flag name uppercased, preceded with <code>CONCOURSE_</code> and dashes (<code>-</code>) replaced with underscores (<code>_</code>). These are also shown in <code>--help</code>.</p> <p>Various sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent and interchangeable. Env vars are simply easier to reference in isolation and are more useful to copy-paste.</p>"},{"location":"docs/install/generating-keys/","title":"Generating Keys","text":""},{"location":"docs/install/generating-keys/#generating-the-keys","title":"Generating the Keys","text":"<p>Concourse's various components use RSA keys to verify tokens and worker registration requests.</p> <p>A minimal deployment will require the following keys:</p> <ul> <li>Session Signing Key<ul> <li>Used by the <code>web</code> node for signing and verifying user session tokens.</li> </ul> </li> <li>TSA Host Key<ul> <li>Used by the <code>web</code> node for the SSH worker registration gateway server (\"TSA\").</li> <li>The public key is given to each <code>worker</code> node to verify the remote host when connecting via   SSH.</li> </ul> </li> <li>Worker Key<ul> <li>Each <code>worker</code> node verifies its registration with the <code>web</code> node via a SSH   key.</li> <li>The public key must be listed in the <code>web</code> node's authorized worker keys file in order for the   worker to register.</li> </ul> </li> </ul> <p>To generate these keys, run:</p> <pre><code>concourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n</code></pre> <p>or use <code>ssh-keygen</code>:</p> <pre><code>ssh-keygen -t rsa -b 4096 -m PEM -f ./session_signing_key\nssh-keygen -t rsa -b 4096 -m PEM -f ./tsa_host_key\nssh-keygen -t rsa -b 4096 -m PEM -f ./worker_key\n</code></pre> <p>At this point you should have the following files:</p> <ul> <li><code>session_signing_key</code></li> <li><code>tsa_host_key</code></li> <li><code>tsa_host_key.pub</code></li> <li><code>worker_key</code></li> <li><code>worker_key.pub</code></li> </ul> <p>You can remove the <code>session_signing_key.pub</code> file if you have one, it is not needed by any process in Concourse.</p>"},{"location":"docs/install/generating-keys/#multiple-worker-keys","title":"Multiple Worker Keys","text":"<p>Currently you have one <code>worker_key</code>. You can use this one key-pair with multiple <code>worker</code> nodes. Another good strategy is to have each worker or group of workers use a key that's unique to that one worker or group of workers.</p> <p>In the second case you will end up with multiple private and public worker keys. The <code>web</code> node needs to know about all of the public worker keys. To pass all public worker keys to the <code>web</code> node create a file that contains all of the worker public keys. A common name for this file is <code>authorized_worker_keys.pub</code>. The file should look like this, with one public key per line.</p> <pre><code>$ cat authorized_worker_keys.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCgKtVnbGRJ7Y63QKoO+loS...\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDU6lA4gSRYIc4MXzphJ2l5...\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDgNU7KBz/QQusPO52pNcea...\n</code></pre> <p>You should now have all the necessary keys needed to deploy Web and Worker nodes.</p>"},{"location":"docs/install/running-postgres/","title":"Running a PostgreSQL Node","text":"<p>Concourse uses PostgreSQL for storing all data and coordinating work in a multi- <code>web</code> node installation.</p>"},{"location":"docs/install/running-postgres/#prerequisites","title":"Prerequisites","text":"<p>PostgreSQL v11 or above is required, though the latest available version is recommended.</p>"},{"location":"docs/install/running-postgres/#running-postgresql","title":"Running PostgreSQL","text":"<p>How this node is managed is up to you; Concourse doesn't actually have much of an opinion on it, it just needs a database. By default Concourse will try connecting to a database named <code>atc</code>.</p> <p>How to install PostgreSQL is really dependent on your platform. Please refer to your Linux distribution or operating system's documentation.</p> <p>For the most part, the instruction on Linux should look something like this:</p> <pre><code>sudo apt install postgresql\nsudo su postgres -c \"createuser $(whoami)\"\nsudo su postgres -c \"createdb --owner=$(whoami) atc\"\n</code></pre> <p>This will install PostgreSQL (assuming your distro uses <code>apt</code>), create a user, and create a database that the current UNIX user can access, assuming this same user is going to be running the<code>web</code> node. This is a reasonable default for distros like Ubuntu and Debian which default PostgreSQL to <code>peer</code> auth.</p>"},{"location":"docs/install/running-postgres/#resource-utilization","title":"Resource utilization","text":"<p>CPU usage: this is one of the most volatile metrics, and one we try pretty hard to keep down. There will be near-constant database queries running, and while we try to keep them very simple, there is always more work to do. Expect to feed your database with at least a couple cores, ideally four to eight. Monitor this closely as the size of your deployment and the amount of traffic it's handling increases, and scale accordingly.</p> <p>Memory usage: similar to CPU usage, but not quite as volatile.</p> <p>Disk usage: pipeline configurations and various bookkeeping metadata for keeping track of jobs, builds, resources, containers, and volumes. In addition, all build logs are stored in the database. This is the primary source of disk usage. To mitigate this, log retention can be defined by pipeline authors by using  <code>job.build_log_retention</code>. Concourse operators can also configure a default Build log retention policy that applies to all pipelines.</p> <p>Bandwidth usage: well, it's a database, so it most definitely uses the network. Something important to consider here is the number of simultaneous connections that the database server itself will allow. Postgres exposes a  <code>max_connections</code> configuration variable, and depending on how many web nodes you are running and the size of their connection pool, you may need to tune these two numbers against each other.</p> <p>Highly available: Up to you. Clustered PostgreSQL is kind of new and probably tricky to deploy, but there are various cloud solutions for this.</p> <p>Outbound traffic: None</p> <p>Inbound traffic: Only ever from the <code>web</code> node</p>"},{"location":"docs/install/running-web/","title":"Running a web Node","text":""},{"location":"docs/install/running-web/#running-a-web-node","title":"Running a <code>web</code> node","text":"<p>The <code>web</code> node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.</p>"},{"location":"docs/install/running-web/#prerequisites","title":"Prerequisites","text":"<p>Nothing special - the <code>web</code> node is a pretty simple Go application that can be run like a 12-factor app.</p>"},{"location":"docs/install/running-web/#running-concourse-web","title":"Running <code>concourse web</code>","text":"<p>The <code>concourse</code> CLI can run as a <code>web</code> node via the <code>web</code> subcommand.</p> <p>Before running it, let's configure a local user so we can log in:</p> <pre><code>CONCOURSE_ADD_LOCAL_USER=myuser:mypass\nCONCOURSE_MAIN_TEAM_LOCAL_USER=myuser\n</code></pre> <p>This will configure a single user, <code>myuser</code>, with the password <code>mypass</code>. You'll probably want to change those to sensible values, and later you may want to configure a proper auth provider - check out Auth &amp; Teams whenever you're ready.</p> <p>Next, you'll need to configure the session signing key, the SSH key for the worker gateway, and the authorized worker key. Check Generating Keys to learn what these are and how they are created.</p> <pre><code>CONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key\nCONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key\nCONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys.pub\n</code></pre> <p>Finally, <code>web</code> needs to know how to reach your Postgres database. This can be set like so:</p> <pre><code>CONCOURSE_POSTGRES_HOST=127.0.0.1 # default\nCONCOURSE_POSTGRES_PORT=5432      # default\nCONCOURSE_POSTGRES_DATABASE=atc   # default\nCONCOURSE_POSTGRES_USER=my-user\nCONCOURSE_POSTGRES_PASSWORD=my-password\n</code></pre> <p>If you're running PostgreSQL locally, you can probably just point it to the socket and rely on the <code>peer</code> auth:</p> <pre><code>CONCOURSE_POSTGRES_SOCKET=/var/run/postgresql\n</code></pre> <p>Now that everything's set, run:</p> <pre><code>concourse web\n</code></pre> <p>All logs will be emitted to <code>stdout</code>, with any panics or lower-level errors being emitted to <code>stderr</code>.</p>"},{"location":"docs/install/running-web/#resource-utilization","title":"Resource utilization","text":"<p>CPU usage: peaks during pipeline scheduling, primarily when scheduling Jobs. Mitigated by adding more <code>web</code> nodes. In this regard, <code>web</code> nodes can be considered compute-heavy more than anything else at large scale.</p> <p>Memory usage: not very well classified at the moment as it's not generally a concern. Give it a few gigabytes and keep an eye on it.</p> <p>Disk usage: none</p> <p>Bandwidth usage: aside from handling external traffic, the <code>web</code> node will at times have to stream bits out from one worker and into another while executing Steps.</p> <p>Highly available: <code>yes</code>; web nodes can all be configured the same (aside from <code>--peer-address</code>) and placed behind a load balancer. Periodic tasks like garbage-collection will not be duplicated for each node.</p> <p>Horizontally scalable: yes; they will coordinate workloads using the database, resulting in less work for each node and thus lower CPU usage.</p> <p>Outbound traffic:</p> <ul> <li><code>db</code> on its configured port for persistence</li> <li><code>db</code> on its configured port for locking and coordinating in a multi-<code>web</code> node deployment</li> <li>other <code>web</code> nodes (possibly itself) on an ephemeral port when a worker   is forwarded through the web node's TSA</li> </ul> <p>Inbound traffic:</p> <ul> <li><code>worker</code> connects to the TSA on port <code>2222</code> for registration</li> <li><code>worker</code> downloads inputs from the ATC during <code>fly execute</code> via   its external URL</li> <li>external traffic to the ATC API via the web UI and <code>fly</code> CLI</li> </ul>"},{"location":"docs/install/running-web/#operating-a-web-node","title":"Operating a <code>web</code> node","text":"<p>The <code>web</code> nodes themselves are stateless - they don't store anything on disk, and coordinate entirely using the database.</p>"},{"location":"docs/install/running-web/#scaling","title":"Scaling","text":"<p>The <code>web</code> node can be scaled up for high availability. They'll also roughly share their scheduling workloads, using the database to synchronize. This is done by just running more <code>web</code> commands on different machines, and optionally putting them behind a load balancer.</p> <p>To run a cluster of <code>web</code> nodes, you'll first need to ensure they're all pointing to the same PostgreSQL server.</p> <p>Next, you'll need to configure a peer address. This is a DNS or IP address that can be used to reach this <code>web</code> node from other <code>web</code> nodes. Typically this uses a private IP, like so:</p> <pre><code>CONCOURSE_PEER_ADDRESS=10.10.0.1\n</code></pre> <p>This address will be used for forwarded worker connections, which listen on the ephemeral port range.</p> <p>Finally, if all of these nodes are going to be accessed through a load balancer, you'll need to configure the external URL that will be used to reach your Concourse cluster:</p> <pre><code>CONCOURSE_EXTERNAL_URL=https://ci.example.com\n</code></pre> <p>Aside from the peer URL, all configuration must be consistent across all <code>web</code> nodes in the cluster to ensure consistent results.</p>"},{"location":"docs/install/running-web/#database-connection-pooling","title":"Database connection pooling","text":"<p>You may wish to configure the max number of parallel database connections that each node makes. There are two pools to configure: one for serving API requests, and one used for all the backend work such as pipeline scheduling.</p> <pre><code>CONCOURSE_API_MAX_CONNS=10     # default\nCONCOURSE_BACKEND_MAX_CONNS=50 # default\n</code></pre> <p>There are some non-configurable connection pools. They take up the following number of connections per pool:</p> <ul> <li>Garbage Collection: 5</li> <li>Lock: 1</li> <li>Worker Registration: 1</li> </ul> <p>The sum of these numbers across all <code>web</code> nodes should not be greater than the maximum number of simultaneous connections your Postgres server will allow. See  <code>db</code> node resource utilization for more information.</p> <p>For example, if 3 <code>web</code> nodes are configured with the values shown above then your PostgreSQL server should be configured with a connection limit of at least 201: <code>(10 + 50 + 5 + 1 + 1) * 3 web nodes</code>.</p>"},{"location":"docs/install/running-web/#reloading-worker-authorized-key","title":"Reloading worker authorized key","text":"<p>While Running <code>concourse web</code>, the authorized worker key file, which contains all public keys for the workers, is loaded at startup. During the lifecycle of a  <code>web</code> node new <code>worker</code> keys might be added or old ones removed. To perform a live reload of this file you can send a <code>SIGHUP</code> signal to the <code>concourse web</code> process. The process will remain running and Concourse will reload the authorized worker key file.</p>"},{"location":"docs/install/running-web/#restarting-upgrading","title":"Restarting &amp; Upgrading","text":"<p>The <code>web</code> nodes can be killed and restarted willy-nilly. No draining is necessary; if the <code>web</code> node was orchestrating a build it will continue where it left off when it comes back, or the build will be picked up by one of the other <code>web</code> nodes.</p> <p>To upgrade a <code>web</code> node, stop its process and start a new one using the newly installed <code>concourse</code>. Any database migrations will be run automatically on start. If <code>web</code> nodes are started in parallel, only one will run the migrations.</p> <p>We don't currently guarantee a lack of funny-business if you're running mixed Concourse versions - database migrations can perform modifications that confuse other <code>web</code> nodes. So there may be some turbulence during a rolling upgrade, but everything should stabilize once all <code>web</code> nodes are running the latest version.</p> <p>If you want more control over when the database migrations happen and know if they were successful you can use the <code>concourse migrate</code> command. The <code>migrate</code> command accepts the same <code>CONCOURSE_POSTGRES_*</code> env vars as the <code>concourse web</code> command.</p>"},{"location":"docs/install/running-web/#downgrading","title":"Downgrading","text":"<p>If you're stuck in a pinch and need to downgrade from one version of Concourse to another, you can use the <code>concourse migrate</code> command.</p> <p>First, grab the desired migration version by running the following:</p> <pre><code># make sure this is the *old* Concourse binary\n$ concourse migrate --supported-db-version\n1551110547\n</code></pre> <p>That number (yours will be different) is the expected migration version for that version of Concourse.</p> <p>Next, run the following with the new Concourse binary:</p> <pre><code>concourse migrate --migrate-db-to-version=1551110547\n</code></pre> <p>This will need the same <code>CONCOURSE_POSTGRES_*</code> configuration described in Running <code>concourse web</code>.</p> <p>Once this completes, switch all <code>web</code> nodes back to the older <code>concourse</code> binary and you should be good to go.</p>"},{"location":"docs/install/running-web/#configuring-the-web-node","title":"Configuring the <code>web</code> node","text":""},{"location":"docs/install/running-web/#giving-your-cluster-a-name","title":"Giving your cluster a name","text":"<p>If you've got many Concourse clusters that you switch between, you can make it slightly easier to notice which one you're on by giving each cluster a name:</p> <pre><code>CONCOURSE_CLUSTER_NAME=production\n</code></pre> <p>When set, this name will be shown in the top bar when viewing the dashboard.</p>"},{"location":"docs/install/running-web/#configuring-ingress-traffic","title":"Configuring ingress traffic","text":"<p>If your web nodes are going to be accessed multiple network layers, you will need to set <code>CONCOURSE_EXTERNAL_URL</code> to a URL accessible by your Concourse users. If you don't set this property, logging in will incorrectly redirect to its default value of <code>127.0.0.1</code>.</p> <p>If your web node(s) will be behind a load balancer or reverse proxy then you will need to ensure connections made by  <code>fly intercept</code> are properly handled by upgrading the connection. Here is a sample nginx configuration that upgrades connections made by <code>fly intercept</code>.</p> <pre><code>server {\n  server_name ci.example.com;\n\n  add_header Strict-Transport-Security \"max-age=31536000\" always;\n  ssl_stapling on;\n  ssl_stapling_verify on;\n\n  # Proxy main concourse traffic\n  location / {\n      proxy_pass http://concourse.local:8080/;\n      proxy_set_header Host $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Protocol $scheme;\n      proxy_set_header X-Forwarded-Host $http_host;\n  }\n\n  # Proxy fly intercept traffic\n  location ~ /hijack$ {\n      proxy_pass http://concourse.local:8080;\n      proxy_set_header Host $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Protocol $scheme;\n      proxy_set_header X-Forwarded-Host $http_host;\n      # Upgrade connection\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection \"upgrade\";\n  }\n}\n</code></pre>"},{"location":"docs/install/running-web/#tls-via-lets-encrypt","title":"TLS via Let's Encrypt","text":"<p>Concourse can be configured to automatically acquire a TLS certificate via Let's Encrypt:</p> <pre><code># Enable TLS\nCONCOURSE_TLS_BIND_PORT=443\n\n# Enable Let's Encrypt\nCONCOURSE_ENABLE_LETS_ENCRYPT=true\n</code></pre> <p>Warning</p> <p>Concourse's Let's Encrypt integration works by storing the TLS certificate and key in the database, so it is  imperative that you enable database encryption as well.</p> <p>By default, Concourse will reach out to Let's Encrypt's ACME CA directory. An alternative URL can be configured like so:</p> <pre><code>CONCOURSE_LETS_ENCRYPT_ACME_URL=https://acme.example.com/directory\n</code></pre> <p>In order to negotiate the certificate, your <code>web</code> node must be reachable by the ACME server. There are intentionally no publicly listed IP addresses to whitelist, so this typically means just making your <code>web</code> node publicly reachable.</p>"},{"location":"docs/install/running-web/#build-log-retention","title":"Build log retention","text":"<p>Build logs are stored in the DB - if they are not cleanup up every once in a while, the storage usage for build logs will continue to grow as more builds run. While this is usually fine for small Concourse instances, as you scale up, you may run into storage concerns.</p> <p>To clean up old build logs, you can configure Concourse to periodically scan for builds whose logs should be reaped based on a log retention policy, skipping over any paused pipelines and jobs. When a build's logs are reaped, they are no longer visible in the UI.</p> <p>Concourse can be configured with a default build log retention policy for all jobs:</p> <pre><code>CONCOURSE_DEFAULT_BUILD_LOGS_TO_RETAIN=50\nCONCOURSE_DEFAULT_DAYS_TO_RETAIN_BUILD_LOGS=14\n</code></pre> <p>With these settings, Concourse will keep the latest 50 builds for each job. If a job runs more than 50 builds in 14 days, all of those builds will be retained until 14 days after they ran.</p> <p>Some jobs have differing retention requirements - you can configure  <code>build_log_retention_policy</code> schema on a job-by-job basis.</p> <p>You can also configure Concourse with maximum values for build log retention policies to prevent jobs from retaining their build logs for too long:</p> <pre><code>CONCOURSE_MAX_BUILD_LOGS_TO_RETAIN=100\nCONCOURSE_MAX_DAYS_TO_RETAIN_BUILD_LOGS=30\n</code></pre> <p>With these settings,  <code>build_log_retention_policy.builds</code> is capped at 100, and  <code>build_log_retention_policy.days</code> is capped at 30.</p>"},{"location":"docs/install/running-web/#enabling-audit-logs","title":"Enabling audit logs","text":"<p>A very simplistic form of audit logging can be enabled with the following vars:</p> <pre><code># Enable auditing for all api requests connected to builds.\nCONCOURSE_ENABLE_BUILD_AUDITING=true\n\n# Enable auditing for all api requests connected to containers.\nCONCOURSE_ENABLE_CONTAINER_AUDITING=true\n\n# Enable auditing for all api requests connected to jobs.\nCONCOURSE_ENABLE_JOB_AUDITING=true\n\n# Enable auditing for all api requests connected to pipelines.\nCONCOURSE_ENABLE_PIPELINE_AUDITING=true\n\n# Enable auditing for all api requests connected to resources.\nCONCOURSE_ENABLE_RESOURCE_AUDITING=true\n\n# Enable auditing for all api requests connected to system transactions.\nCONCOURSE_ENABLE_SYSTEM_AUDITING=true\n\n# Enable auditing for all api requests connected to teams.\nCONCOURSE_ENABLE_TEAM_AUDITING=true\n\n# Enable auditing for all api requests connected to workers.\nCONCOURSE_ENABLE_WORKER_AUDITING=true\n\n# Enable auditing for all api requests connected to volumes.\nCONCOURSE_ENABLE_VOLUME_AUDITING=true\n</code></pre> <p>When enabled, API requests will result in an info-level log line like so:</p> <pre><code>{\"timestamp\":\"2019-05-09T14:41:54.880381537Z\",\"level\":\"info\",\"source\":\"atc\",\"message\":\"atc.audit\",\"data\":{\"action\":\"Info\",\"parameters\":{},\"user\":\"test\"}}\n{\"timestamp\":\"2019-05-09T14:42:36.704864093Z\",\"level\":\"info\",\"source\":\"atc\",\"message\":\"atc.audit\",\"data\":{\"action\":\"GetPipeline\",\"parameters\":{\":pipeline_name\":[\"booklit\"],\":team_name\":[\"main\"]},\"user\":\"test\"}}\n</code></pre>"},{"location":"docs/install/running-web/#configuring-defaults-for-resource-types","title":"Configuring defaults for resource types","text":"<p>Defaults for the \"core\" resource types (those that show up under the Concourse org) that comes with Concourse can be set cluster-wide by passing in a configuration file. The format of the file is the name of the resource type followed by an arbitrary configuration.</p> <p>Documentation for each resource type's configuration is in each implementation's <code>README</code>.</p> <pre><code>CONCOURSE_BASE_RESOURCE_TYPE_DEFAULTS=./defaults.yml\n</code></pre> <p>For example, a <code>defaults.yml</code> that configures the entire cluster to use a registry mirror would have:</p> <pre><code>registry-image:\n  registry_mirror:\n    host: https://registry.mirror.example.com\n</code></pre>"},{"location":"docs/install/running-worker/","title":"Running a worker node","text":""},{"location":"docs/install/running-worker/#running-a-worker-node","title":"Running a <code>worker</code> node","text":"<p>The <code>worker</code> node registers with the <code>web</code> node and is then used for executing builds and performing resource <code>check</code>s. It doesn't really decide much on its own.</p>"},{"location":"docs/install/running-worker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux:<ul> <li>We test and support the following distributions. Minimum kernel version tested is 5.15.<ul> <li>Ubuntu 22.04</li> <li>Ubuntu 24.04</li> </ul> </li> <li>Other Requirements:<ul> <li>User namespaces must be enabled.</li> <li>To enforce memory limits on tasks, memory + swap accounting must be enabled.</li> </ul> </li> </ul> </li> <li>Windows/Darwin:<ul> <li>no special requirements (that we know of).</li> </ul> </li> </ul> <p>Note</p> <p>Windows containers are currently not supported and Darwin does not have native containers. Steps will run inside  a temporary directory on the Windows/Darwin worker. Any dependencies needed for your tasks (e.g. git, .NET, golang,  ssh) should be pre-installed on the worker. Windows/Darwin workers do not come with any resource types.</p>"},{"location":"docs/install/running-worker/#running-concourse-worker","title":"Running <code>concourse worker</code>","text":"<p>The <code>concourse</code> CLI can run as a <code>worker</code> node via the <code>worker</code> subcommand.</p> <p>First, you'll need to configure a directory for the worker to store data:</p> <pre><code>CONCOURSE_WORK_DIR=/opt/concourse/worker\n</code></pre> <p>This is where all the builds run, and where all resources are fetched in to, so make sure it's backed by enough storage.</p> <p>Next, point the worker at your <code>web</code> node like so:</p> <pre><code>CONCOURSE_TSA_HOST=10.0.2.15:2222\nCONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub\nCONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key\n</code></pre> <p>Finally start the worker:</p> <pre><code># run with -E to forward env config, or just set it all as root\nsudo -E concourse worker\n</code></pre> <p>Note that the worker must be run as <code>root</code> because it orchestrates containers.</p> <p>All logs will be emitted to <code>stdout</code>, with any panics or lower-level errors being emitted to <code>stderr</code>.</p>"},{"location":"docs/install/running-worker/#resource-utilization","title":"Resource utilization","text":"<p>CPU usage: almost entirely subject to pipeline workloads. More resources configured will result in more checking, and in-flight builds will use as much CPU as they want.</p> <p>Memory usage: also subject to pipeline workloads. Expect usage to increase with the number of containers on the worker and spike as builds run.</p> <p>Bandwidth usage: again, almost entirely subject to pipeline workloads. Expect spikes from periodic checking, though the intervals should spread out over enough time. Resource fetching and pushing will also use arbitrary bandwidth.</p> <p>Disk usage: arbitrary data will be written as builds run, and resource caches will be kept and garbage collected on their own life cycle. We suggest going for a larger disk size if it's not too much trouble. All state on disk must not outlive the worker itself; it is all ephemeral. If the worker is re-created (i.e. fresh VM/container and all processes were killed), it should be brought back with an empty disk.</p> <p>Highly available: not applicable. Workers are inherently singletons, as they're being used as drivers running entirely different workloads.</p> <p>Horizontally scalable: yes; workers directly correlate to your capacity required by however many pipelines, resources, and in-flight builds you want to run. It makes sense to scale them up and down with demand.</p> <p>Outbound traffic:</p> <ul> <li>External traffic to arbitrary locations as a result of periodic resource checking and running builds</li> <li>External traffic to the <code>web</code> node's configured external URL when downloading the inputs for a  <code>fly execute</code></li> <li>External traffic to the <code>web</code> node's TSA port (<code>2222</code>) for registering the worker</li> <li>If P2P streaming is enabled there will be traffic to other workers.</li> </ul> <p>Inbound traffic:</p> <ul> <li>From the <code>web</code> node on port <code>7777</code> (Garden) and <code>7788</code> (BaggageClaim). These ports do not need to be   exposed, they are forwarded to the web node via the ssh connection on port <code>2222</code>.</li> <li>If P2P streaming is enabled there will be traffic to other workers.</li> </ul>"},{"location":"docs/install/running-worker/#operating-a-worker-node","title":"Operating a <code>worker</code> node","text":"<p>The <code>worker</code> nodes are designed to be stateless and as interchangeable as possible. Tasks and Resources bring their own Docker images, so you should never have to install dependencies on the worker. Windows and Darwin workers are the exception to this. Any dependencies should be pre-installed on Windows and Darwin workers.</p> <p>In Concourse, all important data is represented by Resources, so the workers themselves are dispensable. Any data in the work-dir is ephemeral and should go away when the worker machine is removed - it should not be persisted between worker VM or container re-creates.</p>"},{"location":"docs/install/running-worker/#scaling-workers","title":"Scaling Workers","text":"<p>More workers should be added to accommodate more pipelines. To know when this is necessary you should probably set up Metrics and keep an eye on container counts. If average container count starts to approach 200 or so per worker, you should probably add another worker. Load average is another metric to keep an eye on.</p> <p>To add a worker, just create another machine for the worker and follow the Running <code>concourse worker</code> instructions again.</p> <p>Note</p> <p>It doesn't make sense to run multiple workers on one machine since they'll both be contending for the same  physical resources. Workers should be given their own VMs or physical machines to maximize resource usage.</p>"},{"location":"docs/install/running-worker/#horizontal-vs-vertical-scaling","title":"Horizontal vs Vertical Scaling","text":"<p>The answer to whether you should scale your workers horizontally or vertically depends heavily on what workloads your pipelines are running. Anecdotally though, we have seen that a lot of smaller workers (horizontal scaling) is usually better than a few large workers (vertical scaling).</p> <p>Again, this is not an absolute answer! You will have to test this out against the workloads your pipelines demand and adjust based on the Metrics that you are tracking.</p>"},{"location":"docs/install/running-worker/#worker-heartbeating-stalling","title":"Worker Heartbeating &amp; Stalling","text":"<p>Workers will continuously heartbeat to the Concourse cluster in order to remain registered and healthy. If a worker hasn't checked in after a while, possibly due to a network error, being overloaded, or having crashed, the web node will transition its state to <code>stalled</code> and new workloads will not be scheduled on that worker until it recovers.</p> <p>If the worker remains in this state and cannot be recovered, it can be removed using the  <code>fly prune-worker</code> command.</p>"},{"location":"docs/install/running-worker/#restarting-a-worker","title":"Restarting a Worker","text":"<p>Workers can be restarted in-place by sending <code>SIGTERM</code> to the worker process and starting it back up. Containers will remain running and Concourse will reattach to builds that were in flight.</p> <p>This is a pretty aggressive way to restart a worker, and may result in errored builds - there are a few moving parts involved and we're still working on making this airtight.</p> <p>A safer way to restart a worker is to land it by sending <code>SIGUSR1</code> to the <code>worker</code> process. This will switch the worker to the <code>landing</code> state and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the process will exit.</p> <p>You may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like <code>start-stop-daemon</code>:</p> <pre><code>start-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR1/300/TERM/15/KILL\n</code></pre> <p>This will send <code>SIGUSR1</code>, wait up to 5 minutes, and then send <code>SIGTERM</code>. If it's still running, it will be killed after an additional 15 seconds.</p> <p>Once the timeout is enforced, there's still a chance that builds that were running will continue when the worker comes back.</p>"},{"location":"docs/install/running-worker/#gracefully-removing-a-worker","title":"Gracefully Removing a Worker","text":"<p>When a worker machine is going away, it should be retired. This is similar to landing, except at the end the worker is completely unregistered, along with its volumes and containers. This should be done when a worker's VM or container is being destroyed.</p> <p>To retire a worker, send <code>SIGUSR2</code> to the <code>worker</code> process. This will switch the worker to <code>retiring</code> state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the worker will be removed and the <code>worker</code> process will exit.</p> <p>Just like with landing, you may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like <code>start-stop-daemon</code>:</p> <pre><code>start-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR2/300/TERM/15/KILL\n</code></pre> <p>This will send <code>SIGUSR2</code>, wait up to 5 minutes, and then send <code>SIGTERM</code>. If it's still running, it will be killed after an additional 15 seconds.</p>"},{"location":"docs/install/running-worker/#configuring-the-worker-node","title":"Configuring the <code>worker</code> node","text":""},{"location":"docs/install/running-worker/#tagging-workers","title":"Tagging Workers","text":"<p>If there's something special about your worker and you'd like to target builds at it specifically, you can configure tags like so:</p> <pre><code>CONCOURSE_TAG=\"tag-1,tag-2\"\n</code></pre> <p>A tagged worker is taken out of the default placement logic. Tagged workers will not be used for any untagged Steps.</p> <p>To run build steps on a tagged worker, specify the <code>tags</code> on any particular step in your job.</p> <p>To perform resource <code>check</code>s on a tagged worker, specify  <code>tags</code> on the resource declaration.</p>"},{"location":"docs/install/running-worker/#team-workers","title":"Team Workers","text":"<p>If you want to isolate **all workloads ** for a team then you can configure a worker to belong to a single team like so:</p> <pre><code>CONCOURSE_TEAM=\"lightweavers\"\n</code></pre> <p>Once an untagged team worker is registered Concourse will schedule all untagged builds for that team on its team worker( s). Builds for this team will no longer be scheduled on any untagged, non-team workers.</p> <p>It is possible to have a Concourse cluster made up of only team workers and have zero non-team workers, though this is not a common setup because resource utilization across all workers ends up underutilized. It is useful though if you have a particular team with heavy workloads that usually bothers other teams pipelines.</p>"},{"location":"docs/install/running-worker/#tags-and-team-workers","title":"Tags and Team Workers","text":"<p>When you have a worker configured with tag(s) and a team like so:</p> <pre><code>CONCOURSE_TAG=\"tag-1,tag-2\"\nCONCOURSE_TEAM=\"lightweavers\"\n</code></pre> <p>Only steps that are tagged and from the specified team will be scheduled on such a worker. Any untagged work the team has will land on either:</p> <ol> <li>Untagged team workers belonging to the team, or</li> <li>Untagged workers not configured to a specific team</li> </ol>"},{"location":"docs/install/running-worker/#healthcheck-endpoint","title":"Healthcheck Endpoint","text":"<p>The worker will automatically listen on port <code>8888</code> as its healthcheck endpoint. It will return a <code>HTTP 200</code> status code with an empty body on a successful check. A successful check means the worker can reach the Garden and BaggageClaim servers.</p> <p>The healthcheck endpoint is configurable through three variables:</p> <pre><code>concourse worker --healthcheck-bind-ip=\n# IP address on which to listen for health checking requests. (default: 0.0.0.0)\n\nconcourse worker --healthcheck-bind-port\n# Port on which to listen for health checking requests. (default: 8888)\n\nconcourse worker --healthcheck-timeout\n# HTTP timeout for the full duration of health checking. (default: 5s)\n</code></pre>"},{"location":"docs/install/running-worker/#resource-types","title":"Resource Types","text":"<p>Note</p> <p>The following section only applies to Linux workers. Resource types are simply Linux container images and therefore  can't be run on Windows or Darwin workers.</p>"},{"location":"docs/install/running-worker/#bundled-resource-types","title":"Bundled Resource Types","text":"<p>Workers come prepackaged with a bundle of resource types. They are included in the tarball from the GitHub release page and are part of the concourse/concourse image.</p> <p>To view the resource types available on a worker run:</p> <pre><code>fly workers --details\n</code></pre> <p>If you want more details, like the version number of each resource, you can run:</p> <pre><code>fly curl api/v1/workers\n</code></pre>"},{"location":"docs/install/running-worker/#installing-or-upgrading-bundled-resource-types","title":"Installing or Upgrading Bundled Resource Types","text":"<p>You may want to upgrade the bundled resource types outside of Concourse upgrades or even install additional resource types on your workers to reduce the polling on some external image repository like Docker Hub.</p> <p>We will use the git resource as our example. We will assume your Concourse installation is at <code>/usr/local/concourse</code>.</p> <p>First, pull and create a container of the resource you're installing/upgrading. Grab the ID of the container that Docker creates.</p> <pre><code>$ docker run -d concourse/git-resource\nb253417142565cd5eb43902e94a2cf355d5354b583fbc686488c9a153584c6ba\n</code></pre> <p>Export the containers file system into a gzip compressed tar archive named <code>rootfs.tgz</code></p> <pre><code>docker export b253417142 | gzip &gt; rootfs.tgz\n</code></pre> <p>Create a file called <code>resource_metadata.json</code> and populate it with the following contents. Make sure the <code>type</code> does not conflict with an existing resource type when you're installing a new resource type. In our example here we're calling the type <code>gitv2</code> to avoid conflicting with the pre-existing <code>git</code> resource.</p> <pre><code>{\n  \"type\": \"gitv2\",\n  \"version\": \"1.13.0\",\n  \"privileged\": false,\n  \"unique_version_history\": false\n}\n</code></pre> <p>At this point you should have two files: <code>rootfs.tgz</code> and <code>resource_metadata.json</code>.</p> <p>Create a new directory under the <code>resource-types</code> folder in your Concourse installation directory. By convention, it should be the same name as the <code>type</code>.</p> <pre><code>mkdir /usr/local/concourse/resource-types/gitv2\n</code></pre> <p>Place the <code>rootfs.tgz</code> and <code>resource_metadata.json</code> inside the folder. Restart your worker and verify the new resource type is on there by running one of the following commands:</p> <pre><code>fly workers --details\n# or\nfly curl api/v1/workers\n</code></pre> <p>You can also verify that Concourse can create a container with the <code>rootfs.tgz</code> you made by running a simple pipeline:</p> <pre><code>resources:\n  - name: some-resource\n    type: gitv2 #change to your resource type\n    source:\n      uri: https://github.com/concourse/git-resource.git\n\njobs:\n  - name: simple-job\n    plan:\n      - get: some-resource\n</code></pre>"},{"location":"docs/install/running-worker/#configuring-runtimes","title":"Configuring Runtimes","text":"<p>The worker can be run with multiple container runtimes - containerd, Guardian, and Houdini (the only runtime for Darwin and Windows). Only <code>containerd</code> and <code>Guardian</code> are meant for production use on Linux workers. <code>containerd</code> is the default runtime for Concourse.</p> <p>Note about Architecture</p> <p>The web node (ATC) talks to all 3 runtimes via a single interface called the  Garden server. While Guardian comes packaged with a Garden server and  its flags in Concourse are unfortunately prefixed with <code>--garden-*</code>, Guardian (a runtime) and Garden  (an interface and server) are two separate tools. An analogy for Garden would be the Container Runtime  Interface (CRI) used in  Kubernetes. Kubernetes uses containerd via CRI. Concourse uses containerd via Garden.</p>"},{"location":"docs/install/running-worker/#containerd-runtime","title":"<code>containerd</code> runtime","text":"<p><code>containerd</code> is currently the default runtime for Concourse. It can also be set by setting the <code>--runtime</code> (<code>CONCOURSE_RUNTIME</code>) to <code>containerd</code> on the <code>concourse worker</code> command.</p> <p>Info</p> <p>Prior to v8 of Concourse, <code>containerd</code> was NOT the default runtime. <code>guardian</code> is the default runtime for all versions prior to v8.</p> <p>The following is a list of the <code>containerd</code> runtime specific flags for Concourse that can be set. They are all optional and have default values.</p> <pre><code>Containerd Configuration:\n  --containerd-config=                               Path to a config file to use for the Containerd daemon. [$CONCOURSE_CONTAINERD_CONFIG]\n  --containerd-bin=                                  Path to a containerd executable (non-absolute names get resolved from $PATH). [$CONCOURSE_CONTAINERD_BIN]\n  --containerd-init-bin=                             Path to an init executable (non-absolute names get resolved from $PATH). (default: /usr/local/concourse/bin/init) [$CONCOURSE_CONTAINERD_INIT_BIN]\n  --containerd-cni-plugins-dir=                      Path to CNI network plugins. (default: /usr/local/concourse/bin) [$CONCOURSE_CONTAINERD_CNI_PLUGINS_DIR]\n  --containerd-log-level                             Minimum level of logs to see. (default: info) [$CONCOURSE_CONTAINERD_LOG_LEVEL]\n  --containerd-request-timeout=                      How long to wait for requests to Containerd to complete. 0 means no timeout. (default: 5m) [$CONCOURSE_CONTAINERD_REQUEST_TIMEOUT]\n  --containerd-max-containers=                       Max container capacity. 0 means no limit. (default: 250) [$CONCOURSE_CONTAINERD_MAX_CONTAINERS]\n  --containerd-privileged-mode=                      How many privileges privileged containers get. full is equivalent to root on host. ignore means no extra privileges. fuse-only means enough to use fuse-overlayfs. (default: full) [$CONCOURSE_CONTAINERD_PRIVILEGED_MODE]\n\nContainerd Container Networking:\n  --containerd-external-ip=                          IP address to use to reach container's mapped ports. Autodetected if not specified. [$CONCOURSE_CONTAINERD_EXTERNAL_IP]\n  --containerd-dns-server=                           DNS server IP address to use instead of automatically determined servers. Can be specified multiple times. [$CONCOURSE_CONTAINERD_DNS_SERVER]\n  --containerd-restricted-network=                   Network ranges to which traffic from containers will be restricted. Can be specified multiple times. [$CONCOURSE_CONTAINERD_RESTRICTED_NETWORK]\n  --containerd-additional-hosts=                     Additional entries to add to /etc/hosts in containers. [$CONCOURSE_CONTAINERD_ADDITIONAL_HOSTS]\n  --containerd-network-pool=                         Network range to use for dynamically allocated container subnets. (default: 10.80.0.0/16) [$CONCOURSE_CONTAINERD_NETWORK_POOL]\n  --containerd-mtu=                                  MTU size for container network interfaces. Defaults to the MTU of the interface used for outbound access by the host. [$CONCOURSE_CONTAINERD_MTU]\n  --containerd-allow-host-access                     Allow containers to reach the host's network. This is turned off by default. [$CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS]\n\nDNS Proxy Configuration:\n  --containerd-dns-proxy-enable                      Enable proxy DNS server. Note: this will enable containers to access the host network. [$CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE]\n</code></pre> <p>Warning</p> <p>Make sure to read A note on allowing host access  and DNS proxy to understand the implications of using <code>--containerd-allow-host-access</code> and  <code>--containerd-dns-proxy-enable</code></p>"},{"location":"docs/install/running-worker/#transitioning-from-guardian-to-containerd","title":"Transitioning from Guardian to containerd","text":"<p>If you are transitioning from <code>Guardian</code> to <code>containerd</code> you will need to convert any <code>--garden-*</code> ( <code>CONCOURSE_GARDEN_*</code>) flags to their <code>containerd</code> (<code>CONCOURSE_CONTAINERD_*</code>) counterparts:</p> Guardian Flags Containerd Flags <code>--garden-request-timeout</code><code>CONCOURSE_GARDEN_REQUEST_TIMEOUT</code> <code>--containerd-request-timeout</code><code>CONCOURSE_CONTAINERD_REQUEST_TIMEOUT</code> <code>--garden-dns-proxy-enable</code><code>CONCOURSE_GARDEN_DNS_PROXY_ENABLE</code> -<code>-containerd-dns-proxy-enable</code><code>CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE</code> No equivalent CLI flag<code>CONCOURSE_GARDEN_ALLOW_HOST_ACCESS</code> <code>--containerd-allow-host-access</code><code>CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS</code> <code>--garden-network-pool</code><code>CONCOURSE_GARDEN_NETWORK_POOL</code> <code>--containerd-network-pool</code><code>CONCOURSE_CONTAINERD_NETWORK_POOL</code> <code>--garden-max-containers</code><code>CONCOURSE_GARDEN_MAX_CONTAINERS</code> <code>--containerd-max-containers</code><code>CONCOURSE_CONTAINERD_MAX_CONTAINERS</code> No equivalent CLI flag<code>CONCOURSE_GARDEN_DENY_NETWORKS</code> <code>--containerd-restricted-network</code><code>CONCOURSE_CONTAINERD_RESTRICTED_NETWORK</code> No equivalent CLI flag or ENV option.Configured through <code>garden_config.ini</code> <code>--containerd-additional-hosts</code><code>CONCOURSE_CONTAINERD_ADDITIONAL_HOSTS</code> No equivalent CLI flag<code>CONCOURSE_GARDEN_DNS_SERVER</code> <code>--containerd-dns-server</code><code>CONCOURSE_CONTAINERD_DNS_SERVER</code> No equivalent CLI flag<code>CONCOURSE_GARDEN_EXTERNAL_IP</code> <code>--containerd-external-ip</code><code>CONCOURSE_CONTAINERD_EXTERNAL_IP</code> No equivalent CLI flag<code>CONCOURSE_GARDEN_MTU</code> <code>--containerd-mtu</code><code>CONCOURSE_CONTAINERD_MTU</code>"},{"location":"docs/install/running-worker/#guardian-runtime","title":"<code>Guardian</code> runtime","text":"<p>To use the <code>guardian</code> runtime, set the <code>--runtime</code> flag to <code>guardian</code> on the <code>concourse worker</code> command.</p> <p>Info</p> <p>Prior to v8 of Concourse, <code>guardian</code> was the default runtime. All versions of Concourse &gt;= v8 now use <code>containerd</code> as the default runtime.</p> <p>The <code>concourse worker</code> command automatically configures and runs <code>Guardian</code> using the <code>gdn</code> binary, but depending on the environment you're running Concourse in, you may need to pop open the hood and configure a few things.</p> <p>The <code>gdn</code> server can be configured in two ways:</p> <ol> <li> <p>By creating a <code>config.ini</code> file and passing it as <code>--garden-config</code> (or <code>CONCOURSE_GARDEN_CONFIG</code>). The .ini file    should look something like this:     </p><pre><code>[server]\nflag-name=flag-value \n</code></pre>    To learn which flags can be set, consult <code>gdn server --help</code>. Each flag listed can be set under the <code>[server]</code>    heading.<p></p> </li> <li> <p>By setting <code>CONCOURSE_GARDEN_*</code> environment variables. This is primarily supported for backwards compatibility, and    these variables are not present in <code>concourse worker --help</code>. They are translated to flags passed to <code>gdn server</code> by    lower-casing the <code>*</code> portion and replacing underscores with hyphens.</p> </li> </ol>"},{"location":"docs/install/running-worker/#troubleshooting-and-fixing-dns-resolution","title":"Troubleshooting and fixing DNS resolution","text":"<p>Note</p> <p>The Guardian runtime took care of a lot of container creation operations for Concourse in the past. It was very  user-friendly for the project to use as a container runtime. While implementing the containerd runtime most  reported bugs were actually a difference in containerd's default behaviour compared to Guardian's. Currently  Concourse's containerd runtime mostly behaves like the Guardian runtime did. Most of the following DNS section  should apply to both runtimes.</p> <p>By default, containers created by the Guardian or containerd (will refer to both as runtime) runtime will carry over the <code>/etc/resolv.conf</code> from the host into the container. This is often fine, but some Linux distributions configure a special <code>127.x.x.x</code> DNS resolver (e.g. <code>systemd-resolved</code>).</p> <p>When the runtime copies the <code>resolv.conf</code> over, it removes these entries as they won't be reachable from the container's network namespace. As a result, your containers may not have any valid nameservers configured.</p> <p>To diagnose this problem you can <code>fly intercept</code> into a failing container and check which nameservers are in <code>/etc/resolv.conf</code>:</p> <pre><code>$ fly -t ci intercept -j concourse/concourse\nbash-5.0$ grep nameserver /etc/resolv.conf\nbash-5.0$\n</code></pre> <p>In this case it is empty, as the host only listed a single <code>127.0.0.53</code> address which was then stripped out. To fix this you'll need to explicitly configure DNS instead of relying on the default runtime behavior.</p>"},{"location":"docs/install/running-worker/#pointing-to-external-dns-servers","title":"Pointing to external DNS servers","text":"<p>If you have no need for special DNS resolution within your Concourse containers, you can configure your containers to use specific DNS server addresses external to the VM.</p> <p>The Guardian and containerd runtimes can have their DNS servers configured with flags or envs vars.</p> DNS via flags (containerd)DNS via env vars<code>config.ini</code> (Guardian) <pre><code>concourse worker --containerd-dns-server=\"1.1.1.1\" --containerd-dns-server=\"8.8.8.8\"\n</code></pre> <pre><code># containerd runtime\nCONCOURSE_CONTAINERD_DNS_SERVER=\"1.1.1.1,8.8.8.8\"\n# Guardian runtime\nCONCOURSE_GARDEN_DNS_SERVER=\"1.1.1.1,8.8.8.8\"\n</code></pre> <pre><code>[server]\n; configure Google DNS\ndns-server = 8.8.8.8\ndns-server = 8.8.4.4\n</code></pre> <p>To verify this solves your problem you can <code>fly intercept</code> into a container and check which nameservers are in <code>/etc/resolv.conf</code>:</p> <pre><code>$ fly -t ci intercept -j my-pipeline/the-job\nbash-5.0$ cat /etc/resolv.conf\nnameserver 1.1.1.1\nnameserver 8.8.8.8\nbash-5.0$ ping google.com\nPING google.com (108.177.111.139): 56 data bytes\n64 bytes from 108.177.111.139: seq=0 ttl=47 time=2.672 ms\n64 bytes from 108.177.111.139: seq=1 ttl=47 time=0.911 ms\n</code></pre>"},{"location":"docs/install/running-worker/#using-a-local-dns-server","title":"Using a local DNS server","text":"<p>If you would like to use Consul, <code>dnsmasq</code>, or some other DNS server running on the worker VM, you'll have to configure the LAN address of the VM as the DNS server and allow the containers to reach the address, like so:</p> Local DNS via flags (containerd)Local DNS via env vars<code>config.ini</code> (Guardian) <pre><code>concourse worker --containerd-dns-server=\"10.0.1.3\" --containerd-allow-host-access=\"true\"\n</code></pre> <pre><code># containerd runtime\nCONCOURSE_CONTAINERD_DNS_SERVER=\"10.0.1.3\"\nCONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS=\"true\"\n# Guardian runtime\nCONCOURSE_GARDEN_DNS_SERVER=\"10.0.1.3\"\nCONCOURSE_GARDEN_ALLOW_HOST_ACCESS=\"true\"\n</code></pre> <pre><code>[server]\n; internal IP of the worker machine\ndns-server=10.0.1.3\n\n; allow containers to reach the above IP\nallow-host-access=true\n</code></pre> <p>Warning</p> <p>Make sure to read A note on allowing host access  and DNS proxy to understand the implications of using <code>allow-host-access</code></p> <p>To validate whether the changes have taken effect, you can  <code>fly intercept</code> into any container and check <code>/etc/resolv.conf</code> once again:</p> <pre><code>$ fly -t ci intercept -j my-pipeline/the-job\nbash-5.0$ cat /etc/resolv.conf\nnameserver 10.1.2.3\nbash-5.0$ nslookup concourse-ci.org\nServer:         10.1.2.3\nAddress:        10.1.2.3#53\n\nNon-authoritative answer:\nName:   concourse-ci.org\nAddress: 185.199.108.153\nName:   concourse-ci.org\nAddress: 185.199.109.153\nName:   concourse-ci.org\nAddress: 185.199.110.153\nName:   concourse-ci.org\nAddress: 185.199.111.153\n</code></pre> <p>If <code>nslookup</code> times out or fails, you may need to open up firewalls or security group configuration so that the worker VM can send UDP/TCP packets to itself.</p>"},{"location":"docs/install/running-worker/#a-note-on-allowing-host-access-and-dns-proxy","title":"A note on allowing host access and DNS proxy","text":"<p>Setting <code>allow-host-access</code> will, well, allow containers to access your host VM's network. If you don't trust your container workloads, you may not want to allow this. With host network access, containers will be able to reach out to any other locally running network processes running on the worker including the garden and baggageclaim servers which would allow them to issue commands and manipulate other containers and volumes on the same worker.</p> <p>Setting <code>dns-proxy-enable</code> will also enable <code>allow-host-access</code> (since the dns proxy will be run on the host, therefore requiring host access be enabled).</p>"},{"location":"docs/install/running-worker/#configuring-peer-to-peer-volume-streaming","title":"Configuring Peer-to-Peer Volume Streaming","text":"<p>Peer-to-Peer (P2P) volume streaming enables the workers to stream volumes directly to each other instead of always streaming volumes through the web node(s). This can reduce the time it takes for individual steps in a job to start and reduce the amount of network traffic used by the Concourse cluster.</p> <p>Pre-Requisites</p> <ul> <li>All worker nodes need to be able to reach each other via IP address. This usually means they are on the same LAN. You   can test this by trying to ping one worker from another worker. If even one worker does not meet this requirement then   you cannot use P2P volume streaming.</li> <li>The baggageclaim port (<code>7788</code> is the default) is open to traffic on all worker nodes. You can verify the port is open   and reaching the baggageclaim API server by hitting the <code>/volumes</code> endpoint.    <pre><code>curl http://&lt;worker-IP-address&gt;:7788/volumes\n</code></pre></li> </ul> <p>To enable P2P volume streaming you need to configure some settings on the web and worker nodes. Configure the worker nodes first. Configure the web node(s) last.</p>"},{"location":"docs/install/running-worker/#p2p-worker-configuration","title":"P2P Worker Configuration","text":"<ul> <li><code>CONCOURSE_BAGGAGECLAIM_BIND_IP=0.0.0.0</code> - Required. The worker needs to listen for traffic over <code>127.0.0.1</code> (to   receive info from the web node) as well as its LAN IP in a P2P setup. Therefore, we need to set the IP baggageclaim   binds to <code>0.0.0.0</code>.</li> <li><code>CONCOURSE_BAGGAGECLAIM_P2P_INTERFACE_NAME_PATTERN=eth0</code> - Optional. Regular expression to match a network interface   for P2P streaming. This is how a worker determines its own LAN IP address, by looking it up via the LAN interface   specified by this flag.       You can determine the name of the LAN interface for any worker by listing all network interfaces and noting which   interface has the LAN IP that you want the worker to use.       To view all available network interfaces on your worker:<ul> <li>On Linux run <code>ip addr list</code></li> <li>On MacOS run <code>ifconfig</code></li> <li>On Windows run <code>ipconfig</code>. Windows network interface names are very different from Unix device names. Example   network interface names for Windows include:    <pre><code>Ethernet 4\nLocal Area Connection* 2\nLocal Area Connection* 12\nWi-Fi 5\nBluetooth Network Connection 2\nLoopback Pseudo-Interface 1\n</code></pre></li> </ul> </li> <li><code>CONCOURSE_BAGGAGECLAIM_P2P_INTERFACE_FAMILY=4</code> - Optional. Tells the worker to use IPv4 or IPv6. Defaults to <code>4</code>   for IPv4. Set to <code>6</code> for IPv6.</li> </ul>"},{"location":"docs/install/running-worker/#p2p-web-configuration","title":"P2P Web Configuration","text":"<p>You need to tell the web node(s) to use P2P volume streaming.</p> <pre><code>CONCOURSE_ENABLE_P2P_VOLUME_STREAMING=true\n</code></pre> <p>Once that flag is set and the web node is restarted, P2P volume streaming will start occurring in your Concourse cluster.</p>"},{"location":"docs/install/upgrading-concourse/","title":"Upgrading Concourse","text":"<p>Be careful to check the \"Breaking Changes\" in the release notes - in particular, you'll want to look for any flags that have changed.</p>"},{"location":"docs/install/upgrading-concourse/#upgrading-the-web-node","title":"Upgrading the Web Node","text":"<p>The web node is upgraded by stopping the Concourse process, swapping out the <code>concourse</code> binary with the new one, and re-starting it.</p> <p>Each <code>web</code> node will automatically run database migrations on start-up and lock via the database to ensure only one of the web nodes runs the migrations. We currently do not guarantee zero-downtime upgrades, as migrations may make changes that confuse the older web nodes. This should resolve as each web node is upgraded, and shouldn't result in any inconsistent state.</p> <p>Typically, Concourse can be upgraded from any version to any other version, though around 3.x and 4.x we made some changes to how migrations are run, and as a result the following upgrade paths must be followed:</p> Current Version Upgrade Path &lt; v3.6.0 v3.6.0 -&gt; v4.0.0 -&gt; latest = v3.6.0 v4.0.0 -&gt; latest <p>We'll try to minimize this kind of thing in the future.</p> <p>Lastly, you will want to overwrite the contents of <code>concourse/fly-assets</code> with the contents from the GitHub release tarball so users can  <code>fly sync</code> to the correct version.</p>"},{"location":"docs/install/upgrading-concourse/#upgrading-the-worker-node","title":"Upgrading the Worker Node","text":"<p>The worker node is upgraded by stopping the Concourse process, swapping out the <code>concourse</code> binary with the new one, and re-starting it.</p>"},{"location":"docs/install/upgrading-concourse/#linux-workers","title":"Linux Workers","text":"<p>The Linux tarball from the GitHub release page contains extra assets that you will want to ensure are also upgraded at the same time. Make sure you overwrite the contents of the following directories:</p> <ul> <li><code>concourse/bin/...</code> - Other binaries like <code>gdn</code>, <code>runc</code>, and <code>containerd</code> are in this directory</li> <li><code>concourse/resource-types/...</code> - The location of the   default resource-types included with each Concourse release</li> </ul>"},{"location":"docs/install/upgrading-concourse/#darwin-and-windows-workers","title":"Darwin and Windows Workers","text":"<p>There are no additional steps for upgrading Darwin and Windows workers.</p>"},{"location":"docs/internals/","title":"Internals","text":"<p>This section provides a deeper understanding of some of the concepts surrounding Concourse.</p> <p>An understanding of the basics of Concourse concepts, such as pipelines, jobs, etc, is recommended as parts of this section might assume a level of knowledge from them. This section is not necessary for using Concourse but are more for experienced users that want to dig deeper into how Concourse works.</p>"},{"location":"docs/internals/#basic-architecture","title":"Basic architecture","text":"<p>Concourse is a fairly simple distributed system built up from the following components. You'll see them referenced here and there throughout the documentation, so you may want to skim this page just to get an idea of what they are.</p> <p></p>"},{"location":"docs/internals/#atc-web-ui-build-scheduler","title":"ATC: web UI &amp; build scheduler","text":"<p>The ATC is the heart of Concourse. It runs the web UI and API and is responsible for all pipeline scheduling. It connects to PostgreSQL, which it uses to store pipeline data (including build logs).</p> <p>Multiple ATCs can be running as one cluster; as long as they're all pointing to the same database, they'll synchronize using basic locking mechanisms and roughly spread work across the cluster.</p> <p>The ATC by default listens on port <code>8080</code>, and is usually co-located with the TSA and sitting behind a load balancer.</p> <p>Note</p> <p>For <code>fly intercept</code> to function, make sure your load balancer is configured to do TCP  or SSL forwarding, not HTTP or HTTPS.</p> <p>There are multiple components within the ATC that each have their own set of responsibilities. The main components consist of the checker, scheduler, build tracker, and the garbage collector.</p> <p>The checker's responsibility is to continuously checks for new versions of resources. The scheduler is responsible for scheduling builds for a job and the build tracker is responsible for running any scheduled builds. The garbage collector is the cleanup mechanism for removing any unused or outdated objects, such as containers and volumes.</p> <p>All the components in a Concourse deployment can be viewed in the components table in the database as of v5.7.0. The intervals that the components run at can also be adjusted through editing that table, as well as pausing the component from running entirely.</p>"},{"location":"docs/internals/#tsa-worker-registration-forwarding","title":"TSA: worker registration &amp; forwarding","text":"<p>The TSA is a custom-built SSH server that is used solely for securely registering workers with the ATC.</p> <p>The TSA by default listens on port <code>2222</code>, and is usually co-located with the ATC and sitting behind a load balancer.</p> <p>The TSA implements CLI over the SSH connection, supporting the following commands:</p> <ul> <li>The <code>forward-worker</code> command is used to reverse-tunnel a worker's addresses through the TSA and register the forwarded   connections with the ATC. This allows workers running in arbitrary networks to register securely, so long as they can   reach the TSA. This is much safer than opening the worker up to the outside world.</li> <li>The <code>land-worker</code> command is sent from the worker when landing, and initiates the state change to <code>LANDING</code> through   the ATC.</li> <li>The <code>retire-worker</code> command is sent from the worker when retiring, and initiates the state change to <code>RETIRING</code>   through the ATC.</li> <li>The <code>delete-worker</code> command is sent from the worker when draining is interrupted while a worker is retiring. It   removes the worker from the ATC.</li> <li>The <code>sweep-containers</code> command is sent periodically to facilitate garbage collection of containers which can be   removed from the worker. It returns a list of handles for containers in the <code>DESTROYING</code> state, and it is the worker's   job to subsequently destroy them.</li> <li>The <code>report-containers</code> command is sent along with the list of all container handles on the worker. The ATC uses this   to update the database, removing any <code>DESTROYING</code> containers which are no longer in the set of handles, and marking   any <code>CREATED</code> containers that are not present as missing.</li> <li>The <code>sweep-volumes</code> command is sent periodically to facilitate garbage collection of volumes which can be removed from   the worker. It returns a list of handles for volumes in the <code>DESTROYING</code> state, and it is the worker's job to   subsequently destroy them.</li> <li>The <code>report-volumes</code> command is sent along with the list of all volume handles on the worker. The ATC uses this to   update the database, removing any <code>DESTROYING</code> volumes which are no longer in the set of handles, and marking   any <code>CREATED</code> volumes that are not present as missing.</li> </ul>"},{"location":"docs/internals/#workers-architecture","title":"Workers Architecture","text":"<p>Workers are machines running Garden and Baggageclaim servers and registering themselves via the TSA.</p> <p>Note</p> <p>Windows and Darwin workers also run Garden and Baggageclaim servers but do not run containers. They both use  houdini to fake making containers. Windows containers are not supported and  Darwin does not have native container technology.</p> <p>Workers have no important state configured on their machines, as everything runs in a container and thus shouldn't care about what packages are installed on the host (well, except for those that allow it to be a worker in the first place). This is very different from workers in other non-containerized CI solutions, where the state of packages on the worker is crucial to whether your pipeline works or not.</p> <p>Each worker registers itself with the Concourse cluster via the TSA.</p> <p>Workers by default listen on port <code>7777</code> for Garden and port <code>7788</code> for Baggageclaim. Connections to both servers are forwarded over the SSH connection made to the TSA.</p>"},{"location":"docs/internals/#the-worker-lifecycle","title":"The worker lifecycle","text":""},{"location":"docs/internals/#running","title":"RUNNING","text":"A worker in this state is registered with the cluster and ready to start running containers and storing volumes."},{"location":"docs/internals/#stalled","title":"STALLED","text":"A worker in this state was previously registered with the cluster, but stopped advertising itself for some reason. Usually this is due to network connectivity issues, or the worker stopping unexpectedly. If the worker remains in this state and cannot be recovered, it can be removed using the <code>fly prune-worker</code> command."},{"location":"docs/internals/#landing","title":"LANDING","text":"The <code>concourse land-worker</code> command will put a worker in the <code>LANDING</code> state to safely drain its assignments for temporary downtime. The ATC will wait for builds on the worker for jobs which are uninterruptible to finish, and transition the worker into <code>LANDED</code> state."},{"location":"docs/internals/#landed","title":"LANDED","text":"A worker in this state has successfully waited for all uninterruptible jobs on it after having <code>concourse land-worker</code> called. It will no longer be used to schedule any new containers or create volumes until it registers as <code>RUNNING</code> again."},{"location":"docs/internals/#retiring","title":"RETIRING","text":"The <code>concourse retire-worker</code> command will put a worker in the <code>RETIRING</code> state to remove it from the cluster permanently. The ATC will wait for builds on the worker for jobs which are uninterruptible to finish, and remove the worker."},{"location":"docs/internals/build-tracker/","title":"Build Tracker","text":"<p>The build tracker is the component that runs the execution of a build. It picks up any started builds, which can be orphaned builds (builds that an ATC started but did not finish) or builds that have just been scheduled. There is one build tracker per ATC, which runs on an interval that is defaulted to 10 seconds.</p>"},{"location":"docs/internals/checker/","title":"Resource Checker","text":"<p>Resources represent external state such as a git repository, files in an S3 bucket, or anything else that changes over time. By modelling these as resources, it allows you to use this external state as inputs (or triggers) to your workloads.</p>"},{"location":"docs/internals/checker/#when-are-resources-checked","title":"When are resources checked?","text":"<p>The component that schedules resource checks is called the resource checker. The rate at which these checks happen is called the check interval (configurable via <code>CONCOURSE_LIDAR_SCANNER_INTERVAL</code>). There's an obvious tradeoff, whereby the more frequently you poll, the bigger the strain on Concourse (as well as the external source). However, if you want to pick up those new commits as quickly as possible, then you need to poll as often as possible.</p> <p>The resource checker uses the <code>resource.check_every</code> interval in order to figure out if a resource needs to be checked. A resource's <code>check_every</code> interval dictates how often it should be checked for new versions, with a default of 1 minute. If that seems like a lot of checking, it is, but it's how Concourse keeps everything snappy. You can configure this interval independently for each resource using <code>check_every</code>.</p> <p>If your external service supports it, you can set <code>resource.webhook_token</code> to eliminate the need for periodic checking altogether. If a <code>webhook_token</code> is configured, the external service can notify Concourse when to check for new versions. Note that configuring a <code>webhook_token</code> will not stop Concourse from periodically checking your resource. If you wish to rely solely on webhooks for detecting new versions, you can set <code>check_every</code> to <code>never</code>.</p> <p>On every interval tick, the resource checker will see if there are any resources that need to be checked. It does this by first finding resources which are used as inputs to jobs, and then comparing the current time against the last time each resource was checked. If it has been longer than a resource's configured <code>check_every</code> interval, a new check will be scheduled. In practice this means that if a resource has a <code>check_every</code> of <code>1m</code>, it is not guaranteed to be checked precisely every 60 seconds. <code>check_every</code> simply sets a lower bound on the time between checks.</p> <p>When the resource checker finds a resource to check (either because its <code>check_every</code> interval elapsed, or because its configured <code>webhook_token</code> was triggered), it schedules a new build that invokes the <code>check</code> script of the resource's underlying resource type.</p>"},{"location":"docs/internals/checker/#what-do-resource-checks-produce","title":"What do resource checks produce?","text":"<p>The whole point of running checks is to produce versions. Concourse's Build Scheduler is centered around the idea of resource versions. It's how Concourse determines that something is new and a new build needs to be triggered.</p> <p>The versions produced by each resource are unique to the underlying resource type. For instance, the <code>git</code> resource type uses commit SHAs as versions. The <code>registry-image</code> resource uses the image digest and tag in the version.</p>"},{"location":"docs/internals/garbage-collector/","title":"Garbage Collector","text":"<p>Concourse runs everything in isolated environments by creating fresh containers and volumes to ensure things can safely run in a repeatable environment, isolated from other workloads running on the same worker.</p> <p>This introduces a new problem of knowing when Concourse should remove these containers and volumes. Safely identifying things for removal and then getting rid of them, releasing their resources, is the process of garbage collection.</p>"},{"location":"docs/internals/garbage-collector/#goals","title":"Goals","text":"<p>Let's define our metrics for success:</p> <ul> <li>Safe. There should never be a case where a build is running and a container or volume is removed out from under   it, causing the build to fail. Resource checking should also never result in errors from check containers being   removed. No one should even know garbage collection is happening.</li> <li>Airtight. Everything Concourse creates, whether it's a container or volume on a worker or an entry in the   database, should never leak. Each object should have a fully defined lifecycle such that there is a clear end to its   use. The ATC should be interruptible at any point in time and at the very least be able to remove any state it had   created beforehand.</li> <li>Resilient. Garbage collection should never be outpaced by the workload. A single misbehaving worker should not   prevent garbage collection from being performed on other workers. A slow delete of a volume should not prevent garbage   collecting of other things on the same worker.</li> </ul>"},{"location":"docs/internals/garbage-collector/#how-it-works","title":"How it Works","text":"<p>The garbage collector is a batch operation that runs on an interval with a default of 30 seconds. It's important to note that the collector must be able to run frequently enough to not be outpaced by the workload producing things, and so the batch operation should be able to complete pretty quickly.</p> <p>The batch operation first performs garbage collection within the database alone, removing rows that are no longer needed. The removal of rows from one stage will often result in removals in a later stage. There are individual collectors for each object, such as the volume collector or the container collector, and they are all run asynchronously.</p> <p>After the initial pass of garbage collection in the database, there should now be a set of containers and volumes that meet criteria for garbage collection. These two are a bit more complicated to garbage-collect; they both require talking to a worker, and waiting on a potentially slow delete.</p> <p>Containers and volumes are the costliest resources consumed by Concourse. There are also many of them created over time as builds execute and pipelines perform their resource checking. Therefore it is important to parallelize this aspect of garbage collection so that one slow delete or one slow worker does not cause them to pile up.</p>"},{"location":"docs/internals/scheduler/","title":"Build Scheduler","text":"<p>Warning</p> <p>As of the v6.0.0 release, there have been many changes to the scheduler, so it would be advisable to assume that  this documentation should only be used for Concourse deployments v6.0.0 and above.</p> <p>Builds represent each execution of a job. Figuring out when to schedule a new job build is the responsibility of the build scheduler. The scheduling of new job builds can be dependent on many different factors such as when a new version of a resource is discovered, when a dependent upstream build finishes, or when a user manually triggers a build.</p> <p>The build scheduler is a global component, where it deals with all the jobs within a deployment. It runs on an interval with a default of 10 seconds. If there are multiple ATCs, only one of the ATC's scheduler component will run per interval tick in order to ensure that there will be no duplicated work between ATC nodes.</p> <p>The subcomponent used to figure out whether a build can be scheduled is called the algorithm.</p>"},{"location":"docs/internals/scheduler/#algorithm","title":"Algorithm","text":"<p>The algorithm is a subcomponent of the scheduler which is used to determine the input versions to the next build of a job. There are many factors that contribute to figuring out the next input versions. It can be anything that affects which resource versions will be used to schedule a build, such as <code>version</code> constraints or <code>passed</code> constraints in a <code>get</code> step, disabling versions through the web UI, etc. The algorithm can also fail to determine a successful set of input versions, which the error will be propagated to the preparation view in the build page.</p> <p>If the algorithm computes a successful set of input versions, it will figure out whether the versions it computed can be used to produce a new build. This is done by comparing the trigger-able input versions to the versions used by the previous build and if any of them have a different version, then the scheduler will know to schedule a new build. Conversely, if the input versions produced by the algorithm are the same as the previous build, then the scheduler will not create a new build.</p>"},{"location":"docs/internals/scheduler/#scheduling-behavior","title":"Scheduling behavior","text":"<p>The scheduler will schedule a new build if any of the versions produced by the algorithm for <code>trigger: true</code> resources has not been used in any previous build of the job.</p> <p>What this means is if the algorithm runs and computes an input version, the scheduler will create a new build as long as that version has not been used by any previous build's version for that same input. Even if that version has been used by a build 2 months ago, the scheduler will not schedule a new build because that version has been previously used in a build of the job.</p> <p>If there are any input versions that are different from any previous build, it will trigger a new build.</p>"},{"location":"docs/internals/scheduler/#scheduling-on-demand","title":"Scheduling on demand","text":"<p>The scheduler runs on an interval, but rather than scheduling all the jobs within a deployment on every tick, it only schedules the jobs that need to be scheduled.</p> <p>First, the scheduler determines which jobs need to be scheduled. Below are all the reasons why Concourse will think a job needs to be scheduled:</p> <ul> <li>Detecting new versions of a resource through a check</li> <li>Saving a new version through a put</li> <li>A build finishes for an upstream job (through passed constraints)</li> <li>Enabling/Disabling a resource version that has not been used in a previous build</li> <li>Pinning/Unpinning a resource version that has not been used in a previous build</li> <li>Setting a pipeline</li> <li>Updating a resource's <code>resource_config</code></li> <li>Manually triggering a build</li> <li>Rerunning a build</li> <li>Multiple versions available for a version every constraint</li> </ul> <p>Each job that is scheduled will use the algorithm to determine what inputs its next build should have. Then the build is scheduled and picked up by the Build Tracker.</p>"},{"location":"docs/operation/","title":"Operation","text":"<p>The following sections describes operator-focused features and tools that Concourse provides, such as monitoring and credential management.</p> <p>These concepts are not required to operate Concourse, but are for users that are looking to extend the capabilities of managing a Concourse deployment. For users that are new to these concepts, we do recommend learning how to set up Credential Management and Encryption.</p>"},{"location":"docs/operation/administration/","title":"Administration","text":""},{"location":"docs/operation/administration/#managing-workers","title":"Managing Workers","text":""},{"location":"docs/operation/administration/#fly-workers","title":"<code>fly workers</code>","text":"<p>To list the currently registered workers, including additional metadata, run:</p> <pre><code>fly -t example workers\n</code></pre> <p>This can be useful for monitoring the status of your workers, if you suspect that one keeps dropping out of the pool or getting tasked with too many containers, etc.</p>"},{"location":"docs/operation/administration/#fly-prune-worker","title":"<code>fly prune-worker</code>","text":"<p>To remove a stalled, landing, landed, or retiring worker, run:</p> <pre><code>fly -t example prune-worker --worker worker-name\n</code></pre> <p>To prune all stalled workers, run:</p> <pre><code>fly -t example prune-worker --all-stalled\n</code></pre> <p>This is for those cases where you know a worker is not coming back.</p> <p>Note</p> <p>Running workers cannot be pruned, since they'll just re-register themselves anyway.</p>"},{"location":"docs/operation/administration/#fly-land-worker","title":"<code>fly land-worker</code>","text":"<p>To initiate landing of a worker and eventually (after draining) cause it to exit, run:</p> <pre><code>fly -t example land-worker --worker worker-name\n</code></pre>"},{"location":"docs/operation/administration/#broadcast-message-system","title":"Broadcast Message System","text":"<p>Concourse Admins who operate a big Concourse with many teams often want a way to communicate to everyone that the system is unstable/recovering. Setting a message on the Wall will result in a banner displaying the wall message in the Concourse web UI. The following commands are used to manage the Wall.</p> <p>Fun Fact!</p> <p>\"Wall\" is a reference to the Unix <code>wall</code> CLI.</p>"},{"location":"docs/operation/administration/#fly-set-wall","title":"<code>fly set-wall</code>","text":"<p>Requires being a member of the main team. To set a new wall with a message and expiration, run:</p> <pre><code>fly -t main set-wall --message=\"\u26a0\ufe0f Hello World, there is an error \u26a0\ufe0f\" --ttl=5m\n</code></pre> <p>This will set a wall of \"\u26a0\ufe0f Hello World, there is an error \u26a0\ufe0f\" with an expiration of five minutes.</p>"},{"location":"docs/operation/administration/#fly-get-wall","title":"<code>fly get-wall</code>","text":"<p>To get the current wall, run:</p> <pre><code>fly -t main get-wall\n</code></pre>"},{"location":"docs/operation/administration/#fly-clear-wall","title":"<code>fly clear-wall</code>","text":"<p>Requires being a member of the main team. To clear a current wall, run:</p> <pre><code>fly -t main clear-wall\n</code></pre>"},{"location":"docs/operation/administration/#diagnostic-troubleshooting","title":"Diagnostic / Troubleshooting","text":""},{"location":"docs/operation/administration/#fly-containers","title":"<code>fly containers</code>","text":"<p>To list the active containers across all your workers, run:</p> <pre><code>fly -t example containers\n</code></pre> <p>This can be useful when discovering the containers available for <code>fly intercept</code>ing.</p>"},{"location":"docs/operation/administration/#fly-volumes","title":"<code>fly volumes</code>","text":"<p>To list the active volumes across all your workers, run:</p> <pre><code>fly -t example volumes\n</code></pre> <p>This can be useful to observe the caches warming across your cluster, and could be a good indicator of disk use.</p>"},{"location":"docs/operation/administration/#fly-curl","title":"<code>fly curl</code>","text":"<p>To execute an arbitrary API request, you can run something like the following:</p> <pre><code>fly -t example curl /api/v1/info\n</code></pre> <p>This command is just a shim that runs <code>curl</code> under the hood. To pass flags to <code>curl</code>, pass a <code>--</code> argument after the path so that <code>fly</code> can distinguish them from its own flags:</p> <pre><code>fly -t example curl /api/v1/builds -- \\\n    -X PUT \\\n    -H \"Content-type: application/json\" \\\n    -d @plan.json\n</code></pre> <p>Note</p> <p>If you use this command the assumption is that you know what you're doing. If you find yourself using this command  often, let us know - perhaps there's a missing command!</p>"},{"location":"docs/operation/container-placement/","title":"Container Placement","text":"<p>Each step in a build is executed inside a container. The <code>web</code> node distributes containers across the worker cluster depending on the configured strategy. If no workers satisfy the configured strategy, the step will block until a worker becomes available.</p>"},{"location":"docs/operation/container-placement/#the-volume-locality-strategy","title":"The <code>volume-locality</code> strategy","text":"<p>When using <code>volume-locality</code>, the <code>web</code> node places <code>task</code> step and <code>put</code> step containers on workers where a majority of their inputs are already present. This is the default strategy.</p> <p>The advantage of this approach is that it reduces the likelihood that large artifacts will have to be streamed from one <code>worker</code> node, through the <code>web</code> node, and to the target <code>worker</code> node. For large artifacts, this can result in quite a bit of overhead.</p> <p>The disadvantage of this approach is that it can sometimes result in builds \"gravitating\" to a particular worker and overloading it, at least until the resource caches warm across the worker pool. This disadvantage can be partially mitigated using the (currently experimental) <code>limit-active-volumes</code> strategy in conjunction with Chaining Placement Strategies.</p> <p>If your builds tend to be light on artifacts and heavy on task execution, you may want to try the <code>fewest-build-containers</code> strategy or the (currently experimental) <code>limit-active-tasks</code> strategy.</p>"},{"location":"docs/operation/container-placement/#the-fewest-build-containers-strategy","title":"The <code>fewest-build-containers</code> strategy","text":"<p>When using the <code>fewest-build-containers</code> strategy, step containers (<code>get</code>, <code>put</code>, <code>task</code>) are placed on the worker that has the fewest build containers (i.e. containers for other steps of other builds).</p> <p>Info</p> <p>Containers used for resource checks are not counted because they are long-living containers that get re-used for  multiple checks, and therefore consume very little resources on the worker.</p> <p>To use this strategy, set the following env var on the <code>web</code> node:</p> <pre><code>CONCOURSE_CONTAINER_PLACEMENT_STRATEGY=fewest-build-containers\n</code></pre>"},{"location":"docs/operation/container-placement/#the-random-strategy","title":"The <code>random</code> strategy","text":"<p>With the <code>random</code> strategy, the <code>web</code> node places <code>get</code>, <code>put</code>, and <code>task</code> containers on any worker, ignoring any affinity.</p> <p>As this is truly random, this will be fine until one day it's not fine.</p> <p>To use this strategy, set the following env var on the <code>web</code> node:</p> <pre><code>CONCOURSE_CONTAINER_PLACEMENT_STRATEGY=random\n</code></pre>"},{"location":"docs/operation/container-placement/#the-limit-active-tasks-strategy","title":"The <code>limit-active-tasks</code> strategy","text":"<p>When selecting the <code>limit-active-tasks</code> placement strategy, each <code>task</code> executed on a worker will increase the number of \"active tasks\" on that worker by one. When the task completes the number is decreased by one. The <code>web</code> node then places <code>get</code>, <code>put</code>, and <code>task</code> containers on the worker that currently has the least amount of active tasks.</p> <p>Additionally, <code>max-active-tasks-per-worker</code> can be set to an integer of 1 or more, in which case a worker will not execute more than that amount of tasks. A value of 0 means that there is no limit on the maximum number of active tasks on the workers. If no worker can be selected because all of them already have <code>max-active-tasks-per-worker</code> active tasks, then the task will wait for a free worker, periodically polling the pool. The metric <code>concourse_steps_waiting{type=\"task\"}</code> is emitted to monitor these events. Note that the parameter does not apply to <code>get</code> and <code>put</code> steps which will always be scheduled on the worker with the fewest active tasks.</p> <pre><code>CONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-tasks\n</code></pre> <p>and, optionally</p> <pre><code>CONCOURSE_MAX_ACTIVE_TASKS_PER_WORKER=1\n</code></pre>"},{"location":"docs/operation/container-placement/#the-limit-active-containers-strategy","title":"The <code>limit-active-containers</code> strategy","text":"<p>The <code>limit-active-containers</code> placement strategy rejects workers that already have too many containers. It makes no effort to find the worker with the fewest number of containers present, and is therefore most useful when combined with other placement strategies by Chaining Placement Strategies.</p> <p><code>max-active-containers-per-worker</code> can be set to an integer of 1 or more, in which case a worker will not execute more than that amount of containers. If unset (or set to a value of 0), the <code>limit-active-containers</code> strategy has no effect - if this is your only placement strategy, workers will be chosen at random.</p> <pre><code>CONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-containers\nCONCOURSE_MAX_ACTIVE_CONTAINERS_PER_WORKER=200\n</code></pre>"},{"location":"docs/operation/container-placement/#the-limit-active-volumes-strategy","title":"The <code>limit-active-volumes</code> strategy","text":"<p>The <code>limit-active-volumes</code> placement strategy rejects workers that already have too many volumes. It makes no effort to find the worker with the fewest number of volumes present, and is therefore most useful when combined with other placement strategies by Chaining Placement Strategies.</p> <p><code>max-active-volumes-per-worker</code> can be set to be an integer of 1 or more, in which case a worker will not execute more than that amount of volumes. If unset (or set to a value of 0), the <code>limit-active-volumes</code> strategy has no effect - if this is your only placement strategy, workers will be chosen at random.</p> <pre><code>CONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-volumes\nCONCOURSE_MAX_ACTIVE_VOLUMES_PER_WORKER=200\n</code></pre>"},{"location":"docs/operation/container-placement/#chaining-placement-strategies","title":"Chaining Placement Strategies","text":"<p>Container placement strategies can be chained together to apply multiple strategies in sequence. The first strategy in the chain receives the entire set of workers, filtering the set down in some way, and passing that new set of workers to the next strategy in the chain. If the last strategy in the chain returns multiple workers, one will be chosen at random.</p> <p>For instance, consider the following configuration:</p> <pre><code>CONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-containers,limit-active-volumes,volume-locality,fewest-build-containers\nCONCOURSE_MAX_ACTIVE_CONTAINERS_PER_WORKER=200\nCONCOURSE_MAX_ACTIVE_VOLUMES_PER_WORKER=100\n</code></pre> <p>This defines a chain of 4 placement strategies, plus the implicit <code>random</code> strategy. Let's look at what each strategy accomplishes:</p> <ol> <li><code>limit-active-containers</code> strategy removes all workers that already have    more than 200 containers</li> <li><code>limit-active-volumes</code> strategy removes all remaining workers that already have    more than 100 volumes</li> <li><code>volume-locality</code> strategy keeps only the remaining worker(s) that have the most    inputs locally. This can keep more than one worker in the case of a tie</li> <li><code>fewest-build-containers</code> strategy will attempt to break ties by selecting    the worker with fewer build containers. If all the remaining workers have the exact same number of containers, one    will be selected at random</li> </ol>"},{"location":"docs/operation/encryption/","title":"Encryption","text":"<p>Automating everything means authorizing something to automate many things. This makes CI systems a high-risk target for security leaks.</p> <p>Concourse pipelines are loaded with credentials: resources are configured with private keys, tasks are given credentials to servers they integrate via credential manager variables, <code>task</code> step <code>vars</code>, or <code>task</code> step <code>params</code>, etc. If someone gets their hands on your config, they have access to everything.</p> <p>To mitigate this, Concourse supports encrypting sensitive information before it reaches the database. This way the plaintext credentials only exist in memory for as long as they need to, and if someone gains access to your database, they can't so easily gain the keys to the kingdom.</p> <p>We strongly encourage anyone running Concourse to configure encryption. Going further, it's best to have Concourse not store the credentials in the first place, in which case you may want to configure credential management as well.</p>"},{"location":"docs/operation/encryption/#whats-encrypted","title":"What's encrypted?","text":"<p>The following values are expected to contain credentials, and so will be encrypted:</p> <ul> <li>Resource <code>resource.sources</code>, as they often contain private keys and other   credentials for writing to (or simply granting access to) the resource.</li> <li>Resource type <code>resource_type.sources</code>, for the same reason as   above, though this is probably a less common use case.</li> <li>Pipeline <code>task</code> step <code>vars</code> and <code>task</code> step <code>params</code>, in case they contain   sensitive information such as usernames and/or passwords.</li> <li>Put step <code>put</code> step <code>params</code> and get step <code>get</code> step <code>params</code> are also   encrypted, even though they rarely should contain credentials (they're usually in  <code>resource.source</code>).</li> <li>Team auth configurations, as they often contain things like GitHub or other oAuth client secrets.</li> </ul> <p>Note</p> <p>The actual implementation encrypts things in a more heavy-handed way than the above list implies. For example,  pipeline configs are actually encrypted as one large blob.</p> <p>Notably, the following things are NOT encrypted:</p> <ul> <li>Build logs. If your jobs are outputting credentials, encryption won't help you. We have chosen not to tackle this   initially as it would introduce a performance burden for what is not as much of an obvious win.</li> <li>Resource versions. These should never contain credentials, and are often meaningless on their own.</li> <li>Resource metadata. These are visible to anyone if your pipeline   is exposed, and should never contain   credentials.</li> <li>Pipeline names, job names, etc. - anything else that is not a high-risk target for credential leakage, as opposed to   regular information leaks.      Resources and jobs in particular exist in their own tables, with their names in plaintext, and only their config   encrypted. In this way, names are not protected, even though the pipeline config itself is also stored as one big   encrypted blob.</li> </ul>"},{"location":"docs/operation/encryption/#enabling-encryption","title":"Enabling Encryption","text":"<p>To enable encryption, you'll just need to come up with a 16 or 32-byte random character sequence and configure it as <code>--encryption-key</code> flag to the <code>web</code> command. For BOSH, this is the  <code>encryption_key</code> property.</p> <p>On startup, the <code>web</code> node will encrypt all existing plaintext data, and any new data being written will be encrypted before it's sent over the network to the database.</p> <p>The initial bulk encryption shouldn't take too long, but it will scale linearly with the amount of data that you have, and if another ATC is running it'll suddenly not be able to read the data until it's also given the key. So, expect some downtime.</p>"},{"location":"docs/operation/encryption/#rotating-the-encryption-key","title":"Rotating the Encryption Key","text":"<p>To swap out the encryption key, you'll need to pass the previous key as <code>--old-encryption-key</code> (or  <code>old_encryption_key</code>), and the new key as <code>--encryption-key</code> (or  <code>encryption_key</code>).</p> <p>On startup, the <code>web</code> node will decrypt all existing data and re-encrypt it with the new key, in one go. If it encounters a row which is already encrypted with the new key, it will continue on (as may be the case when restarting with the flags again, or if the ATC died in the middle of rotating).</p> <p>If the ATC encounters a row which cannot be decrypted with neither the old key nor the new one, it will log loudly and fail to start, telling you which row it choked on. This data must be dealt with in some way, either by re-configuring the key the row was encrypted with as the old key, or manually performing database surgery to remove the offending row. Hopefully this doesn't happen to you!</p>"},{"location":"docs/operation/encryption/#disabling-encryption","title":"Disabling Encryption","text":"<p>To opt out of encryption entirely (I'm sure you have your reasons), simply pass <code>--old-encryption-key</code> (or  <code>old_encryption_key</code>) alone. With no new encryption key, the web node will decrypt all existing data on start.</p>"},{"location":"docs/operation/global-resources/","title":"Global Resources","text":"<p>Experimental Feature</p> <p>Global Resources is an experimental feature introduced in Concourse v5.0 . It is enabled by passing the  <code>--enable-global-resources</code> flag to the <code>concourse web</code> command.</p> <p>The basic concept of global resources is to share detected resource versions between all resources that have the same <code>resource.type</code> and <code>resource.source</code> configuration.</p> <p>Before v5.0.0, each pipeline resource had its own version history, associated to the resource by name. This meant that multiple pipelines with the same resource configs would redundantly collect the same version and metadata information.</p> <p>With v5.0.0's experimental 'global resources' feature, resource versions are instead associated to an anonymous 'resource config' i.e. its <code>resource.type</code> and <code>resource.source</code>.</p>"},{"location":"docs/operation/global-resources/#benefits-of-global-resources","title":"Benefits of Global Resources","text":""},{"location":"docs/operation/global-resources/#fewer-resource-checks-to-perform","title":"Fewer resource <code>check</code>s to perform","text":"<p>With global resources, all resources that have the same configuration will share the same version history and share only one checking interval. This reduces load on the worker and on the external services that the resources point to.</p> <p>For example, prior to global resources if there were three resources with the same configuration between three team's pipelines it would result in three check containers performing three resource checks every minute to fetch the versions.</p> <p>With global resources, this configuration will result in only one check container and one resource check every minute to fetch versions for all the resources.</p> <p>Since there will be only one resource check for all resources that have the same configuration, the resource that has the shortest <code>resource.check_every</code> configured will result in its pipeline running the checks for that resource configuration.</p>"},{"location":"docs/operation/global-resources/#complications-with-reusing-containers","title":"Complications with reusing containers","text":"<p>There is an exception to sharing check containers within a deployment, which is workers belonging to a team and workers with tags.</p> <p>If a resource has <code>resource.tags</code> configured, and the resource's check interval ends up acquiring the checking lock, if a check container already exists with the same resource config elsewhere, it will reuse the container, otherwise a container will be created on a worker matching the appropriate tags.</p> <p>Similarly, if a team has its own workers, and their check interval ended up acquiring the lock, it will try to re-use a container with the same resource config from the shared worker pool, rather than creating a new container on the team's workers.</p> <p>This is a bit complicated to reason about, and we plan to stop re-using <code>check</code> containers to simplify all of this. See concourse/concourse#3079 for more information.</p>"},{"location":"docs/operation/global-resources/#reducing-redundant-data","title":"Reducing redundant data","text":"<p>The majority of Concourse resources will benefit from having versions shared globally because most resource versions have an external source of truth.</p> <p>For example, a <code>check</code> for the <code>git</code> resource that pulls in the <code>concourse/concourse</code> repository will always return the same set of versions as an equivalent resource pointing to the same repository. By consolidating the <code>check</code>s and the versions, there will essentially only be one set of versions collected from the repository and saved into the database.</p>"},{"location":"docs/operation/global-resources/#reliable-resource-version-history","title":"Reliable Resource Version History","text":"<p>Prior to global resources, a resource's version history was directly associated to the resource name. This meant that any changes to a resource's configuration without changing its name would basically append the versions from the new configuration after the old versions, which are no longer accurate to the current configuration.</p> <p>Global resources instead associates the resource versions to the resource's <code>resource.type</code> and <code>resource.source</code>. Therefore, whenever a resource definition changes, the versions will \"reset\" and change along with it, resulting in truthful and reliable version histories.</p>"},{"location":"docs/operation/global-resources/#risks-and-side-effects","title":"Risks and Side Effects","text":""},{"location":"docs/operation/global-resources/#sharing-versions-doesnt-work-well-for-all-resource-types","title":"Sharing versions doesn't work well for all resource types","text":"<p>Sharing versions isn't always a good idea. For example, the  <code>time</code> resource is often used to generate versions on an interval so that jobs can fire periodically. If version history were to be shared for all users with e.g. a 10-minute interval, that would lead to a thundering herd of builds storming your workers, leading to load spikes and a lot of unhappy builds.</p> <p>We are working toward a solution to the <code>time</code> resource's thundering herd problem - namely, to not model time as a resource, and instead model it as a  <code>var_source</code>. We are tracking progress toward this goal in concourse/concourse#5815.</p> <p>Another case where version history shouldn't be shared is when resources \"automagically\" learn their auth credentials using things like IAM roles. In these cases, the credentials aren't in the <code>resource.source</code>. If version history were to be shared, anyone could configure the same <code>source:</code>, not specifying any credentials, and see the version history discovered by some other pipeline that ran its checks on workers that had access via IAM roles.</p> <p>For this reason, any resource types that acquire credentials outside of <code>source:</code> should not share version history. Granted, the user won't be able to fetch these versions, but it's still an information leak.</p> <p>IAM roles are a bit of a thorn in our side when it comes to designing features like this. We're planning on introducing support for them in a way that doesn't have this problem in concourse/concourse#3023.</p>"},{"location":"docs/operation/global-resources/#intercepting-check-containers-is-no-longer-safe","title":"Intercepting <code>check</code> containers is no longer safe","text":"<p>Now that <code>check</code> containers are shared across teams, it would be dangerous to allow anyone to <code>fly intercept</code> to <code>check</code> containers. For this reason, this capability is limited to admin users.</p> <p>We recognize that this will make it a bit more difficult for end users to debug things like failing checks. We plan to improve this by introducing a way to provision a new <code>check</code> container to facilitate debugging. See concourse/concourse#3344 for more information.</p>"},{"location":"docs/operation/metrics/","title":"Metrics","text":"<p>Metrics are essential in understanding how any large system is behaving and performing. Concourse can emit metrics about both the system health itself and about the builds that it is running. Operators can tap into these metrics in order to observe the health of the system.</p>"},{"location":"docs/operation/metrics/#configuring-metrics","title":"Configuring Metrics","text":"<p>The <code>web</code> node can be configured to emit metrics on start.</p> <p>Currently supported metrics emitters are InfluxDB, NewRelic, Prometheus, and Datadog. There is also a dummy emitter that will just spit the metrics out in to the logs at <code>DEBUG</code> level, which can be enabled with the <code>--emit-to-logs</code> flag.</p> <p>Regardless of your metrics emitter, you can set <code>CONCOURSE_METRICS_BUFFER_SIZE</code> to determine how many metrics emissions are sent at a time. Increasing this number can be helpful if sending metrics is regularly failing (due to rate limiting or network failures) or if latency is particularly high.</p> <p>There are various flags for different emitters; run <code>concourse web --help</code> and look for \"Metric Emitter\" to see what's available.</p>"},{"location":"docs/operation/metrics/#whats-emitted","title":"What's emitted?","text":"<p>This reference section lists of all the metrics that Concourse emits via the Prometheus emitter.</p> <p>To make this document easy to maintain, Prometheus is used as the \"source of truth\" - primarily because it has help text built-in, making this list easy to generate. Treat this list as a reference when looking for the equivalent metric names for your emitter of choice.</p>"},{"location":"docs/operation/opa-integration/","title":"Open Policy Agent Integration","text":"<p>The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack.</p> <p>OPA allows you to create arbitrary rules within Concourse without having to add a new feature to Concourse. You could even recreate Concourse's RBAC system using OPA.</p> <p>More likely use-cases are to enforce rules your organization may have, such as not using certain container images or disallowing the use of privileged workloads. With OPA you can be as general or fine-grained as you want, enforcing these rules at the team or pipeline level.</p> <p>The next few sections explain how to configure Concourse to talk to an OPA server and how to write OPA rules for Concourse.</p>"},{"location":"docs/operation/opa-integration/#configuring-concourse","title":"Configuring Concourse","text":"<p>There are four configuration options you need to set on the <code>concourse web</code> nodes to have them interact with OPA.</p> <p><code>CONCOURSE_OPA_URL</code></p> <p>The OPA policy check endpoint. Should point to a specific <code>package/rule</code> that contains all Concourse rules for your cluster.</p> <p>Example: <code>http://opa-endpoint.com/v1/data/concourse/decision</code></p> <p><code>CONCOURSE_POLICY_CHECK_FILTER_HTTP_METHOD</code></p> <p>API http methods to go through policy check. You will need to make sure these match up with an API action in the next two configuration options.</p> <p>Example: <code>PUT,POST</code></p> <p><code>CONCOURSE_POLICY_CHECK_FILTER_ACTION</code></p> <p>Actions in this list will go through policy check.</p> <p>Example: <code>ListWorkers,ListContainers</code></p> <p><code>CONCOURSE_POLICY_CHECK_FILTER_ACTION_SKIP</code></p> <p>Actions in this list will not go through policy check</p> <p>Example: <code>PausePipeline,UnpausePipeline</code></p> <p>For the last three configuration options you can refer to this list of routes for a list of API actions and their respective HTTP method. There are also some Special Actions not directly in the API.</p>"},{"location":"docs/operation/opa-integration/#writing-opa-rules","title":"Writing OPA Rules","text":"<p>On the OPA server you'll need to create a package and policy for Concourse. This should match up with the endpoint provided to Concourse. The OPA documentation has a good guide explaining how to generally write OPA rules and set up an OPA server.</p> <p>For any actions that Concourse has been configured to filter it will send a JSON request to the OPA server with the following details. Top-level data directly under the <code>input</code> key will be present for most actions. The information under the <code>data</code> key will differ based on the action being checked.</p> <p>This sample JSON payload is what OPA is sent when a user sets a pipeline. The <code>data</code> key contains the pipeline in JSON format.</p> <pre><code>{\n  \"input\": {\n    \"service\": \"concourse\",\n    \"cluster_name\": \"dev\",\n    \"cluster_version\": \"7.4.0\",\n    \"http_method\": \"PUT\",\n    \"action\": \"SaveConfig\",\n    \"user\": \"test\",\n    \"team\": \"main\",\n    \"pipeline\": \"check-pipeline\",\n    \"data\": {\n      \"jobs\": [\n        {\n          \"name\": \"test\",\n          \"plan\": [\n            {\n              \"get\": \"tiny\"\n            },\n            {\n              \"config\": {\n                \"image_resource\": {\n                  \"source\": {\n                    \"repository\": \"busybox\"\n                  },\n                  \"type\": \"registry-image\"\n                },\n                \"platform\": \"linux\",\n                \"run\": {\n                  \"args\": [\n                    \"-exc\",\n                    \"echo hello\"\n                  ],\n                  \"path\": \"sh\"\n                }\n              },\n              \"task\": \"a-task\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre> <p>An OPA rule can respond to Concourse with three fields:</p> <ul> <li><code>allowed</code> (required): Boolean type. Setting to <code>False</code> will deny the action unless the <code>block</code> field is <code>False</code>.</li> <li><code>block</code> (optional): Boolean type. If set to <code>False</code> and <code>allowed</code> is <code>True</code> this creates a soft-policy enforcement.   The action will be allowed and the <code>reasons</code> will still be printed to the web UI like a warning message. (1)</li> <li><code>reasons</code> (optional): List of string type. If an action is denied based on the <code>allowed</code> field then the reason(s)   will be displayed in the UI.</li> </ul> <ol> <li>Not setting <code>block</code> is the same as setting <code>\"block\": true</code>.</li> </ol> <p>Here is an example OPA policy. By default, it will allow whatever action it has been sent. It will deny the action if one or more of the three deny rules are true.</p> concourse.rego<pre><code>package concourse\n\ndefault decision = {\"allowed\": true}\n\ndecision = {\"allowed\": false, \"reasons\": reasons} {\n  count(deny) &gt; 0\n  reasons := deny\n}\n\ndeny[\"cannot use docker-image types\"] {\n  input.action == \"UseImage\"\n  input.data.image_type == \"docker-image\"\n}\n\ndeny[\"cannot run privileged tasks\"] {\n  input.action == \"SaveConfig\"\n  input.data.jobs[_].plan[_].privileged\n}\n\ndeny[\"cannot use privileged resource types\"] {\n  input.action == \"SaveConfig\"\n  input.data.resource_types[_].privileged\n}\n</code></pre>"},{"location":"docs/operation/opa-integration/#special-actions","title":"Special Actions","text":"<p>Most of the actions you can filter for come directly from the list of API actions. There are currently two special actions you can also filter on.</p>"},{"location":"docs/operation/opa-integration/#useimage","title":"<code>UseImage</code>","text":"<p>Before Concourse starts a container you can check what image it is going to use to create the container. Depending on the <code>image_type</code> the <code>image_source</code> field may contain other fields. The JSON payload for this action will look similar to the following example:</p> <pre><code>{\n  \"input\": {\n    \"service\": \"concourse\",\n    \"cluster_name\": \"dev\",\n    \"cluster_version\": \"7.4.0\",\n    \"action\": \"UseImage\",\n    \"team\": \"main\",\n    \"pipeline\": \"simple\",\n    \"data\": {\n      \"image_type\": \"registry-image\",\n      \"privileged\": true,\n      \"image_source\": {\n        \"repository\": \"alpine\",\n        \"tag\": \"latest\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"docs/operation/opa-integration/#setpipeline","title":"<code>SetPipeline</code>","text":"<p>This action occurs whenever a <code>set_pipeline</code> step is run. The JSON payload for this action will contain the pipeline config in JSON format under the <code>data</code> key:</p> <pre><code>{\n  \"input\": {\n    \"service\": \"concourse\",\n    \"cluster_name\": \"dev\",\n    \"cluster_version\": \"7.4.0\",\n    \"action\": \"SetPipeline\",\n    \"team\": \"main\",\n    \"pipeline\": \"simple\",\n    \"data\": {\n      \"jobs\": [\n        {\n          \"name\": \"test\",\n          \"plan\": [\n            {\n              \"get\": \"tiny\"\n            },\n            {\n              \"config\": {\n                \"image_resource\": {\n                  \"source\": {\n                    \"repository\": \"busybox\"\n                  },\n                  \"type\": \"registry-image\"\n                },\n                \"platform\": \"linux\",\n                \"run\": {\n                  \"args\": [\n                    \"-exc\",\n                    \"echo hello\"\n                  ],\n                  \"path\": \"sh\"\n                }\n              },\n              \"task\": \"a-task\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"docs/operation/security-hardening/","title":"Security Hardening","text":"<p>Concourse tasks run in containers, which provide a degree of isolation from the host. However, if inadequate attention is paid to security, anyone with the ability to update pipelines or modify a script called in a task might be able to escape from the container and take control of the host. From there, they could access other host resources, interfere with pipelines they might not otherwise have access to, and collect credentials.</p> <p>Following the guidance in this section can help you to greatly reduce the risk of a container escape.</p>"},{"location":"docs/operation/security-hardening/#keeping-your-kernel-up-to-date","title":"Keeping your kernel up-to-date","text":"<p>Containers run in different Linux namespaces on the same Linux kernel as the host system. Vulnerabilities in the kernel version you run can allow for local privilege escalation - which in the Concourse context means allowing an escape from a Concourse task to full root privileges on the host.</p> <p>You can greatly reduce the risk of container escapes by staying up to date with your kernel version, tracking either the latest release, or the latest kernel from a Linux distribution with a reputable security programme.</p>"},{"location":"docs/operation/security-hardening/#locking-down-privileged-mode","title":"Locking down privileged mode","text":"<p>By default, privileged mode (i.e. tasks with <code>privileged: true</code> on the task step) grants containers a very wide set of Linux capabilities, without any restrictions on syscalls allowed. These privileges are enough to load a kernel module ( allowing arbitrary privilege escalation and container escape), as well as direct access to all host devices. As such, by default, privileged tasks are equivalent to full root access on the host.</p> <p>If you are running a worker using the containerd container runtime, Concourse provides some options to reduce the risk of container escapes through privileged tasks.</p> <p>The <code>--containerd-privileged-mode=ignore</code> (or by environment variable, <code>CONCOURSE_CONTAINERD_PRIVILEGED_MODE=ignore</code>) option to the worker is the most restrictive, but most secure option. It makes Concourse treat privileged tasks the same as normal tasks (i.e. grants no extra privileges, effectively disabling privileged tasks). While this is secure, it is also restrictive if you want to do things like build or run containers inside tasks.</p> <p>The <code>--containerd-privileged-mode=fuse-only</code> (or by environment variable, <code>CONCOURSE_CONTAINERD_PRIVILEGED_MODE=fuse-only</code>) option to the worker makes it possible to secure privileged tasks against container escape, while still allowing privileged tasks to build container images with buildah, and run them with podman from inside the task.</p> <p>Caution</p> <p>For the fuse-only privileged mode option to be secure against escapes from privileged tasks, you must run your  worker in a container with user namespaces enabled. Privileged containers in fuse-only mode have <code>CAP_SYS_ADMIN</code>  capability, which is harmless when in a non-default user namespace, but equivalent to full root on the host  otherwise. When running the worker in a Docker or podman container, refer to the  Docker or  Podman docs to learn how to set up  user namespaces.</p>"},{"location":"docs/operation/tracing/","title":"Tracing","text":"<p>Experimental Feature</p> <p>Tracing is an experimental feature.</p> <p>Tracing in Concourse enables the delivery of traces related to the internal processes that go into running builds, and other internal operations, breaking them down by time, and component.</p> <p>It leverages the (OpenTelemetry) SDK to allow support for many platforms. Currently tracing can be configured to integrates with:</p> <ul> <li>Jaeger</li> <li>Google Cloud Trace (Stackdriver)</li> <li>Honeycomb.io</li> <li>OpenTelemetry Protocol Exporter</li> </ul>"},{"location":"docs/operation/tracing/#configuring-tracing","title":"Configuring Tracing","text":"<p>To export spans to Jaeger, specify the Thrift HTTP endpoint of the Jaeger collector:</p> <pre><code>CONCOURSE_TRACING_JAEGER_ENDPOINT=http://jaeger:14268/api/traces\n</code></pre> <p>To export spans to Google Cloud Trace, specify the GCP Project ID:</p> <pre><code>CONCOURSE_TRACING_STACKDRIVER_PROJECTID=your-gcp-project-id\n</code></pre> <p>Note that suitable GCP credentials must be available, via the usual  <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, the default location that the <code>gcloud</code> CLI expects, or from GCP's metadata server (if Concourse is deployed on GCP).</p> <p>To export spans the OpenTelemetry Collector via the OTLP Exporter, specify your collector access endpoint:</p> <pre><code>CONCOURSE_TRACING_OTLP_ADDRESS=otel-collector.example.com:4317\nCONCOURSE_TRACING_OTLP_USE_TLS=false\n</code></pre> <p>To export spans to Lightstep via the OTLP Exporter, specify your collector access token and endpoint:</p> <pre><code>CONCOURSE_TRACING_OTLP_ADDRESS=ingest.lightstep.com:443\nCONCOURSE_TRACING_OTLP_HEADERS=lightstep-access-token:mysupersecrettoken\n</code></pre> <p>To export spans to Elastic Observability via the OTLP Exporter, specify your Elastic APM secret token and endpoint:</p> <pre><code>CONCOURSE_TRACING_OTLP_ADDRESS=elastic-apm-server.example.com:443\nCONCOURSE_TRACING_OTLP_HEADERS=Authorization=Bearer your-secret-token\n</code></pre> <p>To export spans to Honeycomb.io, specify the API key, dataset and optionally the service name:</p> <pre><code>CONCOURSE_TRACING_HONEYCOMB_API_KEY=your-honeycomb-api-key\nCONCOURSE_TRACING_HONEYCOMB_DATASET=your-honeycomb-dataset\nCONCOURSE_TRACING_HONEYCOMB_SERVICE_NAME=service-name-for-concourse  # NOTE: Optional. Defaults to \"concourse\"\n</code></pre>"},{"location":"docs/operation/tracing/#trace-context-propagation","title":"Trace context propagation","text":"<p>When tracing is enabled, trace context propagation is activated in pipeline tasks thanks to the injection of the environment variable <code>TRACEPARENT</code> in <code>run</code> commands. The environment variable <code>TRACEPARENT</code> complies with the W3C Trace Context specification.</p>"},{"location":"docs/operation/tracing/#whats-emitted","title":"What's emitted?","text":"<p>Below is a summary of the various operations that Concourse currently traces. They are arranged like a call tree, so that for each operation described, its sub-operations are described indented immediately below.</p> <ul> <li><code>scanner.Run</code> -- An execution of the Resource Checker, responsible for determining which   resources need to be checked.<ul> <li><code>scanner.check</code> -- This operation simply represents inserting the check in the database.</li> </ul> </li> <li><code>scheduler.Run</code> -- This represents one tick of the Build Scheduler.<ul> <li><code>schedule-job</code> -- this is the same operation scoped to a single job.<ul> <li><code>Algorithm.Compute</code> -- this is where the Algorithm determines inputs   for a job. Each of the resolvers below describes a different strategy for determining inputs, depending on the   job's config.<ul> <li><code>individualResolver.Resolve</code> -- This is used to determine versions input to a  <code>get</code> step without <code>passed</code> constraints.</li> <li><code>groupResolver.Resolve</code> -- This is the juicy part of the algorithm, which deals with <code>passed</code> constraints.</li> <li><code>pinnedResolver.Resolve</code> -- This operation is used to determine inputs   when Version Pinning is at play.</li> </ul> </li> <li><code>job.EnsurePendingBuildExists</code> -- This is where a new build, if deemed necessary by scheduling constraints,   will be inserted into the database. This operation follows from checker.Run above and will appear under the   same trace as the check which produced the resource version responsible for triggering the new build.</li> </ul> </li> </ul> </li> <li><code>build</code> -- this is the primary operation performed by the Build Tracker. When a build   is automatically triggered, this span follows from the <code>job.EnsurePendingBuildExists</code> operation which created the   build, appearing in the same trace.<ul> <li><code>get</code> -- this tracks the execution of a <code>get</code> step.</li> <li><code>put</code> -- this tracks the execution of a <code>put</code> step.</li> <li><code>task</code> -- this tracks the execution of a <code>task</code> step.</li> <li><code>set_pipeline</code> -- this tracks the execution of a <code>set_pipeline</code> step.</li> <li><code>load_var</code> -- this tracks the execution of a <code>load_var</code> step.</li> </ul> </li> </ul>"},{"location":"docs/operation/tuning/","title":"Performance Tuning","text":"<p>By default, Concourse is configured to feel very snappy. This is good for when you are first trying out Concourse or using it on a small team with a few dozen pipelines.</p> <p>When you begin trying to scale Concourse is where fires can start breaking out. This section will go over some configuration values in Concourse that you can change to make scaling easier.</p>"},{"location":"docs/operation/tuning/#the-big-caveat","title":"The Big Caveat","text":"<p>Track Metrics! Everything you read next could be all for nothing if you don't have metrics to track where the bottlenecks are in your Concourse system. We highly suggest tracking metrics so you have a clear before and after picture for any changes you make and to clearly see if you're moving things in the right direction.</p>"},{"location":"docs/operation/tuning/#build-logs","title":"Build Logs","text":"<p>Is the size of your database growing dramatically? Can't keep up with the storage costs? Then you should probably configure some default log retention settings.</p> <p>By default, Concourse will not delete any of your logs from your pipelines. You have to opt in to having Concourse automatically delete build logs for you. You can set a time-based retention policy and/or a policy based on the number of logs a job generates.</p>"},{"location":"docs/operation/tuning/#concourse_default_build_logs_to_retain","title":"<code>CONCOURSE_DEFAULT_BUILD_LOGS_TO_RETAIN</code>","text":"<p>Determines how many build logs to retain per job by default. If you set this to <code>10</code> then any jobs in your pipelines that have more than ten builds will have the extra logs for those builds deleted.</p> <p>Users can override this value in their pipelines.</p>"},{"location":"docs/operation/tuning/#concourse_max_build_logs_to_retain","title":"<code>CONCOURSE_MAX_BUILD_LOGS_TO_RETAIN</code>","text":"<p>Determines how many build logs to retain per job. Users cannot override this setting.</p>"},{"location":"docs/operation/tuning/#concourse_default_days_to_retain_build_logs","title":"<code>CONCOURSE_DEFAULT_DAYS_TO_RETAIN_BUILD_LOGS</code>","text":"<p>Determines how old build logs have to be before they are deleted. Setting this to a value like <code>10</code> will result in any build logs older than 10 days to be deleted.</p> <p>Users can override this value in their pipelines.</p>"},{"location":"docs/operation/tuning/#concourse_max_days_to_retain_build_logs","title":"<code>CONCOURSE_MAX_DAYS_TO_RETAIN_BUILD_LOGS</code>","text":"<p>Determines how old build logs have to be before they are deleted. Users cannot override this setting in their pipelines.</p>"},{"location":"docs/operation/tuning/#resource-checking","title":"Resource Checking","text":"<p>By default, Concourse checks any given resource every ~1min. This makes Concourse feel snappy when you first start using it. Once you start trying to scale though the amount of checks can begin to feel aggressive. The following settings can help you reduce the load caused by resource checking.</p>"},{"location":"docs/operation/tuning/#concourse_resource_checking_interval","title":"<code>CONCOURSE_RESOURCE_CHECKING_INTERVAL</code>","text":"<p>This is where the default value for 1min checks comes from. Changing this value changes the default checking interval for all resources. Users can override this value when defining a resource with the  <code>resource.check_every</code> field.</p>"},{"location":"docs/operation/tuning/#concourse_resource_with_webhook_checking_interval","title":"<code>CONCOURSE_RESOURCE_WITH_WEBHOOK_CHECKING_INTERVAL</code>","text":"<p>Same as the previous var but only applies to resources with webhooks. Could use this to disable resource checking of resources that use webhooks by setting it to a large value like <code>99h</code>.</p>"},{"location":"docs/operation/tuning/#concourse_max_checks_per_second","title":"<code>CONCOURSE_MAX_CHECKS_PER_SECOND</code>","text":"<p>Maximum number of checks that can be started per second. This will be calculated as <code>(# of resources)/(resource checking interval)</code>. If you're finding that too many resource checks are running at once and consuming a lot of resources on your workers then you can use this var to reduce the overall load.</p> <p>A value of <code>-1</code> will remove this maximum limit of checks per second.</p>"},{"location":"docs/operation/tuning/#pipeline-management","title":"Pipeline Management","text":"<p>Here are some flags you can set on the web node to help manage the amount of resources pipelines consume. These flags are mostly about ensuring pipelines don't run forever without good reason.</p>"},{"location":"docs/operation/tuning/#concourse_pause_pipelines_after","title":"<code>CONCOURSE_PAUSE_PIPELINES_AFTER</code>","text":"<p>This flag takes a number representing the number of days since a pipeline last ran before it's automatically paused. So specifying <code>90</code> means any pipelines that last ran 91 days ago will be automatically paused.</p> <p>For large instances it can be common for users to set a pipeline and then forget about it. The pipeline may never run another job again and be forgotten forever. Even if the jobs in the pipeline never run Concourse will still be running resource checks for that pipeline, if any resources are defined. By setting this flag you can ensure that any pipelines that meet this criteria will be automatically paused and not consume resources long-term. For some large instances this can mean up to 50% of pipelines eventually being paused.</p>"},{"location":"docs/operation/tuning/#concourse_default_task_cpumemory_limit","title":"<code>CONCOURSE_DEFAULT_TASK_{CPU/MEMORY}_LIMIT</code>","text":"<p>Global defaults for CPU and memory you can set. Only applies to tasks, not resource containers (<code>check/get/put</code> steps). You can read more about how to set these limits on the <code>task</code> step <code>container_limits</code> page.</p> <p>Users can override these values in their pipelines.</p>"},{"location":"docs/operation/tuning/#concourse_default_getputtask_timeout","title":"<code>CONCOURSE_DEFAULT_{GET/PUT/TASK}_TIMEOUT</code>","text":"<p>Global defaults for how long the mentioned step takes to execute. Useful if you're finding your users write pipelines with tasks that get stuck or never end. Ensures that every build eventually finishes.</p> <p>Users can override these values in their pipelines.</p>"},{"location":"docs/operation/tuning/#container-placement","title":"Container Placement","text":"<p>If you find that workers keep crashing due to high CPU and/or memory usage then you could try specifying a custom container placement strategy or strategy chain. The Container Placement page has some examples of container placement strategy chains you can use.</p>"},{"location":"docs/operation/tuning/#garbage-collection","title":"Garbage Collection","text":"<p>When jobs fail or error out in Concourse their resources are not immediately cleaned up. The container and storage space remain on a worker for some period of time before they get garbage collected. If you want to make the garbage collector more aggressive you can change the following settings on your web node:</p>"},{"location":"docs/operation/tuning/#concourse_gc_failed_grace_period","title":"<code>CONCOURSE_GC_FAILED_GRACE_PERIOD</code>","text":"<p>This env var only applies to containers where the job failed and has the longest grace period among all the other GC grace periods. It has a default value of <code>120h</code> (five days).</p> <p>The reason the default value is so long is so users don't feel rushed to investigate their failed job. A job can fail over a weekend and users can investigate the failed jobs containers when they come back on Monday.</p> <p>Failed containers get GC as soon as a new build of the job is kicked off. So you don't have to worry about failed containers always hanging around for five days. They'll only hang around for that long if they're the most recent build of a job.</p> <p>If you notice a lot of containers and volumes hanging around that are tied to failed jobs you can try reducing this setting to fewer days or even a few hours.</p>"},{"location":"docs/operation/tuning/#other-gc-grace-periods","title":"Other GC Grace Periods","text":"<p>Depending on what a container was used for and its exit condition, there are various flags you can adjust to make Concourse GC these resources faster or slower. The following env vars cover the cases where you probably don't need the container hanging around for very long. They have a default value of <code>5m</code>.</p> <ul> <li><code>CONCOURSE_GC_ONE_OFF_GRACE_PERIOD</code> - Period after which one-off build containers will be garbage-collected</li> <li><code>CONCOURSE_GC_MISSING_GRACE_PERIOD</code> - Period after which containers and volumes that were created but went missing   from the worker will be garbage-collected</li> <li><code>CONCOURSE_GC_HIJACK_GRACE_PERIOD</code>- Period after which hijacked containers will be garbage-collected</li> </ul>"},{"location":"docs/operation/tuning/#web-to-worker-ratio","title":"Web To Worker Ratio","text":"<p>This is anecdotal, and you should adjust based on your metrics of your web nodes. A starting ratio of web to workers is 1:6; one web instance for every six workers.</p> <p>The core Concourse team runs two web nodes and 16 workers, a 1:8 ratio. We can get away with this lower web to worker ratio because we don't have that many users actively interacting with the web UI on a daily basis; less than 10 active users. Since we're only one team using the instance we have fewer pipelines than an instance supporting multiple teams would.</p>"},{"location":"docs/operation/creds/","title":"Credential Management","text":"<p>Going beyond Encryption, explicit credential management will provide credentials to your builds for a brief amount of time, without being persisted anywhere. It also allows for credentials to be rotated and managed external to the pipeline or team, and prevents them from being revealed by  <code>fly get-pipeline</code>.</p> <p>Credential management works by replacing the credentials with <code>((vars))</code> in your pipeline or task config. When the Concourse is about to run the step or <code>check</code> that is configured with vars, it will resolve them by fetching the values from the credential manager. If the values are not present, the action will error.</p> <p>The following configurations can be parameterized with a credential manager:</p> <ul> <li>resource.source under pipeline.resources</li> <li>resource_type.source under pipeline.resource_types</li> <li>resource.webhook_token under pipeline.resources</li> <li>task step params on a task step in a pipeline</li> <li>Tasks in their entirety - whether from task step file or task step config in a pipeline, or a config   executed with fly execute</li> </ul> <p>Where these values are looked up and how the credential manager is configured depends on the backend. Consult the relevant section below for whichever backend you want to use.</p> <ul> <li> <p> Vault</p> <p> Configure</p> </li> <li> <p> CredHub</p> <p> Configure</p> </li> <li> <p> AWS SSM</p> <p> Configure</p> </li> <li> <p> AWS Secrets Manager</p> <p> Configure</p> </li> <li> <p> Kubernetes</p> <p> Configure</p> </li> <li> <p> Conjur</p> <p> Configure</p> </li> <li> <p> IDToken</p> <p> Configure</p> </li> </ul>"},{"location":"docs/operation/creds/aws-secrets/","title":"The AWS Secrets Manager credential manager","text":""},{"location":"docs/operation/creds/aws-secrets/#configuration","title":"Configuration","text":"<p>In order to integrate with AWS Secrets Manager for credential management, the web node must be configured with:</p> <ul> <li>an access key and secret key, or a session token</li> <li>the AWS region that your parameters are stored within.</li> </ul> <p>If no access key, secret key, or session token is provided, Concourse will attempt to use environment variables or the instance credentials assigned to the instance.</p> <p>The web node's configuration specifies the following:</p> <code>aws-secretsmanager-access-key</code>: string <p>A valid AWS access key.</p> <p>Environment variable <code>CONCOURSE_AWS_SECRETSMANAGER_ACCESS_KEY</code>.</p> <code>aws-secretsmanager-secret-key</code>: string <p>The secret key that corresponds to the access key defined above.</p> <p>Environment variable <code>CONCOURSE_AWS_SECRETSMANAGER_SECRET_KEY</code>.</p> <code>aws-secretsmanager-session-token</code>: string <p>A valid AWS session token.</p> <p>Environment variable <code>CONCOURSE_AWS_SECRETSMANAGER_SESSION_TOKEN</code>.</p> <code>aws-secretsmanager-region</code>: string <p>The AWS region that requests to Secrets Manager will be sent to.</p> <p>Environment variable <code>CONCOURSE_AWS_SECRETSMANAGER_REGION</code>.</p> <code>aws-secretsmanager-pipeline-secret-template</code>: string <p>The base path used when attempting to locate a pipeline-level secret.</p> <p>Environment variable <code>CONCOURSE_AWS_SECRETSMANAGER_PIPELINE_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>/concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}</code></p> <code>aws-secretsmanager-team-secret-template</code>: string <p>The base path used when attempting to locate a team-level secret.</p> <p>Environment variable <code>CONCOURSE_AWS_SECRETSMANAGER_TEAM_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>/concourse/{{.Team}}/{{.Secret}}</code></p> <p>For example, to launch the ATC and enable Secrets Manager, you may configure:</p> <pre><code>concourse web ... \\\n  --aws-secretsmanager-region us-east-1 \\\n  --aws-secretsmanager-access-key AKIAIOSFODNN7EXAMPLE \\\n  --aws-secretsmanager-secret-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n# or use env variables\nCONCOURSE_AWS_SECRETSMANAGER_REGION=\"us-east-1\" \\\nCONCOURSE_AWS_SECRETSMANAGER_ACCESS_KEY=\"AKIAIOSFODNN7EXAMPLE\" \\\nCONCOURSE_AWS_SECRETSMANAGER_SECRET_KEY=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\nconcourse web ...\n</code></pre> <p>A more secure method is to configure an IAM role for your EC2 ATC instance so that credentials are fetched automatically from the EC2 metadata service.</p>"},{"location":"docs/operation/creds/aws-secrets/#saving-credentials-in-aws","title":"Saving credentials in AWS","text":"<p>It seems to be best to use the 'other type of secret' option and the 'plaintext' entry (otherwise your secrets will be interpolated as JSON) for best results. Make sure your secret locations match the lookup templates exactly; include the leading <code>/</code>, for example.</p>"},{"location":"docs/operation/creds/aws-secrets/#iam-permissions","title":"IAM Permissions","text":"<p>The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the <code>Resource</code> section can contain a wildcard to a secret or be restricted to an individual secret. In order for the health check to work properly (see Scaling), Concourse needs to have access to the <code>__concourse-health-check</code> secret.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowAccessToSecretManagerParameters\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:ListSecrets\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"AllowAccessGetSecret\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:DescribeSecret\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:*:*:secret:/concourse/*\",\n        \"arn:aws:secretsmanager:*:*:secret:__concourse-health-check-??????\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>If you wish to restrict concourse to only have access to secrets for a specific pipeline, you can replace <code>\"arn:aws:secretsmanager:*:*:secret:/concourse/*\"</code> in the example above with:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowAccessToSecretManagerParameters\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:ListSecrets\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"AllowAccessGetSecret\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:DescribeSecret\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:*:*:secret:/concourse/TEAM_NAME/*\",\n        \"arn:aws:secretsmanager:*:*:secret:/concourse/TEAM_NAME/PIPELINE_NAME/*\",\n        \"arn:aws:secretsmanager:*:*:secret:__concourse-health-check-??????\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>where <code>TEAM_NAME</code> and <code>PIPELINE_NAME</code> are replaced with the team and name of the pipeline in question.</p> <p>For more information on how to use IAM roles to restrict access to Secrets Manager, review the official documentation.</p>"},{"location":"docs/operation/creds/aws-secrets/#credential-lookup-rules","title":"Credential Lookup Rules","text":"<p>When resolving a parameter such as <code>((foo_param))</code>, Concourse will look in the following paths, in order:</p> <ul> <li><code>/concourse/TEAM_NAME/PIPELINE_NAME/foo_param</code></li> <li><code>/concourse/TEAM_NAME/foo_param</code></li> </ul> <p>The leading <code>/concourse</code> can be changed by specifying <code>--aws-secretsmanager-pipeline-secret-template</code> or <code>--aws-secretsmanager-team-secret-template</code> variables.</p> <p>Note</p> <p>If Concourse does not have permission to access the pipeline-scoped paths, then credential  lookups will fail even for credentials which are stored at the team level.</p>"},{"location":"docs/operation/creds/aws-secrets/#scaling","title":"Scaling","text":"<p>If your cluster has a large workload, in particular if there are many resources, Concourse can generate a lot of traffic to AWS and subsequently get rate-limited.</p> <p>As long as Concourse has permission to get the value of the <code>__concourse-health-check</code> secret, you should be able to measure an error rate by polling the <code>/api/v1/info/creds</code> endpoint when authenticated as a Concourse Admin.</p> <p>Depending on your workflow for updating secrets and your reliability requirements it may be worth Caching credentials and/or Retrying failed fetches to mitigate rate-limit-related errors.</p>"},{"location":"docs/operation/creds/aws-ssm/","title":"The AWS SSM credential manager","text":""},{"location":"docs/operation/creds/aws-ssm/#configuration","title":"Configuration","text":"<p>The ATC is configured with an access key and secret key or session token and the AWS region that your parameters are stored within. If no access key, secret key, or session token is provided, Concourse will attempt to use environment variables or the instance credentials assigned to the instance.</p> <p>The ATC's configuration specifies the following:</p> <code>aws-ssm-access-key</code>: string <p>A valid AWS access key.</p> <p>Environment variable <code>CONCOURSE_AWS_SSM_ACCESS_KEY</code>.</p> <code>aws-ssm-secret-key</code>: string <p>The secret key that corresponds to the access key defined above.</p> <p>Environment variable <code>CONCOURSE_AWS_SSM_SECRET_KEY</code>.</p> <code>aws-ssm-session-token</code>: string <p>A valid AWS session token.</p> <p>Environment variable <code>CONCOURSE_AWS_SSM_SESSION_TOKEN</code>.</p> <code>aws-ssm-region</code>: string <p>The AWS region that requests to parameter store will be sent to.</p> <p>Environment variable <code>CONCOURSE_AWS_SSM_REGION</code>.</p> <code>aws-ssm-pipeline-secret-template</code>: string <p>The base path used when attempting to locate a pipeline-level secret.</p> <p>Environment variable <code>CONCOURSE_AWS_SSM_PIPELINE_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>/concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}</code></p> <code>aws-ssm-team-secret-template</code>: string <p>The base path used when attempting to locate a team-level secret.</p> <p>Environment variable <code>CONCOURSE_AWS_SSM_TEAM_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>/concourse/{{.Team}}/{{.Secret}}</code></p> <p>For example, to launch the ATC and enable the parameter store, you may configure:</p> <pre><code>concourse web ... \\\n  --aws-ssm-region us-east-1 \\\n  --aws-ssm-access-key AKIAIOSFODNN7EXAMPLE \\\n  --aws-ssm-secret-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n# or use env variables\nCONCOURSE_AWS_SSM_REGION=\"us-east-1\" \\\nCONCOURSE_AWS_SSM_ACCESS_KEY=\"AKIAIOSFODNN7EXAMPLE\" \\\nCONCOURSE_AWS_SSM_SECRET_KEY=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \\\nconcourse web ...\n</code></pre> <p>A more secure method is to configure an IAM role for your EC2 ATC instance so that credentials are fetched automatically from the EC2 metadata service.</p>"},{"location":"docs/operation/creds/aws-ssm/#iam-permissions","title":"IAM Permissions","text":"<p>The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the <code>Resource</code> section can contain a wildcard to a parameter or be restricted to an individual parameter.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowAccessToSsmParameters\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ssm:GetParameter\",\n        \"ssm:GetParametersByPath\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ssm:::parameter/concourse/*\",\n        \"arn:aws:ssm:::parameter/concourse/TEAM_NAME/*\",\n        \"arn:aws:ssm:::parameter/concourse/TEAM_NAME/PIPELINE_NAME/*\"\n      ]\n    },\n    {\n      \"Sid\": \"AllowAccessToDecryptSsmParameters\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:DescribeKey\"\n      ],\n      \"Resource\": \"arn:aws:kms:::key/KMS_KEY_ID\"\n    },\n    {\n      \"Sid\": \"AllowListKeys\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:ListAliases\",\n        \"kms:ListKeys\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Note that the <code>TEAM_NAME</code>, <code>PIPELINE_NAME</code>, and <code>KMS_KEY_ID</code> text above should be replaced to fit your Concourse setup.</p> <p>For more information on how to use IAM roles to restrict access to SSM parameters, review the official documentation.</p>"},{"location":"docs/operation/creds/aws-ssm/#credential-lookup-rules","title":"Credential Lookup Rules","text":"<p>When resolving a parameter such as <code>((foo_param))</code>, Concourse will look in the following paths, in order:</p> <ul> <li><code>/concourse/TEAM_NAME/PIPELINE_NAME/foo_param</code></li> <li><code>/concourse/TEAM_NAME/foo_param</code></li> </ul> <p>The leading <code>/concourse</code> can be changed by specifying <code>--aws-ssm-pipeline-secret-template</code> or <code>--aws-ssm-team-secret-template</code> variables.</p>"},{"location":"docs/operation/creds/caching/","title":"Caching credentials","text":"<p>By default, credentials are fetched each time they're used. When many pipelines are configured this can result in a ton of requests to the credential server.</p> <p>To reduce load on your credential server you may want to enable caching by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_SECRET_CACHE_ENABLED=true\n</code></pre> <p>Enabling secret caching will cache secrets from both credential managers and from var sources.</p> <p>By default, credentials will be cached for one minute at a time. This value can be increased to further reduce load on the server like so:</p> <pre><code>CONCOURSE_SECRET_CACHE_DURATION=5m # increase from 1m default\n</code></pre> <p>Credential cache duration can also be determined by the credential manager itself - for example, if Vault returns a lease duration for a credential, the shorter value between the configured cache duration and the credential's lease duration will be used.</p> <p>By default, the absence of a credential is also cached for 10 seconds so that Concourse doesn't keep looking for a misconfigured credential. This duration can be configured like so:</p> <pre><code>CONCOURSE_SECRET_CACHE_DURATION_NOTFOUND=1s # decrease from 10s default\n</code></pre>"},{"location":"docs/operation/creds/conjur/","title":"The Conjur credential manager","text":""},{"location":"docs/operation/creds/conjur/#configuration","title":"Configuration","text":"<p>Concourse can be configured to pull credentials from a CyberArk Conjur instance.</p> <p>The ATC is configured with a Conjur host username and api key or session token. If no host username, api key, or session token is provided, Concourse will attempt to use environment variables.</p> <p>The ATC's configuration specifies the following:</p> <code>conjur-appliance-url</code>: string <p>URL of the Conjur instance.</p> <p>Environment variable <code>CONCOURSE_CONJUR_APPLIANCE_URL</code>.</p> <code>conjur-account</code>: string <p>The Conjur account.</p> <p>Environment variable <code>CONCOURSE_CONJUR_ACCOUNT</code>.</p> <code>conjur-authn-login</code>: string <p>A valid Conjur host username.</p> <p>Environment variable <code>CONCOURSE_CONJUR_AUTHN_LOGIN</code>.</p> <code>conjur-authn-api-key</code>: string <p>The api key that corresponds to the Conjur host username.</p> <p>Environment variable <code>CONCOURSE_CONJUR_AUTHN_API_KEY</code>.</p> <code>conjur-authn-token-file</code>: string <p>Token file used if Conjur instance is running in k8s or iam.</p> <p>Environment variable <code>CONCOURSE_CONJUR_AUTHN_TOKEN_FILE</code>.</p> <code>conjur-cert-file</code>: string <p>Cert file used if conjur instance is using a self-signed cert.</p> <p>Environment variable <code>CONCOURSE_CONJUR_CERT_FILE</code>.</p> <code>conjur-pipeline-secret-template</code>: string <p>The base path used when attempting to locate a pipeline-level secret.</p> <p>Environment variable <code>CONCOURSE_CONJUR_PIPELINE_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>/concourse/{{.Team}}/{{.Secret}}</code></p> <code>conjur-team-secret-template</code>: string <p>The base path used when attempting to locate a team-level secret.</p> <p>Environment variable <code>CONCOURSE_CONJUR_TEAM_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>/concourse/{{.Team}}/{{.Secret}}</code></p> <code>conjur-secret-template</code>: string <p>The base path used when attempting to locate a vault or safe level secret.</p> <p>Environment variable <code>CONCOURSE_CONJUR_SECRET_TEMPLATE</code>.</p> <p>Example</p> <p>Default: <code>vaultName/{{.Secret}}</code></p> <p>For example, to launch the ATC and enable Conjur, you may configure:</p> <pre><code>concourse web ... \\\n  --conjur-appliance-url https://conjur-master.local \\\n  --conjur-account conjur \\\n  --conjur-authn-login host/concourse/dev \\\n  --conjur-authn-api-key 107eaqz167jkzm2q8wjv4mnyj0z12gfkws9wq9gzsjt29v2sn7yvy\n\n# or use env variables\nCONCOURSE_CONJUR_APPLIANCE_URL=\"https://conjur-master.local\" \\\nCONCOURSE_CONJUR_ACCOUNT=\"conjur\" \\\nCONCOURSE_CONJUR_AUTHN_LOGIN=\"host/concourse/dev\" \\\nCONCOURSE_CONJUR_AUTHN_API_KEY=\"107eaqz167jkzm2q8wjv4mnyj0z12gfkws9wq9gzsjt29v2sn7yvy\" \\\nconcourse web ...\n</code></pre>"},{"location":"docs/operation/creds/conjur/#conjur-permissions","title":"Conjur Permissions","text":"<p>The following is an example Conjur policy that can be used to grant permissions to a Conjur host. In this example <code>host/concourse</code> will have permissions to read and update all the secrets within the <code>TEAM_NAME</code> and <code>PIPELINE_NAME</code> policies.</p> <pre><code>- !host concourse\n- !policy\n  id: concourse\n  owner: !host concourse\n  body:\n    - !policy\n      id: TEAM_NAME\n      body:\n        - !variable team-secret-variable\n        - !policy\n          id: PIPELINE_NAME\n          body:\n            - !variable pipeline-secret-variable\n</code></pre> <p>Note that the <code>TEAM_NAME</code> and <code>PIPELINE_NAME</code> text above should be replaced to fit your Concourse setup.</p> <p>For more information on how to create and load Conjur policies, review the official documentation.</p>"},{"location":"docs/operation/creds/conjur/#credential-lookup-rules","title":"Credential Lookup Rules","text":"<p>When resolving a parameter such as <code>((foo_param))</code>, Concourse will look in the following paths, in order:</p> <ul> <li><code>/concourse/TEAM_NAME/PIPELINE_NAME/foo_param</code></li> <li><code>/concourse/TEAM_NAME/foo_param</code></li> <li><code>vaultName/foo_param</code></li> </ul> <p>The leading <code>/concourse</code> can be changed by specifying <code>--conjur-pipeline-secret-template</code> or <code>--conjur-team-secret-template</code> variables.</p> <p>The leading <code>vaultName</code> can be changed by specifying <code>--conjur-secret-template</code> variable.</p>"},{"location":"docs/operation/creds/credhub/","title":"The CredHub credential manager","text":""},{"location":"docs/operation/creds/credhub/#configuration","title":"Configuration","text":"<p>The ATC is statically configured with a CredHub server URL with TLS and client config.</p> <p>For example, to point the ATC at an internal CredHub server with TLS signed by a local CA, using client id and secret, you may configure:</p> <pre><code>concourse web ... \\\n  --credhub-url https://10.2.0.3:9000 \\\n  --credhub-ca-cert /etc/my-ca.cert \\\n  --credhub-client-id =db02de05-fa39-4855-059b-67221c5c2f63 \\\n  --credhub-client-secret 6a174c20-f6de-a53c-74d2-6018fcceff64\n</code></pre>"},{"location":"docs/operation/creds/credhub/#credential-lookup-rules","title":"Credential Lookup Rules","text":"<p>When resolving a parameter such as <code>((foo_param))</code>, it will look in the following paths, in order:</p> <ul> <li><code>/concourse/TEAM_NAME/PIPELINE_NAME/foo_param</code></li> <li><code>/concourse/TEAM_NAME/foo_param</code></li> </ul> <p>The leading <code>/concourse</code> can be changed by specifying <code>--credhub-path-prefix</code>.</p> <p>CredHub credentials actually have different types, which may contain multiple values. For example, the <code>user</code> type specifies both <code>username</code> and <code>password.</code> You can specify the field to grab via <code>.</code> syntax, e.g. <code>((foo_param.username))</code>.</p> <p>If the action is being run in the context of a pipeline (e.g. a <code>check</code> or a step in a build of a job), the ATC will first look in the pipeline path. If it's not found there, it will look in the team path. This allows credentials to be scoped widely if they're common across many pipelines.</p> <p>If an action is being run in a one-off build, the ATC will only look in the team path.</p>"},{"location":"docs/operation/creds/id-token/","title":"The IDToken credential manager","text":"<p>This idtoken credential manager is a bit special. It doesn't load any credentials from an external source but instead generates JWTs which are signed by concourse and contain information about the pipeline/job that is currently running. It can NOT be used as a cluster-wide credential manager, but must instead be used as a var source.</p> <p>These JWTs can be used to authenticate with external services via \"identity federation\" with the identity of the pipeline.</p> <p>Examples for services that support authentication via JWTs are:</p> <ul> <li>Vault</li> <li>AWS</li> <li>Azure</li> </ul> <p>External services can verify if JWTs are actually issued by your Concourse, by checking the signatures on the JWTs against the public keys published by your Concourse.</p> <p>The public keys for verification are published as JWKS at:</p> <pre><code>https://your-concourse-server.com/.well-known/jwks.json\n</code></pre> <p>Concourse also offers a OIDC Discovery Endpoint, which allows external services to auto-discover the JWKS-URL.</p>"},{"location":"docs/operation/creds/id-token/#usage","title":"Usage","text":"<p>You create a var source of type <code>idtoken</code> with the configuration you want ( see Configuration) in your pipeline. That var source then exposes a single variable with a single field, token, which contains the JWT and can be used in any step of your pipeline.</p> <p>You can also have multiple <code>idtoken</code> var sources in the same pipeline, each with different audiences, lifetimes etc.</p> <pre><code>var_sources:\n  - name: myidtoken\n    type: idtoken\n    config:\n      audience: [ \"sts.amazonaws.com\" ]\n\njobs:\n  - name: print-creds\n    plan:\n      - task: print\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source: { mirror_self: true }\n          run:\n            path: bash\n            args:\n              - -c\n              - |\n                echo myidtoken: ((myidtoken:token))\n</code></pre>"},{"location":"docs/operation/creds/id-token/#configuration","title":"Configuration","text":"<p>You can pass several config options to the <code>idtoken</code> var source to customize the generated JWTs. For example, you can configure the <code>aud</code> claim, token expiration, or granularity of the <code>sub</code> claim. See <code>idtoken</code> var source for all config options.</p>"},{"location":"docs/operation/creds/id-token/#subject-scope","title":"Subject Scope","text":"<p>Some external services (like AWS) only perform exact-matches on a token's sub-claim and ignore most other claims. To enable use-cases like \"all pipelines of a team should be able to assume an AWS-Role\", Concourse offers the option to configure how granular the <code>sub</code> claim's value should be.</p> <p>This is configured via the <code>subject_scope</code> setting of the <code>idtoken</code> var source.</p> <p>Depending on the value of <code>subject_scope</code>, the content of the JWT's <code>sub</code> claim will differ:</p> <code>subject_scope</code> <code>sub</code> Value in JWT <code>team</code> <code>&lt;team_name&gt;</code> <code>pipeline</code> <code>&lt;team_name&gt;/&lt;pipeline_name&gt;</code> <code>instance</code> <code>&lt;team_name&gt;/&lt;pipeline_name&gt;/&lt;instance_vars&gt;</code><sup>1</sup> <code>job</code> <code>&lt;team_name&gt;/&lt;pipeline_name&gt;/&lt;instance_vars&gt;/&lt;job_name&gt;</code><sup>2</sup> <p>This way all your pipelines can simply get a token with <code>subject_scope: team</code> and use this token to assume an AWS-Role that matches on <code>sub: \"your_team_name\"</code>.</p>"},{"location":"docs/operation/creds/id-token/#example-jwt","title":"Example JWT","text":"<p>The generated tokens usually look something like this:</p> <pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJodHRwczovL3lvdXItY29uY291cnNlLmV4YW1wbGUuY29tIiwiZXhwIjoxNzUxMDE1NzM0LCJhdWQiOlsiYXBpOi8vQXp1cmVBRFRva2VuRXhjaGFuZ2UiXSwic3ViIjoibWFpbi9leGFtcGxlLXBpcGVsaW5lIiwidGVhbSI6Im1haW4iLCJwaXBlbGluZSI6ImV4YW1wbGUtcGlwZWxpbmUiLCJqb2IiOiJleGFtcGxlLWpvYiJ9.my7l44tH0wfz8vc6z3fMmzTMxZ8_orhjcsOti3BKSNo\n</code></pre> <p>And after decoding, looks like this:</p> <pre><code>{\n  \"aud\": \"sts.amazonaws.com\",\n  \"exp\": 1751282764,\n  \"iat\": 1751279164,\n  \"iss\": \"https://your-concourse-server.com\",\n  \"job\": \"print-creds\",\n  \"pipeline\": \"mypipeline\",\n  \"sub\": \"main/mypipeline\",\n  \"team\": \"main\"\n}\n</code></pre> <p>Here is a short explanation of the different claims:</p> <ul> <li><code>iss</code>: Who issued the token. Contains the OIDC issuer URL if <code>--oidc-issuer-url</code> is configured, otherwise the external   URL of your Concourse.</li> <li><code>exp</code>: When the token will expire</li> <li><code>aud</code>: Who the token is intended for. (In the above example it's for Azure's Identity Federation API)</li> <li><code>team</code>: The team of the pipeline this token was generated for</li> <li><code>pipeline</code>: The pipeline this token was generated for</li> <li><code>job</code>: The name of the job (inside the pipeline) this token was generated for</li> <li><code>instance_vars</code>: Any instance vars for the pipeline (if it is an instanced pipeline). Will be a comma-separated list   of key-value pairs. e.g. <code>hello:world,my-var:my-value</code></li> <li><code>sub</code>: A combination of team + pipeline + instance_vars + job. Which parts are used here is configurable,   see Subject Scope.</li> </ul>"},{"location":"docs/operation/creds/id-token/#automatic-key-rotation","title":"Automatic Key Rotation","text":"<p>Concourse will automatically rotate the signing keys used for creating the JWTs. The default rotation period is <code>7 days</code>. The previously used keys are being kept around for a while (by default <code>24h</code>) so that verification of currently existing JWTs doesn't fail during key rotation.</p> <p>This behavior can be configured via the following ATC flags:</p> <ul> <li><code>CONCOURSE_SIGNING_KEY_ROTATION_PERIOD</code>: How often to rotate the signing keys. Default: <code>7d</code>. A value of <code>0</code> means   don't rotate at all.</li> <li><code>CONCOURSE_SIGNING_KEY_GRACE_PERIOD</code>: How long to keep previously used signing keys published in the JWKs after they   have been rotated. Default: <code>24h</code>.</li> <li><code>CONCOURSE_SIGNING_KEY_CHECK_INTERVAL</code>: How often to check if new keys are needed or if old ones should be removed.   Default: <code>10m</code></li> </ul>"},{"location":"docs/operation/creds/id-token/#configuring-a-separate-oidc-issuer","title":"Configuring a Separate OIDC Issuer","text":"<p>By default, Concourse uses the <code>--external-url</code>  as the OIDC issuer in generated tokens. You can configure a separate OIDC issuer URL using the <code>--oidc-issuer-url</code> flag:</p> <pre><code>concourse web \\\n    --external-url https://concourse.internal.example.com \\\n    --oidc-issuer-url https://oidc.example.com\n</code></pre> <p>When <code>--oidc-issuer-url</code> is configured:</p> <ul> <li>The <code>iss</code> claim in generated JWT tokens will contain the OIDC issuer URL instead of the external URL</li> <li>The OIDC discovery endpoints (<code>/.well-known/openid-configuration</code> and <code>/.well-known/jwks.json</code>) will return the OIDC   issuer URL</li> <li>Your Concourse web UI and API continue to use the external URL</li> </ul> <p>This is useful for private network deployments where you want to serve OIDC discovery from a separate public endpoint while keeping your Concourse instance private.</p> <p>Warning</p> <p>When the signing keys rotate, Concourse immediately uses the new key for signing tokens. If your OIDC issuer URL is  out of sync with Concourse's JWKS, token verification will fail. The recommended approach is to use a reverse proxy  that forwards requests to your private Concourse in real-time, eliminating sync delays during key rotation.</p>"},{"location":"docs/operation/creds/id-token/#examples","title":"Examples","text":""},{"location":"docs/operation/creds/id-token/#vault","title":"Vault","text":"<p>You can use JWTs to authenticate with HashiCorp Vault. This way your pipelines can directly communicate with Vault and use all of its features, beyond what Concourse's native Vault-integration offers.</p> <p>First enable the JWT auth method in your Vault Server:</p> <pre><code>vault auth enable jwt\n</code></pre> <p>Now configure the JWT auth method to accept JWTs issued by your Concourse (use your <code>--oidc-issuer-url</code> if configured, otherwise your external URL - see Configuring a Separate OIDC Issuer):</p> <pre><code>vault write auth/jwt/config \\\n  oidc_discovery_url=\"https://&lt;external_url_or_oidc_issuer_url&gt;\" \\\n  default_role=\"demo\"\n</code></pre> <p>Lastly, configure a role for JWT auth. Make sure to use the same value in your pipeline that you used for bound_audiences (the best would be the URL of your Vault). bound_subject must be the sub-claim value of your JWT, if you use the subject_scope setting to change the contents of your sub-claim, adapt this accordingly!</p> <pre><code>vault write auth/jwt/role/demo \\\n  role_type=\"jwt\"\\\n  user_claim=\"sub\" \\\n  bound_subject=\"main/your-pipeline\" \\\n  bound_audiences=\"my-vault-server.com\" \\\n  policies=webapps \\\n  ttl=1h\n</code></pre> <p>This role will allow the holder of a JWT with aud: \"<code>my-vault-server.com</code>\" and sub: \"<code>main/your-pipeline</code>\" to get a Vault token with the Vault-policy <code>webapps</code>. If the policy you want to assign has a different name, simply change it in the above example. Make sure to adapt the value for <code>bound_subject</code> according to your team and pipeline name.</p> <p>Pipelines can now do the following:</p> <pre><code>var_sources:\n  - name: vaulttoken\n    type: idtoken\n    config:\n      audience: [ \"my-vault-server.com\" ]\n\njobs:\n  - name: vault-login\n    plan:\n      - task: login\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: hashicorp/vault }\n          run:\n            path: sh\n            args:\n              - -e\n              - -c\n              - |\n                export VAULT_ADDR=https://my-vault-server.com\n                vault write auth/jwt/login \\\n                  role=demo \\\n                  jwt=((vaulttoken:token)) \\\n                  --format=json &gt; vault-response.json\n                echo \"Now do something with the token in vault-response.json\"\n</code></pre> <p>You don't have to create a role and a policy for every single of your pipelines! You can use claims from the JWT with Vault's policy templating feature. This way you can define a policy that allows a pipeline read to all the secrets it would usually have access to using Concourse's native Vault-integration:</p> <pre><code>path \"concourse/metadata/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.team }}\" {\n  capabilities = [\"list\"]\n}\n\npath \"concourse/data/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.team }}/+\" {\n  capabilities = [\"read\"]\n}\n\npath \"concourse/metadata/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.team }}/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.pipeline }}\" {\n  capabilities = [\"list\"]\n}\n\npath \"concourse/metadata/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.team }}/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.pipeline }}/*\" {\n  capabilities = [\"read\", \"list\"]\n}\n\npath \"concourse/data/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.team }}/{{ identity.entity.aliases.&lt;JWT_ACCESSOR&gt;.metadata.pipeline }}/*\" {\n  capabilities = [\"read\", \"list\"]\n}\n</code></pre> <p>Note</p> <p>Make sure to set <code>&lt;JWT_ACCESSOR&gt;</code> to the actual mount-accessor value of your JWT Auth method! You can use <code>vault  auth list --format=json | jq -r '.\"jwt/\".accessor'</code> to get the accessor for your jwt auth method.</p> <p>With a policy like this you don't need to configure <code>bound_subject</code> in your JWT auth role. Every single pipeline can simply use the same role and the policy will take care that they can only access secrets meant for them. However, you need to explicitly configure claim to metadata mapping:</p> <pre><code>vault write auth/jwt/role/demo \\\n  role_type=\"jwt\"\\\n  user_claim=\"sub\" \\\n  bound_subject= \\\n  bound_audiences=\"my-vault-server.com\" \\\n  policies=pipeline-new \\\n  claim_mappings='team=team' \\\n  claim_mappings='pipeline=pipeline' \\\n  ttl=1h\n</code></pre>"},{"location":"docs/operation/creds/id-token/#aws","title":"AWS","text":"<p>AWS supports federation with external identity providers. Using this, you can allow identities managed by an external identity provider to perform actions in your AWS account.</p> <p>In this scenario the external identity provider is Concourse and the identities are teams/pipelines/jobs. This means you are able to grant a specific pipeline or job permission to perform actions in AWS (like deploying something), all without managing IAM users or dealing with long-lived credentials.</p> <p>First you need to create an OpenID Connect identity provider in your AWS Account. Set Provider URL to the external URL of your Concourse server  (or the <code>--oidc-issuer-url</code> if you're using a separate OIDC issuer - see Configuring a Separate OIDC Issuer). For Audience, you can choose any string you like, but using a value like <code>sts.amazonaws.com</code> is recommended. You have to use the same string later in the configuration of your <code>idtoken</code> var source.</p> <p>Next you will need to create an IAM-Role that can be assumed using your JWT. Set Identity Provider to the value you previously set Audience to. Add a condition on the sub-claim with type <code>StringEquals</code> and value <code>yourteam/yourpipeline</code>. This will allow ONLY that specific pipeline (and any instanced versions of it) to assume that IAM Role using a JWT. If you use the <code>subject_scope</code> setting to change the contents of your sub-claim, adapt this condition accordingly! In the next step you will be able to choose which AWS permissions your role will get.</p> <p>Now you can use the AWS AssumeRoleWithWebIdentity API operation to assume your role via a JWT issued by Concourse. The easiest way is to do this is via the assume-role-with-web-identity AWS CLI command:</p> <pre><code>var_sources:\n  - name: awstoken\n    type: idtoken\n    config:\n      audience: [ \"sts.amazonaws.com\" ]\n\njobs:\n  - name: aws-login\n    plan:\n      - task: print\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: amazon/aws-cli }\n          run:\n            path: bash\n            args:\n              - -e\n              - -c\n              - |\n                aws sts assume-role-with-web-identity \\\n                  --role-session-name Concourse \\\n                  --role-arn arn:aws:iam::&lt;your_account&gt;:role/&lt;your_role&gt; \\\n                  --web-identity-token ((awstoken:token)) &gt; creds.json\n                echo \"Now do something with the temporary credentials in creds.json\"\n</code></pre>"},{"location":"docs/operation/creds/id-token/#azure","title":"Azure","text":"<p>Azure also supports a way to grant the holder of a JWT permissions in the Cloud. This is done via a feature called Federated Credentials.</p> <p>First, create an EntraID App Registration. This app registration will be the service principal used by your pipeline.</p> <p>Now create a federated credential for the app registration you just created.</p> <p>For Scenario select \"Other\". For Issuer set it to the external URL of your Concourse server (or the <code>--oidc-issuer-url</code> if you're using a separate OIDC issuer - see Configuring a Separate OIDC Issuer). For Type select \"Explicit subject identifier\" and set Value to <code>&lt;teamname&gt;/&lt;pipelinename&gt;</code> of the pipeline that should be able to use the identity. If you use the <code>subject_scope</code> setting to change the contents of your sub-claim, change this setting here accordingly.</p> <p>You can now assign IAM permissions to the identity of the app registration, which define what the identity is allowed to do in your Azure subscription.</p> <p>Your pipeline can now use the <code>az cli</code> to log in to Azure using a JWT generated by Concourse:</p> <pre><code>var_sources:\n  - name: azuretoken\n    type: idtoken\n    config:\n      audience: [ \"api://AzureADTokenExchange\" ]\n\njobs:\n  - name: azure-deploy\n    plan:\n      - task: login\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: mcr.microsoft.com/azure-cli }\n          run:\n            path: bash\n            args:\n              - -e\n              - -c\n              - |\n                echo ((azuretoken:token))\n                az login --service-principal \\\n                  -u &lt;client_id of your app registration&gt; \\\n                  --tenant &lt;tenant_id of your app registration&gt; \\\n                  --federated-token ((azuretoken:token))\n                echo \"You are now authenticated with Azure. Do something with it!\"\n</code></pre> <ol> <li> <p>Instance vars are rendered as comma-separated key-value pairs. e.g. <code>my-var:my-value,hello:world</code> \u21a9</p> </li> <li> <p>If a path element is empty (for example because you chose <code>job</code> on a pipeline with no instance-vars), the empty  element is still added. e.g. <code>my-team/my-pipeline//my-job</code>. Note the double forward-slashes between the pipeline and job name, where instance vars would go.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/operation/creds/kubernetes/","title":"Kubernetes Credential Manager","text":"<p>Concourse can be configured to pull credentials from Kubernetes <code>secret</code> objects.</p> <p>To configure it, either enable the in-cluster client by setting the following environment variable on the <code>web</code> node:</p> <pre><code>CONCOURSE_KUBERNETES_IN_CLUSTER=true\n</code></pre> <p>or set the path to a <code>kubeconfig</code> file:</p> <pre><code>CONCOURSE_KUBERNETES_CONFIG_PATH=~/.kube/config\n</code></pre>"},{"location":"docs/operation/creds/kubernetes/#credential-lookup-rules","title":"Credential lookup rules","text":"<p>When resolving a parameter such as <code>((foo))</code>, Concourse will look for it in the following order in the namespace configured for that team:</p> <ol> <li> <pre><code>Name:         PIPELINE_NAME.foo\nNamespace:    concourse-TEAM_NAME\nType:         Opaque\n\nData\n====\nvalue:        32 bytes\n</code></pre> </li> <li> <pre><code>Name:         foo\nNamespace:    concourse-TEAM_NAME\nType:         Opaque\n\nData\n====\nvalue:        32 bytes\n</code></pre> </li> </ol> <p>You can also have nested fields if the contents of the secret is JSON, which can be accessed using <code>.</code> syntax (e.g. <code>((foo.bar))</code>).</p> <p>The prefix prepended to the namespace used by Concourse to search for secrets (in the examples above, <code>concourse-</code>) can be changed by configuring the following in the web node:</p> <pre><code>CONCOURSE_KUBERNETES_NAMESPACE_PREFIX=some-other-prefix-\n</code></pre> <p>If an action is being run in a one-off build, Concourse will not include the pipeline name in the secret that it looks for.</p>"},{"location":"docs/operation/creds/kubernetes/#configuring-kubernetes-rbac","title":"Configuring Kubernetes RBAC","text":"<p>As the Web nodes need to retrieve secrets from namespaces that are not their own, they needs extra permissions to do so.</p> <p>If you have k8's RBAC enabled, that means creating the necessary Kubernetes objects to identify the Web nodes and give them access to a predefined list of namespaces where the secrets live.</p> <p>Regardless of how the Kubernetes RBAC-related objects are created, the basic requirement is that <code>web</code> must be able to read secrets in the namespaces where each teams' secrets live.</p> <p>For instance, if you have the following teams which you want to read secrets from:</p> <ul> <li>team-a</li> <li>team-b</li> </ul> <p>Assuming the following web node configuration:</p> <pre><code>CONCOURSE_KUBERNETES_NAMESPACE_PREFIX=myprefix-\n</code></pre> <p>The web node must be able to get secrets from the following namespaces:</p> <ul> <li><code>myprefix-team-a</code></li> <li><code>myprefix-team-b</code></li> </ul> <p>To allow the web node to interpolate credentials for \"team-a\" and \"team-b\", we'd then need to create a few Kubernetes RBAC objects.</p> <p>Starting with identifying the <code>web</code> service as an actor, we can use a ServiceAccount for that:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: web\n  labels:\n    app: web\n</code></pre> <p>To allow actors to do something, in this case, retrieve secrets from a given namespace, a ClusterRole is then needed.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: read-secrets\n  labels:\n    app: web\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\"]\n</code></pre> <p>As that role is useless if not bound to an actor, the next step is creating the the object that represents binding the role to the <code>web</code>'s <code>ServiceAccount</code> that we created before.</p> <p>This is accomplished through the RoleBinding object, which is per-namespace (thus, per-team).</p> <p>Note</p> <p>Even though in this example we're binding to a <code>ClusterRole</code> (which is not tied to any namespace), the use of such cluster role is (see <code>metadata.namespace</code>), making the effective permissions restricted to the namespace applied in the <code>RoleBinding</code>.</p> <pre><code>---\n# Role binding for the first team (`team-b`), allowing `web`\n# to consume secrets from it.\n#\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: web-team-a\n  namespace: myprefix-team-a\n  labels:\n    app: web\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: read-secrets\nsubjects:\n- kind: ServiceAccount\n  name: web\n  namespace: concourse\n\n---\n# Role binding for the second team (`team-b`), allowing `web`\n# to consume secrets from it.\n#\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: web-team-b\n  namespace: myprefix-team-b\n  labels:\n    app: web\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: read-secrets\nsubjects:\n- kind: ServiceAccount\n  name: web\n  namespace: concourse\n</code></pre> <p>To finish the example, we need to associate the <code>web</code> Pod with the service, granting the pod access to those namespaces through the roles that have been bound to it.</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      serviceAccountName: web\n      containers:\n        - name: web\n          image: \"concourse/concourse:latest\"\n          args: [ web ]\n          env:\n            - name: CONCOURSE_KUBERNETES_NAMESPACE_PREFIX\n              value: \"myprefix-\"\n          # ...\n</code></pre>"},{"location":"docs/operation/creds/redacting/","title":"Redacting credentials","text":"<p>Concourse will automatically try to redact credentials from build output.</p> <p>Warning</p> <p>Prior to v8, secret redaction was not enabled by default. To enable it on older versions of Concourse set the following on the web node: </p><pre><code>CONCOURSE_ENABLE_REDACT_SECRETS=true\n</code></pre><p></p> <p>Concourse will keep track of the credential values which were used in a build. These can be secrets referenced in your pipeline or those coming from the <code>load_var</code> step.</p> <p>When writing build logs to the database, it will replace any occurrence of these values with the text <code>((redacted))</code>.</p> <p>Say you're running a task which runs the following script:</p> <pre><code>set -e -u -x\n\necho $SECRET &gt; some-file\nsha1sum some-file\n</code></pre> <p>Note</p> <p>The <code>set -x</code> is the root cause of many accidental credential leaks.</p> <p>Let's say you have a job which runs this task, providing the <code>$SECRET</code> parameter using a credential manager <code>((var))</code>:</p> <pre><code>plan:\n  - task: use-secret\n    file: # ...\n    params:\n      SECRET: ((some-var))\n</code></pre> <p>With <code>hello</code> in <code>some-var</code>, this will result in the following build output:</p> <pre><code>+ echo ((redacted))\n+ sha1sum some-file\nf572d396fae9206628714fb2ce00f72e94f2258f  some-file\n</code></pre> <p>Going a step further, what happens when that value of your secret has multiple lines of output, like <code>\"hello\\ngoodbye\"</code>?</p> <pre><code>plan:\n  - task: use-secret\n    file: # ...\n    params:\n      SECRET: ((some-var)) # -&gt; now equals \"hello\\ngoodbye\"\n</code></pre> <pre><code>+ echo ((redacted)) ((redacted))\n+ sha1sum some-file\n638e5ebcd06a5208906960aa5fbe1d4ebd022771  some-file\n</code></pre> <p>What happened here? Well, because we didn't quote the <code>$SECRET</code> var arg to <code>echo</code>, it squashed the lines together into arguments. This could have confused our redacting logic and resulted in leaking the credential, but because Concourse redacts secret values line-by-line, we're still OK. This will also help with JSON marshalled credential values, which get interspersed with <code>\\n</code> in a string literal.</p> <p>Although Concourse tries to be thorough in its redacting of credentials, the best way to prevent credential leakage is to not accidentally print them in the first place. Think of this as an airbag, not a seatbelt.</p>"},{"location":"docs/operation/creds/retrying-failed/","title":"Retrying failed fetches","text":"<p>When a request to the credential manager fails due to an intermittent error (e.g. a timeout or <code>connection refused</code>), Concourse will automatically try the request again up to 5 times before giving up. After all attempts fail, the error will be surfaced in the UI for the resource check or build step that initiated the request.</p> <p>The retry logic can be configured by specifying the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_SECRET_RETRY_ATTEMPTS=5   # how many times to try\nCONCOURSE_SECRET_RETRY_INTERVAL=10s # how long to wait between attempts\n</code></pre> <p>Vault Credential Manager</p> <p>As Vault API client already does retry which has covered the retry conditions of this general secret fetching retry, if a deployment uses Vault credential manager, <code>CONCOURSE_SECRET_RETRY_ATTEMPTS</code> can be set to 0 (or a small value,  like 1 or 2) in order to avoid duplicate retries.</p>"},{"location":"docs/operation/creds/vault/","title":"The Vault credential manager","text":"<p>Concourse can be configured to pull credentials from a Vault instance.</p> <p>To configure this, first configure the URL of your Vault server by setting the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_VAULT_URL=https://vault.example.com:8200\n</code></pre> <p>You may also need to configure the CA cert for Vault:</p> <pre><code>CONCOURSE_VAULT_CA_CERT=path/to/ca.crt\n</code></pre> <p>You'll also need to configure how the <code>web</code> node authenticates with Vault - see Authenticating with Vault for more details as that step is quite involved.</p>"},{"location":"docs/operation/creds/vault/#credential-lookup-rules","title":"Credential lookup rules","text":"<p>Vault lets you organize secrets into hierarchies, which is useful for when they should be accessible for particular pipelines or teams. When you have a parameter like <code>((foo))</code> in a pipeline definition, Concourse will (by default) look for it in the following paths, in order:</p> <ul> <li><code>/concourse/TEAM_NAME/PIPELINE_NAME/foo</code></li> <li><code>/concourse/TEAM_NAME/foo</code></li> </ul> <p>Vault credentials are actually key-value, so for <code>((foo))</code> Concourse will default to the field name value. You can specify the field to grab via <code>.</code> syntax, e.g. <code>((foo.bar))</code>.</p> <p>If you have multiple, intermediate levels in your path, you can use the <code>/</code> separator to reach your intended field, e.g. <code>((foo/bar/baz.qux))</code>.</p> <p>When executing a one-off task, there is no pipeline: so in this case, only the team path <code>/concourse/TEAM_NAME/foo</code> is searched.</p> <p>There are several ways to customize the lookup logic:</p> <ol> <li>Add a \"shared path\", for secrets common to all teams.</li> <li>Change the team- and pipeline-dependent path templates.</li> <li>Change the path prefix from <code>/concourse</code> to something else.</li> <li>Set a Vault namespace for isolation within a Vault    Enterprise installation.</li> </ol> <p>Each of these can be controlled by Concourse command line flags, or environment variables.</p>"},{"location":"docs/operation/creds/vault/#configuring-a-shared-path","title":"Configuring a shared path","text":"<p>A \"shared path\" can also be configured for credentials that you would like to share across all teams and pipelines, foregoing the default team/pipeline namespacing. Use with care!</p> <pre><code>CONCOURSE_VAULT_SHARED_PATH=some-shared-path\n</code></pre> <p>This path must exist under the configured path prefix. The above configuration would correspond to <code>/concourse/some-shared-path</code> with the default <code>/concourse</code> prefix.</p>"},{"location":"docs/operation/creds/vault/#changing-the-path-templates","title":"Changing the path templates","text":"<p>You can choose your own list of templates, which will expand to team- or pipeline-specific paths. These are subject to the path prefix. By default, the templates used are:</p> <pre><code>CONCOURSE_VAULT_LOOKUP_TEMPLATES=/{{.Team}}/{{.Pipeline}}/{{.Secret}},/{{.Team}}/{{.Secret}}\n</code></pre> <p>When secrets are to be looked up, these are evaluated subject to the configured path prefix, where <code>{{.Team}}</code> expands to the current team, <code>{{.Pipeline}}</code> to the current pipeline (if any), and <code>{{.Secret}}</code> to the name of the secret. So if the settings are:</p> <pre><code>CONCOURSE_VAULT_PATH_PREFIX=/secrets\nCONCOURSE_VAULT_LOOKUP_TEMPLATES=/{{.Team}}/concourse/{{.Pipeline}}/{{.Secret}},/{{.Team}}/concourse/{{.Secret}},/common/{{.Secret}}\n</code></pre> <p>and <code>((password))</code> is used in team <code>myteam</code> and pipeline <code>mypipeline</code>, Concourse will look for the following, in order:</p> <ol> <li><code>/secrets/myteam/concourse/mypipeline/password</code></li> <li><code>/secrets/myteam/concourse/password</code></li> <li><code>/secrets/common/password</code></li> </ol>"},{"location":"docs/operation/creds/vault/#changing-the-path-prefix","title":"Changing the path prefix","text":"<p>The leading <code>/concourse</code> can be changed by specifying the following:</p> <pre><code>CONCOURSE_VAULT_PATH_PREFIX=/some-other-prefix\n</code></pre>"},{"location":"docs/operation/creds/vault/#using-a-vault-namespace","title":"Using a Vault namespace","text":"<p>If you are using Vault Enterprise, you can make secret lookups and authentication happen under a namespace.</p> <pre><code>CONCOURSE_VAULT_NAMESPACE=chosen/namespace/path\n</code></pre> <p>This setting applies to all teams equally.</p>"},{"location":"docs/operation/creds/vault/#configuring-the-secrets-engine","title":"Configuring the secrets engine","text":"<p>Concourse is currently limited to looking under a single path, meaning enabling only one secrets engine is supported: <code>kv</code>, or <code>kv_v2</code>. This may change in the future - we're still collecting ideas in RFC #21.</p> <p>Using kv version 2 enables versioned secrets and the ability to restore previous versions or deleted secrets. Concourse will read the latest version of a secret at all times and if it is deleted it will appear as if the secret does not exist. More information regarding the Vault KV backend and the differences in versions can be found here.</p> <p>So, let's configure the kv secrets engine and mount it at <code>/concourse</code>:</p> <pre><code>vault secrets enable -version=1 -path=concourse kv\n</code></pre> <p>To enable kv_v2 and versioned secrets:</p> <pre><code>vault secrets enable -version=2 -path=concourse kv\n</code></pre> <p>Next, you'll want to create a policy to allow Concourse to read from this path.</p> <pre><code>path \"concourse/*\" {\n  capabilities = [\"read\"]\n}\n</code></pre> <p>Save this to <code>concourse-policy.hcl</code>, and then run:</p> <pre><code>vault policy write concourse ./concourse-policy.hcl\n</code></pre> <p>This configuration will allow Concourse to read all credentials under <code>/concourse</code>. This should match your configured path prefix.</p>"},{"location":"docs/operation/creds/vault/#authenticating-with-vault","title":"Authenticating with Vault","text":"<p>There are many ways to authenticate with a Vault server. The <code>web</code> node can be configured with either a token or an arbitrary auth backend and arbitrary auth params, so just about all of them should be configurable.</p> <p>When the <code>web</code> node acquires a token, either by logging in with an auth backend or by being given one directly, it will continuously renew the token to ensure it doesn't expire. The renewal interval is half of the token's lease duration.</p>"},{"location":"docs/operation/creds/vault/#using-a-periodic-token","title":"Using a periodic token","text":"<p>The simplest way to authenticate is by generating a periodic token:</p> <pre><code>$ vault token create --policy concourse --period 1h\nKey                Value\n---                -----\ntoken              s.mSNnbhGAqxK2ZbMasOQ91rIA\ntoken_accessor     0qsib5YcYvROm86cT08IFxIT\ntoken_duration     1h\ntoken_renewable    true\ntoken_policies     [concourse default]\n</code></pre> <p>Warning</p> <p>Choose your <code>--period</code> wisely, as the timer starts counting down as soon as the token is created. You should also   use a duration long enough to account for any planned <code>web</code> node downtime.</p> <p>Once you have the token, just set the following env on the <code>web</code> node:</p> <pre><code>CONCOURSE_VAULT_CLIENT_TOKEN=s.mSNnbhGAqxK2ZbMasOQ91rIA\n</code></pre> <p>Periodic tokens are the quickest way to get started, but they have one fatal flaw: if the <code>web</code> node is down for longer than the token's configured period, the token will expire and a new one will have to be created and configured. This can be avoided by using the <code>approle</code> auth backend.</p>"},{"location":"docs/operation/creds/vault/#using-the-userpass-auth-backend","title":"Using the <code>userpass</code> auth backend","text":"<p>The <code>userpass</code> backend allows for users (in this case, Concourse) to authenticate with a user pre-configured in Vault.</p> <p>With this backend, the <code>web</code> node is configured with a <code>username</code> corresponding to a pre-configured user, and a <code>password</code> which is used to authenticate and acquire a token.</p> <p>The <code>userpass</code> backend must first be configured in Vault. Vault's <code>userpass</code> backend allows for a few parameters which you may want to set to determine the permissions and lifecycle of its issued tokens:</p> <code>policies=names</code> <p>This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.</p> <code>token_ttl=duration</code> <p>This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.</p> <code>token_max_ttl=duration</code> <p>This sets a maximum lifetime for each token, after which the token can no longer be renewed.</p> <p>If configured, be sure to set the same value on the <code>web</code> node so that it can re-auth before this duration is reached: </p><pre><code>CONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n</code></pre><p></p> <code>period=duration</code> <p>If configured, tokens issued will be periodic . Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both <code>period</code> and <code>token_max_ttl</code> as the max TTL will be ignored.</p> <code>token_num_uses=count</code> <p>This sets a limit on how often a token can be used. We do not recommend setting this value, as it will effectively hamstring Concourse after a few credential acquisitions. The <code>web</code> node does not currently know to re-acquire a token when this limit is reached.</p> <p>For a full list of options refer to userpass api docs. </p><pre><code>$ vault auth enable userpass\nSuccess! Enabled userpass auth method at: userpass/\n$ vault write auth/userpass/users/concourse policies=concourse period=1h password=&lt;....&gt;\nSuccess! Data written to: auth/userpass/users/concourse\n</code></pre><p></p> <p>Now that the backend is configured, we can use the <code>username</code> and <code>password</code>:</p> <p>These should then be set on the <code>web</code> node like so:</p> <pre><code>CONCOURSE_VAULT_AUTH_BACKEND=\"userpass\"\nCONCOURSE_VAULT_AUTH_PARAM=\"username:concourse,password:&lt;....&gt;\"\n</code></pre>"},{"location":"docs/operation/creds/vault/#using-the-approle-auth-backend","title":"Using the <code>approle</code> auth backend","text":"<p>The <code>approle</code> backend allows for an app (in this case, Concourse) to authenticate with a role pre-configured in Vault.</p> <p>With this backend, the <code>web</code> node is configured with a <code>role_id</code> corresponding to a pre-configured role, and a <code>secret_id</code> which is used to authenticate and acquire a token.</p> <p>The <code>approle</code> backend must first be configured in Vault. Vault's <code>approle</code> backend allows for a few parameters which you may want to set to determine the permissions and lifecycle of its issued tokens:</p> <code>policies=names</code> <p>This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.</p> <code>token_ttl=duration</code> <p>This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.</p> <code>token_max_ttl=duration</code> <p>This sets a maximum lifetime for each token, after which the token can no longer be renewed.</p> <p>If configured, be sure to set the same value on the <code>web</code> node so that it can re-auth before this duration is reached: </p><pre><code>CONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n</code></pre><p></p> <code>period=duration</code> <p>If configured, tokens issued will be periodic . Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both <code>period</code> and <code>token_max_ttl</code> as the max TTL will be ignored.</p> <code>token_num_uses=count</code> <p>This sets a limit on how often a token can be used. We do not recommend setting this value, as it will effectively hamstring Concourse after a few credential acquisitions. The <code>web</code> node does not currently know to re-acquire a token when this limit is reached.</p> <code>secret_id_ttl=duration</code> and <code>secret_id_num_uses=count</code> <p>These two configurations will result in the secret ID expiring after the configured time or configured number of log-ins, respectively.</p> <p>You should only set these if you have something periodically re-generating secret IDs and re-configuring your <code>web</code> nodes accordingly.</p> <p>Given all that, a typical configuration may look something like this:</p> <pre><code>$ vault auth enable approle\nSuccess! Enabled approle auth method at: approle/\n$ vault write auth/approle/role/concourse policies=concourse period=1h\nSuccess! Data written to: auth/approle/role/concourse\n</code></pre> <p>Now that the backend is configured, we'll need to obtain the <code>role_id</code> and generate a <code>secret_id</code>:</p> <pre><code>$ vault read auth/approle/role/concourse/role-id\nKey        Value\n---        -----\nrole_id    5f3420cd-3c66-2eff-8bcc-0e8e258a7d18\n$ vault write -f auth/approle/role/concourse/secret-id\nKey                   Value\n---                   -----\nsecret_id             f7ec2ac8-ad07-026a-3e1c-4c9781423155\nsecret_id_accessor    1bd17fc6-dae1-0c82-d325-3b8f9b5654ee\n</code></pre> <p>These should then be set on the <code>web</code> node like so:</p> <pre><code>CONCOURSE_VAULT_AUTH_BACKEND=\"approle\"\nCONCOURSE_VAULT_AUTH_PARAM=\"role_id:5f3420cd-3c66-2eff-8bcc-0e8e258a7d18,secret_id:f7ec2ac8-ad07-026a-3e1c-4c9781423155\"\n</code></pre>"},{"location":"docs/operation/creds/vault/#using-the-cert-auth-backend","title":"Using the <code>cert</code> auth backend","text":"<p>The <code>cert</code> auth method allows authentication using SSL/TLS client certificates.</p> <p>With this backend, the <code>web</code> node is configured with a client cert and a client key. Vault must be configured with TLS, which you should be almost certainly be doing anyway.</p> <p>The <code>cert</code> backend must first be configured in Vault. Vault's <code>cert</code> backend allows for a few parameters which you may want to set to determine the lifecycle of its issued tokens:</p> <code>policies=names</code> <p>This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.</p> <code>ttl=duration</code> <p>This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.</p> <code>max_ttl=duration</code> <p>This sets a maximum lifetime for each token, after which the token can no longer be renewed.</p> <p>If configured, be sure to set the same value on the <code>web</code> node so that it can re-auth before this duration is reached: </p><pre><code>CONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n</code></pre><p></p> <code>period=duration</code> If configured, tokens issued will be  periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both <code>period</code> and <code>max_ttl</code> as the max TTL will be ignored. <pre><code>$ vault auth enable cert\nSuccess! Enabled cert auth method at: cert/\n$ vault write auth/cert/certs/concourse policies=concourse certificate=@out/vault-ca.crt ttl=1h\nSuccess! Data written to: auth/cert/certs/concourse\n</code></pre> <p>Once that's all set up, you'll just need to configure the client cert and key on the <code>web</code> node like so:</p> <pre><code>CONCOURSE_VAULT_AUTH_BACKEND=\"cert\"\nCONCOURSE_VAULT_CLIENT_CERT=vault-certs/concourse.crt\nCONCOURSE_VAULT_CLIENT_KEY=vault-certs/concourse.key\n</code></pre> <p>In this case no additional auth params are necessary, as the Vault's TLS auth backend will check the certificate against all roles if no name is specified.</p>"},{"location":"docs/pipelines/","title":"Pipelines","text":"<p>A pipeline is the result of configuring Jobs and Resources together. When you configure a pipeline, it takes on a life of its own, to continuously detect resource versions and automatically queue new builds for jobs as they have new available inputs.</p> <p>The name of a pipeline has a few restrictions that are outlined here:  <code>identifier</code> schema.</p> <p>Pipelines are configured via <code>fly set-pipeline</code> or the  <code>set_pipeline</code> step as declarative YAML files which  conform to the following schema:</p>"},{"location":"docs/pipelines/#pipeline-schema","title":"<code>pipeline</code> schema","text":"<code>jobs</code>: <code>[job]</code> (required) <code>resources</code>: <code>[resource]</code> <code>resource_types</code>: <code>[resource_type]</code> <code>var_sources</code>: <code>[var_source]</code> <code>groups</code>: <code>[group]</code> <p>A list of job groups to use for organizing jobs in the web UI.</p> <p>Groups have no functional effect on your pipeline. They are purely for making it easier to grok large pipelines  in the web UI.</p> <p>Note</p> <p>Once you have added groups to your pipeline, all jobs must be in a group.</p> Grouping Jobs <p>The following example will make the \"tests\" group the default view (since it's listed first), separating the later jobs into a \"publish\" group:</p> <pre><code>groups:\n  - name: test\n    jobs:\n      - unit\n      - integration\n  - name: publish\n    jobs:\n      - deploy\n      - shipit\n</code></pre> <p>This would display two tabs at the top of the home page: \"test\" and \"publish\".</p> <p>For a real world example of how groups can be used to simplify navigation and provide logical grouping,  see the groups used at the top of the page in the Concourse pipeline.</p> <code>display</code>: <code>display_config</code> <p>Experimental Feature</p> <p>Display was introduced in Concourse v6.6.0. It is considered an experimental feature.</p> <p>Visual configurations for personalizing your pipeline.</p> Background image <p>The following example will display an image in the background of the pipeline it is configured on.</p> <pre><code>display:\n  background_image: https://avatars1.githubusercontent.com/u/7809479?s=400&amp;v=4\n</code></pre>"},{"location":"docs/pipelines/#jobs","title":"<code>jobs</code>","text":"<p>A set of jobs for the pipeline to continuously schedule. At least one job is required for  a pipeline to be valid.</p>"},{"location":"docs/pipelines/#resources","title":"<code>resources</code>","text":"<p>A set of resources for the pipeline to continuously check.</p>"},{"location":"docs/pipelines/#resource_types","title":"<code>resource_types</code>","text":"<p>A set of resource types for resources within the pipeline to use.</p>"},{"location":"docs/pipelines/#var_sources","title":"<code>var_sources</code>","text":"<p>A set of Var sources for the pipeline to use.</p>"},{"location":"docs/pipelines/#group_config-schema","title":"<code>group_config</code> schema","text":"name: <code>identifier</code> <p>A unique name for the group. This should be short and simple as it will be used as the tab name for navigation.</p> jobs: [<code>job.name</code>] <p>A list of jobs that should appear in this group. A job may appear in multiple groups. Neighbours of jobs in the  current group will also appear on the same page in order to give context of the location of the group in  the pipeline.</p> <p>You may also use any valid glob to represent several  jobs, e.g.:</p> <pre><code>groups:\n  - name: develop\n    jobs:\n      - terraform-*\n      - test\n      - deploy-{dev,staging}\n  - name: ship\n    jobs:\n      - deploy-prod\n  - name: all\n    jobs:\n      - \"*\"\n</code></pre> <p>In this example, the <code>develop</code> group will match <code>terraform-apply</code>, <code>terraform-destroy</code>, <code>test</code>, <code>deploy-dev</code>,  <code>deploy-staging</code>. The <code>ship</code> group will only match <code>deploy-prod</code>. The <code>all</code> group will match all jobs in the  pipeline.</p> <p>Note</p> <p>Depending on how it's used, *, {, and } have special meaning in YAML, and may need to be quoted  (as was done in the all job above)</p>"},{"location":"docs/pipelines/#display_config-schema","title":"<code>display_config</code> schema","text":"background_image: <code>string</code> <p>Allows users to specify a custom background image for the pipeline. Must be an http, https, or relative URL.</p> background_filter: <code>string</code> <p>Default <code>opacity(30%) grayscale(100%)</code>. Allows users to specify custom  CSS filters that are applied to the  <code>background_image</code>.</p>"},{"location":"docs/pipelines/grouping-pipelines/","title":"Grouping Pipelines","text":"<p>Info</p> <p>Instance pipelines are not enabled by default if your Concourse is on a version &lt;v8. To enable this feature, set the <code>--enable-pipeline-instances</code> flag or env var <code>CONCOURSE_ENABLE_PIPELINE_INSTANCES</code> to <code>true</code> on the <code>web</code> node.</p> <p>Although pipelines operate independently of one another, it's not uncommon to have several pipelines that are highly related, and possibly derived from the same pipeline template. It's useful to be able to group these pipelines to reduce clutter and improve navigation. For this, Concourse has the concept of Instanced Pipelines and Instance Groups, where an Instance Group composes several related Instanced Pipelines.</p> <p>For instance, suppose you support multiple version lines of your software (v1.0.x and v2.0.x, say), and want a pipeline for each version line in order to facilitate delivering patch releases. You create a common pipeline template that uses Vars to specialize each pipeline:</p> <pre><code>resources:\n  - name: repo\n    type: git\n    source:\n      uri: git@...\n      # The only difference between the pipelines is the git branch to use\n      branch: release/v((version))\n\njobs:\n  - name: test\n    plan: [ ... ]\n\n  - name: deploy-to-staging\n    plan: [ ... ]\n\n  - name: release\n    plan: [ ... ]\n</code></pre> <p>Before Concourse v7.0.0, you might set multiple pipelines with the version information encoded in the pipeline name, e.g.:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline release-1.0.x \\\n    --config template.yml \\\n    --var version=1.0.x\n\nfly -t example set-pipeline \\\n    --pipeline release-2.0.x \\\n    --config template.yml \\\n    --var version=2.0.x\n</code></pre> <p>The downside to this approach is that things can get disorganized quickly as the number of pipelines increases, which can make the UI cluttered and hard to navigate. Additionally, not everything can easily be encoded into the pipeline name, especially with the restrictions on identifiers - while it's readable in this case, it can get unwieldy as the number of variables in the template grows.</p> <p>The recommended approach is to construct an Instance Group where each version has its own Instanced Pipeline:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline release \\\n    --config template.yml \\\n    --instance-var version=1.0.x\n\nfly -t example set-pipeline \\\n    --pipeline release \\\n    --config template.yml \\\n    --instance-var version=2.0.x\n</code></pre> <p>There are only a few differences from the previous approach in terms of creating the pipelines:</p> <ol> <li>We give each Instanced Pipeline the same name (in this case, <code>release</code>), and</li> <li>We use the <code>--instance-var</code> flag instead of <code>--var</code>. Doing so makes the variable name and value a part of the    pipeline's identifier (Managing Instanced Pipelines describes how to work with    Instanced Pipelines in fly)</li> </ol> <p>Warning</p> <p>The <code>-i</code> or <code>--instance-var</code> flag behaves like the <code>-y</code> or <code>--yaml-var</code>, meaning instance vars can hold arbitrary    YAML/JSON data. The <code>-v</code> or <code>--var</code> flag, on the other hand, only defines strings. See    Static vars to learn the difference between the flags</p> <p>Note</p> <p>There are no fly commands for constructing an Instance Group - Concourse logically groups all    Instanced Pipelines with the same name into a single Instance Group. Instanced Pipelines have the same pipeline    semantics as other pipelines - they are just organized and identified in a different way.</p>"},{"location":"docs/pipelines/grouping-pipelines/#managing-instanced-pipelines","title":"Managing Instanced Pipelines","text":"<p>Instanced Pipelines can be managed via fly as described in Managing Pipelines, with one important distinction - since instance vars are a part of the pipeline's identifier, the <code>--pipeline</code> flag must include both the name of the Instance Group as well as the instance vars. The <code>--pipeline</code> flag takes the form:</p> <pre><code>fly ... --pipeline group/var1:value1,var2:value2\n</code></pre> <p>As a concrete example, to pause the <code>release</code> Instanced Pipeline with <code>version:1.0.x</code>, you would issue the following command:</p> <pre><code>fly -t example pause-pipeline --pipeline release/version:1.0.x\n</code></pre> <p>Let's look at a more complicated example - suppose you have an Instanced Pipeline that was set using one of the following commands:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline upgrade \\\n    --config template.yml \\\n    --instance-var version.from=1.0.0 \\\n    --instance-var version.to=2.0.0 \\\n    --instance-var branch=feature/foo\n# ...or equivalently\nfly -t example set-pipeline \\\n    --pipeline upgrade \\\n    --config template.yml \\\n    --instance-var 'version={from: 1.0.0, to: 2.0.0}' \\\n    --instance-var branch=feature/foo\n</code></pre> <p>Tip</p> <p>Using dot-notation here (as in the first command) is recommended since YAML is finicky about spaces.</p> <p>For instance, had we used <code>--instance-var 'version={from:1.0.0, to:2.0.0}'</code> (no spaces between keys and values),    we would end up with the following object (represented as JSON):</p> <pre><code>{\"from:1.0.0\": null, \"to:2.0.0\": null}\n</code></pre> <p>Specifying each field individually using dot-notation is harder to mess up.</p> <p>Here, there are two instance vars: <code>version</code>, that contains the object <code>{\"from\": \"1.0.0\", \"to\": \"2.0.0\"}</code>, and <code>branch</code>, that contains the string <code>\"feature/foo\"</code>. In order to pause this pipeline, you could issue one of the following commands:</p> <pre><code>fly -t example pause-pipeline \\\n    --pipeline 'upgrade/version.from:1.0.0,version.to:2.0.0,branch:\"feature/foo\"'\n# ... or equivalently\nfly -t example pause-pipeline \\\n    --pipeline 'upgrade/version:{from: 1.0.0, to: 2.0.0},branch:\"feature/foo\"'\n</code></pre> <p>For accessing sub-fields of an object, we can either use dot-notation as described in Providing static values for vars, or we can define the object in full as valid YAML.</p> <p>Warning</p> <p>If the instance var name or value contains a \"special character\" (<code>.</code>, <code>,</code>, <code>/</code>, <code>{</code>, <code>}</code>, or whitespace), it    must be surrounded by double quotes <code>\"</code>. Depending on your shell, this usually means the entire flag must be    quoted, since otherwise your shell will try to expand the quotes.</p>"},{"location":"docs/pipelines/grouping-pipelines/#fly-order-instanced-pipelines","title":"<code>fly order-instanced-pipelines</code>","text":"<p>To configure the ordering of instanced pipelines within an individual instance group, run:</p> <pre><code>fly -t example order-instanced-pipelines \\\n    --group group \\\n    --pipeline key1:value1 \\\n    --pipeline key2:value2 \\\n    --pipeline key3:value3\n</code></pre> <p>Note</p> <p>This command only ensures that the given pipelines are in the given order. If there are other pipelines that you    haven't included in the command, they may appear in-between, before, or after the given set.</p> <p>Warning</p> <p>If you want to reorder pipelines outside of an individual instance group, you should use the    <code>fly order-pipelines</code> command.</p>"},{"location":"docs/pipelines/grouping-pipelines/#managing-jobs-and-resources","title":"Managing Jobs and Resources","text":"<p>Managing Jobs and Managing Resources walk you through some of the commands you can use to manage jobs and resources within pipelines. For Instanced Pipelines, we need to encode the instance vars in the <code>--job</code> and <code>--resource</code> flags. These flags now take the form:</p> <pre><code>fly ... --job group/var1:value1,var2:value2/job\n</code></pre> <p>and</p> <pre><code>fly ... --resource group/var1:value1,var2:value2/resource\n</code></pre> <p>For instance, to trigger the <code>test</code> job of <code>release/version:1.0.x</code>, we issue the following command:</p> <pre><code>fly -t example trigger-job --job release/version:1.0.x/test\n</code></pre> <p>To check the <code>repo</code> resource of <code>release/version:1.0.x</code>, we issue the following command:</p> <pre><code>fly -t example check-resource --resource release/version:1.0.x/repo\n</code></pre>"},{"location":"docs/pipelines/managing-pipelines/","title":"Managing Pipelines","text":""},{"location":"docs/pipelines/managing-pipelines/#fly-pipelines","title":"<code>fly pipelines</code>","text":"<p>To list the currently-configured pipelines and their paused state, run:</p> <pre><code>fly -t example pipelines\n</code></pre> <p>By default, archived pipelines are not included in the output of this command. To view archived pipelines, provide <code>--include-archived</code> flag.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-rename-pipeline","title":"<code>fly rename-pipeline</code>","text":"<p>To rename a pipeline, run:</p> <pre><code>fly -t example rename-pipeline \\\n  --old-name my-pipeline \\\n  --new-name my-cool-pipeline\n</code></pre> <p>All job history is retained when renaming a pipeline.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-pause-pipeline","title":"<code>fly pause-pipeline</code>","text":"<p>To pause a pipeline, run:</p> <pre><code>fly -t example pause-pipeline --pipeline my-pipeline\n</code></pre> <p>This will prevent jobs from being scheduled and stop the periodic checking for new versions of resources. Builds that are in-flight will still finish.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-unpause-pipeline","title":"<code>fly unpause-pipeline</code>","text":"<p>To unpause a pipeline, run:</p> <pre><code>fly -t example unpause-pipeline --pipeline my-pipeline\n</code></pre> <p>This will resume job scheduling and resource checking.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-expose-pipeline","title":"<code>fly expose-pipeline</code>","text":"<p>By default, newly configured pipelines are only visible to the pipeline's team. To make a pipeline viewable by other teams and unauthenticated users, run:</p> <pre><code>fly -t example expose-pipeline --pipeline my-pipeline\n</code></pre> <p>This feature is useful if you're using Concourse for an open source project and you'd like your community to be able to see into your build pipeline.</p> <p>To undo this change, see <code>fly hide-pipeline</code>.</p> <p>Exposing a pipeline reveals basically everything except for build output and resource metadata.</p> <p>To expose a resource's metadata, resource.public must be set to <code>true</code>.</p> <p>To expose a job's build output, job.public must be set to <code>true</code>. This will also reveal resource metadata for any <code>get</code> step or <code>put</code> steps in the build output.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-hide-pipeline","title":"<code>fly hide-pipeline</code>","text":"<p>If you realize that you've made a terrible mistake in exposing your pipeline, you can run:</p> <pre><code>fly -t example hide-pipeline --pipeline my-pipeline\n</code></pre> <p>If you're panicking you can run the command's short form, <code>hp</code>, instead.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-get-pipeline","title":"<code>fly get-pipeline</code>","text":"<p>Fly can be used to fetch and update the configuration for your pipelines. This is achieved by using the <code>fly get-pipeline</code> and <code>fly set-pipeline</code> commands. For example, to fetch the current configuration of your <code>my-pipeline</code> Concourse pipeline and print it on <code>STDOUT</code> run the following:</p> <pre><code>fly -t example get-pipeline --pipeline my-pipeline\n</code></pre> <p>To get JSON instead of YAML you can use the <code>-j</code> or <code>--json</code> argument. This can be useful when inspecting your config with jq.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-destroy-pipeline","title":"<code>fly destroy-pipeline</code>","text":"<p>Every now and then you just don't want a pipeline to be around anymore. Running <code>fly destroy-pipeline</code> will stop the pipeline activity and remove all data collected by the pipeline, including build history and collected versions.</p> <p>For example, to destroy the <code>my-pipeline</code> pipeline, you would run:</p> <pre><code>fly -t example destroy-pipeline --pipeline my-pipeline\n</code></pre>"},{"location":"docs/pipelines/managing-pipelines/#fly-order-pipelines","title":"<code>fly order-pipelines</code>","text":"<p>To configure the ordering of pipelines, run:</p> <pre><code>fly -t example order-pipelines \\\n  --pipeline pipeline-1 \\\n  --pipeline pipeline-2 \\\n  --pipeline pipeline-3\n</code></pre> <p>Note that this command only ensures that the given pipelines are in the given order. If there are other pipelines that you haven't included in the command, they may appear in-between, before, or after the given set.</p> <p>Warning</p> <p>If you want to reorder instanced pipelines within an individual instance group, you should use the <code>fly order-instanced-pipelines</code> command.</p>"},{"location":"docs/pipelines/managing-pipelines/#fly-archive-pipeline","title":"<code>fly archive-pipeline</code>","text":"<p>A pipeline can be archived via fly. This means that the pipeline will be paused and hidden from the web UI. The pipeline config will be deleted (so any secrets or interpolated Vars will be removed) while the build logs will be retained.</p> <pre><code>fly -t example archive-pipeline -p pipeline-1\n</code></pre> <p>To unarchive a pipeline, simply set the pipeline again with the same name using fly set-pipeline. If a job in the new pipeline has the same name as a job in the archived pipeline, the old build logs for that job will be restored.</p> <p>Note that because the config is deleted, <code>fly get-pipeline</code> will no longer work for archived pipelines.</p>"},{"location":"docs/pipelines/setting-pipelines/","title":"Setting Pipelines","text":"<p>Pipelines are configured entirely via the <code>fly</code> CLI or the  <code>set_pipeline</code> step. There is no GUI for configuring pipelines.</p>"},{"location":"docs/pipelines/setting-pipelines/#fly-set-pipeline","title":"<code>fly set-pipeline</code>","text":"<p>To submit a pipeline configuration to Concourse from a file on your local disk you can use the <code>-c</code> or <code>--config</code> flag, like so:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml\n</code></pre> <p>This will present a diff of the changes and ask you to confirm the changes. If you accept then Concourse's pipeline configuration will switch to the pipeline definition in the YAML file specified.</p> <p>The <code>-c</code> or <code>--config</code> flag can also take in the value <code>-</code> to indicate reading from <code>stdin</code>:</p> <pre><code>cat pipeline.yml | fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config -\n</code></pre> <p>Note that reading from <code>stdin</code> disables the confirmation prompt - the pipeline will be set automatically.</p>"},{"location":"docs/pipelines/setting-pipelines/#providing-static-values-for-vars","title":"Providing static values for vars","text":"<p>The pipeline configuration can contain Vars which may be replaced with static values or loaded at runtime. This allows for credentials to be extracted from a pipeline config, making it safe to check in to a public repository or pass around.</p> <p>For example, if you have a <code>pipeline.yml</code> as follows:</p> <pre><code>resources:\n  - name: private-repo\n    type: git\n    source:\n      uri: git@...\n      branch: master\n      private_key: ((private-repo-key))\n</code></pre> <p>... you could then configure this pipeline like so:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"private-repo-key=$(cat id_rsa)\"\n</code></pre> <p>Or, if you had a <code>vars.yml</code> as follows:</p> <pre><code>private-repo-key: |\n  -----BEGIN RSA PRIVATE KEY-----\n  ...\n  -----END RSA PRIVATE KEY-----\n</code></pre> <p>... you could configure it like so:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from vars.yml\n</code></pre> <p>You can use nested fields in your <code>pipeline.yml</code> as follows:</p> <pre><code>resources:\n  - name: private-repo\n    type: git\n    source:\n      uri: git@((repo.uri))\n      branch: ((repo.branch))\n      private_key: ((\"github.com\".private-repo-key))\n</code></pre> <p>... you could configure it by <code>--load-vars-from</code> with a <code>vars.yml</code> as follows:</p> <pre><code>repo:\n  uri: github.com/...\n  branch: master\ngithub.com:\n  private-repo-key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    ...\n    -----END RSA PRIVATE KEY-----\n</code></pre> <p>... or you could also configure it by passing the vars as flags:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"repo.uri=github.com\" \\\n    --var \"repo.branch=master\" \\\n    --var \"\\\"github.com\\\".private-repo-key=$(cat id_rsa)\"\n</code></pre> <p>When configuring a pipeline, any vars not provided statically will be left to resolve at runtime. To check that all vars are resolvable, you can pass the <code>--check-creds</code> flag:</p> <pre><code>fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from vars.yml \\\n    --check-creds\n</code></pre> <p>This will fill in all statically-provided vars and then attempt to resolve all remaining vars server-side. If any fail to resolve, configuring the pipeline will fail.</p>"},{"location":"docs/pipelines/setting-pipelines/#fly-validate-pipeline","title":"<code>fly validate-pipeline</code>","text":"<p>To validate a local pipeline configuration without submitting it to Concourse, run <code>validate-pipeline</code>:</p> <pre><code>fly validate-pipeline --config pipeline.yml\n</code></pre> <p>By default, pipeline errors will cause <code>validate-pipeline</code> to fail, but warnings won't. To fail on both errors and warnings, pass the <code>--strict</code> flag.</p>"},{"location":"docs/pipelines/setting-pipelines/#fly-format-pipeline","title":"<code>fly format-pipeline</code>","text":"<p>To format a pipeline config in a \"canonical\" form (i.e. keys are in normal order, with <code>name</code> first for example), run:</p> <pre><code>fly format-pipeline --config pipeline.yml\n</code></pre> <p>This will print the formatted pipeline config to <code>stdout</code>. To update the file in-place, pass <code>--write/-w</code>.</p>"},{"location":"docs/resource-types/","title":"Resource Types","text":"<p>Each resource in a pipeline has a <code>type</code>. The resource's type determines what versions are detected, the bits that are fetched when the resource's <code>get</code> step runs, and the side effect that occurs when the resource's put step runs.</p> <p>Concourse comes with a few \"core\" resource types to cover common use cases like <code>git</code> and <code>s3</code> - the rest are developed and supported by the Concourse community. An exhaustive list of all resource types is available in the Resource Types catalog.</p> <p>A pipeline's resource types are listed under <code>pipeline.resource_types</code> with the following schema:</p>"},{"location":"docs/resource-types/#resource_type-schema","title":"<code>resource_type</code> schema","text":"<code>name</code>: <code>identifier</code> (required) <code>type</code>: <code>resource_type.name</code> | <code>identifier</code> (required) <code>source</code>: <code>config</code> (required) <code>privileged</code>: <code>boolean</code> <code>params</code>: <code>config</code> <code>check_every</code>: <code>duration</code> <code>tags</code>: <code>[string]</code> <code>defaults</code>: <code>config</code> Using a <code>rss</code> resource type to subscript to RSS feeds <p>Resource Types can be used to extend the functionality of your pipeline and provide deeper integrations. This  example uses one to trigger a job whenever a new Dinosaur Comic is out.</p> <pre><code>---\nresource_types:\n  - name: rss\n    type: registry-image\n    source:\n      repository: suhlig/concourse-rss-resource\n      tag: latest\n\nresources:\n  - name: booklit-releases\n    type: rss\n    source:\n      url: http://www.qwantz.com/rssfeed.php\n\njobs:\n  - name: announce\n    plan:\n      - get: booklit-releases\n        trigger: true\n</code></pre>"},{"location":"docs/resource-types/#name","title":"<code>name</code>","text":"<p>The name of the resource type. This should be short and simple. This name will be referenced by  <code>pipeline.resources</code> defined within the same pipeline, and  <code>task-config.image_resource</code>s used by tasks running in the pipeline.</p> <p>Pipeline-provided resource types can override the core resource types by specifying the same name.</p>"},{"location":"docs/resource-types/#type","title":"<code>type</code>","text":"<p>The type of the resource used to provide the resource type's container image.</p> <p>This is a bit meta. Usually this value will be <code>registry-image</code> as the resource type must result in a container  image.</p> <p>A resource type's type can refer to other resource types, and can also use the core type that it's overriding. This  is useful for bringing in a newer or forked <code>registry-image</code> resource.</p>"},{"location":"docs/resource-types/#source","title":"<code>source</code>","text":"<p>The configuration for the resource type's resource. This varies by resource type, and is a black box to Concourse;  it is blindly passed to the resource at runtime.</p> <p>To use <code>registry-image</code> as an example, the source would contain something like <code>repository: username/reponame</code>. See  the Registry Image resource (or whatever resource type your  resource type uses) for more information.</p>"},{"location":"docs/resource-types/#privileged","title":"<code>privileged</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, the resource's containers will be run with full capabilities, as determined by  the worker backend the task runs on.</p> <p>For Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the <code>root</code> user is \"actual\" <code>root</code> (if set to <code>true</code>) or a user namespaced <code>root</code> (if set to <code>false</code>,  the default).</p> <p>This is a gaping security hole; only configure it if the resource type needs it (which should be called out in its  documentation). This is not up to the resource type to decide dynamically, so as to prevent privilege escalation via third-party resource type exploits.</p>"},{"location":"docs/resource-types/#params","title":"<code>params</code>","text":"<p>Arbitrary config to pass when running the <code>get</code> step to fetch the resource type's image. This is  equivalent to <code>get</code> step <code>params</code>.</p>"},{"location":"docs/resource-types/#check_every","title":"<code>check_every</code>","text":"<p>Default <code>1m</code>. The interval on which to check for new versions of the resource. Acceptable interval options are  defined by the time.ParseDuration function.</p>"},{"location":"docs/resource-types/#tags","title":"<code>tags</code>","text":"<p>Default <code>[]</code>. A list of tags to determine which workers the checks will be performed on. You'll want to specify  this if the source is internal to a worker's network, for example. See also  <code>tags</code></p>"},{"location":"docs/resource-types/#defaults","title":"<code>defaults</code>","text":"<p>The default configuration for the resource type. This varies by resource type, and is a black box to Concourse; it  is merged with (duplicate fields are overwritten by) <code>resource.source</code> and  passed to the resource at runtime.</p> Setting default configuration for resources <pre><code>resource_types:\n  - name: gcs\n    type: registry-image\n    source:\n      repository: frodenas/gcs-resource\n    defaults:\n      json_key: ((default_key))\n\nresources:\n  - name: bucket-a\n    type: gcs\n    source:\n      bucket: a\n\n  - name: bucket-b\n    type: gcs\n    source:\n      bucket: b\n\n  - name: bucket-c\n    type: gcs\n    source:\n      bucket: c\n      json_key: ((different_key))\n</code></pre> Overrides default resource types <p>Since it's possible to overwrite the base resource types, it can be used to give defaults to resources at the  pipeline level.</p> <pre><code>resource_types:\n  - name: registry-image\n    type: registry-image\n    source:\n      repository: concourse/registry-image-resource\n    defaults:\n      registry_mirror:\n        host: https://registry.mirror.example.com\n\nresources:\n  - name: mirrored-image\n    type: registry-image\n    source:\n      repository: busybox\n</code></pre> <p>Alternatively, the web node can be configured to use  defaults for base resource types.</p>"},{"location":"docs/resource-types/implementing/","title":"Implementing a Resource Type","text":"<p>A resource type is implemented by a container image with three scripts:</p> <ul> <li><code>/opt/resource/check</code> for checking for new versions of the resource</li> <li><code>/opt/resource/in</code> for pulling a version of the resource down</li> <li><code>/opt/resource/out</code> for idempotently pushing a version up</li> </ul> <p>Distributing resource types as containers allows them to package their own dependencies. For example, the <code>git</code> resource comes with the <code>git</code> binary pre-installed.</p> <p>All resources must implement all three actions, though the actions can just be no-ops (which still must be correctly implemented as detailed below).</p> <p>Resources can emit logs to the user by writing to <code>stderr</code>. ANSI escape codes (coloring, cursor movement, etc.) will be interpreted properly by the web UI, so you should make your output pretty.</p>"},{"location":"docs/resource-types/implementing/#check-check-for-new-versions","title":"<code>check</code>: Check for new versions.","text":"<p>A resource type's <code>check</code> script is invoked to detect new versions of the resource. It is given the configured source and current version on <code>stdin</code>, and must print the array of new versions, in chronological order (oldest first), to <code>stdout</code>, including the requested version if it's still valid.</p> <p>The request body will have the following fields:</p> <ul> <li> <p><code>source</code> is an arbitrary JSON object which specifies the location of the resource (1), including any credentials. This   is passed verbatim from the resource configuration.</p> </li> <li> <p><code>version</code> is a JSON object with <code>string</code> fields, used to uniquely identify an instance of the resource. For <code>git</code> this   would be the commit's SHA.</p> </li> </ul> <ol> <li>For the <code>git</code> resource this would be the repo URI, the branch, and the private key, if necessary.</li> </ol> <p>For example, here's what the input for the <code>git</code> resource may look like:</p> <pre><code>{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": {\n    \"ref\": \"61cbef\"\n  }\n}\n</code></pre> <p>Upon receiving this payload the <code>git</code> resource would probably do something like:</p> <pre><code>[ -d /tmp/repo ] || git clone git://some-uri /tmp/repo\ncd /tmp/repo\ngit pull &amp;&amp; git log 61cbef..HEAD\n</code></pre> <p>Note that it conditionally clones; the container for checking versions is reused between checks, so that it can efficiently pull rather than cloning every time.</p> <p>And the output, assuming <code>d74e01</code> is the commit immediately after <code>61cbef</code>:</p> <pre><code>[\n  {\n    \"ref\": \"61cbef\"\n  },\n  {\n    \"ref\": \"d74e01\"\n  },\n  {\n    \"ref\": \"7154fe\"\n  }\n]\n</code></pre> <p>The list may be empty, if there are no versions available at the source. If the given version is already the latest, an array with that version as the sole entry should be listed.</p> <p>If your resource is unable to determine which versions are newer than the given version (e.g. if it's a git commit that was <code>push -f</code>ed over), then the current version of your resource should be returned (i.e. the new <code>HEAD</code>).</p>"},{"location":"docs/resource-types/implementing/#in-fetch-a-given-resource","title":"<code>in</code>: Fetch a given resource.","text":"<p>The <code>in</code> script is passed a destination directory as command line argument <code>$1</code>, and is given on <code>stdin</code> the configured source and a precise version of the resource to fetch.</p> <p>The script must fetch the resource and place it in the given directory.</p> <p>If the desired resource version is unavailable (for example, if it was deleted), the script must exit with error.</p> <p>The script must emit the fetched version, and may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.</p> <p>The request will contain the following fields:</p> <ul> <li><code>source</code> is the same value as passed to check.</li> <li><code>version</code> is the same type of value passed to check, and specifies the version to   fetch.</li> <li><code>params</code> is an arbitrary JSON object passed along verbatim from get step params on a get step.</li> </ul> <p>Example request, in this case for the <code>git</code> resource:</p> <pre><code>{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": {\n    \"ref\": \"61cebf\"\n  }\n}\n</code></pre> <p>Upon receiving this payload the <code>git</code> resource would probably do something like:</p> <pre><code>git clone --branch develop git://some-uri $1\ncd $1\ngit checkout 61cebf\n</code></pre> <p>And output:</p> <pre><code>{\n  \"version\": {\n    \"ref\": \"61cebf\"\n  },\n  \"metadata\": [\n    {\n      \"name\": \"commit\",\n      \"value\": \"61cebf\"\n    },\n    {\n      \"name\": \"author\",\n      \"value\": \"Hulk Hogan\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/resource-types/implementing/#out-update-a-resource","title":"<code>out</code>: Update a resource.","text":"<p>The <code>out</code> script is passed a path to the directory containing the build's full set of sources as command line argument <code>$1</code>, and is given on <code>stdin</code> the configured params and the resource's source configuration.</p> <p>The script must emit the resulting version of the resource. For example, the <code>git</code> resource emits the SHA of the commit that it has just pushed.</p> <p>Additionally, the script may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.</p> <p>The request will contain the following fields:</p> <ul> <li><code>source</code> is the same value as passed to check.</li> <li><code>params</code> is an arbitrary JSON object passed along verbatim from get step params on a <code>put</code> step.</li> </ul> <p>Example request, in this case for the <code>git</code> resource:</p> <pre><code>{\n  \"params\": {\n    \"branch\": \"develop\",\n    \"repo\": \"some-repo\"\n  },\n  \"source\": {\n    \"uri\": \"git@...\",\n    \"private_key\": \"...\"\n  }\n}\n</code></pre> <p>Upon receiving this payload the <code>git</code> resource would probably do something like:</p> <pre><code>cd $1/some-repo\ngit push origin develop\n</code></pre> <p>And output:</p> <pre><code>{\n  \"version\": {\n    \"ref\": \"61cebf\"\n  },\n  \"metadata\": [\n    {\n      \"name\": \"commit\",\n      \"value\": \"61cebf\"\n    },\n    {\n      \"name\": \"author\",\n      \"value\": \"Mick Foley\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/resource-types/implementing/#metadata","title":"Metadata","text":"<p>When used in a <code>get</code> step or a <code>put</code> step, metadata about the running build is made available via the following environment variables:</p> <code>$BUILD_ID</code> <p>The internal identifier for the build. Right now this is numeric, but it may become a UUID in the future. Treat it as an absolute reference to the build.</p> <code>$BUILD_NAME</code> <p>The build number within the build's job.</p> <code>$BUILD_JOB_NAME</code> <p>The name of the build's job.</p> <code>$BUILD_PIPELINE_NAME</code> <p>The name of the pipeline that the build's job lives in.</p> <code>$BUILD_PIPELINE_INSTANCE_VARS</code> <p>The instance vars of the instanced pipeline that the build's job lives in, serialized as JSON. See Grouping Pipelines for a definition of instanced pipelines.</p> <code>$BUILD_TEAM_NAME</code> <p>The team that the build belongs to.</p> <code>$BUILD_CREATED_BY</code> <p>The username that created the build. By default, it is not available. See <code>expose_build_created_by</code> for how to opt in. This metadata field is not made available to the <code>get</code> step.</p> <code>$ATC_EXTERNAL_URL</code> <p>The public URL for your ATC; useful for debugging.</p> <code>$BUILD_URL</code> <p>The URL of the build using team, pipeline, instance vars, and job name in the URL. Same as what you would see in the web UI. Will be the same value as <code>$BUILD_URL_SHORT</code> if it's a one-off build.</p> <code>$BUILD_URL_SHORT</code> <p>The short URL of the build in the form <code>$ATC_EXTERNAL_URL/builds/$BUILD_ID</code></p> <p>If the build is a one-off, <code>$BUILD_NAME</code>, <code>$BUILD_JOB_NAME</code>, <code>$BUILD_PIPELINE_NAME</code>, and <code>$BUILD_PIPELINE_INSTANCE_VARS</code> will not be set.</p> <p>Additionally, <code>$BUILD_PIPELINE_INSTANCE_VARS</code> will not be set if the build's pipeline has no instance vars (i.e. is not an instanced pipeline).</p> <p>None of these variables are available to check.</p> <p>These variables should be used solely for annotating things with metadata for traceability, i.e. for linking to the build in an alert or annotating an automated commit to facilitate its origin discovery.</p> <p>They should not be used to emulate versioning (e.g. by using the increasing build number). They are not provided to <code>task</code> steps to avoid this anti-pattern.</p>"},{"location":"docs/resource-types/implementing/#certificate-propagation","title":"Certificate Propagation","text":"<p>Certificates can be automatically propagated into each resource container, if the worker is configured to do so. The BOSH release configures this automatically, while the <code>concourse</code> binary must be given a <code>--certs-dir</code> flag pointing to the path containing the CA certificate bundle.</p> <p>The worker's certificate directory will then be always mounted at <code>/etc/ssl/certs</code>, read-only, in each resource container created on the worker. There's no single standard path for this, so we picked one that would work out of the box in most cases.</p> <p>This approach to certificate configuration is similar in mindset to the propagation of <code>http_proxy</code>/<code>https_proxy</code> - certs are kind of a baseline assumption when deploying software, so Concourse should do its best to respect it out-of-the-box, especially as they're often used in tandem with a man-in-the-middle corporate SSL proxy. (In this way it doesn't feel too much like the anti-pattern of hand-tuning workers.)</p>"},{"location":"docs/resource-types/implementing/#testing-resources-locally-using-docker","title":"Testing resources locally using docker","text":"<p>To test an already packaged resource (a docker image) outside concourse, you need to:</p> <ol> <li>If, for instance, you are testing the <code>out</code> behaviour of the <code>git</code> resource, create a json file with <code>source</code>    configuration of the resource and the <code>params</code> the <code>put</code> step expects. Such a file for the <code>git</code> resource would    contain the following (or similar):     <pre><code>{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"params\": {\n    \"repository\": \".\",\n    \"rebase\": true\n  }\n}\n</code></pre>    Save this file to out-config.json in your working directory.</li> <li>Then run the <code>/opt/resource/out</code> script with its inputs provided via <code>stdin</code> like so (using the <code>docker</code> cli as an    example):     <pre><code>docker run --rm -i -v \"${PWD}:${PWD}\" -w \"${PWD}\" \\\n  concourse/git-resource /opt/resource/out . &lt; out-config.json\n</code></pre></li> </ol> <p>Warning</p> <p>This example needs modification depending on the resource you are testing and your local environment. See the notes  below for details.</p> <ol> <li>If you use the exact configuration in this example, the git resource will print an error about the format of the    private key being invalid. Adjust the content <code>out-config.json</code> as necessary to get it working with your resource.</li> <li>If the resource you are testing uses Metadata, you will need to provide the required metadata as    environment variables to your <code>docker run</code> command like so:     <pre><code>docker run --rm -i -e ATC_EXTERNAL_URL=\"https://concourse.example.com\" \\\n  -e BUILD_NAME=620 \\\n  -v \"${PWD}:${PWD}\" \\\n  -w \"${PWD}\" concourse/git-resource /opt/resource/out . &lt; out-config.json\n</code></pre></li> </ol>"},{"location":"docs/resource-types/managing-types/","title":"Managing Resource Types","text":""},{"location":"docs/resource-types/managing-types/#fly-check-resource-type","title":"<code>fly check-resource-type</code>","text":"<p>To force immediate checking for new versions of a resource type, rather than waiting for the periodic checking, run:</p> <pre><code>fly -t example check-resource-type --resource-type my-pipeline/my-resource-type\n</code></pre> <p>This can be useful for forcing an update if you're iterating on your own resource type implementation.</p>"},{"location":"docs/resources/","title":"Resources","text":"<p>Resources are the heart and soul of Concourse. They represent all external inputs to and outputs of jobs in the pipeline.</p> <p>Each resource represents a versioned artifact with an external source of truth. Configuring the same resource in any pipeline on any Concourse cluster will behave the exact same way. Concourse will continuously <code>check</code> each configured resource to discover new versions. These versions then flow through the pipeline via <code>get</code> steps configured on Jobs.</p> <p>More concretely, resources are containers that run on your workers. See Implementing a Resource Type for more details.</p> <p>A pipeline's resources are listed under <code>pipeline.resources</code> with the following schema.</p>"},{"location":"docs/resources/#resource-schema","title":"<code>resource</code> schema","text":"<code>name</code>: <code>identifier</code> (required) <code>type</code>: <code>resource_type.name</code> (required) <code>source</code>: <code>config</code> (required) <code>old_name</code>: <code>identifier</code> <code>icon</code>: <code>string</code> <code>version</code>: <code>version</code> <code>check_every</code>: <code>duration</code> | <code>never</code> <code>check_timeout</code>: <code>duration</code> <code>expose_build_created_by</code>: <code>boolean</code> <code>tags</code>: <code>[string]</code> <code>public</code>: <code>boolean</code> <code>webhook_token</code>: <code>string</code>"},{"location":"docs/resources/#name","title":"<code>name</code>","text":"<p>The name of the resource. This should be short and simple. This name will be referenced by  build plans of jobs in the pipeline.</p>"},{"location":"docs/resources/#type","title":"<code>type</code>","text":"<p>The resource type implementing the resource.</p>"},{"location":"docs/resources/#source","title":"<code>source</code>","text":"<p>The configuration for the resource. This varies by resource type, and is a black box to Concourse; it is blindly  passed to the resource at runtime.</p> <p>To use <code>git</code> as an example, the source may contain the repo URI, the branch of the repo to track, and a private key  to use when pushing/pulling.</p> <p>By convention, documentation for each resource type's configuration is in each implementation's <code>README</code>.</p> <p>You can find the source for the resource types provided with Concourse at the  Concourse GitHub organization.</p>"},{"location":"docs/resources/#old_name","title":"<code>old_name</code>","text":"<p>The old name of the resource. If configured, the history of the old resource will be inherited to the new one. Once  the pipeline is set, this field can be removed as the history has been transferred.</p> Renaming a resource <p>This can be used to rename a resource without losing its history, like so:</p> <pre><code>resources:\n  - name: new-name\n    old_name: current-name\n    type: git\n    source:\n      uri: \"https://github.com/vito/booklit\"\n</code></pre> <p>After the pipeline is set, the resource was successfully renamed, so the <code>old_name</code> field can be removed from  the resource:</p> <pre><code>resources:\n  - name: new-name\n    type: git\n    source:\n      uri: \"https://github.com/vito/booklit\"\n</code></pre>"},{"location":"docs/resources/#icon","title":"<code>icon</code>","text":"<p>The name of a Material Design icon to show next to the resource name in the web  UI. For example, <code>github</code>.</p>"},{"location":"docs/resources/#version","title":"<code>version</code>","text":"<p>A version to pin the resource to across the pipeline. This has the same effect as setting  <code>get</code> step <code>version</code> on every <code>get</code> step referencing the resource.</p> <p>Resources can also be temporarily pinned to a version via the API and web UI. However, this functionality is  disabled if the resource is pinned via configuration, and if a pipeline is configured to have a version pinned while also pinned in the web UI, the configuration takes precedence and will clear out the temporary pin.</p>"},{"location":"docs/resources/#check_every","title":"<code>check_every</code>","text":"<p>Default <code>1m</code>. The interval on which to check for new versions of the resource. Acceptable interval options are  defined by the time.ParseDuration function.</p> <p>If set to <code>never</code> the resource will not be automatically checked. The resource can still be checked manually via the web UI, fly, or webhooks.</p>"},{"location":"docs/resources/#check_timeout","title":"<code>check_timeout</code>","text":"<p>Default <code>1h</code>. The time limit on checking new versions of resources. Acceptable interval options are defined by the time.ParseDuration function.</p>"},{"location":"docs/resources/#expose_build_created_by","title":"<code>expose_build_created_by</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, environment variable  <code>BUILD_CREATED_BY</code> will be available in the metadata of a  <code>put</code> step. This field is not made available to the <code>get</code> step.</p>"},{"location":"docs/resources/#tags","title":"<code>tags</code>","text":"<p>Default <code>[]</code>. A list of tags to determine which workers the checks will be performed on. You'll want to specify  this if the source is internal to a worker's network, for example.</p> <p>Warning</p> <p>This does not apply tags to all <code>get</code> steps or <code>put</code> steps that use the  resource. If you want these steps to use tags, you must set <code>tags</code> for  each step.</p>"},{"location":"docs/resources/#public","title":"<code>public</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, the metadata for each version of the resource will be viewable by  unauthenticated users (assuming the pipeline has been exposed).</p> <p>Resource metadata should never contain credentials or secret information, but this is off by default just to be  safe in case users don't want to show things like commit messages and authors to the public.</p> <p>Note</p> <p>Even when set to <code>false</code>, the versions identifiers will be visible. In addition, if a resource is fetched in a  build whose job is marked <code>job.public</code>, metadata will be visible in the build output.</p>"},{"location":"docs/resources/#webhook_token","title":"<code>webhook_token</code>","text":"<p>If specified, web hooks can be sent to trigger an immediate check of the resource, specifying this value as a  primitive form of authentication via query params.</p> <p>After configuring this value, you would then configure your hook sender with the following painfully long path  appended to your external URL:</p> <pre><code>/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN\n</code></pre> <p>For instance pipelines you will need to include  the pipeline vars for a single pipeline instance. Currently, you can not have webhooks for all instances of a  pipeline.</p> <p>The pipeline vars should be added to the webhook URL as URL  parameters with the  format <code>vars.MY-VAR=\"SOME-VALUE\"</code>. A webhook URL for a pipeline instance may look like this:</p> <pre><code>/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN&amp;vars.my-var=\"some-value\"&amp;vars.second-var=\"two\"\n</code></pre> <p>Note</p> <p>The request payload sent to this API endpoint is entirely ignored. You should configure the resource as if  you're not using web hooks, as the resource config is still the \"source of truth.\"</p>"},{"location":"docs/resources/managing-resources/","title":"Managing Resources","text":""},{"location":"docs/resources/managing-resources/#fly-check-resource","title":"<code>fly check-resource</code>","text":"<p>To force immediate checking for new versions of a resource, rather than waiting for the periodic checking, run:</p> <pre><code>fly -t example check-resource --resource my-pipeline/my-resource\n</code></pre> <p>To check from a particular version, including the given version, append the <code>--from</code> flag like so:</p> <pre><code>fly -t example check-resource --resource my-pipeline/my-resource \\\n    --from ref:abcdef\n</code></pre> <p>This can be useful for collecting versions that are older than the current ones, given that a newly configured resource will only start from the latest version.</p> <p>Note</p> <p>The <code>ref:</code> prefix is resource-dependent. For example, the bosh-io-release resource might use <code>version:11.2</code> in  place of <code>ref:abcdef</code>.</p>"},{"location":"docs/resources/managing-resources/#fly-pin-resource","title":"<code>fly pin-resource</code>","text":"<p>To pin a resource to a specific version of that resource, run:</p> <pre><code>fly -t example pin-resource --resource my-pipeline/my-resource \\\n    --version ref:bceaf\n</code></pre> <p>Note</p> <p>The version needs to be provided as a key-value pair. For the git resource the <code>ref:</code> prefix is used while the registry resource might use <code>digest</code> as a prefix like <code>digest:sha256:94be7d7b</code>.</p> <p>A comment can be provided using the <code>--comment</code> flag, which is then also visible in the UI:</p> <pre><code>fly -t example pin-resource --resource my-pipeline/my-resource \\\n    --version ref:abcdef \\\n    --comment \"Some reason\"\n</code></pre> <p>This can, for example, be used to pull in a fixed version of an external dependency which might break your build in a new release. After the problem has been resolved, the pin can be removed. Another example could be running a build with a set of older inputs when needed.</p> <p>To remove the pin on a resource use:</p> <pre><code>fly -t example unpin-resource --resource my-pipeline/my-resource\n</code></pre> <p>You can also pin a resource via the UI by clicking on the pin button next to the desired version on the resource page. A default comment is automatically generated containing your username and a timestamp. This comment can be edited.</p>"},{"location":"docs/resources/managing-resources/#fly-enable-resource-version","title":"<code>fly enable-resource-version</code>","text":"<p>To enable a specific version of a resource, run:</p> <pre><code>fly -t example enable-resource-version --resource my-pipeline/my-resource \\\n    --version ref:bceaf\n</code></pre> <p>Note</p> <p>The version needs to be provided as a key-value pair. For the git resource the <code>ref:</code> prefix is used while the registry resource might use <code>digest</code> as a prefix like <code>digest:sha256:94be7d7b</code>.</p> <p>This command is idempotent. Enabling an already enabled resource version will do nothing.</p> <p>You can also enable a resource version via the UI by clicking on the check mark button next to the desired version on the resource page.</p>"},{"location":"docs/resources/managing-resources/#fly-disable-resource-version","title":"<code>fly disable-resource-version</code>","text":"<p>To disable a specific version of a resource, run:</p> <pre><code>fly -t example disable-resource-version --resource my-pipeline/my-resource \\\n    --version ref:bceaf\n</code></pre> <p>Note</p> <p>The version needs to be provided as a key-value pair. For the git resource the <code>ref:</code> prefix is used while the registry resource might use <code>digest</code> as a prefix like <code>digest:sha256:94be7d7b</code>.</p> <p>This command is idempotent. Disabling an already disabled resource version will do nothing.</p> <p>You can also disable a resource version via the UI by clicking on the check mark button next to the desired version on the resource page.</p>"},{"location":"docs/resources/managing-resources/#fly-clear-resource-cache","title":"<code>fly clear-resource-cache</code>","text":"<p>If you've got a resource cache that you need to clear out for whatever reason, this can be done like so:</p> <pre><code>fly -t example clear-resource-cache -r my-pipeline/my-resource\n</code></pre> <p>This will immediately invalidate all the caches related to that resource - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.</p> <p>You can also clear out a particular version for the given resource cache, using <code>-v</code>:</p> <pre><code>fly -t example clear-resource-cache \\\n    -r my-pipeline/my-resource \\\n    -v ref:abcdef\n</code></pre> <p>If <code>-v</code> is not specified, all caches for the given resource will be cleared.</p>"},{"location":"docs/resources/resource-versions/","title":"Resource Versions","text":"<p>As you may know, resources represent external state that changes over time. But how do we track those changes in a generic way that will properly represent all the different resource types? That is where resource versions are introduced. Concourse uses versions to represent the exact changes of a resource over time.</p> <p>The versions of a resource are directly dependent on its resource configuration and resource type. Each resource type has its own definition of what its versions should be. For example, the versions of a git resource would be the commits of the GitHub repository and the versions of a docker image resource are the image digests.</p> <p>If you want to figure out what determines the version of a resource type, it is typically outlined in the <code>check</code> behavior for the resource type. For example, the git resource uses commits as versions git resource type check behavior.</p>"},{"location":"docs/resources/resource-versions/#where-do-they-come-from-and-what-are-they-used-for","title":"Where do they come from and what are they used for?","text":"<p>The resource checker is responsible for checking for new versions of a resource. These versions are then saved to the database and can be viewed from the resource page in the web UI.</p> <p>Resource versions are used by the build scheduler in order to schedule new builds for a job.</p>"},{"location":"docs/resources/resource-versions/#version-pinning","title":"Version Pinning","text":"<p>A common job workflow is to use the latest version of a resource in order to trigger new builds. This works most of the time until you run into a situation where you need to run the job using an old version of a resource. Concourse provides a solution to this, which is called resource pinning.</p> <p>There are two different ways to pin a resource: through the pipeline config and through the web UI. Within the pipeline config, you can either pin the resource to a version through the resource configuration or through a get step version configuration. If you would like to pin through the web UI, the functionality can be found in the resource version history page which is accessed through clicking into the resource within the pipeline page.</p> <p>Pinning through the pipeline config is useful for a more permanent pinned state. If a resource is pinned through the pipeline config, it cannot be modified through the web UI and can only be changed through modifying and resetting the pipeline config.</p> <p>Pinning through the web UI is useful for reactionary pinning of a resource. For example, it can be used in the event of a broken upstream dependency.</p> <p>If you had a version pinned in the web UI and then pinned it through the pipeline config, the pipeline config pinned version will take precedence.</p> <p>A pinned version is associated to a resource and can be viewed in the resource page (excluding the case that the version was pinned on a get step). This pinned version will be propagated throughout the pipeline and used by the jobs that take that pinned resource as an input. If there is a job that has a passed constraint on a pinned resource, this means that the input is only valid if that pinned version has been used by the passed constraint job.</p> <p>Let's say we have a pipeline with two jobs and one resource that is being used as a passed constraint between the two jobs. If that resource is pinned to a version, the first job will produce a build using the pinned version of the resource. After that build succeeds, the second job that has a passed constraint on the first will then be able to trigger off a build because the pinned version has been successfully used by the first job.</p>"},{"location":"docs/resources/resource-versions/#unpinning","title":"Unpinning","text":"<p>When a version is unpinned, Concourse will go back to using the latest available version. This means a new build will be queued up if the most recent build used the old pinned version and the input has <code>trigger: true</code>.</p> <p>If you would like to learn more about how version pinning and unpinning works with the build scheduler, you can read more about it in the scheduling behavior section.</p>"},{"location":"docs/resources/resource-versions/#disabling-a-version","title":"Disabling a Version","text":"<p>A resource version can also be disabled through the web UI on the resource version history page. These disabled versions will not be used to schedule any further builds for any jobs that use the resource as an input.</p> <p>Disabled versions can also be re-enabled through the resource version history page.</p> <p>Disabling a version is useful for cases where you know that the version is broken or incompatible.</p>"},{"location":"docs/steps/do/","title":"Do Step","text":""},{"location":"docs/steps/do/#do-step","title":"<code>do</code> Step","text":"<code>do</code>: <code>[step]</code> (required) <p>Performs the given steps serially, with the same semantics as if they were at the top level step listing. Most commonly used with <code>try</code> step, across-step, and step-hooks.</p> Running multiple steps in a try <p>This can be used to perform multiple steps serially in a <code>try</code> step:</p> <pre><code>jobs:\n  - name: with-do\n    plan:\n      - try:\n          do:\n            - get: black-ice\n            - get: control-node\n            - get: cyberdeck\n\nresources:\n  - name: black-ice\n    type: mock\n  - name: control-node\n    type: mock\n  - name: cyberdeck\n    type: mock\n</code></pre>"},{"location":"docs/steps/get/","title":"Get Step","text":""},{"location":"docs/steps/get/#get-step","title":"<code>get</code> Step","text":"<p>Fetches a version of a resource. Expand each section below for more details and examples.</p> <code>get</code>: <code>resource.name</code> | <code>identifier</code> (required) <code>resource</code>: <code>resource.name</code> <code>passed</code>: <code>[job.name]</code> <code>params</code>: <code>config</code> <code>trigger</code>: <code>boolean</code> <code>version</code>: <code>latest</code> | <code>every</code> | <code>version</code>"},{"location":"docs/steps/get/#get","title":"<code>get</code>","text":"<p>The fetched bits will be registered in the build's artifact namespace under the given identifier. Subsequent  <code>task</code> step and <code>put</code> step which list the identifier as an input will have a copy of the bits  in their working directory.</p> Fetching a repo and passing it to a task <p>Almost every simple job will look something like this: fetch my code with a <code>get</code> step and do  something (run tests) with it in a <code>task</code> step.</p> <pre><code>jobs:\n  - name: fetch-repo\n    plan:\n      - get: repo # fetches repo under artifact name \"repo\"\n      - task: ls-repo\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          # pass the \"repo\" artifact into the task\n          inputs:\n            - name: repo\n          run:\n            path: ls\n            args:\n              - \"-lah\"\n              - \"repo\"\n\nresources:\n  - name: repo\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/get/#resource","title":"<code>resource</code>","text":"<p>Defaults to the value of <code>get</code>. The resource to fetch, as configured in  <code>pipeline.resources</code>.</p> <p>Use this attribute to rename a resource from the overall pipeline context into the job-specific context.</p> Re-labeling artifact <pre><code>jobs:\n  - name: fetch-repo\n    plan:\n      - get: thecode # fetches \"repo\" under artifact name \"thecode\"\n        resource: repo\n      - task: ls-repo\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          # pass the \"thecode\" artifact into the task\n          inputs:\n            - name: thecode\n          run:\n            path: ls\n            args:\n              - \"-lah\"\n              - \"thecode\"\n\nresources:\n  - name: repo\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/get/#passed","title":"<code>passed</code>","text":"<p>When specified, only the versions of the resource that made it through the given list of jobs (AND-ed together) will be considered when triggering and fetching.</p> Fanning out and in <p>If multiple <code>get</code>s are configured with <code>passed</code> constraints, all the mentioned jobs are correlated.</p> <pre><code>jobs:\n  - name: lvl-1-firewall\n    plan:\n      - in_parallel:\n          - get: black-ice\n          - get: control-node\n          - get: cyberdeck\n\n  - name: lvl-2-unit\n    plan:\n      - in_parallel:\n          - get: black-ice\n            passed:\n              - lvl-1-firewall\n          - get: control-node\n            passed:\n              - lvl-1-firewall\n          - get: cyberdeck\n            passed:\n              - lvl-1-firewall\n\n  - name: lvl-2-integration\n    plan:\n      - in_parallel:\n          - get: black-ice\n            passed:\n              - lvl-1-firewall\n          - get: control-node\n            passed:\n              - lvl-1-firewall\n          - get: cyberdeck\n            passed:\n              - lvl-1-firewall\n\n  - name: lvl-3-production\n    plan:\n      - in_parallel:\n          - get: black-ice\n            passed:\n              - lvl-2-unit\n              - lvl-2-integration\n          - get: control-node\n            passed:\n              - lvl-2-unit\n              - lvl-2-integration\n          - get: cyberdeck\n            passed:\n              - lvl-2-unit\n              - lvl-2-integration\n\nresources:\n  - name: black-ice\n    type: mock\n    source:\n      initial_version: lvl4\n  - name: control-node\n    type: mock\n    source:\n      initial_version: tower\n  - name: cyberdeck\n    type: mock\n    source:\n      initial_version: mk3\n</code></pre> <p>For the final job, <code>lvl-3-production</code>, only versions that have passed the previous two jobs (<code>lvl-2-unit</code> and  <code>lvl-2-integration</code>) will be passed to <code>lvl-3-production</code>.</p> <p>This is crucial to being able to implement safe \"fan-in\" semantics as things progress through a pipeline.</p>"},{"location":"docs/steps/get/#params","title":"<code>params</code>","text":"<p>Arbitrary configuration to pass to the resource. Refer to the resource type's documentation to see what it supports.</p> Fetching with params <pre><code>jobs:\n  - name: resource-params\n    plan:\n      - get: cyberdeck\n        params:\n          create_files_via_params:\n            version_to_put.txt: \"made-via-params\"\n      - put: cyberdeck\n        params:\n          file: cyberdeck/version_to_put.txt\n\n\nresources:\n  - name: cyberdeck\n    type: mock\n</code></pre>"},{"location":"docs/steps/get/#trigger","title":"<code>trigger</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, new builds of the job will be automatically created when a new version for this input becomes available.</p> <p>Note</p> <p>If none of a job's <code>get</code> steps are set to <code>true</code>, the job can only be manually triggered.</p> Automatically trigger job on new version <pre><code>jobs:\n  - name: fetch-repo\n    plan:\n      - get: repo\n        trigger: true # automatically runs the job\n      - task: ls-repo\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          inputs:\n            - name: repo\n          run:\n            path: ls\n            args: [ \"-lah\",\"repo\" ]\n\nresources:\n  - name: repo\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/get/#version","title":"<code>version</code>","text":"<p>Default <code>latest</code>. The version of the resource to fetch.</p> <p>If set to <code>latest</code>, scheduling will just find the latest available version of a resource and use it, allowing  versions to be skipped. This is usually what you want, e.g. if someone pushes 100 git commits.</p> <p>If set to <code>every</code>, builds will walk through all available versions of the resource. </p> <p>Note</p> <p>If <code>passed</code> is also configured, it will only step through the versions satisfying the constraints.</p> <p>If set to a specific version (e.g. <code>{ref: abcdef123}</code>), only that version will be used. </p> <p>Note</p> <p>The version must be available and detected by the resource, otherwise the input will never be satisfied. You may want to use <code>fly check-resource</code> to force detection of  resource versions, if you need to use an older one that was never detected (as all newly configured resources  start from the latest version).</p>"},{"location":"docs/steps/in-parallel/","title":"In Parallel Step","text":""},{"location":"docs/steps/in-parallel/#in_parallel-step","title":"<code>in_parallel</code> Step","text":"<p>Performs the given steps in parallel. If any sub-steps in a <code>parallel</code> result in a failure or error, the parallel step as a whole is considered to have failed or errored. Expand each section below for more details and examples.</p> <code>in_parallel</code>: <code>[step]</code> | <code>in_parallel_config</code> (required) <p>Steps are either configured as a array or within an <code>in_parallel_config</code> schema.</p> Fetching artifacts in parallel <p>Using the <code>in_parallel</code> step where possible is the easiest way to speeding up a builds.</p> <p>It is often used to fetch all dependent resources together at the start of a build plan:</p> <pre><code>jobs:\n  - name: get-in-parallel\n    plan:\n      - in_parallel:\n          - get: ci\n          - get: repo\n          - get: code\n\nresources:\n  - name: repo\n    type: mock\n  - name: code\n    type: mock\n  - name: ci\n    type: mock\n</code></pre> Running a build matrix <p>If any step in the <code>in_parallel</code> fails, the build will fail, making it useful for build matrices:</p> <pre><code>plan:\n  - get: some-repo\n  - in_parallel:\n      - task: unit-windows\n        file: some-repo/ci/windows.yml\n      - task: unit-linux\n        file: some-repo/ci/linux.yml\n      - task: unit-darwin\n        file: some-repo/ci/darwin.yml\n</code></pre>"},{"location":"docs/steps/in-parallel/#in_parallel_config-schema","title":"<code>in_parallel_config</code> schema","text":"<p>Instead of passing in a list of steps to <code>in_parallel</code> you can pass in the following fields. The list of steps will fall under the <code>steps</code> field.</p> <code>steps</code>: <code>[step]</code> (required) <p>The steps to perform in parallel.</p> Fetching artifacts in parallel <p>Using the <code>in_parallel</code> step where possible is the easiest way to speeding up a builds.</p> <p>It is often used to fetch all dependent resources together at the start of a build plan:</p> <pre><code>jobs:\n  - name: get-in-parallel\n    plan:\n      - in_parallel:\n          limit: 2\n          fail_fast: false\n          steps:\n            - get: ci\n            - get: repo\n            - get: code\n\n\nresources:\n  - name: repo\n    type: mock\n  - name: code\n    type: mock\n  - name: ci\n    type: mock\n</code></pre> <code>limit</code>: <code>number</code> <p>Default unlimited. A semaphore which limits the parallelism when executing the steps in an <code>in_parallel</code> step.  When set, the number of running steps will not exceed the limit.</p> <p>When not specified, <code>in_parallel</code> will execute all steps immediately.</p> Limiting parallelism <p>Using <code>limit</code> is useful for performing parallel execution of a growing number of tasks without overloading your  workers. In the example below, two tasks will be run in parallel and in order until all steps have been  executed:</p> <pre><code>jobs:\n  - name: limit-in-parallel\n    plan:\n      - get: examples\n      - in_parallel:\n          limit: 2\n          steps:\n            - task: print-date\n              file: examples/tasks/print-date.yml\n            - task: hello-world\n              file: examples/tasks/hello-world.yml\n            - task: print-var\n              file: examples/tasks/print-var.yml\n              vars:\n                my-var: hello\n                second-var: good-bye\n\n\nresources:\n  - name: examples\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre> <code>fail_fast</code>: <code>boolean</code> <p>Default <code>false</code>. When enabled, the parallel step will fail fast by returning as soon as any sub-step fails. This  means that running steps will be interrupted and pending steps will no longer be scheduled.</p>"},{"location":"docs/steps/load-var/","title":"Load Var Step","text":""},{"location":"docs/steps/load-var/#load_var-step","title":"<code>load_var</code> Step","text":"<p>Load the value for a var at runtime from a file, making it available to subsequent steps as a local build var named after the given identifier. Expand each section below for more details and examples.</p> <code>load_var</code>: <code>identifier</code> (required) <code>file</code>: <code>file-path</code> (required) <code>format</code>: <code>json</code> | <code>yaml</code> | <code>yml</code> | <code>trim</code> | <code>raw</code> <code>reveal</code>: <code>boolean</code>"},{"location":"docs/steps/load-var/#load_var","title":"<code>load_var</code>","text":"<p>The identifier will be the name of var, available to subsequent steps as a local build var.</p> Loading a simple value as a var <p>The following pipeline loads vars from a text file whose contents are used as a version number to  <code>put</code>.</p> <pre><code>jobs:\n  - name: loading-vars\n    plan:\n      - get: examples\n      - load_var: version\n        file: examples/misc/simple-value.txt\n      - put: img\n        params:\n          version: ((.:version))\n\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n  - name: img\n    type: mock\n</code></pre> <p><code>simple-value.txt</code> looks like this:</p> <pre><code>2.6.0\n</code></pre>"},{"location":"docs/steps/load-var/#file","title":"<code>file</code>","text":"<p>The path to a file whose content shall be read and used as the var's value.</p>"},{"location":"docs/steps/load-var/#format","title":"<code>format</code>","text":"<p>The format of the file's content.</p> <p>If unset, Concourse will try to detect the format from the file extension. If the file format cannot be determined,  Concourse will fallback to <code>trim</code>.</p> <p>If set to <code>json</code>, <code>yaml</code>, or <code>yml</code>, the file content will be parsed accordingly and the resulting structure will be  the value of the var.</p> <p>If set to <code>trim</code>, the var will be set to the content of the file with any trailing and leading whitespace removed.</p> <p>If set to <code>raw</code>, the var will be set to the content of the file without modification (i.e. with any existing  whitespace).</p> Loading a var with multiple fields <p>Let's say we have a file with multiple fields, like this <code>yaml</code> file:</p> <pre><code>first: initial\nnumber: \"9000\"\nhello: HAL\n</code></pre> <p>We could pass these values to subsequent steps by loading it into a var with <code>load_var</code>, which will detect that  it is in YAML format based on the file extension:</p> <pre><code>jobs:\n  - name: loading-vars\n    plan:\n      - get: examples\n      - load_var: version\n        file: examples/pipelines/vars-file.yml\n      - put: img\n        params:\n          version: \"((.:version.hello))-((.:version.number))\"\n\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n  - name: img\n    type: mock\n</code></pre> <p>If the file <code>vars-file.yml</code> was generated in a task and printed these values, they would be automatically  redacted unless <code>reveal: true</code> is set.</p>"},{"location":"docs/steps/load-var/#reveal","title":"<code>reveal</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, allow the var's content to be printed in the build output even with secret  redaction enabled.</p>"},{"location":"docs/steps/put/","title":"Put Step","text":""},{"location":"docs/steps/put/#put-step","title":"<code>put</code> Step","text":"<p>Pushes to the given resource. Expand each section below for more details and examples.</p> <code>put</code>: <code>resource.name</code> | <code>identifier</code> (required) <code>resource</code>: <code>resource.name</code> <code>inputs</code>: <code>detect</code> | <code>all</code> | <code>[identifier]</code> <code>params</code>: <code>config</code> <code>get_params</code>: <code>config</code> <code>no_get</code>: <code>boolean</code>"},{"location":"docs/steps/put/#put","title":"<code>put</code>","text":"<p>When the step succeeds, the version by the step will be immediately fetched via an additional implicit  <code>get</code> step. This is so that later steps in your plan can use the artifact that was produced. The artifact  will be available under the identifier <code>put</code> specifies.</p> Getting and Putting <p>The following plan fetches a version using <code>get</code> and pushes it to another resource using <code>put</code>:</p> <pre><code>jobs:\n  - name: get-and-pull\n    plan:\n      - get: the-ice\n      - put: cyberdeck\n        params:\n          file: the-ice/version.txt\n\nresources:\n  - name: the-ice\n    type: mock\n    source:\n      create_files:\n        version.txt: \"made-via-source\"\n  - name: cyberdeck\n    type: mock\n</code></pre>"},{"location":"docs/steps/put/#resource","title":"<code>resource</code>","text":"<p>Defaults to the value of <code>put</code>. The resource to fetch, as configured in  <code>pipeline.resources</code>.</p> <p>Use this attribute to rename a resource from the overall pipeline context into the job-specific context.</p> Re-labeling Put Resource <pre><code>jobs:\n  - name: fetch-repo\n    plan:\n      # puts to \"repo\" and fetches new version under artifact name \"thecode\"\n      - put: thecode\n        resource: repo\n        params:\n          version: put-only\n      - task: ls-repo\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          # pass the \"thecode\" artifact into the task\n          inputs:\n            - name: thecode\n          run:\n            path: ls\n            args:\n              - \"-lah\"\n              - \"thecode\"\n\nresources:\n  - name: repo\n    type: mock\n</code></pre>"},{"location":"docs/steps/put/#inputs","title":"<code>inputs</code>","text":"<p>Default <code>detect</code>.</p> <p>When not set, or set to <code>detect</code>, the artifacts are detected based on the configured <code>put</code> step <code>params</code> by looking for all string values and using the first path segment as an identifier.</p> <p>If set to <code>all</code>, all artifacts will be provided. This can result in slow performance if the prior steps in the build plan register a bunch of large artifacts before this step, so you may want to consider being explicit.</p> <p>If configured as a list of identifiers, only the listed artifacts will be provided to the container.</p> Put Input Methods <pre><code>jobs:\n  - name: put-input-methods\n    plan:\n      - in_parallel:\n          - get: repo-dev\n          - get: repo-master\n          - get: app-image\n          - get: ci\n      - put: detect-inputs\n        resource: repo\n        inputs: detect # default, will only stream the \"ci\" artifact\n        params:\n          file: ci/version.txt\n      - put: all-inputs\n        resource: repo\n        inputs: all # will stream all artifacts\n        params:\n          file: ci/version.txt\n      - put: explicit-inputs\n        resource: repo\n        inputs: # explicitly list artifacts to stream to put step\n          - ci\n        params:\n          file: ci/version.txt\n\nresources:\n  - name: repo\n    type: mock\n  - name: repo-dev\n    type: mock\n  - name: repo-master\n    type: mock\n  - name: app-image\n    type: mock\n  - name: ci\n    type: mock\n    source:\n      create_files:\n        version.txt: \"42\"\n</code></pre>"},{"location":"docs/steps/put/#params","title":"<code>params</code>","text":"<p>Arbitrary configuration to pass to the resource. Refer to the resource type's documentation to see what it supports.</p> Putting with params <pre><code>jobs:\n  - name: resource-params\n    plan:\n      - put: cyberdeck\n        params:\n          version: \"made-via-params\"\n\nresources:\n  - name: cyberdeck\n    type: mock\n</code></pre>"},{"location":"docs/steps/put/#get_params","title":"<code>get_params</code>","text":"<p>Arbitrary configuration to pass to the resource during the implicit <code>get</code> step. Refer to the resource type's  documentation to see what it supports.</p> Parameterizing the implicit <code>get</code> <p>You can control the settings of the implicit <code>get</code> step by setting <code>get_params</code>. For example, if you did not  want a <code>put</code> step utilizing the <code>registry-image</code> resource  type to download the image, you would implement your  <code>put</code> step as such:</p> <pre><code>plan:\n  - put: app-image\n    params:\n      build: git-resource\n    get_params:\n      skip_download: true\n</code></pre>"},{"location":"docs/steps/put/#no_get","title":"<code>no_get</code>","text":"<p>Skips the get step that usually follows the completion of the put step. This is useful to set if your <code>put</code> steps are at the very end of your job and no further steps would use the artifact generated by the implicit <code>get</code> step.</p>"},{"location":"docs/steps/set-pipeline/","title":"Set Pipeline Step","text":""},{"location":"docs/steps/set-pipeline/#set_pipeline-step","title":"<code>set_pipeline</code> Step","text":"<p>Configures a pipeline. Expand each section below for more details and examples.</p> <p>Pipelines configured with the <code>set_pipeline</code> step are connected to the job that configured them and will be automatically archived in the following scenarios:</p> <ul> <li>When the job that previously set a pipeline runs a successful build which did not configure the pipeline (i.e.   the <code>set_pipeline</code> step was removed for that specific pipeline).</li> <li>When the job is removed from its pipeline configuration (see <code>job.old_name</code> for renaming instead of   removing).</li> <li>When the job's pipeline is archived or destroyed.</li> </ul> <p>This means any job that uses <code>set_pipeline</code> should set all still-desired pipelines in each build, rather than setting them one-by-one through many builds.</p> <p>See <code>fly archive-pipeline</code> for what happens when a pipeline is archived.</p> <code>set_pipeline</code>: <code>identifier</code> | <code>self</code> (required) <code>file</code>: <code>file-path</code> (required) <code>instance_vars</code>: <code>vars</code> <code>vars</code>: <code>vars</code> <code>vars_files</code>: <code>file-path</code> <code>team</code>: <code>identifier</code>"},{"location":"docs/steps/set-pipeline/#set_pipeline","title":"<code>set_pipeline</code>","text":"<p>The identifier specifies the name of the pipeline to configure. Unless <code>set_pipeline</code> step <code>team</code> is set, it will be configured within the current team and be created unpaused. If set to self, the current pipeline will update its  own config.</p> One pipeline configuring another <p>This is a way to ensure a pipeline stays up to date with its definition in a source code repository, eliminating the need to manually run <code>fly set-pipeline</code>.</p> <pre><code>jobs:\n  - name: set-pipeline\n    plan:\n      - get: examples\n        trigger: true\n      - set_pipeline: hello-world  # pipeline's name\n        file: examples/pipelines/hello-world.yml  # pipeline's config\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/set-pipeline/#file","title":"<code>file</code>","text":"<p>The path to the pipeline's configuration file.</p> <p><code>file</code> points at a <code>.yml</code> file containing the pipeline configuration, which allows this to be tracked with your  resources or generated by a <code>task</code> step.</p> <p>The first segment in the path should refer to another artifact from the plan, and the rest of the path is relative to that artifact.</p> Fetching and configuring a pipeline <p>The <code>get</code> step can be used to fetch your configuration from a <code>git</code> repo and autoconfigure it using a  <code>set_pipeline</code> step:</p> <pre><code>jobs:\n  - name: set-pipeline\n    plan:\n      - get: examples\n        trigger: true\n      - set_pipeline: hello-world  # pipeline's name\n        file: examples/pipelines/hello-world.yml  # pipeline's config\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/set-pipeline/#instance_vars","title":"<code>instance_vars</code>","text":"<p>A map of instance vars used to identify instanced pipelines. These vars will  also be interpolated into the pipeline config.</p> <p>Note</p> <p>Variables set with this field will not propagate to tasks configured via <code>task</code> step <code>file</code>. If you  want those variables to be determined at the time the pipeline is set, use <code>task</code> step <code>vars</code> as  well.</p> <p>Info</p> <p>Instance pipelines are not enabled by default if your Concourse is on a version &lt;v8. To enable this feature, set the <code>--enable-pipeline-instances</code> flag or env var <code>CONCOURSE_ENABLE_PIPELINE_INSTANCES</code> to <code>true</code> on the <code>web</code> node.</p> Configuring instance vars <p>The following pipeline will create one instance group with three pipelines. The instance group is called  <code>my-bots</code> and each pipeline has a different set of <code>instance_vars</code> making it distinct from the other pipelines  in the instance group.</p> <pre><code>jobs:\n  - name: set-pipeline-instance-group\n    plan:\n      - get: examples\n      - in_parallel:\n          - set_pipeline: my-bots\n            file: examples/pipelines/pipeline-vars.yml\n            instance_vars:\n              first: initial\n              number: \"9000\"\n              hello: HAL\n          - set_pipeline: my-bots\n            file: examples/pipelines/pipeline-vars.yml\n            instance_vars:\n              first: second\n              number: \"3000\"\n              hello: WALLY-E\n          - set_pipeline: my-bots\n            file: examples/pipelines/pipeline-vars.yml\n            instance_vars:\n              first: the-third\n              number: \"6000\"\n              hello: R2D2\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre> Configuring instance vars and vars <p>Both <code>instance_vars</code> and <code>vars</code> may be statically. The difference between the two fields is that <code>instance_vars</code> are used to identify a pipeline and render the pipeline config. <code>vars</code> are only used for rendering the pipeline  config:</p> <pre><code>jobs:\n  - name: set-pipeline-vars-and-instance-vars\n    plan:\n      - get: examples\n      - set_pipeline: my-bots\n        file: examples/pipelines/pipeline-vars.yml\n        instance_vars:\n          first: initial\n          number: \"9000\"\n        vars:\n          hello: HAL\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/set-pipeline/#vars","title":"<code>vars</code>","text":"<p>A map of template variables to pass to the pipeline config. Unlike <code>instance_vars</code>, <code>vars</code> are solely used to for  interpolation, and do not become a part of the pipeline's identifier.</p> <p>Note</p> <p>Variables set with this field will not propagate to tasks configured via <code>task</code> step <code>file</code>. If you  want those variables to be determined at the time the pipeline is set, use <code>task</code> step <code>vars</code> as  well.</p> Configuring static vars <pre><code>jobs:\n  - name: set-pipeline-vars-only\n    plan:\n      - get: examples\n      - set_pipeline: pipeline-set-with-vars\n        file: examples/pipelines/pipeline-vars.yml\n        vars:\n          first: initial\n          number: \"9000\"\n          hello: HAL\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/set-pipeline/#vars_files","title":"<code>vars_files</code>","text":"<p>A list of paths to <code>.yml</code> files that will be passed to the pipeline config in the same manner as the  <code>--load-vars-from</code> flag to <code>fly set-pipeline</code>. This means that if a variable appears in multiple files, the value from a file that is passed later in the list will override the  values from files earlier in the list.</p> Configuring static vars with a vars file <p>Where the vars file looks like:</p> <pre><code>first: initial\nnumber: \"9000\"\nhello: HAL\n</code></pre> <p>And the pipeline config is:</p> <pre><code>jobs:\n  - name: set-pipeline-vars-only\n    plan:\n      - get: examples\n      - set_pipeline: pipeline-set-with-vars\n        file: examples/pipelines/pipeline-vars.yml\n        var_files:\n          - examples/pipelines/vars-file.yml\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/set-pipeline/#team","title":"<code>team</code>","text":"<p>By default, the <code>set_pipeline</code> step sets the pipeline for the same team that is  running the build.</p> <p>The <code>team</code> attribute can be used to specify another team.</p> <p>Only the <code>main</code> team is allowed to set another team's pipeline. Any team other  than the <code>main</code> team using the <code>team</code> attribute will error, unless they reference  their own team.</p> Setting a pipeline on another team <pre><code>jobs:\n  - name: set-pipeline\n    plan:\n      - get: examples\n        trigger: true\n      - set_pipeline: hello-world\n        file: examples/pipelines/hello-world.yml\n        team: other-team  # name of the team goes here\n\nresources:\n  - name: examples\n    type: git\n    icon: github\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/task/","title":"Task Step","text":""},{"location":"docs/steps/task/#task-step","title":"<code>task</code> Step","text":"<p>Executes a task. Expand each section below for more details and examples.</p> <p>When a task completes, the artifacts specified by <code>task-config.outputs</code> will be registered in the build's artifact namespace. This allows subsequent <code>task</code> steps and <code>put</code> steps to access the result of a task.</p> <code>task</code>: <code>identifier</code> (required) <code>config</code>: <code>task-config</code> <code>file</code>: <code>file-path</code> <code>image</code>: <code>identifier</code> <code>privileged</code>: <code>boolean</code> <code>vars</code>: <code>vars</code> <code>params</code>: <code>env-vars</code> <code>container_limits</code>: <code>container_limits</code> <p>CPU and memory limits to enforce on the task container.</p> <p>Note that these values, when specified, will override any limits set by passing the <code>--default-task-cpu-limit</code> or  <code>--default-task-memory-limit</code> flags to the <code>concourse web</code> command.</p> <p>These values will also override any configuration set on a task's config  <code>container_limits</code>.</p> <code>hermetic</code>: <code>boolean</code> <code>input_mapping</code>: { <code>input.name</code> : <code>identifier</code> } <code>output_mapping</code>: { <code>output.name</code> : <code>identifier</code> }"},{"location":"docs/steps/task/#task","title":"<code>task</code>","text":"<p>The identifier value is just a name - short and sweet. The value is shown in the web UI but otherwise has no effect  on anything. This may change in the future; RFC #32 proposes that the  name be used to reference a file within the project.</p> Functions from inputs to outputs <p>You can think of tasks like functions. They have predefined inputs and outputs and can be written in idempotent ways.</p> <p>The following pipeline contains a function that increments a number. You can think of the task add-one like this pseudo-function:</p> <pre><code>func AddOne(num int) int {\n  return num + 1\n}\n</code></pre> <pre><code>jobs:\n  - name: idempotent-task\n    plan:\n      - get: counter\n      - task: add-one\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          inputs:\n            - name: counter\n          outputs:\n            - name: counter\n          run:\n            path: sh\n            args:\n              - -c\n              - |\n                COUNTER=$(cat counter/version)\n                NEXT=$(($COUNTER + 1))\n                echo \"new version: $NEXT\"\n                echo $NEXT &gt; counter/next\n      - put: counter\n        params:\n          file: counter/next\n\nresources:\n  - name: counter\n    type: mock\n    source:\n      initial_version: \"1\"\n</code></pre>"},{"location":"docs/steps/task/#config","title":"<code>config</code>","text":"<p>The task config to execute.</p> Task Config <pre><code>jobs:\n  - name: job\n    public: true\n    plan:\n      - task: simple-task\n        config: # contains all field in a task config\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          run:\n            path: echo\n            args:\n              - \"Hello world!\"\n</code></pre>"},{"location":"docs/steps/task/#file","title":"<code>file</code>","text":"<p>A dynamic alternative to <code>task</code> step <code>config</code>.</p> <p><code>file</code> points at a <code>.yml</code> file containing the task config, which allows this to be tracked with your resources.</p> <p>The first segment in the path should refer to another source from the plan, and the rest of the path is relative to  that source.</p> <p>The content of the config file may contain template <code>((vars))</code>, which will be filled in using <code>task</code> step <code>vars</code> or  a configured credential manager.</p> Using a task config file <p>Uses this config file:</p> <pre><code>platform: linux\n\nimage_resource:\n  type: mock\n  source: { mirror_self: true }\n\nrun:\n  path: echo\n  args: [\"Hello world!\"]\n</code></pre> <pre><code>jobs:\n  - name: task-config-in-file\n    plan:\n      - get: ci\n      - task: config-from-file\n        file: ci/tasks/hello-world.yml\n\nresources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/task/#image","title":"<code>image</code>","text":"<p>Specifies an artifact source containing an image to use for the task. This overrides any  <code>task-config.image_resource</code> configuration present in the task configuration.</p> <p>This is very useful when part of your pipeline involves building an image, possibly with dependencies pre-baked. You can then propagate that image through the rest of your pipeline, guaranteeing that the correct version (and thus a  consistent set of dependencies) is used throughout your pipeline.</p> Fetching and using an image <p>This can be used to explicitly keep track of dependent images. You could also modify it to build and push the  image in one job and use it in later jobs. See Building and Pushing an  Image.</p> <pre><code>resources:\n  - name: golang\n    type: registry-image\n    source:\n      repository: golang  # could also be the full URL \"docker.io/golang\"\n      tag: \"1.17\"\n\njobs:\n  - name: fetch-and-run-image\n    plan:\n      - get: golang\n      - task: use-fetched-image-in-task\n        image: golang   # reference the image from the get step\n        config:\n          platform: linux\n          run:\n            path: go\n            args:\n              - \"version\"\n</code></pre> Building and using an image <p>Building an Image and Using it in a Task</p>"},{"location":"docs/steps/task/#privileged","title":"<code>privileged</code>","text":"<p>Default <code>false</code>. If set to <code>true</code>, the task will run with escalated capabilities available on the task's platform.</p> <p>Warning</p> <p>Setting <code>privileged: true</code> is a gaping security hole; use wisely and only if necessary. This is not part of the task configuration in order to prevent privilege escalation via pull requests changing the task file.</p> <p>For the <code>linux</code> platform, this determines whether the container will run in a separate user namespace. When set to  <code>true</code>, the container's <code>root</code> user is actual <code>root</code>, i.e. not in a user namespace. This is not recommended, and  should never be used with code you do not trust - e.g. pull requests.</p> <p>For macOS and Windows this field has no effect since workloads on those machines are not containerized.</p>"},{"location":"docs/steps/task/#vars","title":"<code>vars</code>","text":"<p>A map of template variables to pass to an external task. Not to be confused with <code>task</code> step <code>params</code>, which  provides environment variables to the task.</p> <p>This is to be used with external tasks defined in <code>task</code> step <code>file</code>.</p> Parameterized a task config file with vars <p>A var may be statically passed like so:</p> <pre><code>jobs:\n  - name: task-vars\n    plan:\n      - get: ci\n      - task: override-task-vars\n        file: ci/tasks/print-var.yml\n        vars: # statically defined vars\n          my-var: \"Cookies are the best\"\n          second-var: \"chips are a close second\"\n\nresources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre> <p>When run with the following task config:</p> <pre><code>platform: linux\n\nimage_resource:\n  type: mock\n  source:\n    mirror_self: true\n\nparams:\n  MY_VAR: ((my-var))\n\nrun:\n  path: sh\n  args:\n    - -c\n    - |\n      echo ${MY_VAR} and ((second-var))\n</code></pre> <p>The <code>\"((my-var))\"</code> will be resolved to <code>\"Cookies are the best\"</code> and <code>((second-var))</code> will be resolved to  <code>\"chips are a close second\"</code>.</p> <p>This can also be used in combination with Vars from a credential  manager (i.e. Vault) as a way to re-map variable names to match what the task is  expecting:</p> <pre><code>jobs:\n  - name: task-vars\n    plan:\n      - get: ci\n      - task: override-task-vars\n        file: ci/tasks/print-var.yml\n        vars: # re-mapped vars\n          my-var: ((var-from-vault))\n          second-var: ((apple.type))\n\nresources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/task/#params","title":"<code>params</code>","text":"<p>A map of task environment variable parameters to set, overriding those configured in the task's <code>config</code> or <code>file</code>.</p> <p>The difference between <code>params</code> and <code>vars</code> is that <code>vars</code> allows you to interpolate any template variable in an  external task file, while <code>params</code> can be used to overwrite task parameters specifically. Also, <code>params</code> can have  default values declared in the task.</p> Running a task with env var params <p>Let's say we have a task config like so:</p> <pre><code>platform: linux\n\nimage_resource:\n  type: mock\n  source:\n    mirror_self: true\n\nparams:\n  ECHO_ME: \"default text to echo from task config file\"\n  ALSO_ME:\n\nrun:\n  path: sh\n  args:\n    - -c\n    - |\n      echo ${ECHO_ME} and ${ALSO_ME}\n</code></pre> <p>This indicates that there are two params which can be set: <code>ECHO_ME</code>, which has a default, and <code>ALSO_ME</code> which  has no default set.</p> <p>A pipeline could run the task with values passed in like so:</p> <pre><code>jobs:\n  - name: task-params\n    plan:\n      - get: ci\n      - task: constrained-task\n        file: ci/tasks/print-param.yml\n        params:\n          ECHO_ME: \"Eat your fruits\"\n          ALSO_ME: \"veggies\"\n\nresources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre> Using with <code>((vars))</code> <pre><code>jobs:\n  - name: task-params\n    plan:\n      - get: ci\n      - task: constrained-task\n        file: ci/tasks/print-param.yml\n        params:\n          ECHO_ME: ((some-var))\n          ALSO_ME: ((another-var))\n\nresources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/task/#container_limits-schema","title":"<code>container_limits</code> schema","text":"<code>cpu</code>: <code>number</code> <p>The maximum amount of CPU available to the task container, measured in shares. 0 means unlimited.</p> <p>CPU shares are relative to the CPU shares of other containers on a worker. For example, if you have two  containers both with a CPU limit of 2 shares then each container will get 50% of the CPU's time.</p> <pre><code>Container A: 2 shares - 50% CPU\nContainer B: 2 shares - 50% CPU\nTotal CPU shares declared: 4\n</code></pre> <p>If you introduce another container then the number of CPU time per container changes. CPU shares are relative to each other.</p> <pre><code>Container A: 2 shares - 25% CPU\nContainer B: 2 shares - 25% CPU\nContainer C: 4 shares - 50% CPU\nTotal CPU shares declared: 8\n</code></pre> <code>memory</code>: <code>number</code> <p>The maximum amount of memory available to the task container, measured in bytes. 0 means unlimited.</p> Setting CPU and Memory limits <p>This task will only be given 10MB of memory and 2 CPU shares.</p> <pre><code>jobs:\n  - name: limited-resources\n    plan:\n      - task: constrained-task\n        container_limits:\n          cpu: 2 # CPU shares are relative\n          memory: 10000000 # 10MB\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source:\n              repository: busybox\n          run:\n            path: echo\n            args:\n              - \"Hello world!\"\n</code></pre>"},{"location":"docs/steps/task/#hermetic","title":"<code>hermetic</code>","text":"<p>Warning</p> <p>This setting is only supported by the <code>containerd</code> runtime on Linux. For other runtimes this setting has no  effect on container networking. Please contact your Concourse operator to find out what runtime your Concourse  cluster is using.</p> <p>Default <code>false</code>. If set to <code>true</code>, the task will have no outbound network access. Your task will not be able to  reach the internet or any local network resources that aren't also inside the container.</p> <p>For macOS and Windows this field has no effect since workloads on those machines are not containerized</p>"},{"location":"docs/steps/task/#input_mapping","title":"<code>input_mapping</code>","text":"<p>A map from task input names to concrete names in the build plan. This allows a task with generic input names to be  used multiple times in the same plan, mapping its inputs to specific resources within the plan.</p> Generic task input names <p>The following example demonstrates a task with generic <code>main</code> and <code>dev</code> inputs being mapped to more specific  artifact names, <code>repo</code> and <code>repo-dev</code>:</p> <pre><code>jobs:\n  - name: task-input-mapping\n    plan:\n      - in_parallel:\n          - get: repo\n          - get: repo-dev\n          - get: ci\n      - task: list-inputs\n        input_mapping:\n          main: repo\n          dev: repo-dev\n        file: ci/tasks/generic-inputs.yml\n\nresources:\n  - name: repo\n    type: mock\n  - name: repo-dev\n    type: mock\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/task/#output_mapping","title":"<code>output_mapping</code>","text":"<p>A map from task output names to concrete names to register in the build plan. This allows a task with generic output names to be used multiple times in the same plan.</p> Using with <code>input_mapping</code> <p>This is often used together with <code>task</code> step <code>input_mapping</code>:</p> <p>Given this task config:</p> <pre><code>platform: linux\nimage_resource:\n  type: mock\n  source:\n    mirror_self: true\n\ninputs:\n  - name: main\n  - name: dev\n\noutputs:\n  - name: main\n  - name: dev\n\nrun:\n  path: sh\n  args:\n    - -c\n    - |\n      ls -lah\n      echo \"creating versions\"\n      echo \"hello-world\" &gt; main/version\n      echo \"hey there dev\" &gt; dev/version\n</code></pre> <p>This pipeline will map the inputs and outputs of the task to match the name of the resources in the pipeline.</p> <pre><code>jobs:\n  - name: task-output-mapping\n    plan:\n      - in_parallel:\n          - get: repo\n          - get: repo-dev\n          - get: ci\n      - task: create-outputs\n        input_mapping:\n          main: repo\n          dev: repo-dev\n        output_mapping:\n          main: repo\n          dev: repo-dev\n        file: ci/tasks/generic-outputs.yml\n      - in_parallel:\n          - put: repo\n            params:\n              file: repo/version\n          - put: repo-dev\n            params:\n              file: repo-dev/version\n\nresources:\n  - name: repo\n    type: mock\n  - name: repo-dev\n    type: mock\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n</code></pre>"},{"location":"docs/steps/try/","title":"Try Step","text":"<code>try</code>: <code>step</code> (required) <p>Performs the given step, ignoring any failure and masking it with success.</p> <p>This can be used when you want to perform some side effect, but you don't really want the whole build to fail if it doesn't work.</p> Allowing non-critical behavior to fail <p>When emitting logs somewhere for analyzing later, if the destination flakes out it may not really be critical, so we may want to just swallow the error:</p> <pre><code>plan:\n  - task: run-tests\n    config: # ...\n    on_success:\n      try:\n        put: test-logs\n        params:\n          from: run-tests/*.log\n  - task: do-something-else\n    config: # ...\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/","title":"Modifiers and Hooks","text":"<ul> <li> <p>Across</p> <p>Run a step multiple times with different combinations of variable values</p> <p> Schema</p> </li> <li> <p>Timeout</p> <p>End a step after a set amount a time</p> <p> Schema</p> </li> <li> <p>Attempts</p> <p>Run a step up to an amount of times before fully failing</p> <p> Schema</p> </li> <li> <p>Tags</p> <p>Only run this step on a specific worker</p> <p> Schema</p> </li> <li> <p>On Success</p> <p>If the job passes, run this step</p> <p> Schema</p> </li> <li> <p>On Failure</p> <p>If the job fails, run this step</p> <p> Schema</p> </li> <li> <p>On Abort</p> <p>If the job is canceled, run this step</p> <p> Schema</p> </li> <li> <p>On Error</p> <p>If the job results in an error, run this step</p> <p> Schema</p> </li> <li> <p>Ensure</p> <p>Always run this step</p> <p> Schema</p> </li> </ul>"},{"location":"docs/steps/modifier-and-hooks/across/","title":"Across Step Modifier","text":""},{"location":"docs/steps/modifier-and-hooks/across/#across-step-modifier","title":"<code>across</code> Step Modifier","text":"<p>Run a step multiple times with different combinations of variable values.</p> <p>The <code>across</code> step can be combined with the <code>load_var</code> step, the <code>set_pipeline</code> step, and instanced pipelines to maintain a dynamically sized group of related pipelines.</p> <p>More fields are also available for variable interpolation with the across step. See Across Step &amp; Dynamic Vars for details.</p> <p>Note</p> <p>Outputs from steps ran within the across step are not available to steps outside of the across step.</p> <code>across</code>: <code>[across_var]</code> <p>Contains a list of <code>across_var</code> schema.</p> <code>fail_fast</code>: <code>boolean</code> <p>Default <code>false</code>. When enabled, the <code>across</code> step will fail fast by returning as soon as any sub-step fails. This  means that running steps will be interrupted and pending steps will no longer be scheduled.</p> Fail fast <p>The <code>fail_fast</code> key sits at the same level as the <code>across</code> key.</p> <pre><code>plan:\n  - across:\n      - var: var1\n        values:\n          - a\n          - b\n          - c\n      - var: var2\n        values:\n          - 1\n          - 2\n    fail_fast: true\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/across/#across_var-schema","title":"<code>across_var</code> schema","text":"var: <code>identifier</code> (required) <p>The name of the variable that will be added to the \"<code>.</code>\" var source. This variable  will only be accessible in the scope of the step - each iteration of the step gets its own scope.</p> <p>If a variable of the same name already exists in the parent scope, a warning will be printed.</p> values: <code>[value]</code> (required) <p>The list of values that the var will iterate over when running the substep. If multiple vars are configured, all combinations of values across all vars will run.</p> <p>The list of values may also be interpolated. For instance, you may use the <code>load_var</code> step to  first load a list of <code>value</code> schema into a  local var, and then iterate across that dynamic list of values.</p> Value combinations <p>The following <code>across</code> will run the task <code>foo/build.yml</code> for each package defined in  <code>foo/packages-to-build.json</code> with Go 1.15 and 1.16.</p> <pre><code>plan:\n  - get: foo\n  - load_var: packages\n    file: foo/packages-to-build.json\n  - across:\n      - var: package\n        values: ((.:packages))\n      - var: go_version\n        values: [ '1.15', '1.16' ]\n    task: build\n    file: foo/build.yml\n    vars:\n      go_version: ((.:go_version))\n      package: ((.:package))\n</code></pre> <p>Supposing <code>foo/packages-to-build.json</code> had the following content:</p> <pre><code>[\"./cmd/first\", \"./cmd/second\", \"./cmd/third\"]\n</code></pre> <p>...then the task <code>foo/build.yml</code> would be run with the following var combinations:</p> <pre><code>[\n  {\n    package: \"./cmd/first\",\n    go_version: \"1.15\"\n  },\n  {\n    package: \"./cmd/first\",\n    go_version: \"1.16\"\n  },\n  {\n    package: \"./cmd/second\",\n    go_version: \"1.15\"\n  },\n  {\n    package: \"./cmd/second\",\n    go_version: \"1.16\"\n  },\n  {\n    package: \"./cmd/third\",\n    go_version: \"1.15\"\n  },\n  {\n    package: \"./cmd/third\",\n    go_version: \"1.16\"\n  }\n]\n</code></pre> <code>max_in_flight</code>: <code>all</code> | <code>number</code> <p>Default <code>1</code>. If set to <code>all</code>, the substep will run with all combinations of the current var in parallel. If  set to a <code>number</code> schema, only that number of substeps may run in  parallel.</p> Multiple vars <p>If multiple vars are configured, the effective <code>max_in_flight</code> is multiplicative. For instance:</p> <pre><code>plan:\n  - across:\n      - var: var1\n        values:\n          - a\n          - b\n          - c\n        max_in_flight: all\n      - var: var2\n        values:\n          - 1\n          - 2\n      - var: var3\n        values:\n          - foo\n          - bar\n        max_in_flight: 2\n</code></pre> <p>Here, 6 substeps will run in parallel, since all 3 of <code>var1</code>'s values can run in parallel, and 2 of <code>var3</code>'s values can run in parallel.</p>"},{"location":"docs/steps/modifier-and-hooks/across/#examples","title":"Examples","text":"Across with task step <pre><code>jobs:\n  - name: job\n    plan:\n      - across:\n          - var: some-text\n            values: [ \"hello-world\", \"hello-concourse\" ]\n        task: running-((.:some-text))\n        config:\n          platform: linux\n          image_resource:\n            type: mock\n            source:\n              mirror_self: true\n          run:\n            path: echo\n            args: [ \"((.:some-text))\" ]\n</code></pre> Across with input and output mapping <pre><code>resources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n\njobs:\n  - name: job\n    plan:\n      - get: ci\n      - across:\n          - var: pipeline\n            values: [ \"hello-world\", \"time-triggered\" ]\n        do:\n          - task: running-((.:pipeline))\n            input_mapping:\n              ((.:pipeline)): ci\n            output_mapping:\n              ((.:pipeline)): newci\n            config:\n              platform: linux\n              image_resource:\n                type: mock\n                source:\n                  mirror_self: true\n              inputs:\n                - name: ((.:pipeline))\n              outputs:\n                - name: ((.:pipeline))\n              run:\n                path: cat\n                args: [ \"((.:pipeline))/pipelines/((.:pipeline)).yml\" ]\n          - task: newci-((.:pipeline))\n            config:\n              platform: linux\n              image_resource:\n                type: mock\n                source:\n                  mirror_self: true\n              inputs:\n                - name: newci\n              run:\n                path: cat\n                args: [ \"newci/pipelines/((.:pipeline)).yml\" ]\n</code></pre> Across with <code>set_pipeline</code> step <pre><code>resources:\n  - name: ci\n    type: git\n    source:\n      uri: https://github.com/concourse/examples.git\n\njobs:\n  - name: job\n    plan:\n      - get: ci\n      - across:\n          - var: pipeline\n            values: [ \"hello-world\", \"time-triggered\" ]\n        set_pipeline: ((.:pipeline))\n        file: ci/pipelines/((.:pipeline)).yml\n</code></pre> Across with multiple steps <p>Use the <code>do</code> step to across over multiple steps.</p> <pre><code>jobs:\n  - name: job\n    plan:\n      - across:\n          - var: name\n            values: [ \"Kaladin\", \"Jasnah\" ]\n        do: # takes a list of steps\n          - task: saying-hello\n            config:\n              platform: linux\n              image_resource:\n                type: mock\n                source:\n                  mirror_self: true\n              run:\n                path: echo\n                args: [ \"Hello ((.:name))!\" ]\n          - task: saying-bye\n            config:\n              platform: linux\n              image_resource:\n                type: mock\n                source:\n                  mirror_self: true\n              run:\n                path: echo\n                args: [ \"Bye ((.:name))!\" ]\n</code></pre> Multi-branch workflows (instance pipelines) <p>You can use the across step to set a pipeline for each branch in a git repository.</p> <pre><code>plan:\n  - get: release-branches\n    trigger: true\n  - get: ci\n  - load_var: branches\n    file: release-branches/branches.json\n  - across:\n      - var: branch\n        values: ((.:branches))\n    set_pipeline: release\n    file: ci/pipelines/release.yml\n    instance_vars: { branch: ((.:branch.name)) }\n</code></pre> <p>When a new branch is added, a new pipeline will be created. When a branch is deleted, the pipeline will be  automatically archived as described in the <code>set_pipeline</code> step.</p> <p>For a more complete example, refer to Multi-Branch Workflows.</p>"},{"location":"docs/steps/modifier-and-hooks/across/#limitations","title":"Limitations","text":"<p>The <code>across</code> step does not work with the <code>get</code> step or <code>put</code> step. The names of resources are not interpolated within across steps. Trying to do the following will not work.</p> <pre><code>- across:\n    - var: version\n      values: [ \"1.16\", \"1.17\" ]\n  do:\n    - get: go-((.:version))\n    # or this\n    - get: golang\n      resource: go-((.version))\n</code></pre> <p>The main reason this does not work is that Concourse determines the inputs for a job before the job starts. Concourse has no way of determining inputs for a job while it's in the middle of running.</p> <p>Current pipeline validation logic will also block you from setting the pipeline at all since Concourse validates the relationship between all resources and jobs by looking at get and put steps.</p> <p>The above example will return an error like this when trying to set the pipeline:</p> <pre><code>invalid jobs:\n  jobs.job.plan.do[0].across.get(go): unknown resource 'go-((.:version))'\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/attempts/","title":"Attempts Step Modifier","text":""},{"location":"docs/steps/modifier-and-hooks/attempts/#attempts-step-modifier","title":"<code>attempts</code> Step Modifier","text":"<code>attempts</code>: <code>number</code> <p>The total number of times a step should be tried before it should fail, e.g. <code>5</code> will run the <code>step</code> up to 5 times before giving up.</p> <p>Attempts will retry on a Concourse error as well as build failure. When the number of attempts is reached and the step has still not succeeded then the step will fail.</p> Retrying a task <p>The following will run the task and retry it up to 9 times (for a total of 10 attempts) if it fails:</p> <pre><code>plan:\n  - get: foo\n  - task: unit\n    file: foo/unit.yml\n    attempts: 10\n</code></pre> Retrying with a timeout <p>When used in combination with <code>timeout</code>, the timeout applies to each step.</p> <p>This semi-arbitrary decision was made because often things either succeed in a reasonable amount of time or fail due to hanging/flakiness. In this case it seems more useful to allow each attempt the allotted timeout rather than have one very long attempt prevent more attempts.</p> <pre><code>plan:\n  - get: flake\n  - task: flaky-tests\n    file: flake/integration.yml\n    timeout: 10m\n    attempts: 3\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/ensure/","title":"Ensure Step Hook","text":""},{"location":"docs/steps/modifier-and-hooks/ensure/#ensure-step-hook","title":"<code>ensure</code> Step Hook","text":"<code>ensure</code>: <code>step</code> <p>A hook step to execute after the parent step regardless of whether the parent step succeeds, fails, or errors. The step will also be executed if the build was aborted and its parent step was interrupted.</p> <p>If the parent step succeeds and the ensured step fails, the overall step fails.</p> Releasing a lock <p>The following build plan acquires a lock and then <code>ensure</code>s that the lock is released.</p> <pre><code>plan:\n  - put: some-lock\n    params:\n      acquire: true\n  - task: integration\n    file: foo/integration.yml\n    ensure:\n      put: some-lock\n      params:\n        release: some-lock\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/on-abort/","title":"On Abort Step Hook","text":""},{"location":"docs/steps/modifier-and-hooks/on-abort/#on_abort-step-hook","title":"<code>on_abort</code> Step Hook","text":"<code>on_abort</code>: <code>step</code> <p>A hook step to execute if the build is aborted and the parent step was running and then terminated.</p> Cleaning up on abort <p>The following will perform the <code>cleanup</code> task only if the build is aborted while the <code>unit</code> task was running:</p> <pre><code>plan:\n  - get: foo\n  - task: unit\n    file: foo/unit.yml\n    on_abort:\n      task: cleanup\n      file: foo/cleanup.yml\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/on-error/","title":"On Error Step Hook","text":""},{"location":"docs/steps/modifier-and-hooks/on-error/#on_error-step-hook","title":"<code>on_error</code> Step Hook","text":"<code>on_error</code>: <code>step</code> <p>A hook step to execute after the parent step if the parent step terminates abnormally in any way other than those handled by the <code>on_abort</code> or <code>on_failure</code>. This covers scenarios as broad as configuration mistakes, temporary network issues with the workers, or running longer than a <code>timeout</code>.</p> Sending a notification <p>This step can be used to notify folks if their builds errored out:</p> <pre><code>plan:\n  - do:\n      - get: ci\n      - task: unit\n        file: ci/unit.yml\n    on_error:\n      put: slack\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/on-failure/","title":"On Failure Step Hook","text":""},{"location":"docs/steps/modifier-and-hooks/on-failure/#on_failure-step-hook","title":"<code>on_failure</code> Step Hook","text":"<code>on_failure</code>: <code>step</code> <p>A hook step to execute if the parent step fails.</p> <p>This does not \"recover\" the failure - it will still fail even if the hook step succeeds.</p> Alerting on failure <p>The following will perform the <code>alert</code> task only if the <code>unit</code> task fails:</p> <pre><code>plan:\n  - get: foo\n  - task: unit\n    file: foo/unit.yml\n    on_failure:\n      task: alert\n      file: foo/alert.yml\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/on-success/","title":"On Success Step Hook","text":""},{"location":"docs/steps/modifier-and-hooks/on-success/#on_success-step-hook","title":"<code>on_success</code> Step Hook","text":"<code>on_success</code>: <code>step</code> <p>A hook step to execute if the parent step succeeds.</p> Running on success <p>The following will perform the second task only if the first one succeeds:</p> <pre><code>plan:\n  - get: foo\n  - task: unit\n    file: foo/unit.yml\n    on_success:\n      task: alert\n      file: foo/alert.yml\n</code></pre> <p>Note that this is semantically equivalent to the following:</p> <pre><code>plan:\n  - get: foo\n  - task: unit\n    file: foo/unit.yml\n  - task: alert\n    file: foo/alert.yml\n</code></pre> <p>The <code>on_success</code> hook is provided mainly for cases where there is an equivalent <code>on_failure</code>,  and having them next to each other is more clear.</p>"},{"location":"docs/steps/modifier-and-hooks/tags/","title":"Tags Step Modifier","text":""},{"location":"docs/steps/modifier-and-hooks/tags/#tags-step-modifier","title":"<code>tags</code> Step Modifier","text":"<code>tags</code>: <code>[string]</code> <p>The tags by which to match workers. The step will be placed within the pool of workers that match all the given set of tags.</p> <p>For example, if <code>[a, b]</code> is specified, only workers advertising the a and b tags (in addition to any others) will be used for running the step.</p> Running in a private network <p>You may have a private cluster only reachable by special workers running on-premises. To run steps against those workers, just provide a matching tag:</p> <pre><code>plan:\n  - get: my-repo\n  - put: my-site\n    tags:\n      - private\n    params:\n      path: my-repo\n  - task: acceptance-tests\n    tags:\n      - private\n    file: my-repo/ci/acceptance.yml\n</code></pre>"},{"location":"docs/steps/modifier-and-hooks/timeout/","title":"Timeout Step Modifier","text":""},{"location":"docs/steps/modifier-and-hooks/timeout/#timeout-step-modifier","title":"<code>timeout</code> Step Modifier","text":"<code>timeout</code>: <code>duration</code> <p>The amount of time to limit the step's execution to, e.g. <code>30m</code> for 30 minutes.</p> <p>When exceeded, the step will be interrupted, with the same semantics as aborting the build (except the build will be <code>errored</code>, not <code>aborted</code>, to distinguish between human intervention and timeouts being enforced).</p> Giving up <p>The following will run the <code>unit</code> task and cancel it if it takes longer than 1 hour and 30 minutes:</p> <pre><code>plan:\n  - get: foo\n  - task: unit\n    file: foo/unit.yml\n    timeout: 1h30m\n</code></pre>"},{"location":"ecosystem/","title":"Ecosystem","text":"<p>Concourse is utilized by a diverse array of businesses, government agencies, open source projects and non-profit organizations. The applications of Concourse are as varied as its user community, including CI/CD for applications, continuous delivery of infrastructure, release integration, test automation, and numerous other use cases!</p> <p>If you use Concourse, or your organization offers Concourse-related services, we'd appreciate hearing from you. Please submit a pull request adding your organization's name in alphabetical order to one of the lists below, and help us showcase how many people do things continuously with Concourse.</p>"},{"location":"ecosystem/#concourse-as-a-service","title":"Concourse-as-a-Service","text":"<p>The following organizations deliver Concourse as a fully-managed cloud solution, eliminating infrastructure overhead and providing dedicated support.</p> <ul> <li>CentralCI</li> </ul>"},{"location":"ecosystem/#3rd-party-service-providers","title":"3rd Party Service Providers","text":"<p>The following organizations provide various Concourse-related services, including training, consulting, support and managed solutions.</p> <ul> <li>Altoros</li> <li>anynines</li> <li>Cycloid</li> <li>Gstack</li> <li>Pixel Air IO</li> <li>SuperOrbital</li> </ul>"},{"location":"ecosystem/#who-uses-concourse","title":"Who Uses Concourse?","text":"<p>These organizations have either added themselves to this list, or whose use of Concourse is publicly known. There are many additional Concourse users who cannot publicly disclose information about their technology stack (typically financial institutions and security firms).</p> <ol> <li>Altoros</li> <li>anynines</li> <li>Aptomi</li> <li>Armakuni</li> <li>boclips</li> <li>Cerner</li> <li>cloud.gov</li> <li>Cloud Foundry Foundation</li> <li>Comcast</li> <li>Cycloid</li> <li>Electric UI</li> <li>EngineerBetter</li> <li>Express Scripts</li> <li>Fauna</li> <li>Fidelity International</li> <li>Gardener</li> <li>(United     Kingdom) Government Digital Services</li> <li>Gstack</li> <li>The Home Depot</li> <li>IBM</li> <li>LeapYear</li> <li>Napoleon Sports &amp; Casino</li> <li>Nasdaq</li> <li>Nokogiri</li> <li>RabbitMQ</li> <li>Resilient Scale</li> <li>SAP</li> <li>Smarsh</li> <li>Springer Nature</li> <li>Stark &amp; Wayne</li> <li>SUSE</li> <li>SuperOrbital</li> <li>Unit 2 Games</li> <li>United States Air Force - Kessel Run</li> <li>Varian</li> <li>Verizon</li> <li>VMware</li> <li>Webfleet Solutions</li> <li>Yahoo!</li> <li>Zipcar</li> </ol>"},{"location":"project/","title":"Project","text":"<p>Concourse began as a side-project by <code>@vito</code> and <code>@xoebus</code> in 2014. Since then, Concourse has evolved into a dedicated community with contributors from all around the world.</p> <p>Concourse is a project of the Cloud Foundry foundation (CFF), currently lead by Taylor Silva and Derek Richard. The CFF pays for the infrastructure costs of the project. Pixel Air IO, lead by Taylor, is the main developer behind Concourse; reviewing and merging Pull Requests, squashing bugs, and stewarding the project and community.</p>"},{"location":"project/#where-is-everything","title":"Where is everything?","text":"<ul> <li>The Concourse repo houses the main codebase, where planning happens, and   where issues are tracked.</li> <li>The Docs repo contains the source for the website you're reading now!</li> <li>GitHub Discussions are used for support, announcements, idea   sharing, and general conversations.</li> <li>The Concourse blog features tutorials and updates from the development perspective.</li> <li>The Concourse Discord server offers a great space to chat with other contributors.</li> <li>The Concourse working group charter is available in the Cloud   Foundry community repo.</li> <li>The working group holds public monthly meetings. Past meetings can be viewed in   this YouTube playlist and   meeting notes   are here.</li> </ul>"},{"location":"project/#why-make-concourse","title":"Why make Concourse?","text":"<p>When working on a substantial project, having a pipeline to reliably test, deploy, and publish the product is essential for rapid iteration.</p> <p>But with every CI system we tried, we found ourselves repeatedly facing the same problems: complex configs buried in many pages of the web UI, uncertainty about who changed what &amp; when, managing dependencies and state on the workers, build contamination, frustrating UX...</p> <p>Our project was expanding, and with every box we checked and for every worker we manually configured, the anxiety of having to rebuild everything if something failed grew increasingly. We began writing software to manage our CI instead of creating the software for the product we intended to build.</p> <p>We created Concourse to be a CI system that provides peace of mind. A CI that's straightforward enough to fully understand and easy to maintain as your project grows; both in the complexity of the product and the size of your team. We aimed to build a CI with robust abstractions and fewer concepts to learn, making it easier to comprehend and allowing Concourse to evolve gracefully.</p>"},{"location":"project/#how-can-i-help","title":"How can I help?","text":"<p>Concourse is a free and Open Source software project that depends on the contributions of sponsors and volunteers worldwide.</p> <p>If you're interested in contributing, head over to GitHub and check out the contributing docs!</p> <p>If you're able to financially contribute to the continued development of Concourse, you can sponsor Taylor Silva on GitHub.</p>"},{"location":"project/#report-a-security-issue","title":"Report a security issue","text":"<p>To report a security issue, please email security@concourse-ci.org.</p> <p>Security advisories will be published as <code>concourse/concourse</code> GitHub Security Advisories.</p>"},{"location":"project/#thanks","title":"Thanks","text":"<p>It's been a long journey and we're grateful to many people for our continued success. We are deeply indebted to all who help sustain this project, but the extraordinary efforts of the following organizations deserve special recognition.</p>"},{"location":"project/#pivotal","title":"Pivotal","text":"<p>Concourse wouldn't be what it is today without Pivotal. This extends beyond the sponsorship, which began in early 2015 - without the experiences we gained and the practices we learned while working on Cloud Foundry and BOSH, we would have neither the technical expertise nor the strong opinions that led to Concourse's creation.</p>"},{"location":"support/","title":"Support","text":""},{"location":"support/#community-support","title":"Community Support","text":"<p>You can ask the community for support either on Discord or GitHub Discussions.</p>"},{"location":"support/#commercial-support","title":"Commercial Support","text":"<p>The maintainers of Concourse provide commercial support and training for Concourse through Pixel Air IO. If you're interested in commercial support you can book a call with us.</p>"},{"location":"support/#sponsorship","title":"Sponsorship","text":"<p>If you're interested in financially sponsoring Concourse, either as an individual or a corporation, you can sponsor Taylor Silva on GitHub.</p>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2026/#2026","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2020/#2020","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2019/#2019","title":"2019","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/archive/2018/#2018","title":"2018","text":""},{"location":"blog/archive/2017/","title":"2017","text":""},{"location":"blog/archive/2017/#2017","title":"2017","text":""},{"location":"blog/category/tutorials/","title":"tutorials","text":""},{"location":"blog/category/tutorials/#tutorials","title":"tutorials","text":""},{"location":"blog/category/rfcs/","title":"rfcs","text":""},{"location":"blog/category/rfcs/#rfcs","title":"rfcs","text":""},{"location":"blog/category/design/","title":"design","text":""},{"location":"blog/category/design/#design","title":"design","text":""},{"location":"blog/category/product-update/","title":"product-update","text":""},{"location":"blog/category/product-update/#product-update","title":"product-update","text":""},{"location":"blog/category/roadmap/","title":"roadmap","text":""},{"location":"blog/category/roadmap/#roadmap","title":"roadmap","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/2/#blog","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/3/#blog","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/4/#blog","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/page/5/#blog","title":"Blog","text":""},{"location":"blog/page/6/","title":"Blog","text":""},{"location":"blog/page/6/#blog","title":"Blog","text":""},{"location":"blog/page/7/","title":"Blog","text":""},{"location":"blog/page/7/#blog","title":"Blog","text":""},{"location":"blog/page/8/","title":"Blog","text":""},{"location":"blog/page/8/#blog","title":"Blog","text":""},{"location":"blog/page/9/","title":"Blog","text":""},{"location":"blog/page/9/#blog","title":"Blog","text":""},{"location":"blog/page/10/","title":"Blog","text":""},{"location":"blog/page/10/#blog","title":"Blog","text":""},{"location":"blog/page/11/","title":"Blog","text":""},{"location":"blog/page/11/#blog","title":"Blog","text":""},{"location":"blog/archive/2020/page/2/","title":"2020","text":""},{"location":"blog/archive/2020/page/2/#2020","title":"2020","text":""},{"location":"blog/archive/2019/page/2/","title":"2019","text":""},{"location":"blog/archive/2019/page/2/#2019","title":"2019","text":""},{"location":"blog/archive/2018/page/2/","title":"2018","text":""},{"location":"blog/archive/2018/page/2/#2018","title":"2018","text":""},{"location":"blog/archive/2018/page/3/","title":"2018","text":""},{"location":"blog/archive/2018/page/3/#2018","title":"2018","text":""},{"location":"blog/archive/2018/page/4/","title":"2018","text":""},{"location":"blog/archive/2018/page/4/#2018","title":"2018","text":""},{"location":"blog/archive/2018/page/5/","title":"2018","text":""},{"location":"blog/archive/2018/page/5/#2018","title":"2018","text":""},{"location":"blog/archive/2018/page/6/","title":"2018","text":""},{"location":"blog/archive/2018/page/6/#2018","title":"2018","text":""},{"location":"blog/archive/2018/page/7/","title":"2018","text":""},{"location":"blog/archive/2018/page/7/#2018","title":"2018","text":""},{"location":"blog/category/product-update/page/2/","title":"product-update","text":""},{"location":"blog/category/product-update/page/2/#product-update","title":"product-update","text":""},{"location":"blog/category/product-update/page/3/","title":"product-update","text":""},{"location":"blog/category/product-update/page/3/#product-update","title":"product-update","text":""},{"location":"blog/category/product-update/page/4/","title":"product-update","text":""},{"location":"blog/category/product-update/page/4/#product-update","title":"product-update","text":""},{"location":"blog/category/product-update/page/5/","title":"product-update","text":""},{"location":"blog/category/product-update/page/5/#product-update","title":"product-update","text":""},{"location":"blog/category/product-update/page/6/","title":"product-update","text":""},{"location":"blog/category/product-update/page/6/#product-update","title":"product-update","text":""}]}